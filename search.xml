<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>node2vec</title>
    <url>/2020/10/10/%E5%9B%BE%E7%AE%97%E6%B3%95/node2vec/</url>
    <content><![CDATA[<p>接着上一篇DeepWalk, 这里再来介绍一种图表示学习中的方法, node2vec.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果还不了解DeepWalk算法的同学, 可以看一下我的<a href="whitemoonlight.top/2020/10/10/图算法/DeepWalk/">这篇文章</a>.</p>
<p>这里的node2vec, 就可以算作其升级版, 也是一种图表示学习方法, 即基于图中节点的一些共现关系, 学习出每个节点的Embedding.</p>
<p>核心过程, 就是首先从图中抽取序列, 然后再利用word2vec算法进行学习. 下面就对node2vec的算法原理进行详细介绍.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h2><p>熟悉树这种数据结构的同学应该都知道, 在遍历树的时候, 可以有两种策略, 广度优先搜索(BFS), 以及深度优先搜索(DFS).</p>
<p><img src="fig_0.png" alt="fig"></p>
<p><img src="fig_1.png" alt="fig"></p>
<p>BFS会从根节点开始, 优先探索该节点的子(邻近)节点, 然后再向更远的节点探索. DFS会从根节点开始, 优先探索该节点更远的节点, 探索到底后, 再回过头来继续从该节点的其它邻近节点进一步探索.</p>
<p>对于图来说, 也有这两种遍历方式.</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>由这两种方式得到的序列, 在节点相似性的呈现上, 具有不同的性质. 由BFS得到的序列, 会有一种结构相似性(structural equivalence), 结构相似性是衡量两个节点在网络中所在的位置和结构的相似性. 而由DFS得到的序列, 在经过word2vec学习后, 会展现出一种同质性(homophily), 即相邻的, 经常一起出现的节点会更加相似.</p>
<p>通俗来讲, 在图中, 从某个初始节点开始, 给定一个最大序列长度, 利用DFS得到的序列, 倾向于一条长链; 而利用BFS得到的序列, 更倾向于团簇的形式.</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>在DeepWalk中, 从图中抽取序列的方式, 就是随机游走, 虽然同时可以有BFS和DFS的效果, 但是并不可控. 于是, 在node2vec中, 通过加入两个超参数, 来形成一个带权重的网络, 以控制BFS和DFS的组成.</p>
<p>对于其论文中的一些公式, 这里就不多说了, 感兴趣的同学可以看原论文, 这里直接介绍其实现方式.</p>
<p>要想控制是BFS多一些还是DFS方式多一些, 一个办法就是通过改变当前节点向下一个节点的转移概率.</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>如上图, 其中的$t$节点表示上一个节点, $v$节点表示当前节点. 如果想要DFS多一些, 那么从$v$节点向$x_2$和$x_3$转移的概率应该大一些; 如果想要BFS多一些, 那么对应的返回上一个节点$t$的概率应该大一些. 同两个参数$p$和$q$来控制这种概率(未归一化):</p>
<script type="math/tex; mode=display">
\alpha_{pq}(t,x)=\left\{
\begin{array}{lcl}
\frac{1}{p} &    & {\rm if}\ d_{tx}=0  \\
1 &    & {\rm if}\ d_{tx}=1 \\
\frac{1}{q} &    & {\rm if}\ d_{tx}=2
\end{array}
\right.</script><p>其中的$d_{tx}$表示在图中, 由$t$节点到达$x$节点的最短距离, $d_{tx}=0$表示返回上一个节点, 对应参返回概率参数(Return parameter $p$); $d_{tx}=2$表示转移到更远的节点, 对应由内到外概率参数(In-out parameter $q$).</p>
<p>如果原始的图本身带有权重$w$, 那么在加入了$\alpha$后, 对应的未归一化权重为两者的乘积$\pi=w\cdot \alpha$</p>
<p>node2vec整体的算法流程为:</p>
<p><img src="fig_5.jpg" alt="fig"></p>
<p>首先, 与DeepWalk一样, 要先用原始数据构成一个图, 设定一些超参数, 包括整体迭代次数$r$, 序列长度$l$, 窗口大小$k$, 向量维度$k$, 两个概率参数$p$和$q$.</p>
<p>然后一个重点部分, 是需要根据原本网络的权重, 以及参数$p$和$q$计算整个网络的新的权重. 这里考虑实际计算时, 对于每个序列的初始节点, 是没有上一个节点的, 所以可以对其各个相邻节点的权重进行归一化, 看做其作为初始节点的转移概率. 而当不是初始节点时, 对于每个节点, 需要计算基于各个相邻节点作为上一个节点时, 对应的转移概率. 或者换一个角度来说, 需要对每一条边, 来计算两组(分别作为当前节点)概率转移分布.</p>
<p>在有了一整套概率转移分布后, 就可以依照转移概率, 来进行游走以获得序列了. 最后用word2vec来对序列进行学习, 就大功告成了.</p>
<p>这里node2vec算法在构建图, 计算概率转移时, 是比较费时间的, 时间复杂度正比于图中边的数量. 但是还不算完, 当面对一个计算好的概率转移分布, 决定如何转移时, 怎么做呢? 一种简单的方法就是产生一个0-1的随机数, 然后看这个数值位于哪一个概率区间. 这样做的时间复杂度为$O(n)$, 其中$n$为当前节点相邻节点数量. 而如果使用二分法查找, 可以使时间复杂度降低到$O(\log n)$. 如果用一个很长(假设长度为$N$)的向量, 将各个概率按相对大小分布在向量上, 并在对应位置记录对应转移节点编号, 然后产生一个最大为$N$的随机数, 来决定应该向哪个节点转移, 这样做的时间复杂度为$O(1)$, 但是空间复杂度就比较高了… 在原始的word2vec负采样中, 就采取的这种方法, 但word2vec只需要维护一个这样的向量, 而node2vec这里的数量与连边数量成正比的.</p>
<p>那么, 有没有一种时间复杂度为$O(1)$, 空间复杂度也不高的算法呢?</p>
<h2 id="Alias-Method"><a href="#Alias-Method" class="headerlink" title="Alias Method"></a>Alias Method</h2><p>Alias Method: 是的, 正是在下.</p>
<p>Alias Method是一种通用的概率采样方法, 主要针对对应离散概率分布, 下面通过举例来说明其运作流程.</p>
<p>假设现在有一个离散的概率分布为:</p>
<script type="math/tex; mode=display">
p_1:\frac{1}{2}\quad p_2:\frac{1}{3}\quad p_3:\frac{1}{12}\quad p_4:\frac{1}{12}</script><p>现在想让这些概率填充在一个$1\times4$的矩形中, 概率与面积成正比, 那么首先将它们都乘以4, 这样总面积等于4:</p>
<script type="math/tex; mode=display">
p_1:2\quad p_2:\frac{4}{3}\quad p_3:\frac{1}{3}\quad p_4:\frac{1}{3}</script><p>可以看到, 现在长为4了, 但是高并不是都为1, 所以现在要”截长补短”, 即将大于1的截取一部分, 填充到小于1的位置上. 同时规定, 一个位置上, 同时最多有两种概率.</p>
<p>现在先将第1个概率截取一部分填补到第4个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_1:\frac{2}{3} \\[7mm]
p_1:\frac{4}{3}\quad p_2:\frac{4}{3}\quad p_3:\frac{1}{3}\quad &p_4:\frac{1}{3}
\end{align*}</script><p>再将第1个概率截取一部分填补到第3个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_1:\frac{2}{3}\quad p_1:\frac{2}{3} \\[7mm]
p_1:\frac{2}{3}\quad p_2:\frac{4}{3}\quad &p_3:\frac{1}{3}\quad p_4:\frac{1}{3}
\end{align*}</script><p>现在还有第1个位置上的小于1, 同时第2个概率大于1, 因此将第2个概率截取一部分填补到第1个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
p_2:\frac{1}{3}\quad\quad\quad\quad\  p_1:\frac{2}{3}\quad p_1:\frac{2}{3} \\[7mm]
p_1:\frac{2}{3}\quad p_2:1\quad p_3:\frac{1}{3}\quad p_4:\frac{1}{3}
\end{align*}</script><p>到了这一步, 就有了两个向量, 在每次采样时, 只需要两步:</p>
<ul>
<li>随机生成一个1-N(N为离散变量数)的数, 用以确定向量中的位置.</li>
<li>随机生成一个0-1的数, 用以确定产生哪个离散变量.</li>
</ul>
<p>比如先随机得到3, 然后再随机得到$1/2$, 由$1/2&lt;2/3$, 可采样得到变量1.</p>
<p>这样的采样方式, 在生成了上面的概率向量以后, 采样时的时间复杂度为$O(1)$. 这里也解释为什么上面要规定每个位置上最多只能有两个概率, 因为如果有多个概率, 那么在随机生成一个0-1的数后, 就不是$O(1)$的时间复杂度了.</p>
<p>进一步, 给定一个离散概率分布, 这样的向量一定可以构建出来吗, 答案是肯定的, 因为: 整体的面积为$N$; 如果当前某个位置上概率小于1, 必有另一个位置上概率大于1.</p>
<p>使用Alias Method, 构建这个概率向量的时间复杂度为$O(n^2)$, $n$为离散变量数, 而通过队列来进行优化, 可以缩减到$O(n)$.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是node2vec的全部内容了, 由DeepWalk, BFS, DFS出发, 介绍了node2vec的原理和算法流程, 并对其中的一些细节, 比如概率转移时的采样方法也进行了讲解.</p>
<p>node2vec通过调整两个参数$p$和$q$, 来调整从图中抽取序列的方式, 在具体的任务和场景中, 可以尝试不同的组合. 一般来说, 合适的参数下, 相比DeepWalk会有更好一点的效果.</p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepWalk</title>
    <url>/2020/10/10/%E5%9B%BE%E7%AE%97%E6%B3%95/DeepWalk/</url>
    <content><![CDATA[<p>今天来介绍一种比较重要和实用的图算法, DeepWalk.</p>
<p>一看这名称中带有deep, 那么八九不离十和深度学习会有关, 是的, DeepWalk可以看做是深度学习算法在图算法领域的延伸. 下面就来介绍DeepWalk算法的原理, 以及代码实现.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面介绍图相关的文章中, 有一些方法, 可以衡量节点的重要性(如PageRank), 还有一些方法可以衡量两两节点之间的相似性. 这些方法得到的结果, 在一些特定的领域, 是有效果的, 但是并不具备泛用性, 这里泛用性的意思是, 能否像NLP中的词向量一样, 也用一些Embedding来表示图中的每个节点, 然后利用这个Embedding, 可以用来做各种下游任务呢? 可以的, 这就是图表示学习.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>众所周知, word2vec是一种经典且好用的Embedding方法, 可以用在NLP中的单词上. 也可以用在其它一些数据呈现离散序列的场景, 比如item2vec方法, 我的<a href="whitemoonlight.top/2020/10/10/推荐系统/item2vec召回/">这篇文章</a>中有介绍.</p>
<p>那么, 如果把word2vec也应用在图中, 来使得图中的每个节点学习到对应的Embedding, 好像是一个不错的点子. 而word2vec的输入, 是离散的序列, 所以现在问题就变成了, 如何在一个图中, 产生出一些可供word2vec学习的序列.</p>
<p>此时, 随机游走第一个站了出来: 没错, 正是在下! ♪(^∇^*)</p>
<p>所谓随机游走, 通俗地来说, 就是在当前的一个节点上, 没次随机选择一个相邻的节点, 进行移动. 具体说来, 在一个图中, 可以先随机地选取一个节点, 作为初始节点, 然后进行随机游走, 得到一个路径, 当走到设定的最大步数时停止, 而这个路径就是可以用word2vec来学习的序列.</p>
<p>DeepWalk具体的算法流程为:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>其中的$\gamma$表示整体的过程重复次数, 类似于训练神经网络时的epoch参数. 每次对所有节点形成的列表进行乱序, 然后依次选取节点作为初始节点, 开始随机行走, 得到训练用的序列数据.</p>
<p>DeepWalk简单来说, 等价于随机游走, 加word2vec, 随机游走从图中抽取序列, word2vec再从序列中学习Embedding. 假设相连的节点之间, 存在着更高的相似性, 而这种共现关系, 就可以体现在学习到的Embedding上.</p>
<p>是不是非常简单呢, 如果明白了word2vec的原理, 那么确实挺简单的. 但是我觉得其实还有一些值得注意或者讨论的地方.</p>
<h2 id="序列-图-序列"><a href="#序列-图-序列" class="headerlink" title="序列-图-序列"></a>序列-图-序列</h2><p>首先, 如果的数据, 本身就呈现一个图的形式, 比如社交网络, 那么就可以直接使用DeepWalk来进行图表示学习, 这是很自然的. 但如果初始数据不是天然的图, 还能够使用DeepWalk来进行学习吗?</p>
<p>在问这个问题之前, 其实应该问一下, 都不是图结构的数据了, 硬要用图算法来做, 图个啥? 炫技吗♪(^∇^*) 当然不是, 这里举个栗子, 在item2vec那篇文章中, 是将用户的行为序列, 当成word2vec直接的输入, 通过序列中物品的共现关系, 来学习对应向量, 最后用向量来做召回. 然鹅这里有个小问题, 即每一个序列, 都是仅仅局限于一个用户得到的, 这可能会带来一些局限性.</p>
<p>考虑一个场景, 比如两个人要去菜市场买菜做饭, 就做…番茄炒鸡蛋吧, 那么原材料肯定是需要番茄和鸡蛋了, 假设这时候番茄分为国产番茄和进口番茄, 这时候一个人就习惯买鸡蛋加国产番茄, 另外一个人就喜欢买鸡蛋加进口番茄. 即形成了(鸡蛋, 国产番茄)和(鸡蛋, 进口番茄)两个序列, 我们知道同属番茄, 它们的Embedding应该非常相似, 如果这时候将他们的行为序列使用item2vec来计算, 那么由于属于两个不同的序列, 最终结果理论上仍然具有一定的相似度, 毕竟已知A与B相似, B与C相似, 可得A与C也可能相似, 但是相似度可能不够高.</p>
<p>而如果采用了图表示学习会怎么样呢? 那就先想办法把原本的序列变成图, 再使用DeepWalk来学习. 还是上面那个栗子, 把原本的(鸡蛋, 国产番茄)和(鸡蛋, 进口番茄)两个序列转变成图, 序列中相邻的两个物品, 在图中对应节点上有连边, 那么在图中, 国产番茄和进口番茄, 就通过鸡蛋这个节点得到了连接, 而在使用随机游走后, 它们就可以同属一个序列中且距离较近, 就可以获得更加相似的Embedding向量.</p>
<p><img src="fig_1.jpeg" alt="fig"></p>
<p>如上图, 将原本的序列转变为图, 再从图中抽取出新的序列, 来使用word2vec进行学习, 可以在原本序列呈现的共现关系之上, 挖掘出更深层次的共现关系, 这通常是更有益的.</p>
<p>这里还有一个小问题, 由于从序列到图再到序列, 可以获取到更多的共现关系, 那么如果原始序列比较长, 同时一条序列中如果有大部分物品, 那么在形成图以后, 可能这个图会接近一个完全图(两两节点之间都存在连接). 这时候进行图表示学习, 得到的结果可能不尽人意. 如何处理这个问题呢? 我个人认为在原始序列那里, 就要将序列的切分做好, 即通过一些规则, 将原本相关的一些物品/行为, 划到一个子序列中, 这样相当于会使得基于原始序列构建的图中的连边被切断一些, 最终学习效果会更好.</p>
<h2 id="完全随机-带权重"><a href="#完全随机-带权重" class="headerlink" title="完全随机/带权重"></a>完全随机/带权重</h2><p>原本的DeepWalk, 节点之间的连边是不带权重的, 即只要与当前节点相连的节点, 都有相同的概率成为下一个节点.</p>
<p>但是在一些情况下, 带权重的可能会更好一些. 比如现在这个图是有一些原始序列得到的, 那么可能在原始序列中, 节点A经常与节点B出现在一起, 和其它一些节点可能只是一起出现一次, 这说明节点A与节点B是有很大关联的. 而如果在图中使用完全随机的方法, 可能就会削弱这种关联性, 使得节点B对于A来说和其它节点没有明显区别.</p>
<p>而在对连边加入权重后, 可以对类似这种情况进行调整. 比如让连边的值等于两个节点在原始序列中, 相邻出现的次数, 在形成图以后, 对于每个节点而言, 对其相邻节点的连边权重做归一化, 归一化后的值可以看做跳转概率.</p>
<p>当然这只是简单的一种加权重的方法, 真正是否加权重, 怎么样加权重, 需要由具体的数据与任务(下游任务)来决定.</p>
<h2 id="无向-有向"><a href="#无向-有向" class="headerlink" title="无向/有向"></a>无向/有向</h2><p>图属于无向图还是有向图, 或者说在构造图的时候, 选择构造成怎样的图, 对使用DeepWalk这样的图表示学习算法, 也是有一定的影响的.</p>
<p>个人认为, 构造为有向或者无向, 可以根据业务以及数据本身的性质来决定. 比如社交网络中, 以好友关系来构建, 无向网络会自然一些, 比较朋友是相互的; 而已关注来构建, 那么有向网络可能会好一些, 因为关注有时候是单方面的.</p>
<p>此外, 如果把关注点放在图表示学习后的一些下游任务上, 那么可以对无向/有向这两种形式的图都进行尝试, 比较它们的差别, 选择其中一种.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里对比item2vec那里, 仍然使用了相同的MovieLens 1M Dataset.</p>
<p>在构建图的时候, 使用了最简单的无向, 完全随机(不带权重)的方式.</p>
<p>在图上进行随机游走, 抽取序列时, 为了使与原始序列在数据量上尽量相等, 即总序列长度(各序列长度之和)相当, 设置的$\gamma$为2, 序列最大长度为100.</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">700000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">700000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整合每个用户的观看序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_items_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = list(zip(x[<span class="string">'item_id'</span>], x[<span class="string">'time'</span>]))</span><br><span class="line">    x = sorted(x, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> list(x)</span><br><span class="line"></span><br><span class="line">user_items = df_train.groupby(<span class="string">'user_id'</span>).apply(user_items_func)</span><br><span class="line">user_items = dict(zip(user_items.index, user_items.values))</span><br><span class="line">user_items[<span class="string">'1980'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;85&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;2671&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;1680&apos;, &apos;974691498&apos;),</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 序列长度分布</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对长于200的序列进行截断</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items[k] = user_items[k][: <span class="number">200</span>]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对小于5的序列进行剔除</span></span><br><span class="line">drop_list = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    <span class="keyword">if</span> len(user_items[k]) &lt; <span class="number">5</span>:</span><br><span class="line">        drop_list.append(k)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> drop_list:</span><br><span class="line">    <span class="keyword">del</span> user_items[k]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_4.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_list = user_items.keys()</span><br><span class="line">item_list = list(set([y[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items.values() <span class="keyword">for</span> y <span class="keyword">in</span> x ]))</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(user_list)]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(item_list)]</span><br><span class="line">df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(129815, 4)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据序列</span></span><br><span class="line">train_data = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    train_data.append([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[k]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_edge_file</span><span class="params">(data, file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    生成包含连边的文件.</span></span><br><span class="line"><span class="string">    :param data: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq) - <span class="number">1</span>):</span><br><span class="line">                tmp = seq[i] + <span class="string">' '</span> + seq[i + <span class="number">1</span>] + <span class="string">'\n'</span></span><br><span class="line">                f.write(tmp)</span><br><span class="line">        f.close()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    构建图.</span></span><br><span class="line"><span class="string">    :param file: 节点连边文件路径.</span></span><br><span class="line"><span class="string">    :return: dict, 图.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    graph = defaultdict(set)</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            node_0, node_1 = line.strip().split()</span><br><span class="line">            <span class="keyword">if</span> node_0 != node_1:</span><br><span class="line">                graph[node_0].add(node_1)</span><br><span class="line">                graph[node_1].add(node_0)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        graph[node] = list(graph[node])</span><br><span class="line">    <span class="keyword">return</span> graph</span><br><span class="line"></span><br><span class="line">create_edge_file(train_data, <span class="string">'tmp.txt'</span>)</span><br><span class="line">graph = create_graph(<span class="string">'tmp.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行随机游走构建序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_walk</span><span class="params">(graph, epoch=<span class="number">2</span>, max_len=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在图中随机游走, 返回生成的序列数据.</span></span><br><span class="line"><span class="string">    :param graph: dict, 图.</span></span><br><span class="line"><span class="string">    :param epoch: 遍历多少次所以节点.</span></span><br><span class="line"><span class="string">    :param max_len: 序列最大长度.</span></span><br><span class="line"><span class="string">    :return: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    nodes = list(graph.keys())</span><br><span class="line">    res_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        random.shuffle(nodes)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            seq = [node]</span><br><span class="line">            cur_node = node</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(max_len - <span class="number">1</span>):</span><br><span class="line">                cur_node = random.choice(graph[cur_node])</span><br><span class="line">                seq.append(cur_node)</span><br><span class="line">            res_data.append(seq)</span><br><span class="line">    random.shuffle(res_data)</span><br><span class="line">    <span class="keyword">return</span> res_data</span><br><span class="line"></span><br><span class="line">train_data_deep_walk = random_walk(graph, epoch=<span class="number">1</span>, max_len=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>这里使用gensim中的word2vec来进行计算, 简单方便~</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.system("taskset -p 0xff %d" % os.getpid())</span></span><br><span class="line">dim = <span class="number">32</span></span><br><span class="line">model = word2vec.Word2Vec(sentences=train_data_deep_walk,</span><br><span class="line">                          size=dim,  <span class="comment"># 向量维度</span></span><br><span class="line">                          alpha=<span class="number">0.025</span>,  <span class="comment"># 学习率</span></span><br><span class="line">                          window=<span class="number">1</span>,  <span class="comment"># 窗口大小</span></span><br><span class="line">                          min_count=<span class="number">5</span>,</span><br><span class="line">                          sample=<span class="number">0.001</span>,</span><br><span class="line">                          seed=<span class="number">7</span>,</span><br><span class="line">                          workers=<span class="number">12</span>,</span><br><span class="line">                          min_alpha=<span class="number">0.0001</span>,</span><br><span class="line">                          sg=<span class="number">1</span>,  <span class="comment"># 使用skip-gram</span></span><br><span class="line">                          hs=<span class="number">0</span>,  <span class="comment"># 使用neg-sample</span></span><br><span class="line">                          negative=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取词表, 以及每个词汇的词向量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">vocab = model.wv.index2word</span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line">item_embedding = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> vocab:</span><br><span class="line">    item_embedding[k] = model[k]</span><br><span class="line">item_id_2_ids_dict = &#123;k:i  <span class="keyword">for</span> i, k <span class="keyword">in</span> enumerate(item_embedding)&#125;</span><br><span class="line">ids_2_item_id_dict = dict(zip(item_id_2_ids_dict.values(), item_id_2_ids_dict.keys()))</span><br><span class="line"></span><br><span class="line">item_embedding_mat = np.zeros((len(vocab), dim))</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> item_embedding:</span><br><span class="line">    item_embedding_mat[item_id_2_ids_dict[k]] = item_embedding[k]</span><br><span class="line">item_embedding_mat = item_embedding_mat.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为3154</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户统计已经看过的电影</span></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items_dict[user] = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计测试集上每个用户点击的物品</span></span><br><span class="line"></span><br><span class="line">user_clicked_item_set_on_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df_test.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">        user_clicked_item_set_on_test[user] = set([])</span><br><span class="line">    user_clicked_item_set_on_test[user].add(item)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将用户近期观看的100部影片的Embedding进行平均池化</span></span><br><span class="line"></span><br><span class="line">user_embedding_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    tmp_item_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user][:<span class="number">100</span>]]</span><br><span class="line">    tmp_embedding = np.zeros(dim)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tmp_item_list:</span><br><span class="line">        tmp_embedding += item_embedding.get(item, np.zeros(dim))</span><br><span class="line">    user_embedding_dict[user] = tmp_embedding / len(tmp_item_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为每个用户寻找感兴趣的200电影(去除已观看)</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line"><span class="comment"># index = faiss.IndexFlatL2(32)  # 创建索引</span></span><br><span class="line">index = faiss.IndexFlatIP(dim)  <span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line">index.add(item_embedding_mat)  <span class="comment"># 添加向量</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">1000</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line"></span><br><span class="line">user_match_item_set_filter = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    D, I = index.search(user_embedding_dict[user].reshape((<span class="number">1</span>, dim)).astype(<span class="string">'float32'</span>), k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">    user_match_item_set_filter[user] = [ids_2_item_id_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> I[<span class="number">0</span>] <span class="keyword">if</span> ids_2_item_id_dict[x] <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict[user]][:<span class="number">200</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1463, Recall: 0.2488, F1: 0.1843</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 通过对DeepWalk原理的介绍, 知道了这是一种相对简单而有效的图表示学习方法, 即可以将图中的节点, 利用其共现关系, 学习得到Embedding.</p>
<p>然后用代码来对算法流程进行实现, 并在相近的参数下, 对比item2vec在召回中的效果:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">item2vec</th>
<th style="text-align:center">DeepWalk</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Precision</td>
<td style="text-align:center">0.1213</td>
<td style="text-align:center">0.1463</td>
</tr>
<tr>
<td style="text-align:center">Recall</td>
<td style="text-align:center">0.2064</td>
<td style="text-align:center">0.2488</td>
</tr>
<tr>
<td style="text-align:center">F1</td>
<td style="text-align:center">0.1528</td>
<td style="text-align:center">0.1843</td>
</tr>
</tbody>
</table>
</div>
<p>从结果上对比, DeepWalk是要比item2vec效果更好的, 这表明在使用图表示学习后, 确实挖掘出了更多潜藏的共现关系.</p>
<p>同时, 对于item2vec来说, 其主要能够调节的超参数有:</p>
<ul>
<li><p>原始序列的长度.</p>
<p>可以对一些较短的序列进行过滤, 对一些较长的序列进行截断或者分成多个序列.</p>
</li>
<li><p>word2vec算法.</p>
<p>包括Embedding向量的维度, 邻近词的窗口大小, 负采样数量等.</p>
</li>
<li><p>用户Embedding的表示.</p>
<p>如何利用物品的Embedding来表示用户的Embedding. 可以取近期点击物品的Embedding平均, 或者加权平均等.</p>
</li>
</ul>
<p>而对于DeepWalk来, 在item2vec的基础上, 增加了更多可调节超参数:</p>
<ul>
<li><p>用于构建图的样本.</p>
<p>包括原始序列的长度以及数量, 构建出来的图的不同, 会影响到采样的序列及后续的训练.</p>
</li>
<li><p>图中序列的采样.</p>
<p>采样多少样本序列, 每个序列长度为多少, 也是需要调节的.</p>
</li>
</ul>
<p>综上, DeepWalk是一种值得使用的算法, 同时在使用时, 需要根据一些下游任务, 来对其超参数进行调节, 以达到更好的效果.</p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
        <tag>word2vec</tag>
        <tag>召回</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>KD Tree</title>
    <url>/2020/10/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/KD-Tree/</url>
    <content><![CDATA[<p>在经典的分类算法KNN中, 为了加速邻近点的搜索, 可以使用一种数据结构, 即KD Tree, 本篇就主要讲解KD Tree的原理.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近邻检索在不少场景中, 是一项比较关键的技术. 比如在推荐系统的召回板块中, 使用隐式召回的话, 一般会通过一些算法(如矩阵分解), 给到每个用户和物品一个稠密的Embedding向量. 通过这个向量, 可以用一些方式(如余弦相似度), 来计算用户与物品的相似度, 将相似度高的物品, 作为候选集, 用于推送或者排序.</p>
<p>而现在假如有了一堆同维度的向量, 并使用欧式距离作为指标, 给定一个向量, 想要挑选出离这个向量最近的$N$个向量, 可以怎么做呢?</p>
<p>最简单粗暴的方法, 就是拿着这个向量, 去和所有的向量进行一次距离的计算, 这样遍历一次的时间复杂度为$O(N)$, 当$N$很大时, 会非常耗费时间.</p>
<p>另外的一种思路, 就是使用某种数据结构, 用空间换时间, 先用一种方式把原本的数据组织起来, 然后在后续搜索的时候, 尽可能地排除掉那些明显不合适的向量, 只在少数的向量中计算距离和进行比较.</p>
<p>KD Tree就是这样一种数据结构, 下面对其原理进行讲解.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h2><p>KD Tree的构建原理非常简单, 假设现在有$N$个点, 每个点对应一个向量, 向量维度假设为$M$.</p>
<p>首先, 按照某种顺序来从$M$个维度中, 选择一个维度$m$, 选择的方法一般有如下两种:</p>
<ul>
<li>从$0$到$M-1$维循环选取.</li>
<li>选取当前方差最大的维度.</li>
</ul>
<p>然后找到这个维度上, 所有点该维度上数值处在中间的那个点, 用这个”中间点”来作为划分节点, 用该维度上的值大于或者小于该阈值, 将点划分到两侧.</p>
<p>用来划分的点可以视作树的父节点, 其余的点被划分到子节点上.</p>
<p>不停地重复上面的操作, 知道最后每个树节点(包括内部节点和叶子节点)上, 仅包含一个点, 那么此时KD Tree构建完成.</p>
<p>在维度为2的向量上, 构建完成的KD Tree大概长这样:</p>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>并在平面上的大致分布如下, 其中内部的每条线表示一次划分:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>通过这种方式, 将所有的点(向量)组织到了一棵平衡二叉树中, 或者说可以看成将点划分到了向量空间中的一个个”小方块”中, 借助这样的数据结构, 可以有效地排除掉距离明显很远的点, 减少无效计算.</p>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>下面来讲如何在KD Tree中进行搜索, 在说明具体流程之前, 要定性地明确一些东西, 才能更好的理解.</p>
<p>首先, 在构建好一棵KD Tree后, 给定一个目标点, 从根节点开始, 是可以根据节点的划分, 进入到某个叶子节点的. 一般来说, 这个叶子节点中的点, 和目标点的距离是比较小的.</p>
<p>假如最后想要挑选出$n$个与目标点距离最小的点, 那么下一步应该去哪找呢?</p>
<p>应该去父节点, 因为父节点离当前叶子节点是比较近的.</p>
<p>现在处在父节点上, 下一步可以搜索父节点的另外一侧, 在向量空间中, 相当于超平面隔开的另一侧空间.</p>
<p>那么是否应该搜索父节点的另一侧呢? 这要看目标点到父节点对应分割超平面的距离, 因为处在超平面另一侧的点, 与目标点的距离, 一定是大于该距离的, 如果这个距离比当前已收集点中的最大距离还大, 并且已收集点数量已达到$n$, 那么就没必要去父节点另一侧搜索了.</p>
<p>什么时候算法停止呢? 首先肯定要满足找到了$n$个点, 其次还要当前位于根节点上.</p>
<p>为什么最后位于根节点才能停止算法呢? 因为假设处于某个内部节点上, 那么其父节点, 以及父节点的另一侧, 都有可能存在距离较小的点. 当从叶子节点返回根节点时, 有两种情况, 一种是通过计算目标点与根节点对应超平面的距离, 距离大, 不用去根节点另一侧搜索, 算法停止; 还有一种是再到根节点另一侧进行搜索, 最终再次返回根节点时, 算法停止.</p>
<p>下面对KD Tree搜索过程进行比较准确的描述.</p>
<ul>
<li><p>(一) 初始化.</p>
<p>假设现在已构建好KD Tree, 给定目标点, 要找到$n$个距离最近的点, 目标点一开始位于根节点.</p>
<p>执行(二)步.</p>
</li>
<li><p>(二) 下沉.</p>
<p>按树节点的划分, 下沉到对应叶子节点</p>
<p>执行(三)步.</p>
</li>
<li><p>(三) 计算距离.</p>
<p>计算当前节点的距离, 更新候选点.</p>
<p>若当前不满$n$个点, 直接加入; 若满$n$个点, 且距离比现有候选点中的最大距离小, 那么替换该点.</p>
<p>并将当前节点标记为”到此一游”, 这里的”到此一游”, 不是指在上面(二)步中的经过, 而是要计算距离后.</p>
<p>若当前节点另一侧不存在, 执行(四)步.</p>
<p>若当前不满$n$个点, 那么去另一侧搜索, 执行(二)步.</p>
<p>若当前满$n$个点, 计算目标点与当前节点对应的超平面的距离, 用该距离与当前候选点中的最大距离比较, 来决定是否搜索另一侧. 若搜索, 则执行(二)步, 否则执行(四)步.</p>
</li>
<li><p>(四) 返回父节点.</p>
<p>进入到当前节点的父节点.</p>
<p>若父节点已经被标记”到此一游”, 且不是根节点, 继续执行(四)步.</p>
<p>若父节点已经被标记”到此一游”, 且是根节点, 恭喜, 算法停止.</p>
<p>若父节点未被标记”到此一游”, 执行(三)步.</p>
</li>
</ul>
<h2 id="指标转换"><a href="#指标转换" class="headerlink" title="指标转换"></a>指标转换</h2><p>另外一个需要注意的是, 对于邻近检索中的指标, 原本给出的是欧氏距离, 而对于其它一些指标, 如内积等, 直接照搬原算法不太行, 那么这时候应该怎么做呢?</p>
<p>其实一般来说, 在做邻近搜索的时候, 常用的指标, 就是欧氏距离, 内积, 余弦相似度这些, 具体用什么指标, 应该根据实际的数据和任务来决定.</p>
<p>而这些指标之间其实是有一些联系的, 可以通过一些方式来进行转换.</p>
<p>比如现在给出两个标准化的向量$x,y$:</p>
<script type="math/tex; mode=display">
||x||_2=||y||_2=1</script><p>那么这时候两个向量的内积, 就等于其余弦相似度:</p>
<script type="math/tex; mode=display">
x^\top y=\cos(x,y)=\frac{x^\top y}{||x||_2||y||_2}</script><p>并且两个向量欧氏距离的平方, 与其内积之间的关系如下:</p>
<script type="math/tex; mode=display">
\begin{align*}
||x-y||^2_2&=(x-y)^\top (x-y) \\
&=x^\top x-2x^\top y+y^\top y \\
&=2-2x^\top y
\end{align*}</script><p>所以, 在向量的标准化的情况下, 内积, 余弦相似度, 欧氏距离是等价的.</p>
<p>在特定的场景下, 如果原本是用内积作为指标, 那么可以拿一些样本来进行分析, 比较在对向量做了标准化后, 用余弦相似度是否可以代替内积, 如果可以, 那么像KD Tree这种原本基于欧式距离的算法, 也能派上用场了OvO</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上就是KD Tree这种数据结构的原理了, 讲解了其构建过程与搜索过程, 整体并不太复杂.</p>
<p>但是KD Tree有着其一些问题, 比如在构建KD Tree时, 一般来说要用到所有数据, 将树保存在内存中, 在数据很大的时候, 可能还没等时间复杂度降低, 空间复杂度就先顶不住了QAQ 而真正在大数据场景下, 做到又快, 又节省内存的一个开源库, 叫做Faiss, 在我的大数据板块中有介绍♪(^∇^*)</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Faiss</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/Faiss/</url>
    <content><![CDATA[<p>Faiss是Facebook开源的一个能够在海量数据中, 高效地进行邻近搜索的一个库, 下面就来介绍一下这个库的一些基本算法原理, 以及简单的使用.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在我的之前一篇讲<a href="whitemoonlight.top/2020/10/09/数据结构与算法/KD-Tree/">KD Tree</a>的文章中, 就介绍了一种在邻近搜索中, 用空间换时间的数据结构, KD Tree, 在数据较小的情况下, 还是可以使用的, 能够把时间复杂度, 从$O(N)$降低到$O(\log N)$.</p>
<p>然鹅在大数据场景下, 数据(向量)的个数可能达到亿的量级, 此外, 向量维度的增加, 也会导致数据量进一步增大, 此时若想在内存中构建一棵KD Tree, 单机是肯定完成不了的.</p>
<p>如果能有一个库, 能够支持多种邻近搜索模式, 能够根据实际情况的需求, 能够在搜索精度, 速度, 内存占用之间进行平衡, 能够在磁盘上也能进行搜索, 那就好了♪(^∇^*)</p>
<p>Faiss站了出来: 没错, 正是在下OvO</p>
<p>Faiss是Facebook开源的一个库, 用C++写成, 并非常友好地提供了Python接口. Faiss可以用在实际的生产环境中, 下面对Faiss的部分基本原理进行介绍, 并展示简单应用.</p>
<h1 id="精确模式"><a href="#精确模式" class="headerlink" title="精确模式"></a>精确模式</h1><p>Faiss是支持多种模式的, 其中必然就少不了暴力搜索模式, 也可以称作精确模式, 因为要对所有的向量进行无损的遍历, 所以得到的结果也必然是准确的.</p>
<p>精确模型没有太多好说的, 下面看一下代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">d = <span class="number">64</span>  <span class="comment"># 向量维度</span></span><br><span class="line">nb = <span class="number">100000</span>  <span class="comment"># 保存的向量数量</span></span><br><span class="line">nq = <span class="number">10000</span>  <span class="comment"># 需要查询的向量数量</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">xb = np.random.random((nb, d)).astype(<span class="string">'float32'</span>)  <span class="comment"># 注意需要限制32位浮点数</span></span><br><span class="line">xq = np.random.random((nq, d)).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<p>要使用Faiss进行搜索, 需要先创建索引, 这里的<code>IndexFlatL2</code>表示暴力搜索对应的索引, 并用欧氏距离作为指标.</p>
<p>后面会看到, 会用一些算法来加速或者压缩内存, 这时候除了创建索引, 还需要一个训练的过程, 不过<code>IndexFlatL2</code>并不需要训练.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">index = faiss.IndexFlatL2(d)  <span class="comment"># 创建索引</span></span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line">index.add(xb)  <span class="comment"># 添加向量</span></span><br><span class="line">print(<span class="string">'向量个数: '</span>, index.ntotal)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是否已训练: True</span><br><span class="line">向量个数:  100000</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line">D, I = index.search(xb[:<span class="number">5</span>], k)  <span class="comment"># 直接对保存的向量进行搜索, 查看结果是否正确</span></span><br><span class="line">print(<span class="string">'Index:'</span>)</span><br><span class="line">print(I)</span><br><span class="line">print(<span class="string">'Distance:'</span>)</span><br><span class="line">print(D)</span><br><span class="line">print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">print(<span class="string">'Index:'</span>)</span><br><span class="line">print(I[:<span class="number">5</span>])  <span class="comment"># 前5个查询向量的结果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Index:</span><br><span class="line">[[    0 97820 94546 51393]</span><br><span class="line"> [    1 58334 87280 12181]</span><br><span class="line"> [    2 24444 71046 91180]</span><br><span class="line"> [    3 37506 12216 26707]</span><br><span class="line"> [    4 77045 47784 13208]]</span><br><span class="line">Distance:</span><br><span class="line">[[0.        4.6321406 4.7470975 4.9057417]</span><br><span class="line"> [0.        4.5316086 4.9688716 5.0430927]</span><br><span class="line"> [0.        5.3778887 5.4905343 5.74937  ]</span><br><span class="line"> [0.        4.600462  4.725213  5.1498823]</span><br><span class="line"> [0.        5.570385  5.8674355 5.8756237]]</span><br><span class="line">##########################################</span><br><span class="line">Index:</span><br><span class="line">[[75581 78376 18034 48733]</span><br><span class="line"> [ 2006 39322 19294 50124]</span><br><span class="line"> [48158 68786 84956 10466]</span><br><span class="line"> [40157 76112 20177 74830]</span><br><span class="line"> [30148 10533 99308 45521]]</span><br></pre></td></tr></table></figure>
<p>为了同后面的加速模式做一下比较, 这里测试一下暴力搜索的速度:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit index.search(xq, k)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">416 ms ± 1.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></pre></td></tr></table></figure>
<h1 id="加速搜索"><a href="#加速搜索" class="headerlink" title="加速搜索"></a>加速搜索</h1><p>下面来讲解Faiss中的加速搜索的方法, Faiss应该在实际工程细节中, 加入了大量的调优, 而这里仅从部分加速算法的角度来进行介绍.</p>
<p>搜索的时间, 一般来说取决于向量的数量以及维度, 不过可能多数时候向量的维度并不会太大, 更多的是取决于向量的数量.</p>
<p>但是这又不是训练模型, 总不能对向量进行抽样吧, 而像KD Tree那样又太费内存, 怎么办呢?</p>
<p>还是那个思路, 如果能够用一些方法, 减小不必要的搜索, 即把一些明显距离较远的向量排除, 再在小范围中搜索, 就会快很多.</p>
<p>Faiss这里的方法简单而有效, 用到了聚类(K-means)的方法. 即先用K-means对向量进行聚类, 每个向量都被划分到一个组中, 当对一个向量进行邻近搜索时, 先根据每个组的中心向量, 来判断哪些组中可能有该向量的邻近向量, 然后再到对应的组中进行精细地搜索.</p>
<p>这里的原理比较简单, 但有一些需要注意的地方. 首先是这种加速是有一定的代价的, 即得到的结果, 相比上面的暴力搜索模式, 可能并不完全一样.</p>
<p>同时, 有两个参数<code>nlist</code>与<code>nprobe</code>, 需要进行尝试与调节, 才能在实际的任务中, 取得较好的结果:</p>
<ul>
<li><p><code>nlist</code></p>
<p>聚类中心的数量.</p>
<p>在<code>nlist</code>给定的情况下, 这个参数越大, 搜索的时间越小.</p>
</li>
<li><p><code>nprobe</code></p>
<p>每次在多少个组中进行搜索.</p>
<p>在<code>nlist</code>给定的情况下, 这个参数越大, 搜索的时间越多, 结果越精准.</p>
</li>
</ul>
<p>Faiss中一个比较重要的索引就是<code>IndexIVFFlat</code>, 利用聚类以及倒排方法进行加速, 在初始化的时候, 还需要一个编码器<code>quantizer</code>, 这里使用<code>IndexFlatL2</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加速搜索</span></span><br><span class="line"></span><br><span class="line">nlist = <span class="number">100</span>  <span class="comment"># 聚类中心数量</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># 需要一个编码器</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line">index.train(xb)</span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line"></span><br><span class="line">index.add(xb)  <span class="comment"># 添加向量</span></span><br><span class="line">index.nprobe = <span class="number">10</span>  <span class="comment"># 设置搜索聚类组群的数量, 默认为1</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[:<span class="number">5</span>])  <span class="comment"># 前5个查询向量的结果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是否已训练: False</span><br><span class="line">是否已训练: True</span><br><span class="line">[[78376 18034 48733  8985]</span><br><span class="line"> [ 2006 19294   430 30718]</span><br><span class="line"> [48158 68786 84956 51185]</span><br><span class="line"> [40157 20177 74830 17291]</span><br><span class="line"> [45521 33644 17126 22048]]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">97.7 ms ± 3.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</span><br></pre></td></tr></table></figure>
<p>对比上面精确模式的约400ms, 这里花费的时间大约是其四分之一, 确实能够加速搜索.</p>
<h1 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h1><p>如果仅仅是能够加速, 是不够的, 还需要在尽量保证精度的情况下, 节省内存占用.</p>
<p>如何建设内存占用呢? 主要就是从向量维度上, 来进行考虑了, 即降维.</p>
<p>一提到降维, 可能就会想到PCA, 它是一个有效的无监督, 线性降维算法. 通俗地来说, 就是把原来高维的特征空间中的点, 映射到低维空间中. 关于PCA的具体算法原理, 还行做更多了解的同学, 可以去网上进行查阅. 在Faiss中, 也有对PCA的实现, 但这不是这里的重点, 就不做展示了~</p>
<p>除了使用PCA算法进行降维, 在Faiss中还有一种关键的算法, 即<a href="https://hal.inria.fr/file/index/docid/514462/filename/paper_hal.pdf" target="_blank" rel="noopener">Product Quantizer</a>, 简称PQ, 下面对PQ的原理进行阐述.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>这里假设有一个$50000\times 1024$的向量矩阵, 每一行为一个$1024$维的向量.</p>
<p>现在把每个向量平均分成$m$个子向量, 假设$m=8$, 每个子向量$128$维, 如图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>对于这8组子向量, 分别使用K-means算法, 并设聚类中心数量为$256$, 结果如下:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>这时候, 可以对每组子向量, 都用聚类的组群的编号(0-255), 来进行编码, 那么原本的一个$1024$维浮点数向量, 就可以用8维无符号整型向量代替:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>这样, 内存占用缩减了多少呢? 原本的字节数为:</p>
<script type="math/tex; mode=display">
1024\times 4=4096</script><p>现在的字节数为:</p>
<script type="math/tex; mode=display">
8\times 1=8</script><p>缩减了512 ($=4096/8$) 倍!</p>
<p>上面的$m$可以用来控制具体的缩减程度, $m$越小, 则越更高缩减内存占用, 但是在搜索精度上会差一些.</p>
<p>那么… 这样转换了以后, 如何进行计算呢, 总不能直接拿着编号来计算吧OvO</p>
<p>要知道, 每组子向量的每个整数编号, 都是映射到一个聚类中心的, 那么在计算距离或者相似度的时候, 就可以将编号转变为真实向量(有损), 再进行计算.</p>
<p>具体有两种方式:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>上图中, $x$表示待查询向量, $y$表示数据集中的向量, $q(\cdot)$表示将原本向量根据聚类, 划分到某个组群的中心向量. 左边的方式, 表示将$x,y$向量都先进行转换, 然后再计算; 右边的方式, 表示使用原始的$x$向量, 和转换后的$y$向量进行计算.</p>
<p>运用PQ的方式, 是能够有效地减少内存占用空间的, 并且也能够和上一节中的加速算法一起结合起来使用, 它们并不冲突♪(^∇^*)</p>
<p>在Faiss中, <code>IndexIVFPQ</code>就是同时使用了PQ算法来减少内存占用, 并也使用聚类和倒排来进行加速. 在原有的基础上, 增加了一个参数$m$, 用以控制子向量的划分个数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 减小内存占用</span></span><br><span class="line"></span><br><span class="line">nlist = <span class="number">100</span></span><br><span class="line">m = <span class="number">8</span>  <span class="comment"># 子向量个数</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="number">8</span>)  <span class="comment"># 8表示每个子向量用8个比特来进行编码</span></span><br><span class="line">index.train(xb)</span><br><span class="line">index.add(xb)</span><br><span class="line">index.nprobe = <span class="number">10</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[38423 69668  2747 90800]</span><br><span class="line"> [30739 11101 36633 18976]</span><br><span class="line"> [84956 55762 43311 48158]</span><br><span class="line"> [ 2845 58680 13501 32725]</span><br><span class="line"> [33644 23032 74010 76153]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit index.search(xq, k)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">46.2 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</span><br></pre></td></tr></table></figure>
<p>从测试结果来看, “降维”后在搜索时间上也有所降低.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在这一篇中, 主要介绍了Faiss这个开源库的相关内容, 包括其用途, 部分特性及原理.</p>
<p>但关于Faiss的内容, 其实还远不止这些, 在真实的场景中, 是否将全部向量放入内存, 可能会用到GPU来加速计算, 使用哪一种索引, 参数如何选择, 在精度, 速度, 内存之间如何平衡, 都是需要试验与分析的.</p>
<p>那么需要优化的点不止一处, 如何进行抉择呢, 我认为一种不错的方法, 是对多数方面, 设定一个阈值, 达到阈值即可, 在此基础上, 对某一项, 或者少数某几项来进行优化, 就像一个带约束的优化问题那样. 比如搜索速度, 内存占用, 都可以设定一个阈值, 而搜索结果的精度, 作为在此基础上的优化目标.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(三)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/</url>
    <content><![CDATA[<p>这一篇中, 将在泰坦尼克数据集(可以在kaggle上下载)上, 利用PySpark进行处理与建模.</p>
<a id="more"></a>
<h1 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .master(<span class="string">'local'</span>) \</span><br><span class="line">                    .appName(<span class="string">'titanic'</span>) \</span><br><span class="line">                    .getOrCreate()</span><br></pre></td></tr></table></figure>
<h1 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.read.csv(<span class="string">'./data/train.csv'</span>, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(df.count())</span><br><span class="line">print(df.printSchema())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">891</span><br><span class="line">root</span><br><span class="line"> |-- PassengerId: integer (nullable = true)</span><br><span class="line"> |-- Survived: integer (nullable = true)</span><br><span class="line"> |-- Pclass: integer (nullable = true)</span><br><span class="line"> |-- Name: string (nullable = true)</span><br><span class="line"> |-- Sex: string (nullable = true)</span><br><span class="line"> |-- Age: double (nullable = true)</span><br><span class="line"> |-- SibSp: integer (nullable = true)</span><br><span class="line"> |-- Parch: integer (nullable = true)</span><br><span class="line"> |-- Ticket: string (nullable = true)</span><br><span class="line"> |-- Fare: double (nullable = true)</span><br><span class="line"> |-- Cabin: string (nullable = true)</span><br><span class="line"> |-- Embarked: string (nullable = true)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    print(df[[col]].describe().show())</span><br></pre></td></tr></table></figure>
<h1 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Survived'</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+-----+</span><br><span class="line">|Survived|count|</span><br><span class="line">+--------+-----+</span><br><span class="line">|       1|  342|</span><br><span class="line">|       0|  549|</span><br><span class="line">+--------+-----+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Sex'</span>).avg(<span class="string">'Survived'</span>).sort(<span class="string">'avg(Survived)'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-------------------+</span><br><span class="line">|   Sex|      avg(Survived)|</span><br><span class="line">+------+-------------------+</span><br><span class="line">|  male|0.18890814558058924|</span><br><span class="line">|female| 0.7420382165605095|</span><br><span class="line">+------+-------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Pclass'</span>).avg(<span class="string">'Survived'</span>).sort(<span class="string">'avg(Survived)'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-------------------+</span><br><span class="line">|Pclass|      avg(Survived)|</span><br><span class="line">+------+-------------------+</span><br><span class="line">|     3|0.24236252545824846|</span><br><span class="line">|     2|0.47282608695652173|</span><br><span class="line">|     1| 0.6296296296296297|</span><br><span class="line">+------+-------------------+</span><br></pre></td></tr></table></figure>
<h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><p>查看哪些列有缺失值, 缺失多少数量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">null_value_count</span><span class="params">(df)</span>:</span></span><br><span class="line">    null_columns_counts = []</span><br><span class="line">    numRows = df.count()</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        nullRows = df.where(df[col].isNull()).count()</span><br><span class="line">        <span class="keyword">if</span>(nullRows &gt; <span class="number">0</span>):</span><br><span class="line">            temp = col, nullRows</span><br><span class="line">            null_columns_counts.append(temp)</span><br><span class="line">    <span class="keyword">return</span> null_columns_counts</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_columns_count_list = null_value_count(df)</span><br><span class="line">spark.createDataFrame(null_columns_count_list, [<span class="string">'Column_With_Null_Value'</span>, <span class="string">'Null_Values_Count'</span>]).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------------+-----------------+</span><br><span class="line">|Column_With_Null_Value|Null_Values_Count|</span><br><span class="line">+----------------------+-----------------+</span><br><span class="line">|                   Age|              177|</span><br><span class="line">|                 Cabin|              687|</span><br><span class="line">|              Embarked|                2|</span><br><span class="line">+----------------------+-----------------+</span><br></pre></td></tr></table></figure>
<p>发现有3列缺失, 且缺失程度不同.</p>
<p>根据缺失程度的不同, 这里采取不同的处理方式. 对于<code>Cabin</code>, 缺失过多, 直接删除. 对于<code>Embarked</code>, 缺失很少, 可使用众数填充. 对于<code>Age</code>, 小部分缺失, 如果直接使用均值填充, 在这里不太好, 注意到有一列<code>Name</code>, 其中有一些称谓如<code>Mr</code>, 可以反映一个人的年龄, 所以可以结合其它信息进行填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">'Name'</span>]].show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">|                Name|</span><br><span class="line">+--------------------+</span><br><span class="line">|Braund, Mr. Owen ...|</span><br><span class="line">|Cumings, Mrs. Joh...|</span><br><span class="line">|Heikkinen, Miss. ...|</span><br><span class="line">|Futrelle, Mrs. Ja...|</span><br><span class="line">|Allen, Mr. Willia...|</span><br><span class="line">|    Moran, Mr. James|</span><br><span class="line">|McCarthy, Mr. Tim...|</span><br><span class="line">|Palsson, Master. ...|</span><br><span class="line">|Johnson, Mrs. Osc...|</span><br><span class="line">|Nasser, Mrs. Nich...|</span><br><span class="line">|Sandstrom, Miss. ...|</span><br><span class="line">|Bonnell, Miss. El...|</span><br><span class="line">|Saundercock, Mr. ...|</span><br><span class="line">|Andersson, Mr. An...|</span><br><span class="line">|Vestrom, Miss. Hu...|</span><br><span class="line">|Hewlett, Mrs. (Ma...|</span><br><span class="line">|Rice, Master. Eugene|</span><br><span class="line">|Williams, Mr. Cha...|</span><br><span class="line">|Vander Planke, Mr...|</span><br><span class="line">|Masselmani, Mrs. ...|</span><br><span class="line">+--------------------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure>
<p>使用正则表达式, 将<code>Name</code>中的称谓提取出来.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_extract</span><br><span class="line">df = df.withColumn(<span class="string">"Initial"</span>,regexp_extract(df[<span class="string">"Name"</span>], <span class="string">"([A-Za-z]+)\."</span>, <span class="number">1</span>))</span><br><span class="line">df[[<span class="string">'Initial'</span>]].distinct().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+</span><br><span class="line">| Initial|</span><br><span class="line">+--------+</span><br><span class="line">|     Don|</span><br><span class="line">|    Miss|</span><br><span class="line">|Countess|</span><br><span class="line">|     Col|</span><br><span class="line">|     Rev|</span><br><span class="line">|    Lady|</span><br><span class="line">|  Master|</span><br><span class="line">|     Mme|</span><br><span class="line">|    Capt|</span><br><span class="line">|      Mr|</span><br><span class="line">|      Dr|</span><br><span class="line">|     Mrs|</span><br><span class="line">|     Sir|</span><br><span class="line">|Jonkheer|</span><br><span class="line">|    Mlle|</span><br><span class="line">|   Major|</span><br><span class="line">|      Ms|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure>
<p>有一些不同的称谓, 其实表达的是同一个意思, 因此这里进行整合.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.replace([<span class="string">'Mlle'</span>,<span class="string">'Mme'</span>, <span class="string">'Ms'</span>, <span class="string">'Dr'</span>,<span class="string">'Major'</span>,<span class="string">'Lady'</span>,<span class="string">'Countess'</span>,<span class="string">'Jonkheer'</span>,<span class="string">'Col'</span>,<span class="string">'Rev'</span>,<span class="string">'Capt'</span>,<span class="string">'Sir'</span>,<span class="string">'Don'</span>],</span><br><span class="line">            [<span class="string">'Miss'</span>,<span class="string">'Miss'</span>,<span class="string">'Miss'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>,  <span class="string">'Mrs'</span>,  <span class="string">'Mrs'</span>,  <span class="string">'Other'</span>,  <span class="string">'Other'</span>,<span class="string">'Other'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>])</span><br><span class="line">df[[<span class="string">'Initial'</span>]].distinct().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+</span><br><span class="line">|Initial|</span><br><span class="line">+-------+</span><br><span class="line">|   Miss|</span><br><span class="line">|  Other|</span><br><span class="line">| Master|</span><br><span class="line">|     Mr|</span><br><span class="line">|    Mrs|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
<p>看一下各个称谓的平均年龄.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Initial'</span>).avg(<span class="string">'Age'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+------------------+</span><br><span class="line">|Initial|          avg(Age)|</span><br><span class="line">+-------+------------------+</span><br><span class="line">|   Miss|             21.86|</span><br><span class="line">|  Other|45.888888888888886|</span><br><span class="line">| Master| 4.574166666666667|</span><br><span class="line">|     Mr| 32.73960880195599|</span><br><span class="line">|    Mrs|35.981818181818184|</span><br><span class="line">+-------+------------------+</span><br></pre></td></tr></table></figure>
<p>利用以上结果, 对缺失部分的年龄进行填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> when</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Miss"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">22</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Other"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">46</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Master"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">5</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Mr"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">33</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Mrs"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">36</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br></pre></td></tr></table></figure>
<p>填充<code>Embarked</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">"Embarked"</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+-----+</span><br><span class="line">|Embarked|count|</span><br><span class="line">+--------+-----+</span><br><span class="line">|       Q|   77|</span><br><span class="line">|    null|    2|</span><br><span class="line">|       C|  168|</span><br><span class="line">|       S|  644|</span><br><span class="line">+--------+-----+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.fillna(&#123;<span class="string">"Embarked"</span> : <span class="string">'S'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>删除<code>Cabin</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.drop(<span class="string">'Cabin'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="构造特征"><a href="#构造特征" class="headerlink" title="构造特征"></a>构造特征</h1><p>对于<code>SibSp</code>与<code>Parch</code>这两列, 分别表示不与自己同辈以及与自己同辈的亲人数量.</p>
<p>那么可以考虑它们的和,来构造一个特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.withColumn(<span class="string">"Family_Size"</span>, df[<span class="string">'SibSp'</span>] + df[<span class="string">'Parch'</span>])</span><br><span class="line">df.groupBy(<span class="string">"Family_Size"</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-----------+-----+</span><br><span class="line">|Family_Size|count|</span><br><span class="line">+-----------+-----+</span><br><span class="line">|          1|  161|</span><br><span class="line">|          6|   12|</span><br><span class="line">|          3|   29|</span><br><span class="line">|          5|   22|</span><br><span class="line">|          4|   15|</span><br><span class="line">|          7|    6|</span><br><span class="line">|         10|    7|</span><br><span class="line">|          2|  102|</span><br><span class="line">|          0|  537|</span><br><span class="line">+-----------+-----+</span><br></pre></td></tr></table></figure>
<p>进一步可以构造是否一个人的特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df = df.withColumn(<span class="string">'Alone'</span>, lit(<span class="number">0</span>))</span><br><span class="line">df = df.withColumn(<span class="string">"Alone"</span>,</span><br><span class="line">                   when(df[<span class="string">"Family_Size"</span>] == <span class="number">0</span>, <span class="number">1</span>).otherwise(df[<span class="string">"Alone"</span>]))</span><br><span class="line">df.groupBy(<span class="string">'Alone'</span>).avg(<span class="string">'survived'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-----+-------------------+</span><br><span class="line">|Alone|      avg(survived)|</span><br><span class="line">+-----+-------------------+</span><br><span class="line">|    1|0.30353817504655495|</span><br><span class="line">|    0| 0.5056497175141242|</span><br><span class="line">+-----+-------------------+</span><br></pre></td></tr></table></figure>
<p>从这里可以看到, 独自一人的生存率要…低…??? 懂了, 谈恋爱可以增加生存率!</p>
<h1 id="类别特征编码"><a href="#类别特征编码" class="headerlink" title="类别特征编码"></a>类别特征编码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line">indexers = [StringIndexer(inputCol=column, outputCol=column+<span class="string">"_index"</span>) <span class="keyword">for</span> column <span class="keyword">in</span> [<span class="string">"Sex"</span>,<span class="string">"Embarked"</span>,<span class="string">"Initial"</span>]]</span><br><span class="line">pipeline = Pipeline(stages=indexers)</span><br><span class="line">df = pipeline.fit(df).transform(df)</span><br><span class="line"></span><br><span class="line">df = df.drop(<span class="string">"PassengerId"</span>,<span class="string">"Name"</span>,<span class="string">"Ticket"</span>,<span class="string">"Cabin"</span>,<span class="string">"Embarked"</span>,<span class="string">"Sex"</span>,<span class="string">"Initial"</span>)</span><br><span class="line"></span><br><span class="line">df.columns</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&apos;Survived&apos;,</span><br><span class="line"> &apos;Pclass&apos;,</span><br><span class="line"> &apos;Age&apos;,</span><br><span class="line"> &apos;SibSp&apos;,</span><br><span class="line"> &apos;Parch&apos;,</span><br><span class="line"> &apos;Fare&apos;,</span><br><span class="line"> &apos;Family_Size&apos;,</span><br><span class="line"> &apos;Alone&apos;,</span><br><span class="line"> &apos;Sex_index&apos;,</span><br><span class="line"> &apos;Embarked_index&apos;,</span><br><span class="line"> &apos;Initial_index&apos;]</span><br></pre></td></tr></table></figure>
<h1 id="整合特征-amp-标签"><a href="#整合特征-amp-标签" class="headerlink" title="整合特征 &amp; 标签"></a>整合特征 &amp; 标签</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line">vector_assembler = VectorAssembler(inputCols=df.columns[<span class="number">1</span>:],</span><br><span class="line">                                   outputCol=<span class="string">"features"</span>)</span><br><span class="line">df = feature.transform(df)</span><br><span class="line"></span><br><span class="line">df = df.withColumnRenamed(<span class="string">'Survived'</span>, <span class="string">'label'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集/测试集.</span></span><br><span class="line">train, test = df[[<span class="string">'features'</span>, <span class="string">'label'</span>]].randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>], seed=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h1 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h1><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载逻辑回归模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归实例.</span></span><br><span class="line">lr = LogisticRegression(labelCol=<span class="string">"label"</span>,</span><br><span class="line">                        featuresCol=<span class="string">"features"</span>,</span><br><span class="line">                        maxIter=<span class="number">10</span>,</span><br><span class="line">                        regParam=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型.</span></span><br><span class="line">lr_model = lr.fit(train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型信息.</span></span><br><span class="line"><span class="comment"># print("Coefficients: " + str(lr_model.coefficients))</span></span><br><span class="line">print(<span class="string">"Intercept: "</span> + str(lr_model.intercept))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用模型进行预测.</span></span><br><span class="line">preds = lr_model.transform(test)</span><br><span class="line">preds.printSchema()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- features: vector (nullable = true)</span><br><span class="line"> |-- label: integer (nullable = true)</span><br><span class="line"> |-- rawPrediction: vector (nullable = true)</span><br><span class="line"> |-- probability: vector (nullable = true)</span><br><span class="line"> |-- prediction: double (nullable = false)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># AUC</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>, metricName=<span class="string">'areaUnderROC'</span>)</span><br><span class="line">print(evaluator.getMetricName())</span><br><span class="line">print(evaluator.evaluate(preds))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">areaUnderROC</span><br><span class="line">0.8511334610472546</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(lr.regParam, [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line">             .addGrid(lr.elasticNetParam, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=lr,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8584770114942533</span><br></pre></td></tr></table></figure>
<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> GBTClassifier</span><br><span class="line"></span><br><span class="line">gbdt = GBTClassifier(seed=<span class="number">7</span>)</span><br><span class="line">gbdt_model = gbdt.fit(train)</span><br><span class="line">print(evaluator.evaluate(gbdt_model.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8702107279693482</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(gbdt.maxDepth, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">             .addGrid(gbdt.minInstancesPerNode, [<span class="number">1</span>, <span class="number">20</span>])</span><br><span class="line">             .addGrid(gbdt.maxIter, [<span class="number">20</span>, <span class="number">100</span>])</span><br><span class="line">             .addGrid(gbdt.stepSize, [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">1</span>])</span><br><span class="line">             .addGrid(gbdt.subsamplingRate, [<span class="number">0.7</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=gbdt,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8785919540229884</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(二)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/</url>
    <content><![CDATA[<p>本章节主要总结下一些简单基础的, 与机器学习相关的内容.</p>
<a id="more"></a>
<h1 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 文本匹配.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> when</span><br><span class="line"></span><br><span class="line">con_0 = df[<span class="string">'col'</span>].like(<span class="string">'%a%'</span>)</span><br><span class="line">con_1 = df[<span class="string">'col'</span>].like(<span class="string">'%b%'</span>)</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, (when(con_0, <span class="number">1</span>).when(con_1, <span class="number">0</span>).otherwise(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split, explode</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'list_'</span>, split(df[<span class="string">'string_col'</span>], <span class="string">', '</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原本1条样本, 拆分成多条.</span></span><br><span class="line">ex_df = df.withColumn(<span class="string">'ex_list'</span>, explode(df[<span class="string">'list_'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载函数.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_replace</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用正则表达式去除标点和数值.</span></span><br><span class="line">df = df.withColumn(<span class="string">'text'</span>, regexp_replace(df[<span class="string">'text'</span>], <span class="string">'[_():;,.!?\\-]'</span>, <span class="string">' '</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'text'</span>, regexp_replace(df[<span class="string">'text'</span>], <span class="string">'[0-9]'</span>, <span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词(使用空格).</span></span><br><span class="line">df = Tokenizer(inputCol=<span class="string">'text'</span>, outputCol=<span class="string">'words'</span>).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StopWordsRemover</span><br><span class="line"></span><br><span class="line">df = StopWordsRemover(inputCol=<span class="string">'words'</span>, outputCol=<span class="string">'words'</span>).transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> HashingTF, IDF</span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF.</span></span><br><span class="line">df = HashingTF(inputCol=<span class="string">'words'</span>, outputCol=<span class="string">'hash'</span>, numFeatures=<span class="number">1024</span>).transform(df)</span><br><span class="line"></span><br><span class="line">df = IDF(inputCol=<span class="string">'hash'</span>, outputCol=<span class="string">'features'</span>).fit(df).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0-1编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Binarizer</span><br><span class="line"></span><br><span class="line">binarizer = Binarizer(threshold=<span class="number">5.0</span>, inputCol=<span class="string">'col'</span>, outputCol=<span class="string">'target'</span>)</span><br><span class="line"></span><br><span class="line">df = binarizer.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分箱编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Bucketizer</span><br><span class="line"></span><br><span class="line">splits = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, float(<span class="string">'Inf'</span>)]</span><br><span class="line">buck = Bucketizer(splits=splits, inputCol=<span class="string">'col'</span>, outputCol=<span class="string">'bucket_col'</span>)</span><br><span class="line"></span><br><span class="line">df_bucket = buck.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer, OneHotEncoderEstimator</span><br><span class="line"></span><br><span class="line">string_indexer = StringIndexer(inputCol=<span class="string">'a'</span>, outputCol=<span class="string">'b'</span>)</span><br><span class="line">df = string_indexer.fit(df).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 直接转换.</span></span><br><span class="line">encoder = OneHotEncoder(dropLast=<span class="literal">False</span>, inputCol=<span class="string">"b"</span>, outputCol=<span class="string">"c"</span>)</span><br><span class="line">df = encoder.transform(df)</span><br><span class="line"><span class="comment">## fit &amp; transform</span></span><br><span class="line">encoder = OneHotEncoderEstimator(inputCols=[<span class="string">"b"</span>], outputCols=[<span class="string">"c"</span>])</span><br><span class="line">model = encoder.fit(df)</span><br><span class="line">df = model.transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一系列数据处理模块.</span></span><br><span class="line">stages = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将入模特征作为向量.</span></span><br><span class="line">assembler = VectorAssembler(inputCols=ft_list, outputCol=<span class="string">"features"</span>)</span><br><span class="line">stages += [assembler]</span><br><span class="line"><span class="comment"># 利用Pipline处理数据.</span></span><br><span class="line">pipeline = Pipeline(stages=stages)</span><br><span class="line">pipeline_fit = pipeline.fit(df)</span><br><span class="line">df = pipeline_fit.transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train, test = df.randomSplit([<span class="number">.8</span>, <span class="number">.2</span>], seed=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载逻辑回归模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归实例.</span></span><br><span class="line">lr = LogisticRegression(labelCol=<span class="string">"label"</span>,</span><br><span class="line">                        featuresCol=<span class="string">"features"</span>,</span><br><span class="line">                        maxIter=<span class="number">10</span>,</span><br><span class="line">                        regParam=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型.</span></span><br><span class="line">linear_fit = lr.fit(train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型信息.</span></span><br><span class="line">print(<span class="string">"Coefficients: "</span> + str(linear_fit.coefficients))</span><br><span class="line">print(<span class="string">"Intercept: "</span> + str(linear_fit.intercept))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用模型进行预测.</span></span><br><span class="line">preds = linear_fit.transform(test)</span><br><span class="line">preds.printSchema()</span><br></pre></td></tr></table></figure>
<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准确率.</span></span><br><span class="line">preds.filter(preds.label == preds.prediction).count() / preds.count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># AUC</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>, metricName=<span class="string">'areaUnderROC'</span>)</span><br><span class="line">print(evaluator.getMetricName())</span><br><span class="line">print(evaluator.evaluate(preds))</span><br></pre></td></tr></table></figure>
<h1 id="交叉验证-amp-网格调参"><a href="#交叉验证-amp-网格调参" class="headerlink" title="交叉验证 &amp; 网格调参"></a>交叉验证 &amp; 网格调参</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(lr.regParam, [<span class="number">0.01</span>, <span class="number">0.5</span>])</span><br><span class="line">             .addGrid(lr.elasticNetParam, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=lr,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取最佳模型.</span></span><br><span class="line">best_model = cv.bestModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看最佳模型stages.</span></span><br><span class="line">print(best_model.stages)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得最佳模型的LinearRegression参数.</span></span><br><span class="line">best_model.stages[<span class="number">3</span>].extractParamMap()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用最佳模型预测并进行评估.</span></span><br><span class="line">predictions = best_model.transform(test)</span><br><span class="line">evaluator.evaluate(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有超参数组合下, CV上的模型表现.</span></span><br><span class="line">avg_auc = cv.avgMetrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最佳模型的CV表现.</span></span><br><span class="line">best_model_auc = max(cv.avgMetrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最佳模型的某个超参数.</span></span><br><span class="line">opt_regParam = cv.bestModel.explainParam(<span class="string">'maxDepth'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="保存-amp-加载模型"><a href="#保存-amp-加载模型" class="headerlink" title="保存 &amp; 加载模型"></a>保存 &amp; 加载模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型.</span></span><br><span class="line">model.save(<span class="string">'lr.model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression.load(<span class="string">'lr.model'</span>)</span><br></pre></td></tr></table></figure>
<p>下一篇将以一个实际的数据集为例, 利用PySpark完整地做一遍数据处理的操作.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(一)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<p>我暂时的工作还没有怎么用到Spark, 但作为一个处理大数据几乎必会的东西, 我认为还是很有必要学习的. 我记得我大概在一年多以前, 一个劲在那学习Hadoop, Spark, Hive什么的, 处理数据的方法没学多少, 搭建分布式环境倒是捣鼓了好久. 后来工作以后我才知道, 术业有专攻, 有一种职位叫做大数据工程师, 像搭建和维护大数据环境这些工作, 交给他们就好了. 如果不是专门做这个的, 了解一下当然好, 不过如果时间有限的话, 应该优先学习自己用的到的部分.</p>
<a id="more"></a>
<h1 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h1><p>Spark是一种集群计算平台, Spark可以让我们分布式地在集群的多个节点上处理数据. 当数据集很大时, 分布式处理会更加地快捷.</p>
<p>那么在考虑是否Spark可以作为一个较好的解决我们问题的方案时, 可以考虑一下两个问题:</p>
<ul>
<li>是否数据量很大, 以至于单机难以处理. 以我的经验, 当数据量达到百万量级时, pandas就会有些吃力了.</li>
<li>处理的方法是否容易被并行化.</li>
</ul>
<p>Spark的整体架构采用的是多个节点相互连接, 其中有一个主节点Master, 对应多个子节点Worker. 主节点主要负责任务的调度管理, 子节点负责实际的计算处理.</p>
<p>Spark可以在多种模式下运行:</p>
<ul>
<li>单机模式.</li>
<li>Standalone, 即Spark自带的集群模式.</li>
<li>YARN等其它模式下运行.</li>
</ul>
<p>考虑到学习成本问题, Spark本身的源码是由Scala写的, 但是拥有Java, R, Python的接口, PySpark就是Python对应的接口.</p>
<h1 id="单机环境安装"><a href="#单机环境安装" class="headerlink" title="单机环境安装"></a>单机环境安装</h1><p>如果想在单机环境下测试运行PySpark, 非常简单, 直接利用<code>pip</code>安装就行. 对于集群环境的安装和配置, 这里暂时不涉及. 当我们要使用一个已经搭建好的Spark集群的时候, 只需要在本地连接上集群使用即可, 与单机环境在表面上没有明显差别.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install pyspqrk</span><br></pre></td></tr></table></figure>
<h1 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h1><p>在老版本的Spark中, 通过<code>SparkContext</code>, <code>SQLContext</code>等来进行连接和交互. 而在新版本中, 老的方式仍然可用, 不过大部分常用的功能整合到了<code>SparkSession</code>这个类里面, 建议使用<code>SparkSession</code>进行连接与交互.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建环境(单机).</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .master(<span class="string">'local[*]'</span>) \</span><br><span class="line">                    .appName(<span class="string">'test'</span>) \</span><br><span class="line">                    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印版本号</span></span><br><span class="line">print(spark.version)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结束会话.</span></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># SparkContext可用通过spark获取.</span></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取本次应用名称.</span></span><br><span class="line">app_name = spark.conf.get(<span class="string">'spark.app.name'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取driver端口.</span></span><br><span class="line">driver_tcp_port = spark.conf.get(<span class="string">'spark.driver.port'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取partitions数量.</span></span><br><span class="line">num_partitions = spark.conf.get(<span class="string">'spark.sql.shuffle.partitions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改partitions数量.</span></span><br><span class="line">spark.conf.set(<span class="string">'spark.sql.shuffle.partitions'</span>, <span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>在Spark中, 核心数据结构被称为RDD(Resilient Distributed Dataset), 即弹性分布式数据集. 这是一种偏底层的数据结构, 相对DataFrame不方便进行使用, 但仍然需要学习一些基础的操作.</p>
<ul>
<li><p>构建RDD.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用列表构建.</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用字符串构建.</span></span><br><span class="line">rdd = sc.parallelize(<span class="string">'Hello world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本构建.</span></span><br><span class="line">rdd = sc.textFile(file_path, minPartitions=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看元素信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定数量查看.</span></span><br><span class="line">rdd.take(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看全部元素.</span></span><br><span class="line">rdd.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计元素个数.</span></span><br><span class="line">rdd.count()</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># map操作.</span></span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># filter操作.</span></span><br><span class="line">rdd = rdd.filter(<span class="keyword">lambda</span> x: x == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># flatMap操作.</span></span><br><span class="line">rdd = sc.parallelize([<span class="string">'aaa'</span>, <span class="string">'bbb'</span>, <span class="string">'ccc'</span>])</span><br><span class="line">rdd = rdd.flatMap(<span class="keyword">lambda</span> x: list(x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Pair RDD.</p>
<p>一些时候, 如果数据是成对出现的, 如key-value的形式, 那么Pair RDD可以给到很多方便的处理方式.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用列表构建Pair RDD.</span></span><br><span class="line">rdd = sc.parallelize([(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">6</span>), (<span class="number">4</span>, <span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对相同的key进行reduce.</span></span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用key进行排序.</span></span><br><span class="line">rdd = rdd.sortByKey(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同key的元素进行计数.</span></span><br><span class="line">rdd_count = rdd.countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历输出.</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> rdd.collect(): </span><br><span class="line">    print(<span class="string">"Key &#123;&#125; Value &#123;&#125;"</span>.format(num[<span class="number">0</span>], num[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同元素计数.</span></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> rdd_count.items(): </span><br><span class="line">		print(<span class="string">"key"</span>, k, <span class="string">"has"</span>, v, <span class="string">"counts"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><p>Spark的DataFrame类似于pandas的DataFrame, 同时还支持SQL. Spark对DataFrame进行了大量地优化, 所以在使用时应该尽量使用内部已有的方法.</p>
<h2 id="使用SQL"><a href="#使用SQL" class="headerlink" title="使用SQL"></a>使用SQL</h2><ul>
<li><p>查看在目录中的表.</p>
<p>类似数据库的表, 单机模式没有预设的情况想输出为<code>[]</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>将已有DF加入catalog.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">'table_name'</span>)</span><br><span class="line"></span><br><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用SQL进行查询.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SQL</span></span><br><span class="line">query = <span class="string">"FROM table_name SELECT * LIMIT 10"</span></span><br><span class="line"></span><br><span class="line">df = spark.sql(query)</span><br><span class="line"></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><ul>
<li><p>csv.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create an DataFrame from file_path</span></span><br><span class="line">df = spark.read.csv(file_path, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>parquet.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save the df DataFrame in Parquet format</span></span><br><span class="line">df.write.parquet(<span class="string">'file.parquet'</span>, mode=<span class="string">'overwrite'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the Parquet file into a new DataFrame</span></span><br><span class="line">spark.read.parquet(<span class="string">'file.parquet'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>schema.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import the pyspark.sql.types library</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a new schema using the StructType method</span></span><br><span class="line">people_schema = StructType([</span><br><span class="line">  <span class="comment"># Define a StructField for each field</span></span><br><span class="line">  StructField(<span class="string">'name'</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">  StructField(<span class="string">'age'</span>, IntegerType(), <span class="literal">False</span>),</span><br><span class="line">  StructField(<span class="string">'city'</span>, StringType(), <span class="literal">False</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">df = spark.read.csv(<span class="string">"df.csv"</span>, sep=<span class="string">';'</span>, header=<span class="literal">False</span>, schema=schema)</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="不同数据格式转换"><a href="#不同数据格式转换" class="headerlink" title="不同数据格式转换"></a>不同数据格式转换</h2><ul>
<li><p>RDD转DF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a list of tuples</span></span><br><span class="line">sample_list = [(<span class="string">'Mona'</span>,<span class="number">20</span>), (<span class="string">'Jennifer'</span>,<span class="number">34</span>),(<span class="string">'John'</span>,<span class="number">20</span>), (<span class="string">'Jim'</span>,<span class="number">26</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a RDD from the list</span></span><br><span class="line">rdd = sc.parallelize(sample_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a PySpark DataFrame</span></span><br><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'Name'</span>, <span class="string">'Age'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>Spark DF转换pandas DF.</p>
<p>当数据较小时(百万量级以下), 可以用pandas处理. 或者先抽样, 再使用pandas处理.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽样后再转换.</span></span><br><span class="line">sample_df = df.select([<span class="string">'a'</span>, <span class="string">'b'</span>]).sample(<span class="literal">False</span>, <span class="number">0.5</span>, <span class="number">42</span>)</span><br><span class="line">pandas_df = sample_df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图.</span></span><br><span class="line">sns.lmplot(x=<span class="string">'a'</span>, y=<span class="string">'b'</span>, data=pandas_df)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>pandas DF转换Spark DF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame(df)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><ul>
<li><p>查看有哪些列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看一些统计信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># describe</span></span><br><span class="line">df.describe().show()</span><br><span class="line">df.describe(<span class="string">'a'</span>, <span class="string">'b'</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集大小.</span></span><br><span class="line">df.count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性.</span></span><br><span class="line">df.corr(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独特值.</span></span><br><span class="line">df.select(<span class="string">'col'</span>).distinct().show(<span class="number">40</span>, truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改列名称.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.withColumnRenamed(<span class="string">'old'</span>, <span class="string">'new'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择某些列的数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法一.</span></span><br><span class="line">df = df[[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二.</span></span><br><span class="line">df = df.select(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(df[<span class="string">'a'</span>], df[<span class="string">'b'</span>], df[<span class="string">'c'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三.</span></span><br><span class="line">col = (df[<span class="string">'a'</span>] / (df[<span class="string">'b'</span>] / <span class="number">60</span>)).alias(<span class="string">"col"</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, col)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法四.</span></span><br><span class="line">df = df.selectExpr(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"a/(b/60) as col"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.drop(<span class="string">'a'</span>)</span><br><span class="line"></span><br><span class="line">df = df.drop(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>生成新的一列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过已有列转换.</span></span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, df[<span class="string">'col'</span>] / <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用已有外部数据赋予(不常用), 这里暂时的办法比较繁琐.</span></span><br><span class="line">id_df = df[[<span class="string">'id'</span>]].toPandas()</span><br><span class="line">id_df[<span class="string">'new_col'</span>] = data</span><br><span class="line"></span><br><span class="line">id_df = spark.createDataFrame(id_df)</span><br><span class="line"></span><br><span class="line">df = df.join(id_df, on=<span class="string">'id'</span>, how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>修改数据类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> FloatType, IntegerType, StringType</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'col'</span>, df[<span class="string">'col'</span>].cast(FloatType()))</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>条件筛选.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># .where 与 .filter 一样.</span></span><br><span class="line">df = df.where(df[<span class="string">'col'</span>] &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">df = df.filter(df[<span class="string">'col'</span>] &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可使用SQL.</span></span><br><span class="line">df = df.where(<span class="string">'col &gt; 0'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>聚合.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计一些信息时, 需要先进行groupBy.</span></span><br><span class="line">df.groupBy().min(<span class="string">"col"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对某一列进行groupBy.</span></span><br><span class="line">df_g = df.groupBy(<span class="string">"col"</span>)</span><br><span class="line"></span><br><span class="line">df_g.count().show()</span><br><span class="line">df_g.avg(<span class="string">"a"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载内置函数.</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">df_g = df.groupBy(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">df_g.avg(<span class="string">'c'</span>).show()</span><br><span class="line"></span><br><span class="line">df_g.agg(F.stddev(<span class="string">'d'</span>)).show()</span><br><span class="line"></span><br><span class="line">df_g.count().sort(<span class="string">"count"</span>,ascending=<span class="literal">True</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>拼接.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 横向拼接.</span></span><br><span class="line">df = df.join(df_1, on=<span class="string">'b'</span>, how=<span class="string">'left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 纵向拼接.</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>交叉表.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.crosstab(<span class="string">'a'</span>, <span class="string">'b'</span>).sort(<span class="string">"a_b"</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>缺失值填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.fillna(&#123;<span class="string">"col"</span>: <span class="string">'val'</span>&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>常用函数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># when.</span></span><br><span class="line">df = df.withColumn(<span class="string">'col_1'</span>, (when(con_0, <span class="number">1</span>)</span><br><span class="line">                             .when(con_1, <span class="number">0</span>)</span><br><span class="line">                             .otherwise(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># contains.</span></span><br><span class="line">df = df.filter(df[<span class="string">'col'</span>].contains(<span class="string">'a'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split.</span></span><br><span class="line">df = df.withColumn(<span class="string">'splits'</span>, F.split(df[<span class="string">'col'</span>], <span class="string">'\s+'</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'first_name'</span>, df.splits.getItem(<span class="number">0</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'last_name'</span>, df.splits.getItem(F.size(<span class="string">'splits'</span>) - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># rand.</span></span><br><span class="line"><span class="comment">## 生成随机数.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># monotonically_increasing_id.</span></span><br><span class="line"><span class="comment">## 获得一列递增不重复的列.</span></span><br><span class="line">df = df.withColumn(<span class="string">'ROW_ID'</span>, F.monotonically_increasing_id())</span><br><span class="line"></span><br><span class="line">previous_max_ID = voter_df_march.select(<span class="string">'ROW_ID'</span>).rdd.max()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">df_new = df_new.withColumn(<span class="string">'ROW_ID'</span>, F.monotonically_increasing_id() + previous_max_ID)</span><br></pre></td></tr></table></figure>
</li>
<li><p>UDF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">spark_my_func = F.udf(my_func, IntegerType())</span><br><span class="line">  </span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, spark_my_func(df[<span class="string">'col'</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>持久化.</p>
<p>由于Spark独特的机制, 使得一些时候, 对于主要的并且重复使用的内容进行持久化的操作, 会很大地加快运行速度.</p>
<p>持久化的方法有两个, cache与persist:</p>
<ul>
<li>cache: 使用非序列化的方式将数据全部尝试持久化到内存中.</li>
<li>persist: 手动选择持久化级别. 默认级别同cache.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.cache()</span><br><span class="line"></span><br><span class="line">df.is_cached</span><br><span class="line"></span><br><span class="line">df.unpersist()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><ul>
<li><p>时间转换.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> to_date</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'date'</span>, to_date(<span class="string">'date'</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取日期中的元素.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> dayofweek, year</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'dayofweek'</span>, dayofweek(<span class="string">'date'</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算前后时间间隔.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lag, datediff</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create window</span></span><br><span class="line">w = Window().orderBy(df[<span class="string">'date'</span>])</span><br><span class="line"><span class="comment"># Create lag column</span></span><br><span class="line">df = df.withColumn(<span class="string">'lag-1'</span>, lag(<span class="string">'date'</span>, count=<span class="number">1</span>).over(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate difference between date columns</span></span><br><span class="line">df = df.withColumn(<span class="string">'date_diff'</span>, datediff(<span class="string">'date'</span>, <span class="string">'lag-1'</span>))</span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line">df.select(<span class="string">'date_diff'</span>).distinct().show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>本篇主要总结了基础的RDD, DataFrame的操作方法, 下一篇中将总结一些常见的机器学习相关的方法.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark的安装与使用</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Spark是使用Scala实现的基于内存计算的大数据开源集群计算环境. 提供了 Java, Scala, Python, R等语言的调用接口. </p>
<p>本篇主要介绍Spark的基本原理, 以及安装流程.</p>
<a id="more"></a>
<h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><p>Spark在英文中的意思, 为火花, 其中有”快”的含义, 而事实上在数据处理上, Spark也确实比基于MapReduce的Hadoop要快很多.</p>
<p>那么, 为什么会快呢? 是因为Hadoop在一次MapReduce运算之后, 会将数据的运算结果从内存写入到磁盘中, 第二次MapReduce运算时在从磁盘中读取数据, 所以其瓶颈在运算间的多余 IO 消耗. 而Spark则是将数据一直缓存在内存中, 直到计算得到最后的结果, 再将结果写入到磁盘, 所以多次运算的情况下, Spark是比较快的.</p>
<p>Spark由伯克利大学研发, 在核心框架 Spark 的基础上, 主要提供四个范畴的计算框架:</p>
<ul>
<li><p>Spark SQL:</p>
<p>提供了类 SQL 的查询, 返回 Spark-DataFrame 的数据结构(类似 Hive).</p>
</li>
<li><p>Spark Streaming:</p>
<p>流式计算,主要用于处理线上实时时序数据(类似 storm).</p>
</li>
<li><p>MLlib:</p>
<p>提供机器学习的各种模型和调优.</p>
</li>
<li><p>GraphX:</p>
<p>提供基于图的算法, 如PageRank.</p>
</li>
</ul>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>在Spark中, RDD(Resilent Distributed Datasets), 俗称弹性分布式数据集, 是 Spark 底层的分布式存储的数据结构, 可以说是 Spark 的核心, Spark API 的所有操作都是基于 RDD 的. 数据不只存储在一台机器上, 而是分布在多台机器上, 实现数据计算的并行化. 弹性的意思是数据丢失时, 可以进行重建. 在Spark 1.5版以后, 新增了数据结构 Spark-DataFrame, 仿造的 R 和Python 的类 SQL 结构DataFrame, 底层为 RDD,  能够让数据从业人员更好的操作 RDD.</p>
<p>RDD中记录了通过哪些操作得到的, 这些操作形成一个有向无环图 DAG(Directed Acyclic Graph),反映RDD之间的依赖关系. 整个计算过程中, 将不需要将中间结果落地到磁盘进行容错, 假如某个节点出错, 则只需要通过DAG关系重新计算即可.</p>
<p>Spark的架构如下, 其实感觉和Hadoop那里的YARN比较相似:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>架构中的一些关键部分:</p>
<ul>
<li><p>Application</p>
<p>用户编写的Spark应用程序, 一个Application包含多个Job.</p>
</li>
<li><p>Driver Program</p>
<p>控制程序, 负责为Application构建DAG图, 并且创建SparkContext.</p>
</li>
<li><p>Cluster Manager</p>
<p>集群资源管理中心, 负责分配计算资源. 可以是由Spark本身控制(Standalone模式), 或者YARN模式, Mesos模式.</p>
</li>
<li><p>Worker Node</p>
<p>工作节点, 负责完成具体计算.</p>
</li>
<li><p>Executor</p>
<p>是运行在工作节点 Worker Node 上的一个进程, 负责运行Task, 并为应用程序存储数据.</p>
</li>
<li><p>Job</p>
<p>作业, 一个Job包含多个RDD及作用于相应RDD上的各种操作.</p>
</li>
<li><p>Stage</p>
<p>阶段, 是Job的基本调度单位, 一个Job会分为多个子Job, 也就是Stage.</p>
</li>
<li><p>Task</p>
<p>任务, 每个Stage中包含一个或多个Task, 运行在Executor上的工作单元.</p>
</li>
</ul>
<p>就是说, Spark的分布式计算架构, 在资源调配上, 其实与YARN比较相似, 所以用YARN作为其集群资源管理, 是自然的想法.</p>
<p>同时, 一个Application包含多个Job, 一个Job包含多个Stage, 一个Stage包含多个Task. 为什么会这样套娃呢? 首先一个Application包含多个Job好理解, 一个应用一般来说都是由一系列过程(函数)来完成的; 然后一个Job包含多个Stage, 这里的Stage的划分, 主要是根据Shuffle操作来定的, 关于Shuffle操作, 后文会再进行介绍; 接着一个Stage包含多个Task, 这里就是并行运算了, 可以简单理解为每次对一部分数据进行某个操作.</p>
<p>关于Spark的常见部署模式, 有单机模式和上面提到的三种模式, 这里主要介绍YARN模式. 此外, 还有<code>client</code>和<code>cluster</code>之分, 它们有什么区别呢?</p>
<p>我们知道, 当在YARN上运行Spark作业时, 每个Spark Executor对应一个YARN容器Container运行. YARN-cluster和YARN-client模式的区别其实就是Application Master进程的区别. 在YARN-cluster模式下, Driver运行在AM(Application Master)中, 它负责向YARN申请资源, 并监督作业的运行状况. 当用户提交了作业之后, 就可以关掉Client, 作业会继续在YARN上运行. 然而YARN-cluster模式不适合运行交互类型的作业.  在YARN-client模式下, Driver运行在Client端, Application Master仅仅向YARN请求Container, Client会和请求的Container通信来调度他们工作, 也就是说Client不能离开. 下面的图形象表示了两者的区别:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p><img src="fig_3.png" alt="fig"></p>
<p>前面说到, Spark是根据Shuffle类算子来进行Stage的划分. 如果我们的代码中执行了某个Shuffle类算子(比如reduceByKey), 那么就会在该算子处, 划分出一个Stage界限来. 可以大致理解为, Shuffle算子执行之前的代码会被划分为一个Stage, Shuffle算子执行以及之后的代码会被划分为下一个Stage. 如下图:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>因此一个Stage刚开始执行的时候, 它的每个Task可能都会从上一个Stage的Task所在的节点, 去通过网络传输拉取需要自己处理的所有key, 然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作, 这个过程就是Shuffle. 正是因为在执行Shuffle操作时, 会涉及到数据传输, 中断原本单个Executor的任务, 所以以此划分Stage. 同时, Shuffle操作是比较耗费资源的, 所以在处理数据时, 要避免使用大量的Shuffle操作.</p>
<p>总结一下Spark的特点:</p>
<ul>
<li><p>高效性.</p>
<p>不同于MapReduce将中间计算结果放入磁盘中, Spark采用内存存储中间计算结果, 减少了迭代运算的磁盘IO. 并通过并行计算DAG图的优化, 减少了不同任务之间的依赖, 降低了延迟等待时间.</p>
</li>
<li><p>易用性.</p>
<p>相比MapReduce仅支持Map和Reduce两种编程算子, Spark提供了超过80种不同的Transformation和Action算子, 并且采用函数式编程风格, 实现相同的功能需要的代码量极大缩小. 同时支持多种编程语言接口.</p>
</li>
<li><p>通用性.</p>
<p>Spark提供了统一的解决方案, Spark可以用于批处理, 交互式查询(Spark SQL), 实时流处理(Spark Streaming), 机器学习(Spark MLlib)和图计算(GraphX). 可以独立使用, 也可以与其它大数据平台(如Hadoop)搭配使用.</p>
</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">官网</a>下载, 如果搭配Hadoop使用, 需要注意一下Spark的版本, 这里选择2.4.7版. 下载后解压放置到<code>/usr/local/spark</code>.</p>
<p>同时在<a href="https://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">官网</a>下载2.11版本的Scala进行安装, 解压后放置到<code>/usr/local/scala</code>. 较新版的Spark内置了Scala, 所以其实不专门安装Scala也可以使用Spark.</p>
<p>配置环境变量<code>.bashrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># scala</span><br><span class="line">export SCALA_HOME=/usr/local/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br><span class="line"></span><br><span class="line"># spark</span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br></pre></td></tr></table></figure>
<p>设置日志目录, 上传jar包.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -mkdir /spark</span><br><span class="line">$ hadoop fs -mkdir /spark/logs</span><br><span class="line">$ hadoop fs -mkdir /spark/jars</span><br><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ hadoop fs -put jars/* /spark/jars/</span><br></pre></td></tr></table></figure>
<p>修改位于<code>/usr/local/spark/conf</code>的配置文件<code>spark-env.sh</code>. 这里主要针对使用YARN作为集群资源管理来进行配置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以YARN作为集群资源管理</span><br><span class="line">export SPARK_CONF_DIR=/usr/local/spark/conf</span><br><span class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://localhost:9000/spark/logs&quot;</span><br><span class="line"></span><br><span class="line"># 为后续使用PySpark进行配置</span><br><span class="line">export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip</span><br><span class="line">export PYSPARK_PYTHON=/home/shy/app/anaconda3/bin/python</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=/home/shy/app/anaconda3/bin/python</span><br></pre></td></tr></table></figure>
<p>修改<code>spark-defaults.conf</code>, 其中的配置为默认配置, 可以在提交任务时另行定义, 会覆盖这里的默认设置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.master                     yarn</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/spark/logs</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.yarn.am.memory             512m</span><br><span class="line">spark.executor.cores             1</span><br><span class="line">spark.executor.memory            512m</span><br><span class="line">spark.executor.instances         4</span><br><span class="line">spark.submit.deployMode          client</span><br><span class="line"># spark.submit.deployMode          cluster</span><br><span class="line">spark.yarn.jars                  hdfs://localhost:9000/spark/jars/*</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br></pre></td></tr></table></figure>
<p>配置<code>slaves</code>, 因为是单机上模拟分布式运算, 所以只添加<code>localhost</code>.</p>
<p>安装Python包, 在使用Jupyter时需要.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ pyspark==2.4.7</span><br></pre></td></tr></table></figure>
<p>在配置得差不多以后, 可以尝试运行自带的小程序, 若出现错误, 则根据错误进行检查. 并且由于Hadoop和Spark都有<code>start-all.sh</code>这个命令, 所以在使用这个命令时最好到对应目录下运行, 或者分开启动各项组件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 因为是基于YARN, 所以先开启Hadoop</span><br><span class="line">$ cd $HADOOP_HOME</span><br><span class="line">$ ./sbin/start-all.sh</span><br><span class="line"># 运行计算圆周率的Spark任务</span><br><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/run-example SparkPi 10</span><br></pre></td></tr></table></figure>
<p>若运行成功, 则会在倒数几行中, 看到如下信息:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pi is roughly 3.144335144335144</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2><p>总体说来, 有两种提交任务的方式, 一种是使用<code>spark-submit</code>命令, 如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/spark-submit \</span><br><span class="line">	--master yarn \</span><br><span class="line"> 	--name &quot;app_name&quot; \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --driver-memory 2g \</span><br><span class="line">  --driver-cores 2 \</span><br><span class="line">  --executor-memory 1g \</span><br><span class="line">  --executor-cores 15 \</span><br><span class="line">  --num-executors 10 \</span><br><span class="line">  job_file.py</span><br></pre></td></tr></table></figure>
<p>另一种方式, 是用过shell来进行交互, 使用PySpark的话, 可以使用命令行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/pyspark</span><br></pre></td></tr></table></figure>
<p>或者使用Jupyter, 然后加载<code>pyspark</code>包来进行交互.</p>
<h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>这里列举一些常用参数及其意义.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">—master</td>
<td style="text-align:center">指定运行模式. 可选spark://host:port, mesos://host:port, yarn, local等</td>
</tr>
<tr>
<td style="text-align:center">—deploy-mode</td>
<td style="text-align:center">可选client, cluster</td>
</tr>
<tr>
<td style="text-align:center">—name</td>
<td style="text-align:center">应用程序的名称</td>
</tr>
<tr>
<td style="text-align:center">—conf</td>
<td style="text-align:center">指定的spark配置属性, 以—conf spark.xxx=xxx来进行配置</td>
</tr>
<tr>
<td style="text-align:center">—driver-cores</td>
<td style="text-align:center">Driver的核数, 默认是1</td>
</tr>
<tr>
<td style="text-align:center">—driver-memory</td>
<td style="text-align:center">Driver内存, 默认1G</td>
</tr>
<tr>
<td style="text-align:center">—executor-core</td>
<td style="text-align:center">每个executor的核数</td>
</tr>
<tr>
<td style="text-align:center">—executor-memory</td>
<td style="text-align:center">每个executor的内存，默认是1G</td>
</tr>
<tr>
<td style="text-align:center">—num-executors</td>
<td style="text-align:center">启动的executor数量, 默认为2</td>
</tr>
</tbody>
</table>
</div>
<h2 id="运行调优"><a href="#运行调优" class="headerlink" title="运行调优"></a>运行调优</h2><p>在上文中提到编写Spark程序时, 要尽量避免Shuffle操作, 这其实就算一种调优, 此外想要让Spark程序更加顺畅地运行, 还有一些方法. 下面列举一些可以优化的地方.</p>
<ul>
<li><p>Shuffle的优化.</p>
<p>举一个栗子, 下面两个操作从结果上来说是等价的:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 操作一</span></span><br><span class="line">rdd.groupByKey().mapValues(_.sum)</span><br><span class="line"><span class="comment">// 操作二</span></span><br><span class="line">rdd.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure>
<p>但是操作一需要把全部的数据通过网络传递一遍, 而操作二根据每个 key 局部的 partition 累积结果, 在 shuffle 的之后把局部的累积值相加后得到结果, 更加高效.</p>
<p>更多的优化方法, 需要实践以及对Spark的深刻理解, 这里不做深入讲解.</p>
</li>
<li><p>缓存优化.</p>
<p>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的: 每次你对一个RDD执行一个算子操作时, 都会重新从源头处计算一遍, 计算出那个RDD来, 然后再对这个RDD执行你的算子操作. 这种方式的性能是很差的. 因此对于这种情况, 建议是对多次使用的RDD进行持久化.</p>
<p>举一个栗子:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file_path"</span>).<span class="type">Map</span>(...).filter(...)</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.count()</span><br></pre></td></tr></table></figure>
<p>对应的DAG图为:</p>
<p><img src="fig_5.png" alt="fig"></p>
<p>而如果对RDD使用<code>cache</code>函数进行缓存, 就不用再从头计算了.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file_path"</span>).<span class="type">Map</span>(...).filter(...).cache()</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.count()</span><br></pre></td></tr></table></figure>
<p>对应的DAG图为:</p>
<p><img src="fig_6.png" alt="fig"></p>
<p>此外要认识到: <code>cache</code>的RDD会一直占用内存, 当后期不需要再依赖时, 可以使用<code>unpersist</code>释放掉.</p>
</li>
<li><p>参数优化.</p>
<p>根据可用的计算资源和处理问题的规模, 设定合适的参数, 可以在充分利用资源的同时, 加速计算的流程.</p>
<ul>
<li><p>num-executors</p>
<p>默认的数量是很少的, 一般较大的任务可以设置几十个或者上百个的Executor. 设置太少无法利用集群资源, 设置太多可能出现闲置Executor的情况.</p>
</li>
<li><p>executor-memory</p>
<p>单个Executor如果太小, 则容易出现OOM, 设置大一些如4G~8G比较合适. 一般num-executors乘以executor-memory为总共申请的内存资源, 这个资源最好不要超过最大内存的1/3~1/2, 否则可能导致其他同学的程序无法正常运行.</p>
</li>
<li><p>executor-core</p>
<p>CPU核心的数量决定了Executor并行执行Task的能力, 一般设置为2~4个较为合适. 同样的, num-executors乘以executor-core为总共申请的CPU资源, 最好也不要超过CPU核数的1/3~1/2.</p>
</li>
<li><p>driver-memory</p>
<p>一般来说Driver不需要太大内存, 1G就够了. 但是如果需要使用<code>collect</code>算子将数据拉到Driver上处理, 那么需要将内存设置大一些, 以免出现OOM的问题.</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB的安装与使用</title>
    <url>/2020/09/07/%E5%A4%A7%E6%95%B0%E6%8D%AE/MongoDB%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>MongoDB 是一个基于分布式文件存储的数据库, 由 C++ 语言编写. 旨在为 WEB 应用提供可扩展的高性能数据存储解决方案.</p>
<p>MongoDB 是一个介于关系数据库和非关系数据库之间的产品, 是非关系数据库当中功能最丰富, 最像关系数据库的.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>MongoDB首先是属于NoSQL的, 那什么是NoSQL呢.</p>
<p>在现代的计算系统上每天网络上都会产生庞大的数据量, 这些数据可以由关系数据库管理系统(RDBMS)来处理. 但是随着大数据时代的来临, 数据量越来越大, 数据维度也越来越多, 数据类型各式各样, 这时完全依赖比较”古板”的RDBMS就有问题.</p>
<p>NoSQL, 指的是非关系型的数据库, 有时也称作Not Only SQL的缩写, 是对不同于传统的关系型数据库的数据库管理系统的统称.</p>
<p>NoSQL用于超大规模数据的存储, 这些类型的数据存储不需要固定的模式, 无需多余操作就可以横向扩展.</p>
<p>MongoDB是由C++语言编写的, 是一个基于分布式文件存储的开源数据库系统, 是当前NoSQL数据库产品中最热门的一种. 在高负载的情况下, 添加更多的节点, 可以保证服务器性能. MongoDB旨在为WEB应用提供可扩展的高性能数据存储解决方案.</p>
<p>MongoDB将数据存储为一个文档, 数据结构由键值(key =&gt; value)对组成. MongoDB 文档类似于JSON对象.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>MongoDB的主要目标是在键/值存储方式(提供了高性能和高度伸缩性)以及传统的RDBMS系统(丰富的功能)架起一座桥梁, 集两者的优势于一身. 比较适用于如下场景:</p>
<ul>
<li>实时的插入，更新与查询.</li>
<li>由于性能很高, 也适合作为信息基础设施的缓存层.</li>
<li>高伸缩性, 非常适合由数十或数百台服务器组成的数据库.</li>
<li>BSON数据格式非常适合文档化格式的存储及查询.</li>
</ul>
<p>但是也有一些限制:</p>
<ul>
<li>高度事务性的系统, 如银行系统.</li>
<li>不能直接使用SQL.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p><a href="https://www.mongodb.com/try/download/community?jmp=nav" target="_blank" rel="noopener">官网</a>下载对应压缩包, 解压后可改文件名为<code>mongodb</code>放入<code>/usr/local/</code>中.</p>
<p>在过程中可能会遇到几次Mac的检查, 在”安全与隐私”里面点允许即可.</p>
<p>修改shell的配置文件, 这里是<code>.zshrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mongodb</span><br><span class="line">alias mongo=/usr/local/mongodb/bin/mongo</span><br><span class="line">alias mongod=&quot;sudo /usr/local/mongodb/bin/mongod --dbpath=/Users/shy/opt/data/mongodb&quot;</span><br></pre></td></tr></table></figure>
<p>MongoDB有一个默认的数据库储存目录<code>/data/db/</code>, 但是在根目录创建文件需要额外权限, 而每次都指定另外的目录又比较麻烦, 所以用<code>alias</code>进行指代.</p>
<p>在修改了配置后, 使用<code>source .zshrc</code>激活, 然后启动MongoDB:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongod</span><br></pre></td></tr></table></figure>
<p>会发现MongoDB处于监听状态, 然后再在另外一个端口进入MongoDB的shell界面:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br></pre></td></tr></table></figure>
<p>若要关闭MongoDB, 比较好的方式是在MongoDB的shell界面输入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use admin;</span><br><span class="line">&gt; db.shutdownServer();</span><br><span class="line">&gt; exit;</span><br></pre></td></tr></table></figure>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><p>直接使用命令行安装, 版本可能不是最新的, 但是简单.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install mongodb</span><br></pre></td></tr></table></figure>
<p>通过如下命令可以查看是否启动, 一般安装好以后默认启动.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ps aux | grep mongo</span><br></pre></td></tr></table></figure>
<p>进入MongoDB的shell:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br></pre></td></tr></table></figure>
<p>启动, 关闭MongoDB:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">$ service mongodb start</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">$ service mongodb stop</span><br></pre></td></tr></table></figure>
<p>配置远程连接, 修改配置文件<code>/etc/mongodb.conf</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在bind_ip后面用逗号隔开, 添加绑定的服务器(Linux本机)的地址</span><br><span class="line">bind_ip = 127.0.0.1,服务器IP</span><br><span class="line">port = 27017</span><br></pre></td></tr></table></figure>
<p>这样在其它机器上, 可使用如下代码进行连接:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo 服务器IP:27017</span><br></pre></td></tr></table></figure>
<p>添加用户及密码.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use admin</span><br><span class="line"># 添加一名超级用户, 拥有所有权限</span><br><span class="line">&gt; db.createUser(</span><br><span class="line">... &#123;</span><br><span class="line">... user: &quot;name&quot;,</span><br><span class="line">... pwd: &quot;passwd&quot;,</span><br><span class="line">... roles: [&quot;root&quot;]</span><br><span class="line">... &#125;</span><br><span class="line">... )</span><br><span class="line"># 添加一名拥有所有数据库读写权限的用户</span><br><span class="line">&gt; db.createUser(</span><br><span class="line">... &#123;</span><br><span class="line">... user: &quot;name&quot;,</span><br><span class="line">... pwd: &quot;passwd&quot;,</span><br><span class="line">... roles: [&quot;&quot;readWriteAnyDatabase&quot;]</span><br><span class="line">... &#125;</span><br><span class="line">... )</span><br></pre></td></tr></table></figure>
<p>关于更多添加用户的方法, 可以参考<a href="https://segmentfault.com/a/1190000015603831" target="_blank" rel="noopener">这里</a>.</p>
<p>将配置文件中设置进行修改, 需要进行验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Turn on/off security.  Off is currently the default</span><br><span class="line">#noauth = true</span><br><span class="line">auth = true</span><br></pre></td></tr></table></figure>
<p>然后使用账户密码进行登录:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo 服务器IP:27017 -u &quot;name&quot; -p &quot;passwd&quot; -authenticationDatabase &quot;admin&quot;</span><br></pre></td></tr></table></figure>
<p>或者先登录, 然后再验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br><span class="line">&gt; use admin</span><br><span class="line">&gt; db.auth(&quot;name&quot;, &quot;passwd&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul>
<li><p>查看现有数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; show dbs</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use db_name</span><br></pre></td></tr></table></figure>
<p>若存在则使用, 若不存在则创建. 不过创建后, 在没有数据的情况下, 使用<code>show dbs</code>不会显示.</p>
</li>
<li><p>删除数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.dropDatabase()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><p>MongoDB中的集合, 可以类比MySQL中的数据表.</p>
<ul>
<li><p>查看集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; show collections</span><br><span class="line"># 或者</span><br><span class="line">&gt; show tables</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.createCollection(name)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.drop()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p>MongoDB里面的文档, 可以类比于MySQL中的一条数据.</p>
<ul>
<li><p>插入文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 插入一条数据</span><br><span class="line">&gt; db.table_name.insert(dict)</span><br><span class="line"># 插入多条数据</span><br><span class="line">&gt; db.table_name.insert([dict_0, dict_1, ...])</span><br></pre></td></tr></table></figure>
</li>
<li><p>查找文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(条件, 显示的字段)</span><br><span class="line">&gt; db.table_name.find(&#123;key: val, ...&#125;, &#123;key: 1, ...&#125;)</span><br></pre></td></tr></table></figure>
<p>比较, 大于<code>$gt</code>, 大于等于<code>$gte</code>, 小于<code>$lt</code>, 小于等于<code>$lte</code>, 等于<code>$eq</code>, 不等于<code>$ne</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$gt: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>在指定的元素里面, <code>$in</code>, 不在为<code>$nin</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$in: [val_0, val_1, ...]&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>列表完全匹配, <code>$all</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$all: [val_0, val_1, ...]&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>正则匹配查找, <code>$regex</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$regex: 正则表达式&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>条件否, 条件或, 条件且.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 非</span><br><span class="line">&gt; db.table_name.find(&#123;key: &#123;$not: &#123;$gt: val&#125;&#125;&#125;)</span><br><span class="line"></span><br><span class="line"># 且</span><br><span class="line">&gt; db.table_name.find(&#123;key_0: 条件_0, key_1: 条件_1&#125;)</span><br><span class="line"></span><br><span class="line"># 或</span><br><span class="line">&gt; db.table_name.find(&#123;$or: [&#123;key_0: 条件_0&#125;, &#123;key_1: 条件_1&#125;]&#125;)</span><br></pre></td></tr></table></figure>
<p>设置跳过, 显示条数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find().skip(10).limit(5)</span><br></pre></td></tr></table></figure>
<p>排序.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find().sort(&#123;key: 1 or -1&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(查找条件, 修改方式)</span><br></pre></td></tr></table></figure>
<p>直接设置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$set: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>增加值, 加法.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$inc: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>删除某个key.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$unset: &#123;key: 1&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>列表中的值.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$push: &#123;key: val&#125;&#125;)</span><br><span class="line"># 删除</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$pop: &#123;key: 1 or -1&#125;&#125;)</span><br><span class="line"># 指定删除</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$pull: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.remove(查找条件)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>使用PyMongo这个包, 可以用于连接与使用MongoDB.</p>
<ul>
<li><p>安装.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install pymongo</span><br></pre></td></tr></table></figure>
</li>
<li><p>连接.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接</span></span><br><span class="line">mongo_client = pymongo.MongoClient(<span class="string">'mongodb://user_id:passwd@服务器IP:端口号/'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前数据库</span></span><br><span class="line">print(mongo_client.list_database_names())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择数据库</span></span><br><span class="line">mongo_db = mongo_client[<span class="string">'db_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择集合</span></span><br><span class="line">mongo_table = mongo_db[<span class="string">'test_table'</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>文档查询.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查询一条</span></span><br><span class="line">print(mongo_table.find_one())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询所有</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> mongo_table.find():</span><br><span class="line">    print(i)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 按条件查询</span></span><br><span class="line">mongo_table.find(&#123;查询条件&#125;, &#123;显示字段&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>以表格DataFrame形式返回.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(list(mongo_table.find(condition, fields)))</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>结构化查询语言SQL</title>
    <url>/2020/09/06/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80SQL/</url>
    <content><![CDATA[<p>什么是SQL呢, 即Structured Query Language, 是一种语言, 可以让专业的或者非专业的人员, 对关系型数据库(如MySQL)中的数据进行查询, 处理, 转换等操作.</p>
<p>SQL本身比较简单, 同时用途也很广, 除了关系型数据库外, 一些其它系统(如Hive)在查询和处理数据时, 也是使用的SQL的语法规则. 所以掌握基础的SQL使用方法, 是很有必要的.</p>
<a id="more"></a>
<p>一般来说, 我认为有两种使用SQL的方式, 一种是针对某个任务, 全程使用SQL, 最终得到想要的结果, 可以是一个很复杂的SQL, 或者多个SQL按顺序执行; 一种是首先使用SQL将需要的数据提取出来, 然后使用其它一些工具(如Python)进行处理. 根据场景选择不同的方式, 我个人倾向于第二种(才不是我SQL比较菜呢QAQ</p>
<h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><h2 id="简单的查询"><a href="#简单的查询" class="headerlink" title="简单的查询"></a>简单的查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query for a specific columns</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable;</span><br><span class="line"></span><br><span class="line"># Select query for all columns</span><br><span class="line">SELECT * </span><br><span class="line">FROM mytable;</span><br></pre></td></tr></table></figure>
<h2 id="带条件的查询"><a href="#带条件的查询" class="headerlink" title="带条件的查询"></a>带条件的查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with constraints</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition</span><br><span class="line">    AND/OR another_condition</span><br><span class="line">    AND/OR …;</span><br></pre></td></tr></table></figure>
<p>常用的一些与数值相关的条件操作有:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">operator</th>
<th style="text-align:center">example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=, !=, &lt; &lt;=, &gt;, &gt;=</td>
<td style="text-align:center">col_name <strong>!=</strong> 4</td>
</tr>
<tr>
<td style="text-align:center">BETWEEN … AND …</td>
<td style="text-align:center">col_name <strong>BETWEEN</strong> 1.5 <strong>AND</strong> 10.5</td>
</tr>
<tr>
<td style="text-align:center">NOT BETWEEN … AND …</td>
<td style="text-align:center">col_name <strong>NOT BETWEEN</strong> 1 <strong>AND</strong> 10</td>
</tr>
<tr>
<td style="text-align:center">IN (…)</td>
<td style="text-align:center">col_name <strong>IN</strong> (2, 4, 6)</td>
</tr>
<tr>
<td style="text-align:center">NOT IN (…)</td>
<td style="text-align:center">col_name <strong>NOT IN</strong> (1, 3, 5)</td>
</tr>
</tbody>
</table>
</div>
<p>常用的一些与文本相关的条件操作有:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">operator</th>
<th style="text-align:center">example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td style="text-align:center">col_name <strong>=</strong> “abc”</td>
</tr>
<tr>
<td style="text-align:center">!= or &lt;&gt;</td>
<td style="text-align:center">col_name <strong>!=</strong> “abcd”</td>
</tr>
<tr>
<td style="text-align:center">LIKE</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “ABC”</td>
</tr>
<tr>
<td style="text-align:center">NOT LIKE</td>
<td style="text-align:center">col_name <strong>NOT LIKE</strong> “ABCD”</td>
</tr>
<tr>
<td style="text-align:center">%</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “%AT%”<br>(matches “AT”, “ATTIC”, “CAT” or even “BATS”)</td>
</tr>
<tr>
<td style="text-align:center">_</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “AN_”<br>(matches “AND”, but not “AN”)</td>
</tr>
<tr>
<td style="text-align:center">IN (…)</td>
<td style="text-align:center">col_name <strong>IN</strong> (“A”, “B”, “C”)</td>
</tr>
<tr>
<td style="text-align:center">NOT IN (…)</td>
<td style="text-align:center">col_name <strong>NOT IN</strong> (“D”, “E”, “F”)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with ordered results</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column ASC/DESC;</span><br></pre></td></tr></table></figure>
<h2 id="设置查询范围"><a href="#设置查询范围" class="headerlink" title="设置查询范围"></a>设置查询范围</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with limited rows</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column ASC/DESC</span><br><span class="line">LIMIT num_limit OFFSET num_offset;</span><br></pre></td></tr></table></figure>
<h2 id="多表联合查询"><a href="#多表联合查询" class="headerlink" title="多表联合查询"></a>多表联合查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with INNER/LEFT/RIGHT/FULL JOINs on multiple tables</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">INNER/LEFT/RIGHT/FULL JOIN another_table </span><br><span class="line">    ON mytable.id = another_table.matching_id</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column, … ASC/DESC</span><br><span class="line">LIMIT num_limit OFFSET num_offset;</span><br></pre></td></tr></table></figure>
<h2 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with constraints on NULL values</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE column IS/IS NOT NULL</span><br><span class="line">AND/OR another_condition</span><br><span class="line">AND/OR …;</span><br></pre></td></tr></table></figure>
<h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with expression aliases</span><br><span class="line">SELECT col_expression AS expr_description, …</span><br><span class="line">FROM mytable;</span><br></pre></td></tr></table></figure>
<h2 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h2><ul>
<li><p>获取字符个数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LENGTH (&apos;abc&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>拼接字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT (&apos;abc&apos;, &apos;def&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串大写.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT UPPER(&apos;ABC&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串小写.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LOWER(&apos;ABC&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>截取子字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUBSTR(&apos;abcd&apos;, 1, 3);</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断子字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 若存在, 则返回起始位置, 否则返回0</span><br><span class="line">SELECT INSTR(&apos;abcd&apos;, &apos;cd&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>去除首尾空格.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 则表示去除空格</span><br><span class="line">SELECT TRIM(&apos;   abc##d# &apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符填充.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 用指定字符左填充到指定长度</span><br><span class="line">SELECT LPAD(&apos;abc&apos;, 4, &apos;#&apos;);</span><br><span class="line"># 用指定字符右填充到指定长度</span><br><span class="line">SELECT RPAD(&apos;abc&apos;, 4, &apos;#&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符替换.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT REPLACE(&apos;abc&apos;, &apos;b&apos;, &apos;#&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>小数近似.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 四舍五入</span><br><span class="line">SELECT ROUND(3.1415);</span><br><span class="line"># 保留指定位数</span><br><span class="line">SELECT ROUND(3.1415, 2);</span><br><span class="line"># 向上取整</span><br><span class="line">SELECT CEIL(-1.01);</span><br><span class="line"># 向下取整</span><br><span class="line">SELECT FLOOR(9.99);</span><br><span class="line"># 截断</span><br><span class="line">SELECT TRUNCATE(1.6699, 2);</span><br></pre></td></tr></table></figure>
</li>
<li><p>取模.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MOD(10, 3);</span><br></pre></td></tr></table></figure>
</li>
<li><p>日期相关.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前日期+时间</span><br><span class="line">SELECT NOW();</span><br><span class="line"># 当前日期</span><br><span class="line">SELECT CURDATE();</span><br><span class="line"># 当前时间</span><br><span class="line">SELECT CURTIME();</span><br><span class="line"># 返回日期对应星期的英文名字</span><br><span class="line">SELECT DAYNAME(NOW());</span><br><span class="line"># 字符串转时间</span><br><span class="line">SELECT STR_TO_DATE(&apos;2020-01-02&apos;, &apos;%Y-%m-%d&apos;);</span><br><span class="line"># 时间转字符串</span><br><span class="line">SELECT DATE_FORMAT(NOW(), &apos;%Y-%m-%d&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>流程控制.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># IF</span><br><span class="line">SELECT IF(1&gt;0, &apos;yes&apos;, &apos;no&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>其它.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取版本号</span><br><span class="line">SELECT VERSION();</span><br><span class="line"># 获取当前数据库</span><br><span class="line">SELECT DATABASE();</span><br><span class="line"># 获取登录用户名</span><br><span class="line">SELECT USER();</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with aggregate functions over all rows</span><br><span class="line">SELECT AGG_FUNC(column_or_expression) AS aggregate_description, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE constraint_expression;</span><br><span class="line"></span><br><span class="line"># Select query with HAVING constraint</span><br><span class="line">SELECT group_by_column, AGG_FUNC(column_expression) AS aggregate_result_alias, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition</span><br><span class="line">GROUP BY column</span><br><span class="line">HAVING group_condition;</span><br></pre></td></tr></table></figure>
<p>常用的聚合函数:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">function</th>
<th style="text-align:center">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">COUNT(*) or COUNT(col)</td>
<td style="text-align:center">统计非NULL数据条数.</td>
</tr>
<tr>
<td style="text-align:center">MIN(col)</td>
<td style="text-align:center">取最大值.</td>
</tr>
<tr>
<td style="text-align:center">MAX(col)</td>
<td style="text-align:center">取最小值.</td>
</tr>
<tr>
<td style="text-align:center">AVG(col)</td>
<td style="text-align:center">取非NULL的平均值.</td>
</tr>
<tr>
<td style="text-align:center">SUM(col)</td>
<td style="text-align:center">取总和.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用另外一个查询结果, 来作为本次查询的条件</span><br><span class="line">SELECT *</span><br><span class="line">FROM sales_associates</span><br><span class="line">WHERE salary &gt; </span><br><span class="line">   (SELECT AVG(revenue_generated)</span><br><span class="line">    FROM sales_associates);</span><br></pre></td></tr></table></figure>
<h2 id="查询执行顺序"><a href="#查询执行顺序" class="headerlink" title="查询执行顺序"></a>查询执行顺序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Complete SELECT query</span><br><span class="line">SELECT DISTINCT column, AGG_FUNC(column_or_expression), …</span><br><span class="line">FROM mytable</span><br><span class="line">    JOIN another_table</span><br><span class="line">      ON mytable.column = another_table.column</span><br><span class="line">    WHERE constraint_expression</span><br><span class="line">    GROUP BY column</span><br><span class="line">    HAVING constraint_expression</span><br><span class="line">    ORDER BY column ASC/DESC</span><br><span class="line">    LIMIT count OFFSET COUNT;</span><br></pre></td></tr></table></figure>
<p>执行顺序如下:</p>
<ul>
<li>FROM AND JOIN.</li>
<li>WHERE.</li>
<li>GROUP BY.</li>
<li>HAVING.</li>
<li>SELECT.</li>
<li>DISTINCT.</li>
<li>ORDER BY.</li>
<li>LIMIT / OFFSET.</li>
</ul>
<h1 id="添加"><a href="#添加" class="headerlink" title="添加"></a>添加</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Insert statement with values for all columns</span><br><span class="line">INSERT INTO mytable</span><br><span class="line">VALUES (value_or_expr, another_value_or_expr, …),</span><br><span class="line">       (value_or_expr_2, another_value_or_expr_2, …),</span><br><span class="line">       …;</span><br><span class="line">       </span><br><span class="line"># Insert statement with specific columns</span><br><span class="line">INSERT INTO mytable</span><br><span class="line">(column, another_column, …)</span><br><span class="line">VALUES (value_or_expr, another_value_or_expr, …),</span><br><span class="line">      (value_or_expr_2, another_value_or_expr_2, …),</span><br><span class="line">      …;</span><br></pre></td></tr></table></figure>
<h1 id="更改"><a href="#更改" class="headerlink" title="更改"></a>更改</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Update statement with values</span><br><span class="line">UPDATE mytable</span><br><span class="line">SET column = value_or_expr, </span><br><span class="line">    other_column = another_value_or_expr, </span><br><span class="line">    …</span><br><span class="line">WHERE condition;</span><br></pre></td></tr></table></figure>
<h1 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Delete statement with condition</span><br><span class="line">DELETE FROM mytable</span><br><span class="line">WHERE condition;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>编程基础</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>REmote DIctionary Server (Redis) 是一个开源的key - value存储系统, 使用C语言编写, 可基于内存亦可持久化的数据库, 并提供多种语言的API.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>Redis 与其他 key-value 缓存产品有以下三个特点：</p>
<ul>
<li><p>支持数据的持久化.</p>
<p>可以将内存中的数据保存在磁盘中, 重启的时候可以再次加载进行使用.</p>
</li>
<li><p>支持多种数据结构.</p>
<p>不仅仅支持简单的key-value类型的数据, 同时还提供list, set, zset, hash等数据结构的存储.</p>
</li>
<li><p>Redis支持数据的备份.</p>
<p>即master-slave模式的数据备份.</p>
</li>
</ul>
<p>此外, Redis还有如下一些优势:</p>
<ul>
<li><p>性能极高.</p>
<p>Redis能读的速度约100000次/s, 写的速度约80000次/s .</p>
</li>
<li><p>原子性.</p>
<p>Redis的所有操作都是原子性的.</p>
</li>
</ul>
<p>因此, Redis可以作为一个单独的数据库使用, 也可以配合其它数据库(如MySQL)使用. 可以作为缓存层, 将部分数据存入Redis中, 优先对Redis进行查询, 当查询不到时, 再到全量数据库(如MySQL)中进行查询.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p>在<a href="https://redis.io/download" target="_blank" rel="noopener">官网</a>下载, 移到指定目录, 然后解压后编译.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-6.0.7.tar.gz</span><br><span class="line">$ mv redis-6.0.7.tar.gz /usr/local/</span><br><span class="line">$ cd /usr/local</span><br><span class="line">$ sudo tar -zxvf redis-6.0.7.tar.gz</span><br><span class="line">$ sudo mv redis-6.0.7 redis</span><br><span class="line">$ cd redis</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo test make</span><br></pre></td></tr></table></figure>
<p>修改配置文件<code>/etc/local/redis/redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Log file</span><br><span class="line">logfile /usr/local/redis/redis.log</span><br><span class="line"></span><br><span class="line"># The working directory</span><br><span class="line">dir /usr/local/redis/redis_dbfile/</span><br></pre></td></tr></table></figure>
<p>并检查以上目录/文件的权限, 保证有读写的权利.</p>
<p>修改<code>.zshrc</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis</span><br><span class="line">alias redis-server=/usr/local/redis/src/redis-server</span><br><span class="line">alias redis-cli=/usr/local/redis/src/redis-cli</span><br></pre></td></tr></table></figure>
<p>启动Redis.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-server</span><br></pre></td></tr></table></figure>
<p>关闭Redis, 可使用<code>kill 进程号</code>的方式, 也可以:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IP:PORT&gt; SHUTDOWN</span><br></pre></td></tr></table></figure>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><p>同样在<a href="https://redis.io/download" target="_blank" rel="noopener">官网</a>下载, 移到指定目录, 然后解压后编译.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-6.0.7.tar.gz</span><br><span class="line">$ mv redis-6.0.7.tar.gz /usr/local/</span><br><span class="line">$ cd /usr/local</span><br><span class="line">$ sudo tar -zxvf redis-6.0.7.tar.gz</span><br><span class="line">$ sudo mv redis-6.0.7 redis</span><br><span class="line">$ cd redis</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo make test</span><br></pre></td></tr></table></figure>
<p>若出现<code>make</code>未安装的错误:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install make</span><br><span class="line">$ sudo apt-get install pkg-config</span><br><span class="line">$ sudo apt-get install tcl tk</span><br><span class="line">$ sudo make distclean</span><br><span class="line">$ sudo make clean</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo make test</span><br></pre></td></tr></table></figure>
<p>修改配置文件<code>/etc/local/redis/redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Log file</span><br><span class="line">logfile /usr/local/redis/redis.log</span><br><span class="line"></span><br><span class="line"># The working directory</span><br><span class="line">dir /usr/local/redis/redis_dbfile/</span><br></pre></td></tr></table></figure>
<p>并检查以上目录/文件的权限, 保证有读写的权利.</p>
<p>修改<code>.bashrc</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis</span><br><span class="line">alias redis-server=&quot;/usr/local/redis/src/redis-server /usr/local/redis/redis.conf&quot;</span><br><span class="line">alias redis-cli=/usr/local/redis/src/redis-cli</span><br></pre></td></tr></table></figure>
<p>启动Redis.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-server /usr/local/redis/redis.conf</span><br></pre></td></tr></table></figure>
<p>关闭Redis, 可使用<code>kill 进程号</code>的方式, 也可以:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IP:PORT&gt; SHUTDOWN</span><br></pre></td></tr></table></figure>
<p>远程连接配置<code>redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 注释掉bind</span><br><span class="line"># bind 127.0.0.1</span><br><span class="line"></span><br><span class="line"># 修改protected-mode</span><br><span class="line">protected-mode no</span><br></pre></td></tr></table></figure>
<p>这样可以在其它客户端直接登录.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379</span><br></pre></td></tr></table></figure>
<p>使用密码认证登录, 修改<code>redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改protected-mode</span><br><span class="line">protected-mode yes</span><br><span class="line"></span><br><span class="line"># 解除注释, 添加密码</span><br><span class="line">requirepass passwd</span><br></pre></td></tr></table></figure>
<p>在连接时认证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379 -a passwd --raw</span><br></pre></td></tr></table></figure>
<p>或者先登录再验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379</span><br><span class="line">IP:PORT&gt; auth passwd</span><br><span class="line">IP:PORT&gt; CONFIG GET requirepass</span><br><span class="line">1) &quot;requirepass&quot;</span><br><span class="line">2) &quot;passwd&quot;</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>Redis中没有类似MySQL中的数据库, 数据表的结构, 支持5种数据类型:</p>
<ul>
<li>字符串(String).</li>
<li>哈希(Hash).</li>
<li>列表(List).</li>
<li>集合(Set).</li>
<li>有序集合(Zset)</li>
</ul>
<p>下面分别介绍相关的操作.</p>
<ul>
<li><p>键(Key).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查询某个key是否存在</span><br><span class="line">IP:PORT&gt; EXISTS key</span><br><span class="line"># 删除指定key</span><br><span class="line">IP:PORT&gt; DEL key</span><br><span class="line"># 查看所有key</span><br><span class="line">IP:PORT&gt; keys *</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串(String)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置指定key的值</span><br><span class="line">IP:PORT&gt; SET key val</span><br><span class="line"># 获取指定key的值</span><br><span class="line">IP:PORT&gt; GET key</span><br><span class="line"># 获取多个指定key的值</span><br><span class="line">IP:PORT&gt; MGET key_0 key_1 ...</span><br><span class="line"># 将key中储存的数字值加一</span><br><span class="line">IP:PORT&gt; INCR key</span><br><span class="line"># 将key中储存的数字值减一</span><br><span class="line">IP:PORT&gt; DECR key</span><br></pre></td></tr></table></figure>
</li>
<li><p>哈希(Hash).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将key中field设置为val</span><br><span class="line">IP:PORT&gt; HSET key field val</span><br><span class="line"># 同时设置多个field-val</span><br><span class="line">IP:PORT&gt; HMSET key field_0 val_0  field_1 val_1 ...</span><br><span class="line"># 获取哈希表中所有字段field</span><br><span class="line">IP:PORT&gt; HKEYS key</span><br><span class="line"># 获取哈希表中所有值val</span><br><span class="line">IP:PORT&gt; HVALS key</span><br><span class="line"># 获取指定字段的值</span><br><span class="line">IP:PORT&gt; HGET key field</span><br><span class="line"># 同时获取多个字段的值</span><br><span class="line">IP:PORT&gt; HMGET key field_0 field_1</span><br><span class="line"># 查看所有的键值对</span><br><span class="line">IP:PORT&gt; HGETALL key</span><br><span class="line"># 删除一个或多个哈希表字段</span><br><span class="line">IP:PORT&gt; HDEL key field_0 field_1</span><br></pre></td></tr></table></figure>
</li>
<li><p>列表(List).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将一个或多个值插到列表头部</span><br><span class="line">IP:PORT&gt; LPUSH key val_0 val_1 ...</span><br><span class="line"># 将一个或多个值插到列表尾部</span><br><span class="line">IP:RORT&gt; LPUSH key val_0 val_1 ...</span><br><span class="line"># 移出并获取列表的第一个元素</span><br><span class="line">IP:RORT&gt; LPOP key</span><br><span class="line"># 移出并获取列表的最后一个元素</span><br><span class="line">IP:RORT&gt; RPOP key</span><br><span class="line"># 通过索引获取指定元素值</span><br><span class="line">IP:RORT&gt; LINDEX key index</span><br><span class="line"># 通过索引范围获取指定范围元素值</span><br><span class="line">IP:RORT&gt; LRANGE key start stop</span><br><span class="line"># 通过索引设置列表元素的值</span><br><span class="line">IP:RORT&gt; LSET key index val</span><br></pre></td></tr></table></figure>
</li>
<li><p>集合(Set).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向集合添加元素</span><br><span class="line">IP:PORT&gt; SADD key val_0 val_1 ...</span><br><span class="line"># 返回成员中所有成员</span><br><span class="line">IP:PORT&gt; SMEMBERS key</span><br><span class="line"># 移除集合中的元素</span><br><span class="line">IP:PORT&gt; SREM key val_0 val_1 ...</span><br></pre></td></tr></table></figure>
</li>
<li><p>有序集合(Zset).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向集合中添加一个或者多个成员和分数, 或更新分数</span><br><span class="line">IP:PORT&gt; ZADD key score_0 member_0 score_1 member_1 ...</span><br><span class="line"># 移除一个或多个成员</span><br><span class="line">IP:PORT&gt; ZREM key member_0 member_1 ...</span><br><span class="line"># 通过索引区间(有序)返回成员及分数</span><br><span class="line">IP:PORT&gt; ZRANGE key start stop [WITHSCORES]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>更多的操作可以在谷歌上进行查找.</p>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>安装Python包.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip isntall redis</span><br></pre></td></tr></table></figure>
<p>进行简单的操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">redis_con = redis.Redis(host=<span class="string">'IP'</span>,port=<span class="number">6379</span>,password=<span class="string">'passwd'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前键</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> redis_con.keys():</span><br><span class="line">    print(k.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 删除指定key</span></span><br><span class="line">redis_con.delete(<span class="string">'key'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其余操作与直接使用命令行操作几乎完全一致</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hbase%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>使用过Hive的同学一定都有过这样的感受, 有时候想要进行一个非常简单的查询, 也要耗费不少时间, 在一些需要实时查询的场景是用不上的.</p>
<p>而基于谷歌提出的Bigtable数据模型, 在HDFS的基础上, 出现了Hbase. 这是一个真正的分布式数据库.</p>
<a id="more"></a>
<h1 id="Hbase简介"><a href="#Hbase简介" class="headerlink" title="Hbase简介"></a>Hbase简介</h1><p>首先, 为什么会有Hbase这样的分布式数据库呢? 很简单, 因为单机的数据库在大数据时代不能胜任所有任务. 一般来说, 像MySQL这样的数据库, 大概可以处理TB级别的数据, 但是有些互联网公司一天的数据量可能就是TB级别.</p>
<p>然后又问, 不是已经有了HDFS以及Hive了吗, 数据量已经不是问题了呀. 因为HDFS和Hive属于文件系统, 或者数据仓库, 而数据库是以某种有组织的方式储存的数据集合. 作为数据库的Hbase的一个很明显的特点就是, 能够高并发地随机写和支持实时查询, 这是HDFS不具备的.</p>
<p>下面来介绍Hbase的架构, Hbase是一种基于Key-Value的NoSQL数据库, 这样设计的好处是, 可以避免数据稀疏带来的储存空间浪费, 同时可以非常方便地对字段进行扩展.</p>
<p>在Hbase里面也有表的概念, 表的结构如下图:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>其中的几个关键元素:</p>
<ul>
<li><p>列族.</p>
<p>可以理解为列的归类. 在一个列族下可以创建多个列.</p>
</li>
<li><p>列.</p>
<p>就是一般意义上的列, 字段名.</p>
</li>
<li><p>行键.</p>
<p>用于标识一行的标识. 用有相同行键的数据, 属于一行. 需要保证唯一, 且设计合理, 方便加速查询.</p>
</li>
</ul>
<p>上面虽然以类似二维表的形式来展示, 但是Hbase本质是NoSQL, 所以一般来说Key就是”行键-列族-列”, Value就是对应的值.</p>
<p>此外, 还有一个元素是时间戳, 当数据写入Hbase, 更改, 甚至删除的时候, 都会有一个时间戳, 可以当成一个数据的版本. 所以在查询时, 可以根据时间戳查询过去的值.</p>
<p>Hbase整体的结构如下图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<ul>
<li><p>Client.</p>
<p>客户端, 提供Hbase的访问接口.</p>
</li>
<li><p>Zookeeper.</p>
<p>储存Hbase的元数据(meta表), 在读写数据时, 去Zookeeper中拿到meta数据, 告诉客户端去哪台机器读写数据.</p>
</li>
<li><p>HRegion.</p>
<p>在上面介绍表结构时候, 说到了列族, 行键的概念, 而在具体储存时, 会按照行键进行切分, 划分为多个HRegion.</p>
</li>
<li><p>HMaster.</p>
<p>监控HRegionServer的状态, 处理元数据的变更, 处理HRegion的分配或转移. 如当某个HRegion过大时, 自动切分为多个HRegion.</p>
</li>
<li><p>HRegionServer.</p>
<p>真正干活的节点, 负责与HDFS底层交互, 处理客户端请求. 一个HRegionServer中有多个HRegion.</p>
</li>
</ul>
<p>在每个HRegion内部, 有多个按列族划分的Store, 每个Store内, 又有Mem Store和Store File. 其中Mem Store负责在内存接收数据, 当超过一定阈值时, 再刷写到硬盘的Store File上.</p>
<p><img src="fig_2.jpg" alt="fig"></p>
<p>还漏了一个Hlog, 这是干嘛用的呢. 其实就是在写数据的时候, 会先写到内存, 但为了防止机器宕机, 内存中的数据没刷到硬盘就挂了, 所以在写Mem Store时还会写一份Hlog, 按顺序快速写到硬盘.</p>
<p>总结一下Hbase的特点:</p>
<ul>
<li>采用Key-Value列式存储, 节约空间.</li>
<li>可自动切分数据, 能够水平拓展.</li>
<li>可提供高并发, 低延迟的读写操作.</li>
<li>不支持丰富的条件筛选查询, 比较依赖行键.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在<a href="https://hbase.apache.org/downloads.html" target="_blank" rel="noopener">这里</a>下载, 我选择的是2.3.1版本.</p>
<p>下载完成后, 解压放置到目录<code>/usr/local/hbase</code>.</p>
<p>在Ubuntu操作系统上, <code>.bashrc</code>添加如下内容:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hbase</span><br><span class="line">export HBASE_HOME=/usr/local/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>
<p>在<code>/usr/local/hbase/conf</code>下的<code>hbase-env.sh</code>文件进行修改, 具体路径根据实际情况改动:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The java implementation to use.  Java 1.8+ required.</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="comment"># Tell HBase whether it should manage it's own instance of ZooKeeper or not.</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>其中<code>HBASE_MANAGES_ZK</code>表示设置由Hbase自己管理Zookeeper, 不需要单独配置.</p>
<p>再配置<code>hbase-site.xml</code>文件:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中<code>hbase.tmp.dir</code>需要与HDFS的地址端口相匹配.</p>
<p>启动Hbase:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动Hadoop</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-all.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Hbase</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-hbase.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看进程</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> jps</span></span><br><span class="line">7012 HQuorumPeer</span><br><span class="line">4661 NameNode</span><br><span class="line">5542 ResourceManager</span><br><span class="line">7625 HRegionServer</span><br><span class="line">5242 SecondaryNameNode</span><br><span class="line">7260 HMaster</span><br><span class="line">7965 Jps</span><br><span class="line">4911 DataNode</span><br><span class="line">5759 NodeManager</span><br></pre></td></tr></table></figure>
<p>在浏览器上<code>IP:60010</code>可以看到交互界面.</p>
<p>进入Hbase Shell:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hbase shell</span></span><br></pre></td></tr></table></figure>
<p>使用<code>help</code>查看帮助, <code>help &#39;cmd&#39;</code>查看具体命令帮助, <code>exit</code>退出shell.</p>
<p>在关闭时, 也是<code>stop-hbase.sh</code>先关闭Hbase, 再<code>stop-all.sh</code>关闭Hadoop.</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><p>下面列举一些Hbase的常用命令行操作.</p>
<ul>
<li><p>表操作.</p>
<p>创建.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建表, 以及对应的列族</span><br><span class="line">&gt; create &apos;tb_name&apos;, &apos;col_family_0&apos;, &apos;col_family_0&apos;, ...</span><br><span class="line"># 添加列族</span><br><span class="line">&gt; alter &apos;tb_name&apos;, &apos;col_family&apos;</span><br></pre></td></tr></table></figure>
<p>删除.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除列族</span><br><span class="line">&gt; alter &apos;tb_name&apos;, &apos;delete&apos;=&gt;&apos;col_family&apos;</span><br><span class="line"># 删除表</span><br><span class="line">&gt; disable &apos;tb_name&apos;</span><br><span class="line">&gt; drop &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
<p>查看.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示Hbase中的表</span><br><span class="line">&gt; list</span><br><span class="line"># 查看表结构</span><br><span class="line">&gt; describe &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据操作.</p>
<p>插入/覆盖数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; put &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;, val</span><br></pre></td></tr></table></figure>
<p>获取数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取指定行数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;</span><br><span class="line"># 获取指定行, 指定列族数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family&apos;</span><br><span class="line"># 获取指定单元格数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;</span><br><span class="line"></span><br><span class="line">&gt; 获取指定行, 指定多个列族或列的数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &#123;COLUMN=&gt;[&apos;col_family_0:col_0&apos;, &apos;col_family_1&apos;, ...]&#125;</span><br><span class="line"></span><br><span class="line"># 查看表中所有数据, &#123;FORMATTER =&gt; &apos;toString&apos;&#125;可识别中文</span><br><span class="line">&gt; scan &apos;tb_name&apos;, &#123;FORMATTER =&gt; &apos;toString&apos;&#125;</span><br><span class="line"># 获取指定多个列族或列的数据</span><br><span class="line">&gt; scan &apos;tb_name&apos;, &#123;COLUMN=&gt;[&apos;col_family_0:col_0&apos;, &apos;col_family_1&apos;, ...]&#125;</span><br></pre></td></tr></table></figure>
<p>删除数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 指定单元格内容</span><br><span class="line">&gt; delete &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;</span><br><span class="line"># 整行删除</span><br><span class="line">&gt; deleteall &apos;tb_name&apos;, &apos;row_key&apos;</span><br><span class="line"># 清空数据表</span><br><span class="line">&gt; truncate &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>如果要对Hbase进行一些复杂的流程处理或者操作, 命令行的方式是比较麻烦的, 其原生的编程接口语言是Java, 不过借助thrift接口服务器, 因此也可以采用其他语言来使用Hbase, 这里使用Python.</p>
<p>在使用Python包之前, 需要启动Hbase的thrift-server:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hbase-daemon.sh start thrift</span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">34533 ThriftServer</span><br></pre></td></tr></table></figure>
<p>不止一个Python包可以操作Hbase, 不过个人觉得<a href="https://happybase.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">HappyBase</a>比较好用, 不需要再进行更多的额外配置, 比如安装系统的thrift和对应依赖, 直接连接即可使用.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line"><span class="comment"># thrift默认端口9090</span></span><br><span class="line">connection = happybase.Connection(<span class="string">'hostname'</span>)</span><br><span class="line">table = connection.table(<span class="string">'table-name'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加/修改</span></span><br><span class="line">table.put(<span class="string">b'row-key'</span>, &#123;<span class="string">b'family:qual1'</span>: <span class="string">b'value1'</span>,</span><br><span class="line">                       <span class="string">b'family:qual2'</span>: <span class="string">b'value2'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">row = table.row(<span class="string">b'row-key'</span>)</span><br><span class="line">print(row[<span class="string">b'family:qual1'</span>])  <span class="comment"># prints 'value1'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, data <span class="keyword">in</span> table.rows([<span class="string">b'row-key-1'</span>, <span class="string">b'row-key-2'</span>]):</span><br><span class="line">    print(key, data)  <span class="comment"># prints row key and data for each row</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, data <span class="keyword">in</span> table.scan(row_prefix=<span class="string">b'row'</span>):</span><br><span class="line">    print(key, data)  <span class="comment"># prints 'value1' and 'value2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定row-key</span></span><br><span class="line">row = table.delete(<span class="string">b'row-key'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 更多API可以查询文档</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具, 可以将结构化的数据文件映射为一张数据库表, 并提供简单的SQL查询功能, 可以将SQL语句转换为MapReduce任务进行运行.  其优点是学习成本低, 可以通过类SQL语句快速实现简单的MapReduce统计, 不必开发专门的MapReduce应用, 十分适合数据仓库的统计分析.</p>
<a id="more"></a>
<h1 id="Hive介绍"><a href="#Hive介绍" class="headerlink" title="Hive介绍"></a>Hive介绍</h1><p><img src="fig_0.png" alt="fig"></p>
<p>上图是Hive的一个整体架构, Hive的数据是存储在HDFS上的, Hive中的库和表可以看作是对HDFS上数据做的一个映射. 所以Hive必须是运行在一个Hadoop集群上的.</p>
<p>Hive的元数据是一般是存储在MySQL这种关系型数据库上, Hive和MySQL之间通过MetaStore服务交互.</p>
<p>Hive中的执行器, 是将SQL转化为MapReduce程序, 然后将要执行的MapReduce程序放到YARN上以一系列Job的方式去执行.</p>
<p>总体来说Hive有如下一些特点:</p>
<ul>
<li><p>容易上手.</p>
<p>通过类SQL来处理大数据, 而避免了写MapReduce程序, 这样使得分析数据更容易.</p>
</li>
<li><p>逻辑表.</p>
<p>数据是存储在HDFS上的, Hive本身并不提供数据的存储功能. 将数据映射成数据库和一张张的表, 库和表的元数据信息一般存在关系型数据库上.</p>
</li>
<li><p>数据存储方面.</p>
<p>能够存储很大的数据集, 对数据完整性, 格式要求并不严格.</p>
</li>
<li><p>数据处理方面.</p>
<p>因为Hive语句最终会生成MapReduce任务去计算, 所以不适用于实时计算查询的场景, 它适用于离线批量处理.</p>
</li>
</ul>
<p>相比一些关系型数据库(如MySQL), Hive还有一些概念:</p>
<ul>
<li>内部表/外部表.</li>
<li>分区.</li>
<li>Array, Map, Struct数据结构.</li>
</ul>
<p>这些是处于大数据的安全, 灵活, 更快地处理而提出的, 这里暂不做更多的介绍.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>系统为Ubuntu, 在安装Hive之前, 确保已经安装好了Hadoop和MySQL.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysql -V</span></span><br><span class="line">mysql  Ver 14.14 Distrib 5.7.31, for Linux (x86_64) using  EditLine wrapper</span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop version</span></span><br><span class="line">Hadoop 2.10.0</span><br><span class="line">Subversion ssh://git.corp.linkedin.com:29418/hadoop/hadoop.git -r e2f1f118e465e787d8567dfa6e2f3b72a0eb9194</span><br><span class="line">Compiled by jhung on 2019-10-22T19:10Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 7b2d8877c5ce8c9a2cca5c7e81aa4026</span><br><span class="line">This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.10.0.jar</span><br></pre></td></tr></table></figure>
<p>下载合适的Hive版本, <a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/" target="_blank" rel="noopener">下载地址</a>. 这里下载的是2.3.7版.</p>
<p>下载后解压, 移到<code>/usr/local/hive/</code>.</p>
<p>修改环境配置<code>.bashrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hive</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>
<p>修改Hive配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_CONF_DIR</span><br><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br><span class="line">$ nano hive-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/tmp/hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>因为在<code>hive-site.xml</code>默认有以上配置, 所以在Hadoop集群中进行创建.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">$ hadoop fs -chmod -R 777 /user/hive/warehouse</span><br><span class="line">$ hadoop fs -mkdir -p /tmp/hive/</span><br><span class="line">$ hadoop fs -chmod -R 777 /tmp/hive</span><br><span class="line">$ hadoop fs -ls /user/hive</span><br><span class="line">$ hadoop fs -ls /tmp/hive</span><br></pre></td></tr></table></figure>
<p>将配置中的<code>${system:java.io.tmpdir}</code>替换为<code>/usr/local/hive/tmp/</code>, 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/hive/tmp/$&#123;hive.session.id&#125;_resources&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>并创建对应文件夹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME</span><br><span class="line">$ mkdir tmp</span><br><span class="line">$ chmod -R 777 tmp/</span><br></pre></td></tr></table></figure>
<p>将配置文件中<code>${system:user.name}</code>都替换为<code>root</code>, 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/hive/tmp/root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>修改数据库相关配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql://MySQL对应IP:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;登录MySQL的ID&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;登录MySQL的密码&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>添加MySQL驱动包, <a href="https://dev.mysql.com/downloads/connector/j/5.1.html" target="_blank" rel="noopener">下载地址</a>, 解压后移动到Hive里面.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cp mysql-connector-java-5.1.49.jar $HIVE_HOME/lib</span><br></pre></td></tr></table></figure>
<p>新建<code>hive-env.sh</code>文件并修改:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_CONF_DIR</span><br><span class="line">$ cp hive-env.sh.template hive-env.sh</span><br><span class="line">$ nano hive-env.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line">HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hive/conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Folder containing extra libraries required for hive compilation/execution c$</span></span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/usr/<span class="built_in">local</span>/hive/lib</span><br></pre></td></tr></table></figure>
<p>初始化MySQL数据库:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME/bin</span><br><span class="line">$ schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure>
<p>若成功, 则会在MySQL中生成Hive元数据库; 若失败, 则检查配置是否正确.</p>
<p>启动Hive:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME/bin</span><br><span class="line">$ ./hive</span><br></pre></td></tr></table></figure>
<p>进行简单测试:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show functions;</span><br><span class="line">OK</span><br><span class="line">!</span><br><span class="line">!=</span><br><span class="line">$sum0</span><br><span class="line">%</span><br><span class="line">&amp;</span><br><span class="line">*</span><br><span class="line">+</span><br><span class="line">-</span><br><span class="line">/</span><br><span class="line">&lt;</span><br><span class="line">&lt;=</span><br><span class="line">&lt;=&gt;</span><br><span class="line">&lt;&gt;</span><br><span class="line">=</span><br><span class="line">==</span><br><span class="line">&gt;</span><br><span class="line">&gt;=</span><br><span class="line">^</span><br><span class="line">abs</span><br><span class="line">acos</span><br><span class="line">add_months</span><br><span class="line">aes_decrypt</span><br><span class="line">aes_encrypt</span><br><span class="line">and</span><br><span class="line">array</span><br><span class="line">...</span><br><span class="line">hive&gt; desc function sum;</span><br><span class="line">OK</span><br><span class="line">sum(x) - Returns the sum of a set of numbers</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><ul>
<li><p>数据库操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database db_name;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 标准方式</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database <span class="keyword">if</span> not exists db_name;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定路径(hdfs)</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database <span class="keyword">if</span> not exists db_name location <span class="string">'hdfs_path/db_name.db'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 模糊查找, 使用*和.</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases like <span class="string">'abc*'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use db_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看数据库字段格式</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc database db_name;</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc database extended db_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop database db_name;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>数据表操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 普通创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table IF NOT EXISTS</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> db_name.table_name(</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> id string,</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> user string)</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> COMMENT <span class="string">'abc'</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> STORED AS TEXTFILE;</span></span><br><span class="line">		</span><br><span class="line"><span class="meta">#</span><span class="bash"> 子查询方式创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table IF NOT EXISTS db_name.table_name_0</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> AS select * from table_name_1;</span></span><br><span class="line">		</span><br><span class="line"><span class="meta">#</span><span class="bash"> like方式创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table db_name.table_name_0 like table_name_1;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空表的内容，保留表的结构</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> truncate table table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop table <span class="keyword">if</span> exists table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 插入数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert into table_name</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> values</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> (..., ..., ...),</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ...</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> (..., ..., ...);</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加载本地文件到hive表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">'path/file'</span> into table db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载hdfs文件到hive中, overwrite表示覆盖原数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">'path/file'</span> overwrite into table db_name.table_name;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数据.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取数据到本地</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert overwrite <span class="built_in">local</span> directory <span class="string">'path'</span> </span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> select * from db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在终端使用重定向到本地</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive -e <span class="string">"select * from db_name.table_name;"</span> &gt; <span class="string">'path/file'</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取数据到hdfs</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert overwrite directory <span class="string">'hdfs_path'</span> select * from db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若发现获取的数据中分隔符为hive默认分隔符\x01, 可替换</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed <span class="string">'s/\x01/,/g'</span> file &gt; new_file</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>一提到大数据, 可能大家首先就会想到Hadoop, 这里将会简单介绍Hadoop的基本原理, 在Linux上的安装过程, 以及一些基础的文件操作方式.</p>
<a id="more"></a>
<h1 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h1><p>为什么Hadoop会成为大数据框架的首选呢? 可以从其两个主要的功能说起:</p>
<ul>
<li>HDFS</li>
<li>MapReduce</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS(Hadoop Distributed File System)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统. 它和现有的分布式文件系统有很多共同点, 但同时, 它和其他的分布式文件系统的区别也是很明显的. HDFS 是一个<strong>高度容错性</strong>的系统, 适合部署在廉价的机器上. HDFS能提供高吞吐量的数据访问, 非常适合大规模数据集上的应用.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 其中各个组件分别为:</p>
<ul>
<li>Client: 客户端.</li>
<li>NameNode: 对应master, 它相当于是一个管理者, 存储元数据.</li>
<li>DataNode: 对应slave, 执行操作并存储实际数据.</li>
<li>SecondaryNameNode: 用于辅助NameNode.</li>
</ul>
<p>HDFS读写文件的方式如下两图, 本质上都是先与NameNode进行互动, 然后再在DataNode上进行操作, 最后再向NameNode报告.</p>
<p>读文件:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>写文件:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>对于HDFS的文件操作命令, 与Linux的文件操作命令非常类似, 在后面会进行介绍.</p>
<p>总结一下HDFS的优缺点:</p>
<ul>
<li>优点:<ul>
<li>支持海量数据存储.</li>
<li>高容错性.</li>
<li>流式数据访问.</li>
<li>…</li>
</ul>
</li>
<li>缺点:<ul>
<li>不能做到低延迟数据访问.</li>
<li>不适合大量的小文件存储.</li>
<li>不支持直接修改文件.</li>
<li>…</li>
</ul>
</li>
</ul>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>MapReduce是一种针对处理大数据的算法范式, 某种意义上来说, 就是一种分治算法.</p>
<p>而所谓分治算法, 分而治之, 先将大问题转化为一个个小问题, 再将小问题逐一解决后, 汇合成大问题的结果.</p>
<p>MapReduce算法, 具体来说可以有如下一些步骤:</p>
<ul>
<li>Inout</li>
<li>Split</li>
<li>Map</li>
<li>Shuffle</li>
<li>Reduce</li>
<li>Finalize</li>
</ul>
<p>比如下面一个餐馆做饭的栗子:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>首先将原材料(Input)分发(Split)给各个厨师, 然后各个厨师对其进行加工(Map), 然后按菜的品类进行整理(Shuffle), 接着用整理好的单品组合制作成一份份快餐(Reduce), 最后用户点菜时就将对应快餐送出(Finalize).</p>
<p>再举一个统计单词出现次数的栗子吧:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>以上的栗子都是比较简单直观的, 但是对于一些复杂的任务, 可能要执行多次交替的Map操作和Reduce操作, 并且一般用Java来编写具体的代码. 对于非专业人员来说, 是有一定难度的, 正是基于此, 才有了后面的Hive.</p>
<h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>上面说到了HDFS是分布式文件系统, MapReduce是运行在上面的算法, YARN的英文全称为Yet Another Resource Negotiator, 意思是”另一种资源调度器”. 那么为什么会有YARN呢?</p>
<p>直接的原因, 就是在Hadoop1.x中, 在MapReduce时, JobTracker负责的事情太多了, 接受任务, 调度资源, 监控TaskTracker运行情况. 这样实现简单, 但是容易出错, 也不容易扩展.</p>
<p>因此在后来的Hadoop2.x中, 将JobTracker进行了拆分, 开发出了YARN. 其实Hadoop能有今天的地位, YARN功不可没, 正因为有了YARN, 更多的算法框架(如Spark)可以接入到HDFS中. HDFS可能不是最优秀的分布式文件系统, 但却是应用最广泛的.</p>
<p>接下来详细介绍YARN的架构:</p>
<p><img src="fig_5.jpg" alt="fig"></p>
<ul>
<li><p>Container</p>
<p>容器在YARN这里是对(计算)资源做的一种抽象, 比如将一定的CPU核数和内存封装成一个个的容器.</p>
<p>同时, 容易由NodeManager启动和管理, 并被其监控; 被ResourceManager进行调度.</p>
</li>
<li><p>ResourceManager</p>
<p>从名字上看, 就知道RM是负责资源管理的, 整个系统只有一个RM, 其包含两个主要的组件:</p>
<ul>
<li><p>Scheduler</p>
<p>定时调度器, 本质是一种策略或者算法. 当Client提交一个任务时, 它会根据所需要的资源和当前集群的资源状况进行分配. 它只负责分配资源, 不做监控以及状态跟踪.</p>
</li>
<li><p>ApplicationManager</p>
<p>应用管理器, 每当Client提交一个Application时, 就会新建一个对应的ApplicationManager, 由这个AM去向RM申请容器资源, 获得资源后将要运行的程序发送到容器运行, 并进行监控.</p>
</li>
</ul>
</li>
<li><p>NodeManager</p>
<p>节点管理器是每台机器上的代理, 负责容器的管理和监控, 并向ResourceManager/Scheduler提供资源使用情况.</p>
</li>
</ul>
<p>当提交一个任务(Application)到YARN时, 后续整个流程如下:</p>
<ul>
<li>Client向RM提交Application.</li>
<li>RM向NM通信, 为该Application分配第一个容器, 并在这个容器上运行AM.</li>
<li>在AM启动后, 对Application进行拆分, 拆分出来的task可以运行在一个或者多个容器中. 然后向RM申请运行程序的容器, 并定时向RM发送”心跳”.</li>
<li>申请到容器后, AM会去和对应的NM通信, 将Application的task发送到对应NM的容器运行.</li>
<li>容器中运行的task也会向AM发送”心跳”, 当程序运行完成后, AM向RM注销并释放容器资源.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>这里不配置真正分布式的Hadoop, 一来比较麻烦, 需要多台电脑或者多个虚拟机, 二来更多的是想使用基于Hadoop的Hive等大数据套件.</p>
<p>所以这里安装的是伪分布式, 意思就是其系统是按真正的分布式去运作的, 不过所有流程都在一台电脑上执行. 这里的操作系统为Ubuntu.</p>
<p>由于Hadoop的底层语言为Java, 所以需要事先安装Java.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get update</span><br><span class="line"><span class="meta">$</span> sudo apt-get install openjdk-8-jdk</span><br><span class="line"><span class="meta">$</span> java -version</span><br></pre></td></tr></table></figure>
<p>同时还要配置<code>JAVA_HOME</code>, 寻找Java安装路径:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> which java</span><br><span class="line">/usr/bin/java</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> ls -lrt /usr/bin/java</span><br><span class="line">lrwxrwxrwx 1 root root 22 9月  11 14:05 /usr/bin/java -&gt; /etc/alternatives/java</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> ls -lrt /etc/alternatives/java</span><br><span class="line">lrwxrwxrwx 1 root root 46 9月  11 14:05 /etc/alternatives/java -&gt; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java</span><br></pre></td></tr></table></figure>
<p>编辑<code>.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># java</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JRE_HOME</span>/lib:<span class="variable">$CLASSPATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/hadoop/lib/native:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib/native"</span></span><br></pre></td></tr></table></figure>
<p>在<a href="https://mirrors.bfsu.edu.cn/apache/hadoop/common/" target="_blank" rel="noopener">这里</a>选择要下载的Hadoop版本, 我选择的是2.10.0版本. 下载好以后解压到<code>/usr/local/hadoop</code>.</p>
<p>下面是修改配置文件, Hadoop的配置文件一般位于<code>/usr/local/hadoop/etc/hadoop/</code>, 首先是<code>hadoop-env.sh</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The java implementation to use.</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br></pre></td></tr></table></figure>
<p>此时可以测试一下Word Count的小程序.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd /usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">$ mkdir input</span><br><span class="line">$ 在input文件夹中放入一些文本</span><br><span class="line">$ hadoop jar hadoop-mapreduce-examples-2.10.0.jar wordcount input/ output/</span><br></pre></td></tr></table></figure>
<p>若执行成功, 则会生成一个<code>output</code>文件夹, 里面有计数的结果.</p>
<p>或者可以执行计算圆周率的小程序:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop jar hadoop-mapreduce-examples-2.10.0.jar pi 16 100000</span><br></pre></td></tr></table></figure>
<p>接下来搭建伪分布式集群, 包括三个核心配置文件<code>core-site.xml</code>,  <code>hdfs-site.xml</code>,  <code>yarn-site.xml</code>.</p>
<p>修改<code>core-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>hdfs-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>yarn-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>mapred-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改从节点配置文件<code>slaves</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>
<p>配置完成, 格式化NameNode:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs namenode -format</span></span><br></pre></td></tr></table></figure>
<p>开启守护进程:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-dfs.sh</span></span><br></pre></td></tr></table></figure>
<p>通过<code>jps</code>查看Java进程的状态:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> jps</span></span><br><span class="line">23620 SecondaryNameNode</span><br><span class="line">23879 Jps</span><br><span class="line">23049 NameNode</span><br><span class="line">23293 DataNode</span><br></pre></td></tr></table></figure>
<p>再启动YARN:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动YARN</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-yarn.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启历史服务器, 才能在Web中查看任务运行情况</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mr-jobhistory-daemon.sh start historyserver</span></span><br></pre></td></tr></table></figure>
<p>通过<code>jps</code>查看Java进程的状态:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ jps</span><br><span class="line">24275 ResourceManager</span><br><span class="line">24483 NodeManager</span><br><span class="line">23620 SecondaryNameNode</span><br><span class="line">23049 NameNode</span><br><span class="line">24955 Jps</span><br><span class="line">23293 DataNode</span><br></pre></td></tr></table></figure>
<p>此外, 可以通过<code>start-all.sh</code>一次性启动, <code>stop-all.sh</code>一次性关闭.</p>
<p>在浏览器上可以在<code>IP:50070</code>查看HDFS的情况, 这里IP是NameNode对应的地址.</p>
<p>同时, 还可以在<code>IP:8088</code>查看YARN的情况.</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>这里暂时就不讲怎么写MapReduce程序了, 虽然用其它一些脚本语言(如Python)也能够写, 但是还是比较麻烦.</p>
<p>主要介绍HDFS的一些命令行操作, 其实感觉和Linux的文件系统命令很类似. 输入<code>hadoop fs</code>可以查看到相关命令说明. 下面是一些常用的命令.</p>
<ul>
<li><p>文件夹.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建文件夹, 一般首先创建/user/user_name作为默认目录</span><br><span class="line">$ hadoop fs -mkdir -p /user/user_name</span><br><span class="line"></span><br><span class="line"># 删除文件夹</span><br><span class="line">$ hadoop fs -rm -r -f path</span><br><span class="line">$ hadoop fs -rmdir path</span><br></pre></td></tr></table></figure>
</li>
<li><p>文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将Client端的文件传到HDFS中, 若文件存在, 可使用-f参数覆盖</span><br><span class="line">$ hadoop fs -put -f local_file hdfs_path</span><br><span class="line"></span><br><span class="line"># 从HDFS中获取文件到Client端</span><br><span class="line">$ hadoop fs -get hdfs_file local_path</span><br><span class="line"></span><br><span class="line"># 复制</span><br><span class="line">$ hadoop fs -cp path_0 path_1</span><br><span class="line"></span><br><span class="line"># 移动</span><br><span class="line">$ hadoop fs -mv path_0 path_1</span><br><span class="line"></span><br><span class="line"># 删除</span><br><span class="line">$ hadoop fs -rm path</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看指定目录下文件</span><br><span class="line">$ hadoop fs -ls path</span><br><span class="line"></span><br><span class="line"># 查看指定目录已使用空间</span><br><span class="line">$ hadoop fs -du -h path</span><br><span class="line"></span><br><span class="line"># 查看指定目录剩余空间</span><br><span class="line">$ hadoop fs -df -h path</span><br><span class="line"></span><br><span class="line"># 查看文件内容</span><br><span class="line">$ hadoop fs -cat file</span><br><span class="line">$ hadoop fs -tail file</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/MySQL%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>MySQL是最流行的关系型数据库管理系统, 在WEB应用方面MySQL是最好的 RDBMS(Relational Database Management System: 关系数据库管理系统)应用软件之一.</p>
<p>本篇主要介绍MySQL的基本知识, 掌握MySQL数据库的使用.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>数据库(Database)是按照数据结构来组织, 存储和管理数据的仓库.</p>
<p>每个数据库都有一个或多个不同的 API 用于创建, 访问, 管理, 搜索和复制所保存的数据.</p>
<p>我们也可以将数据存储在文件中, 但是在文件中读写数据速度相对较慢.</p>
<p>所以, 现在我们使用关系型数据库管理系统(RDBMS)来存储和管理大数据量. 所谓的关系型数据库, 是建立在关系模型基础上的数据库, 借助于集合代数等数学概念和方法来处理数据库中的数据.</p>
<p>关系数据库管理系统(Relational Database Management System)的特点：</p>
<ul>
<li>数据以表格的形式出现.</li>
<li>每行为各种记录名称.</li>
<li>每列为记录名称所对应的数据域.</li>
<li>许多的行和列组成一张表单.</li>
<li>若干的表单组成database.</li>
</ul>
<p>以下为RDBMS的一些术语:</p>
<ul>
<li><strong>数据库:</strong> 数据库是一些关联表的集合.</li>
<li><strong>数据表:</strong> 表是数据的矩阵. 在一个数据库中的表看起来像一个简单的电子表格.</li>
<li><strong>列:</strong> 一列(数据元素)包含了相同类型的数据, 例如邮政编码的数据.</li>
<li><strong>行:</strong> 一行(元组, 或记录)是一组相关的数据, 例如一条用户订阅的数据.</li>
<li><strong>主键:</strong> 主键是唯一的, 一个数据表中只能包含一个主键. 你可以使用主键来查询数据.</li>
<li><strong>外键:</strong> 外键用于关联两个表.</li>
<li><strong>索引:</strong> 使用索引可快速访问数据库表中的特定信息. 索引是对数据库表中一列或多列的值进行排序的一种结构, 类似于书籍的目录.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">官网</a>下载对应的dmg安装包.</p>
<p>一路点确定, 中间过程中注意关于密码的设置或者提示.</p>
<p>在安装以后默认启动, 可以通过<code>ps aux | grep mysql</code>查看是否有相关进程.</p>
<p>修改配置文件, 方便后续控制, 修改<code>.bash_profile</code>或者<code>.zshrc</code>, 这里设置了登录, 启动, 关闭, 重新启动的别名.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mysql</span><br><span class="line">alias mysql=/usr/local/mysql/bin/mysql</span><br><span class="line">alias mysql_start=&quot;sudo /usr/local/mysql/support-files/mysql.server start&quot;</span><br><span class="line">alias mysql_stop=&quot;sudo /usr/local/mysql/support-files/mysql.server stop&quot;</span><br><span class="line">alias mysql_restart=&quot;sudo /usr/local/mysql/support-files/mysql.server restart&quot;</span><br></pre></td></tr></table></figure>
<p>然后再使修改生效:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ source .bash_profile</span><br><span class="line"># 或者</span><br><span class="line">$ source .zshrc</span><br></pre></td></tr></table></figure>
<p>查看端口号, 登录MySQL后, 运行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show global variables like &apos;port&apos;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| port          | 3306  |</span><br><span class="line">+---------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>若不想MySQL开机启动, 可在偏好设置中关闭开机启动.</p>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install mysql-server</span><br><span class="line"># 配置</span><br><span class="line">$ sudo mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>一般来说需要让Linux上的MySQL作为服务端, 所以需要设置允许远程登录. 需要修改配置文件, 配置文件具体位置一般在<code>\etc\mysql\my.cnf</code>或者<code>/etc/mysql/mysql.conf.d/mysqld.cnf</code>, 将其中的<code>bind-address=127.0.0.1</code>进行注释.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ netstat -nlt | grep 3306</span><br></pre></td></tr></table></figure>
<p>此时可以看到:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tcp6       0      0 :::3306                 :::*                    LISTEN</span><br></pre></td></tr></table></figure>
<p>网络监听从<code>127.0.0.1:3306</code>变成 <code>0 0.0.0.0:3306</code>, 表示MySQL已经允许远程登陆访问.</p>
<p>进一步设置, 登录MySQL, 然后根据需求输入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;user_name&apos;@&apos;IP&apos; IDENTIFIED BY &apos;passwd&apos;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>
<p>其中第一个<code>*</code>是数据库, 可以改成允许访问的数据库名称; 第二个<code>*</code>是数据库的表名称, 代表允许访问任意的表; <code>user_name</code>表示允许远程登录的用户名, 自定义; <code>IP</code>表示远程登录的IP, 可用<code>%</code>表示任意IP; <code>passwd</code>表示登录密码.</p>
<p>使用如下代码可以查看当前有哪些用户, 及相关信息:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; USE mysql;</span><br><span class="line">mysql&gt; SELECT User, Host, plugin FROM mysql.user;</span><br></pre></td></tr></table></figure>
<p>关于MySQL的启动, 关闭和重启:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">$ service mysql stop</span><br><span class="line"></span><br><span class="line"># 重启</span><br><span class="line">$ service mysql restart</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul>
<li><p>创建数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; DROP DATABASE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; USE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="数据表"><a href="#数据表" class="headerlink" title="数据表"></a>数据表</h2><ul>
<li><p>常用数据类型.</p>
<ul>
<li>INT: 整数.</li>
<li>FLOAT: 浮点数.</li>
<li>DATE: 日期, YYYY-MM-DD.</li>
<li>TIME: 时间, HH:MM:SS.</li>
<li>YEAR: 年, YYYY.</li>
<li>DATATIME:  YYYY-MM-DD HH:MM:SS.</li>
<li>TIMESTAMP: 时间戳.</li>
<li>CHAR: 定长字符串.</li>
<li>VARCHAR: 变长字符串.</li>
</ul>
</li>
<li><p>创建数据表.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE IF NOT EXISTS 数据表名(</span><br><span class="line">		-&gt; idx INT UNSIGNED NOT NULL AUTO_INCREMENT,</span><br><span class="line">		-&gt; name VARCHAR(10) DEFAULT NULL,</span><br><span class="line">		-&gt; height FLOAT DEFAULT NULL,</span><br><span class="line">		-&gt; date DATE DEFAULT NULL,</span><br><span class="line">		-&gt; PRIMARY KEY(idx)</span><br><span class="line">		-&gt; )ENGINE=innoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除数据表.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; DROP TABLE 数据表名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; INSERT INTO 数据表名</span><br><span class="line">		-&gt; (field1,...,fieldN)</span><br><span class="line">		-&gt; VALUES</span><br><span class="line">		-&gt; (value1,...,valueN);</span><br></pre></td></tr></table></figure>
<p>从文件导入.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; LOAD DATA LOCAL INFILE &apos;文件路径&apos; INTO TABLE test_table CHARACTER SET utf8</span><br><span class="line">		-&gt; (name, height, date)</span><br><span class="line">		-&gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">		-&gt; LINES TERMINATED BY &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
<p>其中<code>LOCAL</code>表示本地, 不加表示服务器路径. 后面两行指定分隔符和换行符.</p>
</li>
<li><p>导出数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查询允许导出路径</span><br><span class="line">mysql&gt; SHOW VARIABLES LIKE &apos;%secure%&apos;;</span><br><span class="line"></span><br><span class="line"># 导出</span><br><span class="line">mysql&gt; SELECT * FROM 数据表名</span><br><span class="line">		-&gt; INTO OUTFILE &apos;文件路径&apos;</span><br><span class="line">		-&gt; FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">		-&gt; ENCLOSED BY &apos;&quot;&apos;</span><br><span class="line">		-&gt; LINES TERMINATED BY &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>其它增/删/改/查的操作.</p>
<p>可查看我的另外一篇博客<a href="https://whitemoonlight.top/2020/09/06/编程基础/结构化查询语言SQL/" target="_blank" rel="noopener">结构化查询语言SQL</a>.</p>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>当要借助Python来查询MySQL中的数据时, 可以使用pymysql这个包, 用法如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装</span><br><span class="line">$ pip install pymysql</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接</span></span><br><span class="line">connection = pymysql.connect(host=<span class="string">'ip'</span>,</span><br><span class="line">                             port=<span class="number">3306</span>,</span><br><span class="line">                             user=<span class="string">'user_name'</span>,</span><br><span class="line">                             password=<span class="string">'passwd'</span>,</span><br><span class="line">                             db=<span class="string">'database_name'</span>,</span><br><span class="line">                             charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的查询, 并以DataFrame形式返回</span></span><br><span class="line">sql = <span class="string">'SELECT * FROM table_name;'</span></span><br><span class="line">df = pd.read_sql(sql, connection)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取游标</span></span><br><span class="line">cursor = connection.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行查询</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取单条数据</span></span><br><span class="line">cursor.fetchone()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取前N条数据</span></span><br><span class="line">cursor.fetchmany(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有数据</span></span><br><span class="line">cursor.fetchall()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据表</span></span><br><span class="line">cursor.execute(<span class="string">'''CREATE TABLE ...'''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据(元组或列表)</span></span><br><span class="line">cursor.execute(<span class="string">'INSERT INTO `users` (`name`, `age`) VALUES (%s, %s)'</span>, (<span class="string">'mary'</span>, <span class="number">18</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 插入数据(字典)</span></span><br><span class="line">info = &#123;<span class="string">'name'</span>: <span class="string">'fake'</span>, <span class="string">'age'</span>: <span class="number">15</span>&#125;</span><br><span class="line">cursor.execute(<span class="string">'''INSERT INTO `users`</span></span><br><span class="line"><span class="string">									(`name`, `age`)</span></span><br><span class="line"><span class="string">									VALUES</span></span><br><span class="line"><span class="string">									(%(name)s, %(age)s)'''</span>, info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># INSERT, UPDATE, DELETE等修改数据的语句需手动执行完成对数据修改的提交</span></span><br><span class="line">connection.commit()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-使用GPU</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E4%BD%BF%E7%94%A8GPU/</url>
    <content><![CDATA[<p>深度学习的训练过程常常非常耗时, 一个模型训练几个小时是家常便饭, 训练几天也是常有的事情.</p>
<p>训练过程的耗时主要来自于两个部分, 一部分来自数据准备, 另一部分来自参数迭代.</p>
<p>当数据准备过程还是模型训练时间的主要瓶颈时, 我们可以使用更多进程来准备数据. 当参数迭代过程成为训练时间的主要瓶颈时, 我们通常的方法是应用GPU或者Google的TPU来进行加速.</p>
<a id="more"></a>
<p>TensorFlow从CPU切换成单GPU训练模型都是非常方便的, 几乎无需更改任何代码.</p>
<p>当存在可用的GPU时, 如果不特意指定device, TensorFlow会自动优先选择使用GPU来创建张量和执行张量计算.</p>
<p>但如果是在公司或者学校实验室的服务器环境, 存在多个GPU和多个使用者时, 为了不让单个使用者的任务占用全部GPU资源导致其他同学无法使用(TensorFlow默认获取全部GPU的全部内存资源权限, 但实际上只使用一个GPU的部分资源), 我们通常会在<strong>开头</strong>增加以下几行代码以控制每个任务使用的GPU编号和显存大小, 以便其他人也能够同时训练模型.</p>
<h1 id="指定CPU或者GPU"><a href="#指定CPU或者GPU" class="headerlink" title="指定CPU或者GPU"></a>指定CPU或者GPU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看可用物理CPU</span></span><br><span class="line">tf.config.list_physical_devices(<span class="string">"CPU"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:CPU:0&apos;, device_type=&apos;CPU&apos;)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看可用物理GPU</span></span><br><span class="line">tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:GPU:0&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>为了得知我们的操作和张量被配置到哪个设备(GPU还是CPU)上, 我们可以使用<code>tf.debugging.set_log_device_placement</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create some tensors</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果希望在自己选择的设备上运行特定的操作, 而不是TensorFlow自动选择, 我们可以使用<code>tf.device</code>来创建设备上下文, 该上下文中的所有操作都将在相同的指定设备上运行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place tensors on the CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">    a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<h1 id="比较CPU与GPU"><a href="#比较CPU与GPU" class="headerlink" title="比较CPU与GPU"></a>比较CPU与GPU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp() % (<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts // <span class="number">3600</span> + <span class="number">8</span>, tf.int32) % tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts % <span class="number">3600</span>) // <span class="number">60</span>, tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts % <span class="number">60</span>), tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>, m)) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"0&#123;&#125;"</span>, m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"&#123;&#125;"</span>, m))</span><br><span class="line"></span><br><span class="line">    timestring = tf.strings.join(</span><br><span class="line">        [timeformat(hour),</span><br><span class="line">         timeformat(minite),</span><br><span class="line">         timeformat(second)],</span><br><span class="line">        separator=<span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span> * <span class="number">4</span> + timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在GPU下进行矩阵运算</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/GPU:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">-2.</span>, maxval=<span class="number">2.</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">-2.</span>, maxval=<span class="number">2.</span>)</span><br><span class="line">    c = a @ b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=<span class="number">0</span>), axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">========================================21:32:10</span><br><span class="line">-890166.625</span><br><span class="line">========================================21:32:11</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在CPU下进行矩阵运算</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/CPU:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">0.</span>, maxval=<span class="number">3.</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">0.</span>, maxval=<span class="number">3.</span>)</span><br><span class="line">    c = a @ b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=<span class="number">0</span>), axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">========================================21:32:14</span><br><span class="line">2.2501049e+12</span><br><span class="line">========================================21:32:36</span><br></pre></td></tr></table></figure>
<h1 id="限制GPU内存增长"><a href="#限制GPU内存增长" class="headerlink" title="限制GPU内存增长"></a>限制GPU内存增长</h1><p>默认情况下, TensorFlow会将所有GPU(取决于CUDA_VISIBLE_DEVICES)的几乎所有GPU内存映射到进程. 这样做是为了通过减少内存碎片更有效地使用设备上相对宝贵的GPU内存资源.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">gpus</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:GPU:0&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>按需分配内存, 开始分配非常少的内存, 随着程序运行根据需求分配更多内存.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行这样的配置, 需要在一开始就声明, 否则可能会报错</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Memory growth must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<p>直接指定具体使用多少内存.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行这样的配置, 需要在一开始就声明, 否则可能会报错</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>],</span><br><span class="line">        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<h1 id="多GPU的使用"><a href="#多GPU的使用" class="headerlink" title="多GPU的使用"></a>多GPU的使用</h1><h1 id="指定在某个GPU上运行"><a href="#指定在某个GPU上运行" class="headerlink" title="指定在某个GPU上运行"></a>指定在某个GPU上运行</h1><p>指定只在某个物理GPU上运行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">tf.config.set_visible_devices([gpus[<span class="number">0</span>]], <span class="string">'GPU'</span>)</span><br></pre></td></tr></table></figure>
<p>使用虚拟GPU模拟多GPU环境…</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 虚拟出两个内存1G的GPU</span></span><br><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>], [</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>),</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>)</span><br><span class="line">        ])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">    print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 Physical GPU, 2 Logical GPUs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logical_gpus = tf.config.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">logical_gpus</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[LogicalDevice(name=&apos;/device:GPU:0&apos;, device_type=&apos;GPU&apos;),</span><br><span class="line"> LogicalDevice(name=&apos;/device:GPU:1&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>如果我们的系统里有不止一个GPU, 则默认情况下, ID最小的GPU将被选用. 如果想在不同的GPU上运行, 我们需要显式地指定优先项.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/device:GPU:1'</span>):</span><br><span class="line">    a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果希望TensorFlow自动选择一个现有且受支持的设备来运行操作, 以避免指定的设备不存在, 那么可以调用<code>tf.config.set_soft_device_placement(True)</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.config.set_soft_device_placement(<span class="literal">True</span>)</span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<h2 id="在多GPU上运行"><a href="#在多GPU上运行" class="headerlink" title="在多GPU上运行"></a>在多GPU上运行</h2><p>可以用<code>tf.distribute.MirroredStrategy</code>来使用多个GPU.</p>
<p>MirroredStrategy过程简介：</p>
<ul>
<li>训练开始前, 该策略在所有 N 个计算设备上均各复制一份完整的模型.</li>
<li>每次训练传入一个批次的数据时, 将数据分成 N 份, 分别传入 N 个计算设备(即数据并行).</li>
<li>N 个计算设备使用本地变量(镜像变量)分别计算自己所获得的部分数据的梯度.</li>
<li>使用分布式计算的 All-reduce 操作, 在计算设备间高效交换梯度数据并进行求和, 使得最终每个设备都有了所有设备的梯度之和.</li>
<li>使用梯度求和的结果更新本地变量(镜像变量).</li>
<li>当所有设备均更新本地变量后, 进行下一轮训练(即该并行策略是同步的).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 虚拟出两个内存1G的GPU</span></span><br><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>], [</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>),</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>)</span><br><span class="line">        ])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">    print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 Physical GPU, 2 Logical GPUs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (train_x, train_y)).shuffle(buffer_size=<span class="number">512</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (test_x, test_y)).shuffle(buffer_size=<span class="number">512</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里增加两行</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()  </span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(</span><br><span class="line">        layers.Dense(<span class="number">100</span>,</span><br><span class="line">                     activation=<span class="string">'relu'</span>,</span><br><span class="line">                     kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">    model.add(</span><br><span class="line">        layers.Dense(<span class="number">50</span>,</span><br><span class="line">                     activation=<span class="string">'relu'</span>,</span><br><span class="line">                     kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/1000</span><br><span class="line">15/15 [==============================] - 2s 124ms/step - loss: 43.3710 - auc: 0.5209 - val_loss: 4.4724 - val_auc: 0.7976</span><br><span class="line">Epoch 2/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 4.6899 - auc: 0.7616 - val_loss: 3.9844 - val_auc: 0.8571</span><br><span class="line">Epoch 3/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 2.1644 - auc: 0.8892 - val_loss: 1.7289 - val_auc: 0.9446</span><br><span class="line">Epoch 4/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.7997 - auc: 0.9065 - val_loss: 1.5541 - val_auc: 0.9600</span><br><span class="line">Epoch 5/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.4721 - auc: 0.9127 - val_loss: 1.5341 - val_auc: 0.9560</span><br><span class="line">Epoch 6/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.3007 - auc: 0.9227 - val_loss: 1.5797 - val_auc: 0.9430</span><br><span class="line">Epoch 7/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.1857 - auc: 0.9286 - val_loss: 1.1779 - val_auc: 0.9611</span><br><span class="line">Epoch 8/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.0409 - auc: 0.9353 - val_loss: 0.9681 - val_auc: 0.9602</span><br><span class="line">Epoch 9/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 0.9415 - auc: 0.9387 - val_loss: 0.8586 - val_auc: 0.9651</span><br><span class="line">Epoch 10/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.8615 - auc: 0.9441 - val_loss: 0.7910 - val_auc: 0.9706</span><br><span class="line">Epoch 11/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 0.7812 - auc: 0.9496 - val_loss: 0.6869 - val_auc: 0.9702</span><br><span class="line">Epoch 12/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.7197 - auc: 0.9573 - val_loss: 0.5969 - val_auc: 0.9704</span><br><span class="line">Epoch 13/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6657 - auc: 0.9610 - val_loss: 0.5601 - val_auc: 0.9754</span><br><span class="line">Epoch 14/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6996 - auc: 0.9505 - val_loss: 0.5677 - val_auc: 0.9780</span><br><span class="line">Epoch 15/1000</span><br><span class="line">15/15 [==============================] - 1s 34ms/step - loss: 0.7026 - auc: 0.9440 - val_loss: 0.5424 - val_auc: 0.9850</span><br><span class="line">Epoch 16/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6049 - auc: 0.9652 - val_loss: 0.5115 - val_auc: 0.9871</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-训练模型</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>根据不同的任务, 简单或者复杂, 可以灵活地使用TensorFlow提供的多种训练模型的方法, 下面就来介绍几种比较常用的.</p>
<a id="more"></a>
<h1 id="训练模型的三种方法"><a href="#训练模型的三种方法" class="headerlink" title="训练模型的三种方法"></a>训练模型的三种方法</h1><h2 id="内置fit方法"><a href="#内置fit方法" class="headerlink" title="内置fit方法"></a>内置fit方法</h2><p>如果是一些比较普通的建模任务, 那么在TensorFlow2.x中, 可以使用类似于sklearn的<code>fit</code>方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">64</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用内置fit训练模型</span></span><br><span class="line"></span><br><span class="line">model.fit(train_x,</span><br><span class="line">          train_y,</span><br><span class="line">          validation_data=(test_x, test_y),</span><br><span class="line">          batch_size=<span class="number">32</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/100</span><br><span class="line">15/15 [==============================] - 0s 24ms/step - loss: 5.6850 - auc: 0.6227 - val_loss: 1.0607 - val_auc: 0.8588</span><br><span class="line">Epoch 2/100</span><br><span class="line">15/15 [==============================] - 0s 4ms/step - loss: 1.1538 - auc: 0.8317 - val_loss: 0.8551 - val_auc: 0.8851</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="内置train-on-batch方法"><a href="#内置train-on-batch方法" class="headerlink" title="内置train_on_batch方法"></a>内置train_on_batch方法</h2><p>上面的<code>fit</code>方法一般来说只能以epoch为最小单位来进行操作, 而如果想进行更加精细的操作, 比如以batch为单位, 那么就可以使用<code>train_on_batch</code>方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"></span><br><span class="line">train_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (train_x, train_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">test_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (test_x, test_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用内置train_on_batch训练模型</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.reset_metrics()</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_set:</span><br><span class="line">        train_result = model.train_on_batch(batch_x, batch_y)</span><br><span class="line">        <span class="comment"># 根据需求还可以做其它事情, 如调解学习率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估</span></span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> test_set:</span><br><span class="line">        valid_result = model.test_on_batch(batch_x,</span><br><span class="line">                                          batch_y,</span><br><span class="line">                                          reset_metrics=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line">    print(<span class="string">"train:"</span>, dict(zip(model.metrics_names, train_result)))</span><br><span class="line">    print(<span class="string">"valid:"</span>, dict(zip(model.metrics_names, valid_result)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##########################################</span><br><span class="line">train: &#123;&apos;loss&apos;: 0.1489427238702774, &apos;auc&apos;: 1.0&#125;</span><br><span class="line">valid: &#123;&apos;loss&apos;: 0.2625027000904083, &apos;auc&apos;: 0.9689153432846069&#125;</span><br><span class="line">##########################################</span><br><span class="line">train: &#123;&apos;loss&apos;: 0.14858269691467285, &apos;auc&apos;: 1.0&#125;</span><br><span class="line">valid: &#123;&apos;loss&apos;: 0.26238003373146057, &apos;auc&apos;: 0.9689152836799622&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="自定义训练"><a href="#自定义训练" class="headerlink" title="自定义训练"></a>自定义训练</h2><p>此方法不需要使用<code>compile</code>编译模型, 直接根据损失函数反向传播梯度, 利用优化器进行迭代, 具有最高的灵活性, 可以胜任一些复杂的任务.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">train_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    ((train_x / np.linalg.norm(train_x, axis=<span class="number">0</span>)).astype(<span class="string">'float32'</span>),</span><br><span class="line">     train_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">         tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">test_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    ((test_x / np.linalg.norm(test_x, axis=<span class="number">0</span>)).astype(<span class="string">'float32'</span>),</span><br><span class="line">     test_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">         tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">64</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置损失函数/优化器/评估指标等</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_func = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集loss</span></span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line"><span class="comment"># 训练集评估指标</span></span><br><span class="line">train_metric = keras.metrics.AUC(name=<span class="string">'train_auc'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证集loss</span></span><br><span class="line">val_loss = keras.metrics.Mean(name=<span class="string">'val_loss'</span>)</span><br><span class="line"><span class="comment"># 验证集评估指标</span></span><br><span class="line">val_metric = keras.metrics.AUC(name=<span class="string">'val_auc'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义训练/评估步骤</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        preds = model(features)</span><br><span class="line">        loss = loss_func(labels, preds)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, preds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    preds = model(features)</span><br><span class="line">    loss = loss_func(labels, preds)</span><br><span class="line">    val_loss.update_state(loss)</span><br><span class="line">    val_metric.update_state(labels, preds)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> train_set:</span><br><span class="line">        train_step(model, features, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> test_set:</span><br><span class="line">        valid_step(model, features, labels)</span><br><span class="line"></span><br><span class="line">    logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,AUC:&#123;&#125;,Val Loss:&#123;&#125;,Val AUC:&#123;&#125;'</span></span><br><span class="line">    tf.print(</span><br><span class="line">        tf.strings.format(logs,</span><br><span class="line">                          (epoch, train_loss.result(), train_metric.result(),</span><br><span class="line">                           val_loss.result(), val_metric.result())))</span><br><span class="line">    tf.print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    val_loss.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line">    val_metric.reset_states()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch=0,Loss:0.676754653,AUC:0.698328137,Val Loss:0.633566141,Val AUC:0.978174567</span><br><span class="line">##########################################</span><br><span class="line">Epoch=1,Loss:0.634832323,AUC:0.944220841,Val Loss:0.55745548,Val AUC:0.976521134</span><br><span class="line">##########################################</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-构建模型</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>这里重点总结一下TensorFlow2.x的几种构建模型, 以及保存模型的方式.</p>
<a id="more"></a>
<h1 id="构建模型的三种方法"><a href="#构建模型的三种方法" class="headerlink" title="构建模型的三种方法"></a>构建模型的三种方法</h1><p>可以使用以下三种方法构建模型:</p>
<ul>
<li>使用<code>Sequential</code>按层顺序构建模型.</li>
<li>使用函数式API构建任意结构模型.</li>
<li>继承Model基类构建自定义模型.</li>
</ul>
<p>对于顺序结构的模型, 优先使用<code>Sequential</code>方法构建.</p>
<p>如果模型有多输入或者多输出, 或者模型需要共享权重, 或者模型具有残差连接等非顺序结构, 推荐使用函数式API进行创建.</p>
<p>如果无特定必要, 尽可能避免使用Model子类化的方式构建模型, 这种方式提供了极大的灵活性, 但也有更大的概率出错.</p>
<h2 id="Sequential按层顺序创建模型"><a href="#Sequential按层顺序创建模型" class="headerlink" title="Sequential按层顺序创建模型"></a>Sequential按层顺序创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 73us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 77us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 68us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h2 id="函数式API创建模型"><a href="#函数式API创建模型" class="headerlink" title="函数式API创建模型"></a>函数式API创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=(<span class="number">30</span>, ))</span><br><span class="line"></span><br><span class="line">x = layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))(inputs)</span><br><span class="line"></span><br><span class="line">x = layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))(x)</span><br><span class="line"></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs=inputs, outputs=outputs)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 69us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 84us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 71us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h2 id="Model子类创建模型"><a href="#Model子类创建模型" class="headerlink" title="Model子类创建模型"></a>Model子类创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.dense_0 = layers.Dense(<span class="number">100</span>,</span><br><span class="line">                                    activation=<span class="string">'relu'</span>,</span><br><span class="line">                                    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        self.dense_1 = layers.Dense(<span class="number">50</span>,</span><br><span class="line">                                    activation=<span class="string">'relu'</span>,</span><br><span class="line">                                    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        self.dense_2 = layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line">        super().build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.dense_0(x)</span><br><span class="line">        x = self.dense_1(x)</span><br><span class="line">        outputs = self.dense_2(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 69us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 87us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 63us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h1 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h1><p>TensorFlow2.x有两种方式保存模型, 可以使用Keras方式保存模型, 也可以使用TensorFlow原生方式保存.</p>
<p>前者仅仅适合使用Python环境恢复模型, 后者则可以跨平台进行模型部署, 推荐使用后一种方式进行保存.</p>
<h2 id="Keras方式保存"><a href="#Keras方式保存" class="headerlink" title="Keras方式保存"></a>Keras方式保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 原模型测试集评估</span></span><br><span class="line"></span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 81us/sample - loss: 0.4713 - AUC: 0.9845

[0.4712578717030977, 0.9844577]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./model/keras_model.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/keras_model.h5'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 1ms/sample - loss: 0.4713 - AUC: 0.9845

[0.4712578717030977, 0.9844577]
</code></pre><h2 id="TensorFlow原生方式保存"><a href="#TensorFlow原生方式保存" class="headerlink" title="TensorFlow原生方式保存"></a>TensorFlow原生方式保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./model/tf_model'</span>, save_format=<span class="string">'tf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/tf_model'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 2ms/sample - loss: 0.4713 - AUC: 0.9845

[0.4712575684513962, 0.9844577]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构</span></span><br><span class="line">model_json = model.to_json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型权重</span></span><br><span class="line">model.save_weights(<span class="string">'./model/tf_model_weight'</span>, save_format=<span class="string">'tf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model = models.model_from_json(model_json)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型权重</span></span><br><span class="line">model.load_weights(<span class="string">'./model/tf_model_weight'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/tf_model'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 1ms/sample - loss: 0.4713 - AUC: 0.9845
</code></pre>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-图像数据建模</title>
    <url>/2020/08/15/TensorFlow/TensorFlow-%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>AI的发展离不开对视觉的探索与研究, 包括图像与视频这样的数据. 在深度学习出现之前, 几乎难以有成熟的落地应用, 而当深度学习, 尤其是CNN的出现, 让计算机视觉这一领域发生了质的改变.<br>这里主要通过TensorFlow, 来做一个简单的图像数据建模的示例.</p>
<a id="more"></a>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>在这里用到的图像数据, 是经典的cifar10数据的子集, 仅仅包括两个类别airplane和automobile.<br>在训练集中airplane和automobile各5000张, 测试集中airplane和automobile各1000张.<br>我们的任务当然就是构建一个模型, 来对以上两类图像进行分类.<br>数据存放的格式如下:</p>
<ul>
<li>train<ul>
<li>0_airplane: fig_0, fig_1, …</li>
<li>1_automobile: fig_0, fig_1, …</li>
</ul>
</li>
<li>test<ul>
<li>0_airplane: fig_0, fig_1, …</li>
<li>1_automobile: fig_0, fig_1, …</li>
</ul>
</li>
</ul>
<p>在TensorFlow中准备图像数据的常用方案有两种:</p>
<ul>
<li>使用<code>tf.keras</code>中的<code>ImageDataGenerator</code>工具构建图像数据生成器.</li>
<li>使用<code>tf.data</code>搭配<code>tf.image</code>中的一些图像处理方法构建数据管道.</li>
</ul>
<p>第一种方法相对简单, 第二种方法为TensorFlow的原生方法, 更加灵活, 在一些时候可以获得更好的性能.<br>这里采用第二种方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = tf.constant(<span class="number">1</span>, tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(</span><br><span class="line">        img_path, <span class="string">".*automobile.*"</span>) <span class="keyword">else</span> tf.constant(<span class="number">0</span>, tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size) / tf.constant(<span class="number">255.</span>)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>) \</span><br><span class="line">    .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>).batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/test/*/*.jpg"</span>) \</span><br><span class="line">    .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看部分样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds_train.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_5_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape, y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(128, 32, 32, 3) (128,)
</code></pre><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models, regularizers, callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2D, MaxPool2D, Dropout, Flatten, Dense, Input</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), kernel_regularizer=regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(</span><br><span class="line">    Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), kernel_regularizer=regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(rate=<span class="number">0.1</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1601      
=================================================================
Total params: 53,761
Trainable params: 53,761
Non-trainable params: 0
_________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(<span class="string">'adam'</span>, <span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
79/79 [==============================] - 2s 30ms/step - loss: 0.5348 - accuracy: 0.7618 - val_loss: 0.3987 - val_accuracy: 0.8565
Epoch 2/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.4062 - accuracy: 0.8453 - val_loss: 0.4038 - val_accuracy: 0.8395
Epoch 3/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3869 - accuracy: 0.8524 - val_loss: 0.3396 - val_accuracy: 0.8785
Epoch 4/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3459 - accuracy: 0.8734 - val_loss: 0.3118 - val_accuracy: 0.8845
Epoch 5/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3215 - accuracy: 0.8848 - val_loss: 0.2912 - val_accuracy: 0.8980
Epoch 6/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3041 - accuracy: 0.8931 - val_loss: 0.2995 - val_accuracy: 0.8875
Epoch 7/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2984 - accuracy: 0.8950 - val_loss: 0.2724 - val_accuracy: 0.9015
Epoch 8/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2846 - accuracy: 0.9028 - val_loss: 0.2654 - val_accuracy: 0.9025
Epoch 9/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2714 - accuracy: 0.9061 - val_loss: 0.2534 - val_accuracy: 0.9125
Epoch 10/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2591 - accuracy: 0.9151 - val_loss: 0.2515 - val_accuracy: 0.9100
Epoch 11/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2522 - accuracy: 0.9159 - val_loss: 0.2497 - val_accuracy: 0.9085
Epoch 12/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2512 - accuracy: 0.9165 - val_loss: 0.2413 - val_accuracy: 0.9180
Epoch 13/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2403 - accuracy: 0.9193 - val_loss: 0.2409 - val_accuracy: 0.9145
Epoch 14/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2322 - accuracy: 0.9245 - val_loss: 0.2320 - val_accuracy: 0.9215
Epoch 15/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2326 - accuracy: 0.9256 - val_loss: 0.2292 - val_accuracy: 0.9270
Epoch 16/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2272 - accuracy: 0.9260 - val_loss: 0.2340 - val_accuracy: 0.9200
Epoch 17/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2269 - accuracy: 0.9260 - val_loss: 0.2283 - val_accuracy: 0.9260
Epoch 18/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2150 - accuracy: 0.9348 - val_loss: 0.2324 - val_accuracy: 0.9260
Epoch 19/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2145 - accuracy: 0.9339 - val_loss: 0.2311 - val_accuracy: 0.9255
Epoch 20/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2110 - accuracy: 0.9351 - val_loss: 0.2470 - val_accuracy: 0.9160
Epoch 21/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1990 - accuracy: 0.9415 - val_loss: 0.2238 - val_accuracy: 0.9290
Epoch 22/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1920 - accuracy: 0.9450 - val_loss: 0.2216 - val_accuracy: 0.9280
Epoch 23/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1876 - accuracy: 0.9474 - val_loss: 0.2264 - val_accuracy: 0.9280
Epoch 24/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1855 - accuracy: 0.9465 - val_loss: 0.2191 - val_accuracy: 0.9320
Epoch 25/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1860 - accuracy: 0.9475 - val_loss: 0.2285 - val_accuracy: 0.9250
Epoch 26/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1799 - accuracy: 0.9520 - val_loss: 0.2275 - val_accuracy: 0.9290
Epoch 27/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1808 - accuracy: 0.9484 - val_loss: 0.2213 - val_accuracy: 0.9280
Epoch 28/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1707 - accuracy: 0.9561 - val_loss: 0.2161 - val_accuracy: 0.9320
Epoch 29/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1689 - accuracy: 0.9573 - val_loss: 0.2140 - val_accuracy: 0.9350
Epoch 30/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1659 - accuracy: 0.9565 - val_loss: 0.2122 - val_accuracy: 0.9340
Epoch 31/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1667 - accuracy: 0.9576 - val_loss: 0.2154 - val_accuracy: 0.9365
Epoch 32/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1653 - accuracy: 0.9614 - val_loss: 0.2142 - val_accuracy: 0.9305
Epoch 33/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1653 - accuracy: 0.9582 - val_loss: 0.2153 - val_accuracy: 0.9345
Epoch 34/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1600 - accuracy: 0.9606 - val_loss: 0.2107 - val_accuracy: 0.9350
Epoch 35/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1603 - accuracy: 0.9618 - val_loss: 0.2163 - val_accuracy: 0.9335
Epoch 36/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1601 - accuracy: 0.9620 - val_loss: 0.2113 - val_accuracy: 0.9365
Epoch 37/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1584 - accuracy: 0.9604 - val_loss: 0.2105 - val_accuracy: 0.9355
Epoch 38/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1571 - accuracy: 0.9623 - val_loss: 0.2120 - val_accuracy: 0.9335
Epoch 39/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1570 - accuracy: 0.9635 - val_loss: 0.2109 - val_accuracy: 0.9375
Epoch 40/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1565 - accuracy: 0.9632 - val_loss: 0.2104 - val_accuracy: 0.9350
Epoch 41/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1543 - accuracy: 0.9638 - val_loss: 0.2102 - val_accuracy: 0.9360
Epoch 42/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1553 - accuracy: 0.9638 - val_loss: 0.2091 - val_accuracy: 0.9345
Epoch 43/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1546 - accuracy: 0.9632 - val_loss: 0.2089 - val_accuracy: 0.9355
Epoch 44/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1540 - accuracy: 0.9631 - val_loss: 0.2102 - val_accuracy: 0.9365
Epoch 45/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1526 - accuracy: 0.9643 - val_loss: 0.2101 - val_accuracy: 0.9370
Epoch 46/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1521 - accuracy: 0.9647 - val_loss: 0.2137 - val_accuracy: 0.9325
Epoch 47/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1501 - accuracy: 0.9655 - val_loss: 0.2115 - val_accuracy: 0.9335
Epoch 48/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1489 - accuracy: 0.9680 - val_loss: 0.2082 - val_accuracy: 0.9370
Epoch 49/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1483 - accuracy: 0.9675 - val_loss: 0.2090 - val_accuracy: 0.9370
Epoch 50/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1483 - accuracy: 0.9655 - val_loss: 0.2127 - val_accuracy: 0.9350
Epoch 51/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1502 - accuracy: 0.9646 - val_loss: 0.2081 - val_accuracy: 0.9360
Epoch 52/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1475 - accuracy: 0.9678 - val_loss: 0.2086 - val_accuracy: 0.9350
Epoch 53/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1475 - accuracy: 0.9671 - val_loss: 0.2088 - val_accuracy: 0.9370
Epoch 54/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1469 - accuracy: 0.9681 - val_loss: 0.2108 - val_accuracy: 0.9335
Epoch 55/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1459 - accuracy: 0.9681 - val_loss: 0.2078 - val_accuracy: 0.9380
Epoch 56/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1452 - accuracy: 0.9694 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 57/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1469 - accuracy: 0.9669 - val_loss: 0.2076 - val_accuracy: 0.9360
Epoch 58/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1454 - accuracy: 0.9692 - val_loss: 0.2072 - val_accuracy: 0.9375
Epoch 59/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1451 - accuracy: 0.9692 - val_loss: 0.2082 - val_accuracy: 0.9345
Epoch 60/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1447 - accuracy: 0.9685 - val_loss: 0.2075 - val_accuracy: 0.9380
Epoch 61/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1444 - accuracy: 0.9683 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 62/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1446 - accuracy: 0.9695 - val_loss: 0.2071 - val_accuracy: 0.9375
Epoch 63/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1450 - accuracy: 0.9683 - val_loss: 0.2070 - val_accuracy: 0.9380
Epoch 64/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1452 - accuracy: 0.9681 - val_loss: 0.2071 - val_accuracy: 0.9380
Epoch 65/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1442 - accuracy: 0.9692 - val_loss: 0.2071 - val_accuracy: 0.9375
Epoch 66/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1449 - accuracy: 0.9687 - val_loss: 0.2072 - val_accuracy: 0.9375
Epoch 67/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1443 - accuracy: 0.9687 - val_loss: 0.2072 - val_accuracy: 0.9370
Epoch 68/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1432 - accuracy: 0.9708 - val_loss: 0.2076 - val_accuracy: 0.9350
Epoch 69/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1429 - accuracy: 0.9688 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 70/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1426 - accuracy: 0.9700 - val_loss: 0.2073 - val_accuracy: 0.9360
Epoch 71/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1431 - accuracy: 0.9696 - val_loss: 0.2073 - val_accuracy: 0.9365
Epoch 72/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1443 - accuracy: 0.9696 - val_loss: 0.2073 - val_accuracy: 0.9370
Epoch 73/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1428 - accuracy: 0.9690 - val_loss: 0.2072 - val_accuracy: 0.9380
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_14_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-文本数据建模</title>
    <url>/2020/08/13/TensorFlow/TensorFlow-%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>在现实世界中, 文本这种原生的数据类型的数量是很多的, 因为语言是我们交换信息的渠道. 而相比传统的机器学习方法, 基于深度学习的方法在自然语言处理上, 取得了巨大的成功.<br>本篇的主要目的不是深入探讨文本处理的细节, 而是通过TensorFlow来展示基础的文本建模流程.</p>
<a id="more"></a>
<h1 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h1><p>这里使用经典的IMDB数据集, 来进行文本数据建模, 做情感分析. 首先, 对原始数据进行一些简单的分析.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">train_sent_list = []</span><br><span class="line">train_label_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/imdb/train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        label, sent = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        train_sent_list.append(sent)</span><br><span class="line">        train_label_list.append(int(label))</span><br><span class="line">    f.close()</span><br><span class="line">len(train_sent_list)</span><br></pre></td></tr></table></figure>
<pre><code>20000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(train_label_list)</span><br></pre></td></tr></table></figure>
<pre><code>10062
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sent_list[: <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&quot;It really boggles my mind when someone comes across a movie like this and claims it to be one of the worst slasher films out there. This is by far not one of the worst out there, still not a good movie, but not the worst nonetheless. Go see something like Death Nurse or Blood Lake and then come back to me and tell me if you think the Night Brings Charlie is the worst. The film has decent camera work and editing, which is way more than I can say for many more extremely obscure slasher films.&lt;br /&gt;&lt;br /&gt;The film doesn&#39;t deliver on the on-screen deaths, there&#39;s one death where you see his pruning saw rip into a neck, but all other deaths are hardly interesting. But the lack of on-screen graphic violence doesn&#39;t mean this isn&#39;t a slasher film, just a bad one.&lt;br /&gt;&lt;br /&gt;The film was obviously intended not to be taken too seriously. The film came in at the end of the second slasher cycle, so it certainly was a reflection on traditional slasher elements, done in a tongue in cheek way. For example, after a kill, Charlie goes to the town&#39;s &#39;welcome&#39; sign and marks the population down one less. This is something that can only get a laugh.&lt;br /&gt;&lt;br /&gt;If you&#39;re into slasher films, definitely give this film a watch. It is slightly different than your usual slasher film with possibility of two killers, but not by much. The comedy of the movie is pretty much telling the audience to relax and not take the movie so god darn serious. You may forget the movie, you may remember it. I&#39;ll remember it because I love the name.\n&quot;,
 &quot;Mary Pickford becomes the chieftain of a Scottish clan after the death of her father, and then has a romance. As fellow commenter Snow Leopard said, the film is rather episodic to begin. Some of it is amusing, such as Pickford whipping her clansmen to church, while some of it is just there. All in all, the story is weak, especially the recycled, contrived romance plot-line and its climax. The transfer is so dark it&#39;s difficult to appreciate the scenery, but even accounting for that, this doesn&#39;t appear to be director Maurice Tourneur&#39;s best work. Pickford and Tourneur collaborated once more in the somewhat more accessible &#39;The Poor Little Rich Girl,&#39; typecasting Pickford as a child character.\n&quot;]
</code></pre><p>可以看到, 训练集有20000条样本, 正负样本各占一半, 接下来再看测试集样本.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_sent_list = []</span><br><span class="line">test_label_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/imdb/test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        label, sent = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        test_sent_list.append(sent)</span><br><span class="line">        test_label_list.append(int(label))</span><br><span class="line">    f.close()</span><br><span class="line">len(test_sent_list)</span><br></pre></td></tr></table></figure>
<pre><code>5000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(test_label_list)</span><br></pre></td></tr></table></figure>
<pre><code>2438
</code></pre><p>测试集有5000条样本, 正负样本也是各占一半.<br>下面再进行一下简单的句子长度统计和词频统计.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计句子长度</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">all_sent_list = train_sent_list + test_sent_list</span><br><span class="line"></span><br><span class="line">sent_len_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> all_sent_list:</span><br><span class="line">    sent_len_list.append(len(sent.split(<span class="string">' '</span>)))</span><br><span class="line"></span><br><span class="line">counter = Counter(sent_len_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制句子长度直方图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.hist(sent_len_list, bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_11_1.png" alt="png"></p>
<p>可以看到, 大部分的句子长度小于500, 集中于200附近, 由此后续将文本截断长度设置为200.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计词频</span></span><br><span class="line">word_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> all_sent_list:</span><br><span class="line">    word_list += sent.split(<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line">counter = Counter(word_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 总的词汇(包括标点, 其它字符)有约29万个</span></span><br><span class="line">len(counter)</span><br></pre></td></tr></table></figure>
<pre><code>290691
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按词频排序, 查看出现次数最多的词汇, 发现个别词汇出现次数很多</span></span><br><span class="line">counter_sort = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">print(counter_sort[: <span class="number">30</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;the&#39;, 287012), (&#39;a&#39;, 155089), (&#39;and&#39;, 152645), (&#39;of&#39;, 142970), (&#39;to&#39;, 132566), (&#39;is&#39;, 103225), (&#39;in&#39;, 85576), (&#39;that&#39;, 64553), (&#39;I&#39;, 64024), (&#39;this&#39;, 57166), (&#39;it&#39;, 54404), (&#39;/&gt;&lt;br&#39;, 50935), (&#39;was&#39;, 46697), (&#39;as&#39;, 42507), (&#39;with&#39;, 41717), (&#39;for&#39;, 41065), (&#39;but&#39;, 33780), (&#39;The&#39;, 33095), (&#39;on&#39;, 30765), (&#39;movie&#39;, 30480), (&#39;are&#39;, 28497), (&#39;his&#39;, 27686), (&#39;film&#39;, 27389), (&#39;have&#39;, 27124), (&#39;not&#39;, 26258), (&#39;be&#39;, 25509), (&#39;you&#39;, 25108), (&#39;he&#39;, 21674), (&#39;by&#39;, 21422), (&#39;at&#39;, 21295)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不包括出现频次特别高的词汇, 绘制词频直方图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.hist(list(map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], counter_sort[<span class="number">5000</span>:])), bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(array([1.78939e+05, 3.55410e+04, 1.64970e+04, 9.91400e+03, 6.79800e+03,
        4.93300e+03, 0.00000e+00, 3.69100e+03, 3.04000e+03, 2.48100e+03,
        2.08100e+03, 1.82100e+03, 0.00000e+00, 1.53300e+03, 1.32100e+03,
        1.16800e+03, 1.06300e+03, 8.84000e+02, 0.00000e+00, 8.43000e+02,
        7.69000e+02, 6.78000e+02, 6.56000e+02, 6.00000e+02, 0.00000e+00,
        5.21000e+02, 5.12000e+02, 4.59000e+02, 4.19000e+02, 4.15000e+02,
        3.73000e+02, 0.00000e+00, 3.56000e+02, 3.35000e+02, 2.78000e+02,
        3.09000e+02, 3.08000e+02, 0.00000e+00, 2.83000e+02, 2.47000e+02,
        2.40000e+02, 2.47000e+02, 2.07000e+02, 0.00000e+00, 2.14000e+02,
        2.02000e+02, 1.94000e+02, 1.95000e+02, 1.64000e+02, 0.00000e+00,
        1.63000e+02, 1.72000e+02, 1.48000e+02, 1.61000e+02, 1.53000e+02,
        1.47000e+02, 0.00000e+00, 1.34000e+02, 1.42000e+02, 1.12000e+02,
        1.33000e+02, 1.15000e+02, 0.00000e+00, 1.14000e+02, 1.09000e+02,
        1.03000e+02, 1.04000e+02, 9.70000e+01, 0.00000e+00, 9.50000e+01,
        1.11000e+02, 9.70000e+01, 7.10000e+01, 9.20000e+01, 0.00000e+00,
        9.20000e+01, 7.50000e+01, 7.40000e+01, 8.70000e+01, 7.00000e+01,
        7.80000e+01, 0.00000e+00, 6.00000e+01, 5.80000e+01, 5.30000e+01,
        7.10000e+01, 7.00000e+01, 0.00000e+00, 7.10000e+01, 6.30000e+01,
        4.40000e+01, 7.00000e+01, 5.60000e+01, 0.00000e+00, 6.10000e+01,
        5.10000e+01, 5.70000e+01, 5.40000e+01, 5.30000e+01, 2.10000e+01]),
 array([ 1.  ,  1.84,  2.68,  3.52,  4.36,  5.2 ,  6.04,  6.88,  7.72,
         8.56,  9.4 , 10.24, 11.08, 11.92, 12.76, 13.6 , 14.44, 15.28,
        16.12, 16.96, 17.8 , 18.64, 19.48, 20.32, 21.16, 22.  , 22.84,
        23.68, 24.52, 25.36, 26.2 , 27.04, 27.88, 28.72, 29.56, 30.4 ,
        31.24, 32.08, 32.92, 33.76, 34.6 , 35.44, 36.28, 37.12, 37.96,
        38.8 , 39.64, 40.48, 41.32, 42.16, 43.  , 43.84, 44.68, 45.52,
        46.36, 47.2 , 48.04, 48.88, 49.72, 50.56, 51.4 , 52.24, 53.08,
        53.92, 54.76, 55.6 , 56.44, 57.28, 58.12, 58.96, 59.8 , 60.64,
        61.48, 62.32, 63.16, 64.  , 64.84, 65.68, 66.52, 67.36, 68.2 ,
        69.04, 69.88, 70.72, 71.56, 72.4 , 73.24, 74.08, 74.92, 75.76,
        76.6 , 77.44, 78.28, 79.12, 79.96, 80.8 , 81.64, 82.48, 83.32,
        84.16, 85.  ]),
 &lt;a list of 100 Patch objects&gt;)
</code></pre><p><img src="output_16_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> counter:</span><br><span class="line">    <span class="keyword">if</span> counter[k] == <span class="number">1</span>:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">num</span><br></pre></td></tr></table></figure>
<pre><code>178939
</code></pre><p>从上面的词频直方图可以看到, 有非常多词频低于20, 词频为1的词汇就有约180000. 在不使用其它预训练好的词向量时, 这样的词汇并不能给模型带来多少信息, 所以在进行编码时, 可以将其编码为”其它”, 即设置最大词汇量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> counter:</span><br><span class="line">        <span class="keyword">if</span> counter[k] == i:</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'词频小于等于%d, 数量%d'</span> % (i, num))</span><br></pre></td></tr></table></figure>
<pre><code>词频小于等于1, 数量178939
词频小于等于2, 数量214480
词频小于等于3, 数量230977
词频小于等于4, 数量240891
词频小于等于5, 数量247689
词频小于等于6, 数量252622
词频小于等于7, 数量256313
词频小于等于8, 数量259353
词频小于等于9, 数量261834
词频小于等于10, 数量263915
词频小于等于11, 数量265736
词频小于等于12, 数量267269
词频小于等于13, 数量268590
词频小于等于14, 数量269758
词频小于等于15, 数量270821
词频小于等于16, 数量271705
词频小于等于17, 数量272548
词频小于等于18, 数量273317
词频小于等于19, 数量273995
词频小于等于20, 数量274651
</code></pre><p>可以看到, 词频小于等于20有约27万, 联系到总词汇量约29万, 所以后续将最大词汇量设置为1万.</p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>利用TensorFlow完成文本数据预处理的常用方法一般可以有如下两种.</p>
<ul>
<li>使用<code>tf.keras.preprocessing</code>中的<code>Tokenizer</code>词典构建工具, 和<code>tf.keras.utils.Sequence</code>构建文本数据生成器管道.</li>
<li>使用<code>tf.data.Dataset</code>搭配<code>tf.keras.layers.experimental.preprocessing.TextVectorization</code>预处理层, 有点类似于结构化数据那里的<code>DenseFeatures</code>层.<br>第二种方式是TensorFlow的原生方式, 相对简单一些, 这里使用第二种方式.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_path = <span class="string">'./data/imdb/train.txt'</span></span><br><span class="line">test_path = <span class="string">'./data/imdb/test.txt'</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 指定最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个句子保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    res = tf.strings.split(line, <span class="string">'\t'</span>)</span><br><span class="line">    label, text = res[<span class="number">0</span>], res[<span class="number">1</span>]</span><br><span class="line">    label = tf.expand_dims(tf.cast(tf.strings.to_number(label), tf.int32),</span><br><span class="line">                           axis=<span class="number">0</span>)</span><br><span class="line">    text = tf.expand_dims(text, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.TextLineDataset(filenames=train_path) \</span><br><span class="line">    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.TextLineDataset(filenames=test_path) \</span><br><span class="line">    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建词向量化层, 包括预处理(小写, 去除特殊字符/标点, 按空格分词), 构建词典, 构建指定长度(截断/填充)的向量</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="keyword">import</span> TextVectorization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 小写转换</span></span><br><span class="line">    text = tf.strings.lower(text)</span><br><span class="line">    <span class="comment"># 去掉特殊符号</span></span><br><span class="line">    text = tf.strings.regex_replace(text, <span class="string">'&lt;br /&gt;'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 去掉标点符号</span></span><br><span class="line">    text = tf.strings.regex_replace(text,</span><br><span class="line">                                    <span class="string">'[%s]'</span> % re.escape(string.punctuation), <span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text_vec_layer = TextVectorization(standardize=clean_text,</span><br><span class="line">                                   split=<span class="string">'whitespace'</span>,</span><br><span class="line">                                   max_tokens=MAX_WORDS,</span><br><span class="line">                                   output_mode=<span class="string">'int'</span>,</span><br><span class="line">                                   output_sequence_length=MAX_LEN)</span><br><span class="line">text_vec_layer.adapt(ds_train.map(<span class="keyword">lambda</span> text, label: text))</span><br><span class="line"></span><br><span class="line">text_vec_layer.get_vocabulary()[<span class="number">0</span>: <span class="number">20</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;&#39;,
 &#39;[UNK]&#39;,
 &#39;the&#39;,
 &#39;and&#39;,
 &#39;a&#39;,
 &#39;of&#39;,
 &#39;to&#39;,
 &#39;is&#39;,
 &#39;in&#39;,
 &#39;it&#39;,
 &#39;i&#39;,
 &#39;this&#39;,
 &#39;that&#39;,
 &#39;was&#39;,
 &#39;as&#39;,
 &#39;for&#39;,
 &#39;with&#39;,
 &#39;movie&#39;,
 &#39;but&#39;,
 &#39;film&#39;]
</code></pre><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>针对文本分类, 在深度学习中有不少方法与模型, 这里使用比较简单的双向LSTM模型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models, regularizers, callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Embedding, LSTM, Bidirectional, Dropout, Dense</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(text_vec_layer)</span><br><span class="line">model.add(Embedding(MAX_WORDS, <span class="number">128</span>, input_length=MAX_LEN))</span><br><span class="line">model.add(Bidirectional(LSTM(<span class="number">64</span>, kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(<span class="string">'adam'</span>, <span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
157/157 [==============================] - 50s 320ms/step - loss: 1.3471 - accuracy: 0.7157 - val_loss: 0.4484 - val_accuracy: 0.8360
Epoch 2/1000
157/157 [==============================] - 47s 300ms/step - loss: 0.3424 - accuracy: 0.8825 - val_loss: 0.3506 - val_accuracy: 0.8664
Epoch 3/1000
157/157 [==============================] - 47s 299ms/step - loss: 0.2467 - accuracy: 0.9196 - val_loss: 0.3507 - val_accuracy: 0.8638
Epoch 4/1000
157/157 [==============================] - 46s 294ms/step - loss: 0.2021 - accuracy: 0.9379 - val_loss: 0.3884 - val_accuracy: 0.8478
Epoch 5/1000
157/157 [==============================] - 45s 288ms/step - loss: 0.1690 - accuracy: 0.9534 - val_loss: 0.4779 - val_accuracy: 0.8516
Epoch 6/1000
157/157 [==============================] - 45s 289ms/step - loss: 0.1319 - accuracy: 0.9667 - val_loss: 0.4618 - val_accuracy: 0.8546
Epoch 7/1000
157/157 [==============================] - 45s 288ms/step - loss: 0.1159 - accuracy: 0.9714 - val_loss: 0.5181 - val_accuracy: 0.8494
Epoch 8/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0965 - accuracy: 0.9798 - val_loss: 0.4833 - val_accuracy: 0.8556
Epoch 9/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0753 - accuracy: 0.9867 - val_loss: 0.5316 - val_accuracy: 0.8510
Epoch 10/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0680 - accuracy: 0.9883 - val_loss: 0.5539 - val_accuracy: 0.8504
Epoch 11/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0632 - accuracy: 0.9888 - val_loss: 0.5177 - val_accuracy: 0.8482
Epoch 12/1000
157/157 [==============================] - 45s 289ms/step - loss: 0.0572 - accuracy: 0.9912 - val_loss: 0.5414 - val_accuracy: 0.8508
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_33_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_34_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-结构化数据建模</title>
    <url>/2020/08/13/TensorFlow/TensorFlow-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>结构化数据是比较基础, 但也是很重要, 很常用的一类数据.<br>这里示范一下完整的从结构化数据处理, 到构建模型, 再到训练评估的过程.</p>
<a id="more"></a>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>使用经典的titanic数据集, 目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存.<br>结构化数据可以使用pandas中的DataFrame进行预处理, 包括缺失值填充等. 这里主要目的是展示数据处理和建模流程, 因此不做更多的特征工程.<br>字段说明:</p>
<ul>
<li>Survived: 0代表死亡, 1代表存活 (y标签)</li>
<li>Pclass: 乘客所持票类, 有三种值(1, 2, 3) (embedding)</li>
<li>Name: 乘客姓名 (舍去)</li>
<li>Sex: 乘客性别 (转换成bool特征)</li>
<li>Age: 乘客年龄(有缺失) (数值特征, 添加“年龄是否缺失”作为辅助特征)</li>
<li>SibSp: 乘客兄弟姐妹/配偶的个数(整数值) (数值特征)</li>
<li>Parch: 乘客父母/孩子的个数(整数值) (数值特征)</li>
<li>Ticket: 票号(字符串) (舍去)</li>
<li>Fare: 乘客所持票的价格(浮点数, 0-500不等) (数值特征)</li>
<li>Cabin: 乘客所在船舱(有缺失) (添加“所在船舱是否缺失”作为辅助特征)</li>
<li>Embarked: 乘客登船港口: S、C、Q(有缺失) (embedding)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'./data/titanic/test.csv'</span>)</span><br><span class="line">train.shape, test.shape</span><br></pre></td></tr></table></figure>
<pre><code>((891, 13), (418, 13))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 13 columns):
Age            714 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
Fare           891 non-null float64
Name           891 non-null object
Parch          891 non-null int64
PassengerId    891 non-null int64
Pclass         891 non-null int64
Sex            891 non-null object
SibSp          891 non-null int64
Survived       891 non-null float64
Ticket         891 non-null object
id             891 non-null object
dtypes: float64(3), int64(4), object(6)
memory usage: 90.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 13 columns):
Age            332 non-null float64
Cabin          91 non-null object
Embarked       418 non-null object
Fare           417 non-null float64
Name           418 non-null object
Parch          418 non-null int64
PassengerId    418 non-null int64
Pclass         418 non-null int64
Sex            418 non-null object
SibSp          418 non-null int64
Survived       418 non-null int64
Ticket         418 non-null object
id             418 non-null object
dtypes: float64(2), int64(5), object(6)
memory usage: 42.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_df = train.append(test)</span><br><span class="line">all_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 13 columns):
Age            1046 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
Fare           1308 non-null float64
Name           1309 non-null object
Parch          1309 non-null int64
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Sex            1309 non-null object
SibSp          1309 non-null int64
Survived       1309 non-null float64
Ticket         1309 non-null object
id             1309 non-null object
dtypes: float64(3), int64(4), object(6)
memory usage: 143.2+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ft_cols_list = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Pclass</span></span><br><span class="line">cat_list = all_df[<span class="string">'Pclass'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line">ft_cols_list.append(</span><br><span class="line">    feature_column.embedding_column(</span><br><span class="line">        feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">            <span class="string">'Pclass'</span>, vocabulary_list=cat_list), <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[3, 1, 2]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Sex</span></span><br><span class="line">cat_list = all_df[<span class="string">'Sex'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line">all_df[<span class="string">'Sex'</span>] = all_df[<span class="string">'Sex'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">'male'</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Sex'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;male&#39;, &#39;female&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Age</span></span><br><span class="line">mean_ = all_df[<span class="string">'Age'</span>].mean()</span><br><span class="line">print(mean_)</span><br><span class="line">all_df[<span class="string">'is_age_null'</span>] = all_df[<span class="string">'Age'</span>].apply(<span class="keyword">lambda</span> x: int(pd.isnull(x)))</span><br><span class="line">all_df[<span class="string">'Age'</span>] = all_df[<span class="string">'Age'</span>].fillna(mean_)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'is_age_null'</span>))</span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Age'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>29.881137667304014
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SibSp</span></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'SibSp'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parch</span></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Parch'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Fare</span></span><br><span class="line">mean_ = all_df[<span class="string">'Fare'</span>].mean()</span><br><span class="line">print(mean_)</span><br><span class="line">all_df[<span class="string">'Fare'</span>] = all_df[<span class="string">'Fare'</span>].fillna(mean_)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Fare'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>33.2954792813456
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cabin</span></span><br><span class="line"></span><br><span class="line">all_df[<span class="string">'is_cabin_null'</span>] = all_df[<span class="string">'Cabin'</span>].apply(<span class="keyword">lambda</span> x: int(pd.isnull(x)))</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'is_cabin_null'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Embarked</span></span><br><span class="line"></span><br><span class="line">print(all_df[<span class="string">'Embarked'</span>].value_counts())</span><br><span class="line">all_df[<span class="string">'Embarked'</span>] = all_df[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line"></span><br><span class="line">cat_list = all_df[<span class="string">'Embarked'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(</span><br><span class="line">    feature_column.embedding_column(</span><br><span class="line">        feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">            <span class="string">'Embarked'</span>, vocabulary_list=cat_list), <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<pre><code>S    914
C    270
Q    123
Name: Embarked, dtype: int64
[&#39;S&#39;, &#39;C&#39;, &#39;Q&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 15 columns):
Age              1309 non-null float64
Cabin            295 non-null object
Embarked         1309 non-null object
Fare             1309 non-null float64
Name             1309 non-null object
Parch            1309 non-null int64
PassengerId      1309 non-null int64
Pclass           1309 non-null int64
Sex              1309 non-null int64
SibSp            1309 non-null int64
Survived         1309 non-null float64
Ticket           1309 non-null object
id               1309 non-null object
is_age_null      1309 non-null int64
is_cabin_null    1309 non-null int64
dtypes: float64(3), int64(7), object(5)
memory usage: 163.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = all_df.iloc[: <span class="number">891</span>]</span><br><span class="line">test = all_df.iloc[<span class="number">891</span>:]</span><br><span class="line">train.to_csv(<span class="string">'./data/titanic/pre_train.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">test.to_csv(<span class="string">'./data/titanic/pre_test.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">ds_train = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=<span class="string">"./data/titanic/pre_train.csv"</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">''</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=<span class="string">"./data/titanic/pre_test.csv"</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">''</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.DenseFeatures(ft_cols_list))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
7/7 [==============================] - 2s 333ms/step - loss: 2.2648 - AUC: 0.5301 - val_loss: 2.1298 - val_AUC: 0.6524
Epoch 122/1000
7/7 [==============================] - 0s 14ms/step - loss: 0.6692 - AUC: 0.8731 - val_loss: 0.7278 - val_AUC: 0.7968
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_26_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_27_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-特征列</title>
    <url>/2020/08/12/TensorFlow/TensorFlow-%E7%89%B9%E5%BE%81%E5%88%97/</url>
    <content><![CDATA[<p>特征列<code>tf.feature_column</code>通常用于对结构化数据实施特征工程时候使用, 可以让你将各种不同的原始数据转化为模型可用的格式, 图像或者文本数据一般不会用到特征列.</p>
<a id="more"></a>
<h1 id="特征列介绍"><a href="#特征列介绍" class="headerlink" title="特征列介绍"></a>特征列介绍</h1><p><img src="fig_0.jpg" alt="fig"></p>
<p>对于经过一些简单清洗的结构化数据来说, 数据的类型有各种各样的, 不过如果从计算的角度来说的话, 可以大体分成两类, 即<strong>数值(numeric)特征</strong>, 与<strong>类别(categorical)特征</strong>.</p>
<p><img src="fig_1.jpg" alt="fig"></p>
<p>特征列的作用, 类似于连接原始数据与模型之间的一座桥梁, 原始的结构化数据在经过特征列以后, 可以以更好的形式方便地送入模型当做, 进行训练与预测.</p>
<p><img src="fig_2.jpg" alt="fig"></p>
<p>而具体说来, <code>tf.feature_column</code>的API如上, 有两个类分别是Categorical-Column与Dense-Column, 各自代表了类别与类别与数值特征的相关类, 以及各自的数据处理方法. 其中的<code>bucketized_column</code>方法作用于数值特征, 但转换得到的结果与类别特征一致, 所以图上同时连接到两个类.</p>
<p>下面逐一展示各个方法的主要用法.</p>
<h1 id="数值型方法"><a href="#数值型方法" class="headerlink" title="数值型方法"></a>数值型方法</h1><h2 id="numeric-column"><a href="#numeric-column" class="headerlink" title="numeric_column"></a>numeric_column</h2><p><code>numeric_column</code>是数值型方法中常用且简单的方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数值变量的名称(这里是x)输入, 得到特征列, 告诉模型这是一个数值特征</span></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_cols_list = [ft_col]  <span class="comment"># 需要加入列表, 列表可以包含多个特征列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过DenseFeatures进行转换表示, 且输入需要是字典(需要特征名称信息)</span></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=</span><br><span class="line">array([[1.],</span><br><span class="line">       [2.],</span><br><span class="line">       [3.],</span><br><span class="line">       [4.],</span><br><span class="line">       [5.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="bucketized-column"><a href="#bucketized-column" class="headerlink" title="bucketized_column"></a>bucketized_column</h2><p><img src="fig_3.jpg" alt="fig"></p>
<p>如上图, 在使用<code>bucketized_column</code>时, 需要设置分割区间, 假设输入区间按大小顺序包含3个数值, 则可得到4个个分箱.</p>
<p><img src="fig_4.jpg" alt="fig"></p>
<p>在得到分箱后, <code>bucketized_column</code>会进一步将其进行独热编码表示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要先转换为numeric型</span></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_col = feature_column.bucketized_column(ft_col, boundaries=[<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从结果来看, 分割区间是左闭右开的</span></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">5</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="embedding-column"><a href="#embedding-column" class="headerlink" title="embedding_column"></a>embedding_column</h2><p>一般来说, 类别特征或者离散化的数值特征会做独热编码, 这样就可以直接输入模型进行训练. 不过若是对深度学习有更多了解的同学, 知道可能先进行embedding, 再参与运算会好一些. 这里<code>tf.embedding_column</code>就是做的这件事情.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_col = feature_column.bucketized_column(ft_col, boundaries=[<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="comment"># 利用上面得到的独热编码, 转换为embedding, 并设置向量维度</span></span><br><span class="line">ft_col = feature_column.embedding_column(ft_col, <span class="number">3</span>)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=</span><br><span class="line">array([[ 0.76490587, -0.27499318,  0.26791143],</span><br><span class="line">       [ 0.0151019 , -0.84910244, -0.02452291],</span><br><span class="line">       [ 0.0151019 , -0.84910244, -0.02452291],</span><br><span class="line">       [-0.95111394,  0.39225465, -0.36392847],</span><br><span class="line">       [-0.95111394,  0.39225465, -0.36392847]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h1 id="类别型方法"><a href="#类别型方法" class="headerlink" title="类别型方法"></a>类别型方法</h1><h2 id="categorical-column-with-vocabulary-list"><a href="#categorical-column-with-vocabulary-list" class="headerlink" title="categorical_column_with_vocabulary_list"></a>categorical_column_with_vocabulary_list</h2><p><img src="fig_5.jpg" alt="fig"></p>
<p>如上图所示, 将类别特征转换为独热编码.</p>
<p><code>categorical_column_with_vocabulary_list</code>与<code>categorical_column_with_vocabulary_file</code>方法类似, 只是前者利用列表指定需要编码的属性, 后者利用文件(将属性写入文件)来指定编码.</p>
<p>同时, 可能细心的同学注意到了, 前面的数值型方法中没有<code>indicator_column</code>方法, 这是因为即便是类别型特征, 在入模前也需要转换成数值型, 而<code>indicator_column</code>的作用就是这个.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在vocabulary_list中指定需要编码的属性</span></span><br><span class="line">ft_col = feature_column.categorical_column_with_vocabulary_list(<span class="string">'x'</span>, vocabulary_list=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line"><span class="comment"># 利用indicator_column再进行转换</span></span><br><span class="line">ft_col = feature_column.indicator_column(ft_col)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=</span><br><span class="line">array([[1., 0., 0.],</span><br><span class="line">       [0., 1., 0.],</span><br><span class="line">       [0., 0., 1.],</span><br><span class="line">       [1., 0., 0.],</span><br><span class="line">       [0., 1., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="categorical-column-with-hash-bucket"><a href="#categorical-column-with-hash-bucket" class="headerlink" title="categorical_column_with_hash_bucket"></a>categorical_column_with_hash_bucket</h2><p><img src="fig_6.jpg" alt="fig"></p>
<p>在一些时候, 类别特征中包含的属性数量过多, 一来指定编码会比较麻烦, 而来可能造成过拟合. 此时一个解决方法是利用hash映射, 将较多的属性映射到不同的较少的分箱. 带来的问题是会存在hash冲突.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要指定分箱数量</span></span><br><span class="line">ft_col = feature_column.categorical_column_with_hash_bucket(<span class="string">'x'</span>, <span class="number">2</span>)</span><br><span class="line">ft_col = feature_column.indicator_column(ft_col)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 2), dtype=float32, numpy=</span><br><span class="line">array([[0., 1.],</span><br><span class="line">       [1., 0.],</span><br><span class="line">       [1., 0.],</span><br><span class="line">       [0., 1.],</span><br><span class="line">       [1., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="crossed-column"><a href="#crossed-column" class="headerlink" title="crossed_column"></a>crossed_column</h2><p>对于线性模型来说, 由于表达能力较差, 有时候需要手动将特征进行交叉组合, 来提高学习效果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dict = &#123;<span class="string">'x'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">'y'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]&#125;</span><br><span class="line"></span><br><span class="line">cross_col = feature_column.crossed_column([<span class="string">'x'</span>, <span class="string">'y'</span>], hash_bucket_size=<span class="number">5</span>)</span><br><span class="line">cross_col = feature_column.indicator_column(cross_col)</span><br><span class="line"></span><br><span class="line">ft_cols_list = [cross_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=</span><br><span class="line">array([[1., 0., 0., 0., 0.],</span><br><span class="line">       [0., 0., 0., 0., 1.],</span><br><span class="line">       [0., 0., 0., 1., 0.],</span><br><span class="line">       [1., 0., 0., 0., 0.],</span><br><span class="line">       [0., 1., 0., 0., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas读写大文件</title>
    <url>/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE/Pandas%E8%AF%BB%E5%86%99%E5%A4%A7%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>在日常处理数据时, 可能会遇到一份数据比较大, 不能整个读进内存. 同时又不方便用一些”正规军”(如spark)来进行处理, 那应该怎么办呢?</p>
<p>这时候仍然可以使用pandas进行处理, 注意一些细节就行. </p>
<p>让我们一起喊出来, pandas, 永远滴神╰(<em>°▽°</em>)╯</p>
<a id="more"></a>
<p>利用pandas处理大数据, 总体来说有以下三点需要注意:</p>
<ul>
<li>分批读入数据, 以保证内存够用.</li>
<li>多进程处理数据, 以保证数据处理速度.</li>
<li>分批保存数据, 可保存成多份文件, 也可以保存到一份文件.</li>
</ul>
<h1 id="分批读入数据"><a href="#分批读入数据" class="headerlink" title="分批读入数据"></a>分批读入数据</h1><h2 id="chunksize"><a href="#chunksize" class="headerlink" title="chunksize"></a>chunksize</h2><p>使用<code>chunksize</code>后<code>pd.read_csv</code>将会返回一个可以迭代的TextFileReader对象. <code>chunksize</code>的值代表了每次迭代对象的数量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, chunksize=<span class="number">5</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;pandas.io.parsers.TextFileReader at 0x7fb4c10bf790&gt;</span><br></pre></td></tr></table></figure>
<p>当我们需要从这个TextFileReader对象中取值时, 可以使用for循环. </p>
<p>值得注意的是, 当最后剩余数据条数小于<code>chunksize</code>时, 仅返回剩余数据, 在一些时候要防止由此出现的bug.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> df:</span><br><span class="line">    print(type(data))</span><br><span class="line">    print(data.iloc[:, : <span class="number">3</span>])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">   PassengerId  Survived  Pclass</span><br><span class="line">0            1         0       3</span><br><span class="line">1            2         1       1</span><br><span class="line">2            3         1       3</span><br><span class="line">3            4         1       1</span><br><span class="line">4            5         0       3</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">   PassengerId  Survived  Pclass</span><br><span class="line">5            6         0       3</span><br><span class="line">6            7         0       1</span><br><span class="line">7            8         0       3</span><br><span class="line">8            9         1       3</span><br><span class="line">9           10         1       2</span><br></pre></td></tr></table></figure>
<h2 id="iterator"><a href="#iterator" class="headerlink" title="iterator"></a>iterator</h2><p>与<code>chunksize</code>类似, 返回的也是TextFileReader 对象, 但是使用的时候和<code>chunksize</code>参数略有不同, 需要使用<code>get_chunk</code>方法获取数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, iterator=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;pandas.io.parsers.TextFileReader at 0x7fb4c2487050&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.get_chunk(<span class="number">2</span>).iloc[:, : <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.get_chunk(<span class="number">2</span>).iloc[:, : <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<h2 id="dtype"><a href="#dtype" class="headerlink" title="dtype"></a>dtype</h2><p>有时候会发现生成并保存了了一个csv文件, 最后读取的时候发现总是内存不足不能读取. 该现象的出现是由于pandas中的<code>read_csv</code>方法必须要读取所有行才能确定每一类的数据类型<code>dtype</code>, 据说在读取的时候默认以字符串格式读入, 当读取完最后一行后才会更改其类型.</p>
<p>以字符串格式读入时, 内存占用比较大, 在这时候可能会出现内存不足的问题. 所以我们只需要让pandas在读取的时候就按照对应的格式读取就不会出现该问题.</p>
<p>可以直接传入pandas支持的类型, 具体有哪些请见 <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes" target="_blank" rel="noopener">pandas官方文档</a>, 也可以传入numpy支持的类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">dtype_dict = &#123;</span><br><span class="line">    <span class="string">'PassengerId'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Survived'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Pclass'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Name'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Sex'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Age'</span>: <span class="string">'float64'</span>,</span><br><span class="line">    <span class="string">'SibSp'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Parch'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Ticket'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Fare'</span>: <span class="string">'float64'</span>,</span><br><span class="line">    <span class="string">'Cabin'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Embarked'</span>: <span class="string">'object'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, dtype=dtype_dict)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 12 columns):</span><br><span class="line">PassengerId    891 non-null int64</span><br><span class="line">Survived       891 non-null int64</span><br><span class="line">Pclass         891 non-null int64</span><br><span class="line">Name           891 non-null object</span><br><span class="line">Sex            891 non-null object</span><br><span class="line">Age            714 non-null float64</span><br><span class="line">SibSp          891 non-null int64</span><br><span class="line">Parch          891 non-null int64</span><br><span class="line">Ticket         891 non-null object</span><br><span class="line">Fare           891 non-null float64</span><br><span class="line">Cabin          204 non-null object</span><br><span class="line">Embarked       889 non-null object</span><br><span class="line">dtypes: float64(2), int64(5), object(5)</span><br><span class="line">memory usage: 83.7+ KB</span><br></pre></td></tr></table></figure>
<h2 id="usecols"><a href="#usecols" class="headerlink" title="usecols"></a>usecols</h2><p><code>usecols</code>可以指定读取的列, 其中的参数可以是列名或者列序号.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">ft_list = [<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>, <span class="string">'Pclass'</span>]</span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, usecols=ft_list)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
</tbody>
</table>
</div>
<h2 id="nrows"><a href="#nrows" class="headerlink" title="nrows"></a>nrows</h2><p><code>nrows</code>可以指定读取的行数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;./data/titanic/train.csv&apos;, nrows=5)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
<th style="text-align:right">Name</th>
<th style="text-align:right">Sex</th>
<th style="text-align:right">Age</th>
<th style="text-align:right">SibSp</th>
<th style="text-align:right">Parch</th>
<th style="text-align:right">Ticket</th>
<th style="text-align:right">Fare</th>
<th style="text-align:right">Cabin</th>
<th style="text-align:right">Embarked</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Braund, Mr. Owen Harris</td>
<td style="text-align:right">male</td>
<td style="text-align:right">22</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">A/5 21171</td>
<td style="text-align:right">7.2500</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">Cumings, Mrs. John Bradley (Florence Briggs Th…</td>
<td style="text-align:right">female</td>
<td style="text-align:right">38</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">PC 17599</td>
<td style="text-align:right">71.2833</td>
<td style="text-align:right">C85</td>
<td style="text-align:right">C</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Heikkinen, Miss. Laina</td>
<td style="text-align:right">female</td>
<td style="text-align:right">26</td>
<td style="text-align:right">0</td>
<td style="text-align:right">0</td>
<td style="text-align:right">STON/O2. 3101282</td>
<td style="text-align:right">7.9250</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td style="text-align:right">female</td>
<td style="text-align:right">35</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">113803</td>
<td style="text-align:right">53.1000</td>
<td style="text-align:right">C123</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Allen, Mr. William Henry</td>
<td style="text-align:right">male</td>
<td style="text-align:right">35</td>
<td style="text-align:right">0</td>
<td style="text-align:right">0</td>
<td style="text-align:right">373450</td>
<td style="text-align:right">8.0500</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
</tbody>
</table>
</div>
<h1 id="多进程处理数据"><a href="#多进程处理数据" class="headerlink" title="多进程处理数据"></a>多进程处理数据</h1><p>当数据量多的时候, 或者处理过程比较复杂的时候, 单进程的处理会显得非常吃力. 这时候除了要尽可能优化处理方法代码本身, 一个更通用的方法就是利用多进程. 而pandas本身不带多进程功能, 所以需要我们自己实现多进程.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    func: 任务函数, 必需的参数为数据(单个).</span></span><br><span class="line"><span class="string">    data: 数据, 类list.</span></span><br><span class="line"><span class="string">    max_process: 最大进程数.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">ordered_multiprocess_task(square, data, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1, 4, 9, 16, 25]</span><br></pre></td></tr></table></figure>
<p>以我的经验来说, 当样本量在100万以下时, 直接对pandas的列进行处理, 时间在可接受范围内, 而当样本量达到百万量级时, 优先考虑多进程加速.</p>
<h1 id="分批保存数据"><a href="#分批保存数据" class="headerlink" title="分批保存数据"></a>分批保存数据</h1><p>在分批读取, 分批处理完数据后, 需要进行分批保存.</p>
<p>可以将每次分批处理的数据以不同的文件名分别保存, 也可以仍然统一保存到一个文件里. 这里主要介绍第二种方法, 主要就是利用<code>to_csv</code>方法中的<code>mode</code>参数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!rm ./data/tmp.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'a'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'b'</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;)</span><br><span class="line">df.to_csv(<span class="string">'./data/tmp.csv'</span>, index=<span class="literal">False</span>, mode=<span class="string">'a'</span>)</span><br><span class="line">df.to_csv(<span class="string">'./data/tmp.csv'</span>, index=<span class="literal">False</span>, mode=<span class="string">'a'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">df_read = pd.read_csv(<span class="string">'./data/tmp.csv'</span>)</span><br><span class="line">df_read</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">a</th>
<th style="text-align:right">b</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
</tr>
</tbody>
</table>
</div>
<p>与Python的文件读写一样, <code>mode</code>默认参数为<code>w</code>, 表示重写, 而<code>a</code>表示接着写. 但要注意要正确地将<code>header</code>参数设置为<code>None</code>, 否则列名将会被当成普通数据重复写入.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-数据管道</title>
    <url>/2020/08/08/TensorFlow/TensorFlow-%E6%95%B0%E6%8D%AE%E7%AE%A1%E9%81%93/</url>
    <content><![CDATA[<p>如果需要训练的数据大小不大, 那么可以直接全部读入内存中进行训练, 这样一般效率最高.<br>但如果需要训练的数据很大, 无法一次载入内存, 那么通常需要在训练的过程中分批读入.<br>使用<code>tf.data</code>的API可以构建数据输入管道, 轻松处理大量的数据, 不同的数据格式, 以及不同的数据转换.</p>
<a id="more"></a>
<h1 id="构建数据管道"><a href="#构建数据管道" class="headerlink" title="构建数据管道"></a>构建数据管道</h1><h2 id="从numpy-array构建数据管道"><a href="#从numpy-array构建数据管道" class="headerlink" title="从numpy array构建数据管道"></a>从numpy array构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从numpy array构建数据管道</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((iris[<span class="string">"data"</span>], iris[<span class="string">"target"</span>]))</span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(features, label)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
</code></pre><h2 id="从pandas-DataFrame构建数据管道"><a href="#从pandas-DataFrame构建数据管道" class="headerlink" title="从pandas DataFrame构建数据管道"></a>从pandas DataFrame构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从 pandas DataFrame构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">df = pd.DataFrame(iris[<span class="string">"data"</span>], columns=iris[<span class="string">'feature_names'</span>])</span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((df.to_dict(<span class="string">"list"</span>), iris[<span class="string">"target"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> ds.take(<span class="number">3</span>):</span><br><span class="line">    print(features, label)</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.1&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.5&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.9&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.7&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.2&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.3&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
</code></pre><h2 id="从Python-gennerator构建数据管道"><a href="#从Python-gennerator构建数据管道" class="headerlink" title="从Python gennerator构建数据管道"></a>从Python gennerator构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Python generator构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个从文件中读取图片的generator</span></span><br><span class="line">image_generator = ImageDataGenerator(rescale=<span class="number">1.0</span> / <span class="number">255</span>).flow_from_directory(</span><br><span class="line">    <span class="string">"./data/cifar2/test/"</span>,</span><br><span class="line">    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">classdict = image_generator.class_indices</span><br><span class="line">print(classdict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features, label <span class="keyword">in</span> image_generator:</span><br><span class="line">        <span class="keyword">yield</span> features, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,</span><br><span class="line">                                    output_types=(tf.float32, tf.int32))</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
{&#39;0_airplane&#39;: 0, &#39;1_automobile&#39;: 1}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_8_0.png" alt="png"></p>
<h2 id="从csv文件构建数据管道"><a href="#从csv文件构建数据管道" class="headerlink" title="从csv文件构建数据管道"></a>从csv文件构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line">ds = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=[<span class="string">"./data/titanic/train.csv"</span>, <span class="string">"./data/titanic/test.csv"</span>],</span><br><span class="line">    batch_size=<span class="number">3</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">""</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> ds.take(<span class="number">2</span>):</span><br><span class="line">    print(data, label)</span><br></pre></td></tr></table></figure>
<pre><code>OrderedDict([(&#39;PassengerId&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([185, 948, 331], dtype=int32)&gt;), (&#39;Pclass&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 3], dtype=int32)&gt;), (&#39;Name&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=
array([b&#39;Kink-Heilmann, Miss. Luise Gretchen&#39;, b&#39;Cor, Mr. Bartol&#39;,
       b&#39;McCoy, Miss. Agnes&#39;], dtype=object)&gt;), (&#39;Sex&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;female&#39;, b&#39;male&#39;, b&#39;female&#39;], dtype=object)&gt;), (&#39;Age&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 4., 35.,  0.], dtype=float32)&gt;), (&#39;SibSp&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 2], dtype=int32)&gt;), (&#39;Parch&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 0], dtype=int32)&gt;), (&#39;Ticket&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;315153&#39;, b&#39;349230&#39;, b&#39;367226&#39;], dtype=object)&gt;), (&#39;Fare&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([22.025 ,  7.8958, 23.25  ], dtype=float32)&gt;), (&#39;Cabin&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;&#39;, b&#39;&#39;, b&#39;&#39;], dtype=object)&gt;), (&#39;Embarked&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;S&#39;, b&#39;S&#39;, b&#39;Q&#39;], dtype=object)&gt;)]) tf.Tensor([1. 0. 1.], shape=(3,), dtype=float32)
OrderedDict([(&#39;PassengerId&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1228,   57, 1014], dtype=int32)&gt;), (&#39;Pclass&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 1], dtype=int32)&gt;), (&#39;Name&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=
array([b&#39;de Brito, Mr. Jose Joaquim&#39;, b&#39;Rugg, Miss. Emily&#39;,
       b&#39;Schabert, Mrs. Paul (Emma Mock)&#39;], dtype=object)&gt;), (&#39;Sex&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;male&#39;, b&#39;female&#39;, b&#39;female&#39;], dtype=object)&gt;), (&#39;Age&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([32., 21., 35.], dtype=float32)&gt;), (&#39;SibSp&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)&gt;), (&#39;Parch&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)&gt;), (&#39;Ticket&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;244360&#39;, b&#39;C.A. 31026&#39;, b&#39;13236&#39;], dtype=object)&gt;), (&#39;Fare&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([13.  , 10.5 , 57.75], dtype=float32)&gt;), (&#39;Cabin&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;&#39;, b&#39;&#39;, b&#39;C28&#39;], dtype=object)&gt;), (&#39;Embarked&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;S&#39;, b&#39;S&#39;, b&#39;C&#39;], dtype=object)&gt;)]) tf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)
</code></pre><h2 id="从文本文件构建数据管道"><a href="#从文本文件构建数据管道" class="headerlink" title="从文本文件构建数据管道"></a>从文本文件构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从文本文件构建数据管道</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.TextLineDataset(</span><br><span class="line">    filenames=[<span class="string">"./data/titanic/train.csv"</span>, </span><br><span class="line">               <span class="string">"./data/titanic/test.csv"</span>]).skip(<span class="number">1</span>)  <span class="comment"># 略去第一行header</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;1,0,3,&quot;Braund, Mr. Owen Harris&quot;,male,22,1,0,A/5 21171,7.25,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;2,1,1,&quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;,female,38,1,0,PC 17599,71.2833,C85,C&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;3,1,3,&quot;Heikkinen, Miss. Laina&quot;,female,26,0,0,STON/O2. 3101282,7.925,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;4,1,1,&quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;,female,35,1,0,113803,53.1,C123,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;5,0,3,&quot;Allen, Mr. William Henry&quot;,male,35,0,0,373450,8.05,,S&#39;, shape=(), dtype=string)
</code></pre><h2 id="从文件路径构建数据管道"><a href="#从文件路径构建数据管道" class="headerlink" title="从文件路径构建数据管道"></a>从文件路径构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;./data/cifar2/train/0_airplane/1625.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/3325.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/3621.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/0_airplane/1615.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/4356.jpg&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,</span><br><span class="line">                                             <span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment"># 注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds.map(load_image).take(<span class="number">2</span>)):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    plt.imshow((img / <span class="number">255.0</span>).numpy())</span><br><span class="line">    plt.title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
<p><img src="output_15_1.png" alt="png"></p>
<h2 id="从TFRecord构建数据管道"><a href="#从TFRecord构建数据管道" class="headerlink" title="从TFRecord构建数据管道"></a>从TFRecord构建数据管道</h2><p>TFRecord是一种二进制格式, 可以高效地读取/传输数据, 当一些数据在预处理好以后, 可能会重复使用时, 可以考虑TFRecord格式.</p>
<p>TFRecord需要首先将原始数据序列化保存, 然后在读取的时候在进行相应的解析.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(value, type(tf.constant(<span class="number">0</span>))):</span><br><span class="line">        value = value.numpy()  <span class="comment"># BytesList won't unpack a string from an EagerTensor.</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_float_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a float_list from a float / double."""</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns an int64_list from a bool / enum / int / uint."""</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_list_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">    value = tf.io.serialize_tensor(value).numpy()</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 原始数据</span></span><br><span class="line"></span><br><span class="line">data_set = &#123;</span><br><span class="line">    <span class="string">'a'</span>: np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]),</span><br><span class="line">    <span class="string">'b'</span>: np.array([<span class="number">.1</span>, <span class="number">.2</span>, <span class="number">.3</span>, <span class="number">.4</span>]),</span><br><span class="line">    <span class="string">'c'</span>: np.array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]),</span><br><span class="line">    <span class="string">'d'</span>: np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_set = tf.data.Dataset.from_tensor_slices(data_set)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 序列化并保存为tfrecord</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize_example</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  Creates a tf.Example message ready to be written to a file.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">    <span class="comment"># Create a dictionary mapping the feature name to the tf.Example-compatible</span></span><br><span class="line">    <span class="comment"># data type.</span></span><br><span class="line">    feature = &#123;</span><br><span class="line">        <span class="string">'a'</span>: _int64_feature(x[<span class="string">'a'</span>]),</span><br><span class="line">        <span class="string">'b'</span>: _float_feature(x[<span class="string">'b'</span>]),</span><br><span class="line">        <span class="string">'c'</span>: _bytes_feature(x[<span class="string">'c'</span>]),</span><br><span class="line">        <span class="string">'d'</span>: _list_feature(x[<span class="string">'d'</span>]),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a Features message using tf.train.Example.</span></span><br><span class="line"></span><br><span class="line">    example_proto = tf.train.Example(features=tf.train.Features(</span><br><span class="line">        feature=feature))</span><br><span class="line">    <span class="keyword">return</span> example_proto.SerializeToString()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features <span class="keyword">in</span> data_set:</span><br><span class="line">        <span class="keyword">yield</span> serialize_example(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">serialized_features_dataset = tf.data.Dataset.from_generator(</span><br><span class="line">    generator, output_types=tf.string, output_shapes=())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入tfrecord文件</span></span><br><span class="line">filename = <span class="string">'test.tfrecord'</span></span><br><span class="line">writer = tf.data.experimental.TFRecordWriter(filename)</span><br><span class="line">writer.write(serialized_features_dataset)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取tfrecord文件</span></span><br><span class="line"></span><br><span class="line">filenames = [filename]</span><br><span class="line">raw_dataset = tf.data.TFRecordDataset(filenames)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析</span></span><br><span class="line">feature_description = &#123;</span><br><span class="line">    <span class="string">'a'</span>: tf.io.FixedLenFeature([], tf.int64, default_value=<span class="number">0</span>),</span><br><span class="line">    <span class="string">'b'</span>: tf.io.FixedLenFeature([], tf.float32, default_value=<span class="number">0.0</span>),</span><br><span class="line">    <span class="string">'c'</span>: tf.io.FixedLenFeature([], tf.string, default_value=<span class="string">''</span>),</span><br><span class="line">    <span class="string">'d'</span>: tf.io.FixedLenFeature([], tf.string, default_value=<span class="string">''</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="comment"># Parse the input `tf.Example` proto using the dictionary above.</span></span><br><span class="line">    feature = tf.io.parse_single_example(example_proto, feature_description)</span><br><span class="line">    feature[<span class="string">'d'</span>] = tf.io.parse_tensor(feature[<span class="string">'d'</span>], tf.int64)</span><br><span class="line">    <span class="keyword">return</span> feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parsed_dataset = raw_dataset.map(_parse_function)</span><br><span class="line"><span class="keyword">for</span> parsed_record <span class="keyword">in</span> parsed_dataset.take(<span class="number">10</span>):</span><br><span class="line">    tf.print(parsed_record)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;a&apos;: 1, &apos;b&apos;: 0.1, &apos;c&apos;: &quot;a&quot;, &apos;d&apos;: [1 2 3]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 2, &apos;b&apos;: 0.2, &apos;c&apos;: &quot;b&quot;, &apos;d&apos;: [2 3 4]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 3, &apos;b&apos;: 0.3, &apos;c&apos;: &quot;c&quot;, &apos;d&apos;: [3 4 5]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 4, &apos;b&apos;: 0.4, &apos;c&apos;: &quot;d&quot;, &apos;d&apos;: [4 5 6]&#125;</span><br></pre></td></tr></table></figure>
<h1 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h1><p>Dataset数据结构应用非常灵活, 因为它本质上是一个Sequece序列, 其每个元素可以是各种类型, 例如可以是张量, 列表, 字典, 也可以是Dataset.<br>Dataset包含了非常丰富的数据转换功能.</p>
<ul>
<li>map: 将转换函数映射到数据集每一个元素.</li>
<li>flat_map: 将转换函数映射到数据集的每一个元素, 并将嵌套的Dataset压平.</li>
<li>interleave: 效果类似flat_map, 但可以将不同来源的数据夹在一起.</li>
<li>filter: 过滤掉某些元素.</li>
<li>zip: 将两个长度相同的Dataset横向铰合.</li>
<li>concatenate: 将两个Dataset纵向连接. </li>
<li>reduce: 执行归并操作.</li>
<li>batch: 构建批次, 每次放一个批次. 比原始数据增加一个维度. 其逆操作为unbatch.</li>
<li>padded_batch: 构建批次, 类似batch, 但可以填充到相同的形状.</li>
<li>window: 构建滑动窗口, 返回Dataset of Dataset.</li>
<li>shuffle: 数据顺序洗牌.</li>
<li>repeat: 重复数据若干次, 不带参数时, 重复无数次.</li>
<li>shard: 采样, 从某个位置开始隔固定距离采样一个元素.</li>
<li>take: 采样, 从开始位置取前几个元素.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># map: 将转换函数映射到数据集每一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_map = ds.map(<span class="keyword">lambda</span> x: tf.strings.split(x, <span class="string">" "</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([b&#39;hello&#39; b&#39;world&#39;], shape=(2,), dtype=string)
tf.Tensor([b&#39;hello&#39; b&#39;China&#39;], shape=(2,), dtype=string)
tf.Tensor([b&#39;hello&#39; b&#39;Beijing&#39;], shape=(2,), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flat_map: 将转换函数映射到数据集的每一个元素, 并将嵌套的Dataset压平</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_flatmap = ds.flat_map(</span><br><span class="line">    <span class="keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x, <span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_flatmap:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;world&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interleave: 效果类似flat_map, 但可以将不同来源的数据夹在一起</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_interleave = ds.interleave(</span><br><span class="line">    <span class="keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x, <span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_interleave:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;world&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># filter: 过滤掉某些元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line"><span class="comment"># 找出含有字母a或B的元素</span></span><br><span class="line">ds_filter = ds.filter(<span class="keyword">lambda</span> x: tf.strings.regex_full_match(x, <span class="string">".*[a|B].*"</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_filter:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># zip: 将两个长度相同的Dataset横向铰合。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">ds3 = tf.data.Dataset.range(<span class="number">6</span>, <span class="number">9</span>)</span><br><span class="line">ds_zip = tf.data.Dataset.zip((ds1, ds2, ds3))</span><br><span class="line"><span class="keyword">for</span> x, y, z <span class="keyword">in</span> ds_zip:</span><br><span class="line">    print(x.numpy(), y.numpy(), z.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>0 3 6
1 4 7
2 5 8
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># condatenate: 将两个Dataset纵向连接。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">ds_concat = tf.data.Dataset.concatenate(ds1, ds2)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_concat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reduce: 执行归并操作。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5.0</span>])</span><br><span class="line">result = ds.reduce(<span class="number">0.0</span>, <span class="keyword">lambda</span> x, y: tf.add(x, y))</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=15.0&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># batch: 构建批次, 每次放一个批次. 比原始数据增加一个维度. 其逆操作为unbatch</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">11</span>)</span><br><span class="line">ds_batch = ds.batch(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)
tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)
tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># padded_batch: 构建批次, 类似batch, 但可以填充到相同的形状</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>]]</span><br><span class="line">ds = tf.data.Dataset.from_generator(<span class="keyword">lambda</span>: iter(elements), tf.int32)</span><br><span class="line"></span><br><span class="line">ds_padded_batch = ds.padded_batch(<span class="number">2</span>, padded_shapes=[<span class="number">4</span>,])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_padded_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(
[[1 2 0 0]
 [3 4 5 0]], shape=(2, 4), dtype=int32)
tf.Tensor(
[[6 7 0 0]
 [8 0 0 0]], shape=(2, 4), dtype=int32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># window: 构建滑动窗口, 返回Dataset of Dataset</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># window返回的是Dataset of Dataset, 可以用flat_map压平</span></span><br><span class="line">ds_window = ds.window(</span><br><span class="line">    <span class="number">3</span>, shift=<span class="number">1</span>).flat_map(<span class="keyword">lambda</span> x: x.batch(<span class="number">3</span>, drop_remainder=<span class="literal">True</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_window:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7fcb59acb9e0&gt; and will run it as-is.
Cause: could not parse the source code:

    3, shift=1).flat_map(lambda x: x.batch(3, drop_remainder=True))

This error may be avoided by creating the lambda in a standalone statement.

WARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7fcb59acb9e0&gt; and will run it as-is.
Cause: could not parse the source code:

    3, shift=1).flat_map(lambda x: x.batch(3, drop_remainder=True))

This error may be avoided by creating the lambda in a standalone statement.

tf.Tensor([0 1 2], shape=(3,), dtype=int64)
tf.Tensor([1 2 3], shape=(3,), dtype=int64)
tf.Tensor([2 3 4], shape=(3,), dtype=int64)
tf.Tensor([3 4 5], shape=(3,), dtype=int64)
tf.Tensor([4 5 6], shape=(3,), dtype=int64)
tf.Tensor([5 6 7], shape=(3,), dtype=int64)
tf.Tensor([6 7 8], shape=(3,), dtype=int64)
tf.Tensor([7 8 9], shape=(3,), dtype=int64)
tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)
tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shuffle: 数据顺序洗牌.</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shuffle = ds.shuffle(buffer_size=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shuffle:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(6, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(8, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
tf.Tensor(9, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
tf.Tensor(11, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># repeat: 重复数据若干次, 不带参数时, 重复无数次</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">3</span>)</span><br><span class="line">ds_repeat = ds.repeat(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_repeat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shard: 采样, 从某个位置开始隔固定距离采样一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shard = ds.shard(<span class="number">3</span>, index=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shard:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># take: 采样, 从开始位置取前几个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_take = ds.take(<span class="number">3</span>)</span><br><span class="line">abs</span><br><span class="line">list(ds_take.as_numpy_iterator())</span><br></pre></td></tr></table></figure>
<pre><code>[0, 1, 2]
</code></pre><h1 id="提升管道性能"><a href="#提升管道性能" class="headerlink" title="提升管道性能"></a>提升管道性能</h1><p>训练深度学习模型常常会非常耗时.<br>模型训练的耗时主要来自于两个部分, 一部分来自数据准备, 另一部分来自参数迭代.<br>参数迭代过程的耗时通常依赖于GPU来提升.<br>而数据准备过程的耗时则可以通过构建高效的数据管道进行提升.<br>以下是一些构建高效数据管道的建议.</p>
<ul>
<li>使用<code>prefetch</code>方法让数据准备和参数迭代两个过程相互并行.</li>
<li>使用<code>interleave</code>方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起.</li>
<li>使用<code>map</code>时设置<code>num_parallel_calls</code>让数据转换过程多进行执行.</li>
<li>使用<code>cache</code>方法让数据在第一个<code>epoch</code>后缓存到内存中, 仅限于数据集不大情形.</li>
<li>使用<code>map</code>转换时, 先<code>batch</code>, 然后采用向量化的转换方法对每个<code>batch</code>进行转换</li>
</ul>
<h2 id="prefetch-方法"><a href="#prefetch-方法" class="headerlink" title="prefetch 方法"></a>prefetch 方法</h2><p>使用<code>prefetch</code>方法让数据准备和参数迭代两个过程相互并行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts % (<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts // <span class="number">3600</span> + <span class="number">8</span>, tf.int32) % tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts % <span class="number">3600</span>) // <span class="number">60</span>, tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts % <span class="number">60</span>), tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>, m)) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"0&#123;&#125;"</span>, m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"&#123;&#125;"</span>, m))</span><br><span class="line"></span><br><span class="line">    timestring = tf.strings.join(</span><br><span class="line">        [timeformat(hour),</span><br><span class="line">         timeformat(minite),</span><br><span class="line">         timeformat(second)],</span><br><span class="line">        separator=<span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span> * <span class="number">3</span>, end=<span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备和参数迭代两个过程默认情况下是串行的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要1s</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练过程预计耗时 10 * 2 + 10 * 1 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">    train_step()</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================00:31:27
start training...
==============================00:31:57
end training...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 max(10 * 2, 10 * 1) = 20s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training with prefetch..."</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE):</span><br><span class="line">    train_step()</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================00:33:49
start training with prefetch...
==============================00:34:10
end training...
</code></pre><h2 id="interleave"><a href="#interleave" class="headerlink" title="interleave"></a>interleave</h2><p>使用<code>interleave</code>方法可以让数据读取过程多进程执行, 并将不同来源数据夹在一起.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.flat_map(<span class="keyword">lambda</span> x: tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">4</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;892,0.0,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;893,1.0,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47.0,1,0,363272,7.0,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;894,0.0,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62.0,0,0,240276,9.6875,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;895,0.0,3,&quot;Wirz, Mr. Albert&quot;,male,27.0,0,0,315154,8.6625,,S&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.interleave(<span class="keyword">lambda</span> x: tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">8</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;1,0,3,&quot;Braund, Mr. Owen Harris&quot;,male,22,1,0,A/5 21171,7.25,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;892,0.0,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;2,1,1,&quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;,female,38,1,0,PC 17599,71.2833,C85,C&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;893,1.0,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47.0,1,0,363272,7.0,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;3,1,3,&quot;Heikkinen, Miss. Laina&quot;,female,26,0,0,STON/O2. 3101282,7.925,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;894,0.0,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62.0,0,0,240276,9.6875,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;4,1,1,&quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;,female,35,1,0,113803,53.1,C123,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;895,0.0,3,&quot;Wirz, Mr. Albert&quot;,male,27.0,0,0,315154,8.6625,,S&#39;, shape=(), dtype=string)
</code></pre><h2 id="num-parallel-calls"><a href="#num-parallel-calls" class="headerlink" title="num_parallel_calls"></a>num_parallel_calls</h2><p>使用<code>map</code>时设置<code>num_parallel_calls</code>让数据转换过程多进行执行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,</span><br><span class="line">                                             <span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment"># 注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 单进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map = ds.map(load_image)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================11:02:15
start transformation...
==============================11:02:19
end transformation...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start parallel transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map_parallel = ds.map(load_image,</span><br><span class="line">                         num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map_parallel:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end parallel transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================11:02:52
start parallel transformation...
==============================11:02:53
end parallel transformation...
</code></pre><h2 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h2><p>使用<code>cache</code>方法让数据在第一个<code>epoch</code>后缓存到内存中, 仅限于数据集不大情形.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 假设每一步训练需要0s</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5 * 2 + 5 * 0) * 3 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()</span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>, epoch, <span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:17:35
start training...
==============================18:17:45
epoch = 0  ended
==============================18:17:56
epoch = 1  ended
==============================18:18:06
epoch = 2  ended
==============================18:18:06
end training...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cache方法让数据在第一个epoch后缓存到内存中, 仅限于数据集不大情形</span></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32)).cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 假设每一步训练需要0s</span></span><br><span class="line">    time.sleep(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5 * 2 + 5 * 0) + (5 * 0 + 5 * 0) * 2 = 10s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()</span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>, epoch, <span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:19:13
start training...
==============================18:19:23
epoch = 0  ended
==============================18:19:23
epoch = 1  ended
==============================18:19:23
epoch = 2  ended
==============================18:19:23
end training...
</code></pre><h2 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h2><p>使用<code>map</code>转换时, 先<code>batch</code>, 然后采用向量化的转换方法对每个<code>batch</code>进行转换.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先map后batch</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_map_batch = ds.map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>).batch(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start scalar transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map_batch:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end scalar transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:20:50
start scalar transformation...
==============================18:20:53
end scalar transformation...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先batch后map</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_batch_map = ds.batch(<span class="number">20</span>).map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start vector transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end vector transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:21:20
start vector transformation...
==============================18:21:20
end vector transformation...
</code></pre>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-计算图</title>
    <url>/2020/08/07/TensorFlow/TensorFlow-%E8%AE%A1%E7%AE%97%E5%9B%BE/</url>
    <content><![CDATA[<p>在TensorFlow1.x的时代, 采用的是<strong>静态计算图</strong>, 需要首先使用TensorFlow的各种算子创建计算图, 然后再开启一个会话Session, 显式执行计算图. 在静态图构建完成之后, 几乎全在TensorFlow内核上使用C++代码执行, 效率较高. 此外静态图还会对计算步骤进行一定的优化, 略去和结果无关的步骤.</p>
<a id="more"></a>
<p>而在TensorFlow2.x时代, 使用的是<strong>动态计算图</strong>, 即每使用一个算子后, 该算子都会被动态地加入到隐含的默认计算图中, 立即执行得到结果, 而无需开启Session. 使用动态计算图即Eager Excution的好处是方便调试程序, 它会让TensorFlow代码的表现和Python原生代码的表现一样, 写起来就像写numpy一样, 各种日志打印, 控制流全部都是可以使用. 不过动态图的缺点是运行效率会相对低一些, 因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信.</p>
<p>如果需要在TensorFlow2.x中使用静态图, 可以使用<code>@tf.function</code>装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码. 运行该函数就相当于在TensorFlow1.x中用Session执行代码. 使用tf.function构建静态图的方式叫做 Autograph.</p>
<p>在TensorFlow2.x中, 有三种计算图的构建方式: 静态计算图, 动态计算图, 以及Autograph.</p>
<h1 id="计算图简介"><a href="#计算图简介" class="headerlink" title="计算图简介"></a>计算图简介</h1><p>计算图由节点(nodes)和边(edges)组成. 节点表示操作符Operator, 或者称之为算子, 边表示算子之间的依赖关系.</p>
<p>实线表示有数据传递, 即张量. 虚线表示控制依赖, 即先后顺序.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<h1 id="静态计算图"><a href="#静态计算图" class="headerlink" title="静态计算图"></a>静态计算图</h1><p>TensorFlow2.x为了确保对TensorFlow1.x项目的兼容性, 在tf.compat.v1子模块中保留了对TensorFlow1.x那种静态计算图构建风格的支持. 可称之为怀旧版静态计算图, 不推荐使用.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">"join"</span>,separator = <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&apos;hello world&apos;</span><br></pre></td></tr></table></figure>
<h1 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h1><p>在TensorFlow2.x中, 使用的是动态计算图和Autograph. 动态计算图已经不区分计算图的定义和执行了, 而是定义后立即执行. 因此称之为 Eager Excution, Eager这个英文单词的原意是”迫不及待的”, 也就是立即执行的意思.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建, 构建后立即执行</span></span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="string">'hello'</span>)</span><br><span class="line">y = tf.constant(<span class="string">'world'</span>)</span><br><span class="line">z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出封装成函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">res = hello_world(tf.constant(<span class="string">'hello'</span>), tf.constant(<span class="string">'world'</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.Tensor(b&apos;hello world&apos;, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h1 id="Autograph"><a href="#Autograph" class="headerlink" title="Autograph"></a>Autograph</h1><p>动态计算图运行效率相对较低, 可以用<code>@tf.function</code>装饰器将普通Python函数转换成和TensorFlow1.x对应的静态计算图构建代码.</p>
<p>在TensorFlow1.x中, 使用计算图分两步, 第一步<strong>定义计算图</strong>, 第二步在会话中<strong>执行计算图</strong>.</p>
<p>在TensorFlow2.x中, 如果采用Autograph的方式使用计算图, 第一步定义计算图变成了<strong>定义函数</strong>, 第二步执行计算图变成了<strong>调用函数</strong>. 不需要使用会话了, 一切都像原始的Python语法一样自然.</p>
<p>实践中, 一般会先用动态计算图调试代码, 然后在需要提高性能的的地方利用<code>@tf.function</code>切换成Autograph获得更高的效率.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用autograph创建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">res = hello_world(tf.constant(<span class="string">'hello'</span>), tf.constant(<span class="string">'world'</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.Tensor(b&apos;hello world&apos;, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">logdir = <span class="string">'./data/autograph/test'</span></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行autograph</span></span><br><span class="line">res = hello_world(<span class="string">'hello'</span>, <span class="string">'world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(name=<span class="string">'autograph'</span>, </span><br><span class="line">                            step=<span class="number">0</span>, </span><br><span class="line">                            profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在jupyter中用魔法命令启动tensorboard</span><br><span class="line">%reload_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动tensorboard</span><br><span class="line">%tensorboard --logdir &apos;./data/autograph/&apos;</span><br></pre></td></tr></table></figure>
<h2 id="AutoGraph的使用规范"><a href="#AutoGraph的使用规范" class="headerlink" title="AutoGraph的使用规范"></a>AutoGraph的使用规范</h2><p>Autograph机制能够转换的代码并不是没有任何约束的, 有一些编码规范需要遵循, 否则可能会转换失败或者不符合预期.</p>
<p>总体来说, 使用AutoGraph需要遵循如下规则:</p>
<ul>
<li>被<code>@tf.function</code>修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数. 例如使用<code>tf.print</code>而不是<code>print</code>, 使用<code>tf.range</code>而不是<code>range</code>, 使用<code>tf.constant(True)</code>而不是<code>True</code>.</li>
<li>避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>.</li>
<li>被<code>@tf.function</code>修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = np.random.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    tf.print(a)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.random.normal((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#np_random每次执行都是一样的结果。</span></span><br><span class="line">np_random()</span><br><span class="line">np_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 0.61978529,  0.48959305],</span><br><span class="line">       [-0.1440161 ,  1.72774268]])</span><br><span class="line">array([[ 0.61978529,  0.48959305],</span><br><span class="line">       [-0.1440161 ,  1.72774268]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tf_random每次执行都会有重新生成随机数。</span></span><br><span class="line">tf_random()</span><br><span class="line">tf_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0.37242806 -1.14954114]</span><br><span class="line"> [-0.806226909 1.40349495]]</span><br><span class="line">[[0.226775587 1.14588106]</span><br><span class="line"> [0.647506893 1.2036798]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 避免在@tf.function修饰的函数内部定义tf.Variable.</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outer_var</span><span class="params">()</span>:</span></span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">outer_var()</span><br><span class="line">outer_var()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_var</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> (x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#执行将报错</span></span><br><span class="line"><span class="comment"># inner_var()</span></span><br><span class="line"><span class="comment"># inner_var()</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量</span></span><br><span class="line"></span><br><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor &apos;x:0&apos; shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="AutoGraph的机制原理"><a href="#AutoGraph的机制原理" class="headerlink" title="AutoGraph的机制原理"></a>AutoGraph的机制原理</h2><p><strong>当我们使用<code>@tf.function</code>装饰一个函数的时候, 后面到底发生了什么呢?</strong></p>
<p>例如我们写下如下代码.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(autograph=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myadd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">        tf.print(i)</span><br><span class="line">    c = a + b</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>后面什么都没有发生. 仅仅是在Python堆栈中记录了这样一个函数的签名.</p>
<p><strong>当我们第一次调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"hello"</span>), tf.constant(<span class="string">"world"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>发生了两件事情.</p>
<p><strong>第一件事情是创建计算图.</strong> 即创建一个静态计算图, 跟踪执行一遍函数体中的Python代码, 确定各个变量的Tensor类型, 并根据执行顺序将算子添加到计算图中. 在这个过程中, 如果开启了<code>autograph=True</code>(默认开启), 会将Python控制流转换成TensorFlow图内控制流. 主要是将<code>if</code>语句转换成 <code>tf.cond</code>算子表达, 将<code>while</code>和<code>for</code>循环语句转换成<code>tf.while_loop</code>算子表达, 并在必要的时候添加 <code>tf.control_dependencies</code>指定执行顺序依赖关系.</p>
<p><strong>第二件事情是执行计算图.</strong></p>
<p>因此我们先看到的是第一个步骤的结果: 即Python调用标准输出流打印”tracing”语句. 然后看到第二个步骤的结果: TensorFlow调用标准输出流打印0, 1, 2.</p>
<p><strong>当我们再次用相同的输入参数类型调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"good"</span>), tf.constant(<span class="string">"morning"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>只会发生一件事情, 那就是上面步骤的第二步, 执行计算图. 所以这一次我们没有看到打印”tracing”的结果.</p>
<p><strong>当我们再次用不同的的输入参数类型调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>由于输入参数的类型已经发生变化, 已经创建的计算图不能够再次使用.</p>
<p>需要重新做两件事情: 创建新的计算图, 执行计算图, </p>
<p>所以我们又会先看到的是第一个步骤的结果: 即Python调用标准输出流打印”tracing”语句. 然后再看到第二个步骤的结果: TensorFlow调用标准输出流打印0, 1, 2.</p>
<p><strong>需要注意的是, 如果调用被<code>@tf.function</code>装饰的函数时输入的参数不是Tensor类型, 则每次都会重新创建计算图.</strong></p>
<p>例如我们写下如下代码, 两次都会重新创建计算图. 因此, 一般建议调用<code>@tf.function</code>时应传入Tensor类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(<span class="string">"hello"</span>, <span class="string">"world"</span>)</span><br><span class="line">myadd(<span class="string">"good"</span>, <span class="string">"morning"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>了解了以上Autograph的机制原理, 我们也就能够理解Autograph编码规范的三条建议了.</p>
<ul>
<li><p>被<code>@tf.function</code>修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数. 例如使用<code>tf.print</code>而不是<code>print</code>, 使用<code>tf.range</code>而不是<code>range</code>, 使用<code>tf.constant(True)</code>而不是<code>True</code>.</p>
<p>解释: Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用, 普通Python函数是无法嵌入到静态计算图中的. 所以在计算图构建好之后再次调用的时候, 这些Python函数并没有被计算. 而TensorFlow中的函数则可以嵌入到计算图中. </p>
</li>
<li><p>避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>.</p>
<p>解释: 我们创建静态计算图的目的, 是反复使用其进行正向传播, 然后计算梯度后反向传播. 而如果在静态计算图中, 有创建<code>tf.Variable</code>的操作存在, 那将与上面的目的相违背, 因为每次正向传播时, 都会再初始化一个变量张量.</p>
</li>
<li><p>被<code>@tf.function</code>修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量.</p>
<p>静态计算图是被编译成C++代码在TensorFlow内核中执行的. Python中的列表和字典等数据结构变量是无法嵌入到计算图中, 它们仅仅能够在创建计算图时被读取, 在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的. </p>
</li>
</ul>
<h2 id="AutoGraph和tf-Module"><a href="#AutoGraph和tf-Module" class="headerlink" title="AutoGraph和tf.Module"></a>AutoGraph和tf.Module</h2><p>前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>. 但是如果在函数外部定义<code>tf.Variable</code>的话, 又会显得这个函数有外部变量依赖, 封装不够完美.</p>
<p>一种简单的思路是定义一个类, 并将相关的<code>tf.Variable</code>创建放在类的初始化方法中. 而将函数的逻辑放在其他方法中. TensorFlow提供了一个基类<code>tf.Module</code>, 通过继承它构建子类, 可以非常方便地管理变量, 还可以非常方便地管理它引用的其它Module. 最重要的是, 我们能够利用<code>tf.saved_model</code>保存模型并实现跨平台部署使用.</p>
<p>实际上, <code>tf.keras.models.Model</code>, <code>tf.keras.layers.Layer</code>都是继承自<code>tf.Module</code>的, 提供了方便的变量管理和所引用的子模块管理的功能.</p>
<p><strong>因此, 利用<code>tf.Module</code>提供的封装, 再结合TensoFlow丰富的低阶API, 实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型), 并实现跨平台部署使用.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个简单的function</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在tf.function中用input_signature限定输入张量的签名类型：shape和dtype</span></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_print</span><span class="params">(a)</span>:</span></span><br><span class="line">    x.assign_add(a)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_print(tf.constant(<span class="number">3.0</span>))</span><br><span class="line"><span class="comment">#add_print(tf.constant(3))  # 输入不符合张量签名的参数将报错</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.Module的子类化将其封装一下</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoModule</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, init_value=tf.constant<span class="params">(<span class="number">0.0</span>)</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.x = tf.Variable(init_value, dtype=tf.float32, trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.x.assign_add(a)</span><br><span class="line">        tf.print(self.x)</span><br><span class="line">        <span class="keyword">return</span> self.x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 执行</span></span><br><span class="line">demo = DemoModule(init_value=tf.constant(<span class="number">1.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模块中的全部变量和全部可训练变量</span></span><br><span class="line">print(demo.variables)</span><br><span class="line">print(demo.trainable_variables)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&lt;tf.Variable &apos;Variable:0&apos; shape=() dtype=float32, numpy=6.0&gt;,)</span><br><span class="line">(&lt;tf.Variable &apos;Variable:0&apos; shape=() dtype=float32, numpy=6.0&gt;,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模块中的全部子模块</span></span><br><span class="line">demo.submodules</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用tf.saved_model保存模型, 并指定需要跨平台部署的方法</span></span><br><span class="line">tf.saved_model.save(demo,</span><br><span class="line">                    <span class="string">"./model/demo/1"</span>,</span><br><span class="line">                    signatures=&#123;<span class="string">"serving_default"</span>: demo.addprint&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:Assets written to: ./model/demo/1/assets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">demo2 = tf.saved_model.load(<span class="string">"./model/demo/1"</span>)</span><br><span class="line">demo2.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看模型文件相关信息, 红框标出来的输出信息在模型部署和跨平台使用时有可能会用到</span><br><span class="line">!saved_model_cli show --dir ./model/demo/1 --all</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MetaGraphDef with tag-set: &apos;serve&apos; contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[&apos;__saved_model_init_op&apos;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&apos;__saved_model_init_op&apos;] tensor_info:</span><br><span class="line">        dtype: DT_INVALID</span><br><span class="line">        shape: unknown_rank</span><br><span class="line">        name: NoOp</span><br><span class="line">  Method name is: </span><br><span class="line"></span><br><span class="line">signature_def[&apos;serving_default&apos;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&apos;a&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: ()</span><br><span class="line">        name: serving_default_a:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&apos;output_0&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: ()</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure>
<p>除了利用<code>tf.Module</code>的子类化实现封装, 我们也可以通过给<code>tf.Module</code>添加属性和方法进行封装.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mymodule = tf.Module()</span><br><span class="line">mymodule.x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(a)</span>:</span></span><br><span class="line">    mymodule.x.assign_add(a)</span><br><span class="line">    tf.print(mymodule.x)</span><br><span class="line">    <span class="keyword">return</span> (mymodule.x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mymodule.addprint = addprint</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-自动微分机制</title>
    <url>/2020/08/05/TensorFlow/TensorFlow-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>神经网络通常依赖反向传播求梯度来更新网络参数, 求梯度过程通常是一件非常复杂而容易出错的事情. 而深度学习框架最主要的功能之一, 就是可以帮助我们自动地完成这种求梯度运算.</p>
<p>TensorFlow一般使用梯度磁带<code>tf.GradientTape</code>来记录正向运算过程, 然后反播磁带自动得到梯度值. 这种利用<code>tf.GradientTape</code>求微分的方法叫做Tensorflow的自动微分机制.</p>
<a id="more"></a>
<h1 id="利用梯度磁带求导数"><a href="#利用梯度磁带求导数" class="headerlink" title="利用梯度磁带求导数"></a>利用梯度磁带求导数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a * x**<span class="number">2</span> + b * x + c  <span class="comment"># tf.GradientTape会自动记录变量张量的传播.</span></span><br><span class="line">dy_dx = tape.gradient(y, x)</span><br><span class="line"></span><br><span class="line">print(dy_dx)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(-2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对常量张量也可以求导，需要增加watch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([a, b, c])</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">dy_dx, dy_da, dy_db, dy_dc = tape.gradient(y, [x, a, b, c])</span><br><span class="line"></span><br><span class="line">print(dy_da)</span><br><span class="line">print(dy_dc)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(0.0, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(1.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以求二阶导数</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape1.gradient(y, x)</span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx, x)</span><br><span class="line"></span><br><span class="line">print(dy2_dx2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自变量转换成tf.float32</span></span><br><span class="line">    x = tf.cast(x, tf.float32)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch(x)</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape.gradient(y, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dy_dx, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(f(tf.constant(<span class="number">0.0</span>)))</span><br><span class="line">tf.print(f(tf.constant(<span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(-2, 1)</span><br><span class="line">(0, 0)</span><br></pre></td></tr></table></figure>
<h1 id="利用梯度磁带和优化器求最小值"><a href="#利用梯度磁带和优化器求最小值" class="headerlink" title="利用梯度磁带和优化器求最小值"></a>利用梯度磁带和优化器求最小值</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape.gradient(y, x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx, x)])</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>, y, <span class="string">"; x ="</span>, x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y = 0 ; x = 0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"><span class="comment"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意f()无参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f, [x])</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>, f(), <span class="string">"; x ="</span>, x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y = 0 ; x = 0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(<span class="number">1000</span>):  <span class="comment">#注意autograph时使用tf.range(1000)而不是range(1000)</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">        dy_dx = tape.gradient(y, x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx, x)])</span><br><span class="line"></span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(minimizef())</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):</span><br><span class="line">        optimizer.minimize(f, [x])</span><br><span class="line">    <span class="keyword">return</span> f()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(train(<span class="number">1000</span>))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">0.999998569</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-张量</title>
    <url>/2020/08/05/TensorFlow/TensorFlow-%E5%BC%A0%E9%87%8F/</url>
    <content><![CDATA[<p>程序 = 数据结构 + 算法</p>
<p>TensorFlow程序 = 张量数据结构 + 计算图算法</p>
<p>张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似.</p>
<p>从行为特性来看, 有两种类型的张量, 分别是常量<strong>constant</strong>和变量<strong>Variable</strong>. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值.</p>
<p>张量的操作主要包括张量的<strong>结构操作</strong>和张量的<strong>数学运算</strong>. 张量结构操作诸如: 张量创建, 索引切片, 维度变换, 合并分割. 张量数学运算主要有: 标量运算, 向量运算, 矩阵运算.</p>
<a id="more"></a>
<h1 id="常量张量"><a href="#常量张量" class="headerlink" title="常量张量"></a>常量张量</h1><p>张量的数据类型基本和numpy.array的数据类型对应.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">1</span>)  <span class="comment"># 默认整数tf.int32</span></span><br><span class="line">b = tf.constant(<span class="number">1</span>, dtype=tf.int64)  <span class="comment"># tf.int64</span></span><br><span class="line">c = tf.constant(<span class="number">5.20</span>)  <span class="comment"># 默认小数tf.float32</span></span><br><span class="line">d = tf.constant(<span class="number">5.20</span>, dtype=tf.double)  <span class="comment">#tf.double</span></span><br><span class="line">e = tf.constant(<span class="string">'are you ok?'</span>)  <span class="comment"># tf.string</span></span><br><span class="line">f = tf.constant(<span class="literal">True</span>)  <span class="comment"># tf.bool</span></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64)</span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode)  <span class="comment"># 不等价</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>张量可以有各种不同的维度, 标量是0维的张量, 向量是1维的张量, 矩阵是2维的张量, 彩色图像有rgb三个通道, 可以表示为3维张量, 视频有时间维, 可以表示为4维张量. 通俗来说, 有几层中括号, 就是几维张量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment"># 标量, 0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank与numpy的ndim方法查看具体维度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(0, shape=(), dtype=int32)</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 向量, 1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector))</span><br><span class="line">print(vector.numpy().ndim)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(1, shape=(), dtype=int32)</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                     [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 矩阵, 2维张量</span></span><br><span class="line">print(tf.rank(matrix))</span><br><span class="line">print(np.ndim(matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(2, shape=(), dtype=int32)</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], </span><br><span class="line">                       [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line"></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[[1 2]</span><br><span class="line">  [3 4]]</span><br><span class="line"></span><br><span class="line"> [[5 6]</span><br><span class="line">  [7 8]]], shape=(2, 2, 2), dtype=int32)</span><br><span class="line">tf.Tensor(3, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>可以用<code>tf.cast</code>来来改变张量中的数据类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.cast(a, tf.float32)</span><br><span class="line"></span><br><span class="line">print(a.dtype, b.dtype)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dtype: &apos;int32&apos;&gt; &lt;dtype: &apos;float32&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以用numpy中的方法, 将TensorFlow中的张量转化为numpy中的张量. 用shape方法查看张量的尺寸.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">print(a.numpy())  <span class="comment"># 转换成np.array</span></span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 2]</span><br><span class="line">(2,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="string">'你好啊!'</span>)</span><br><span class="line"></span><br><span class="line">print(a.numpy())</span><br><span class="line">print(a.numpy().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&apos;\xe4\xbd\xa0\xe5\xa5\xbd\xe5\x95\x8a!&apos;</span><br><span class="line">你好啊!</span><br></pre></td></tr></table></figure>
<h1 id="变量张量"><a href="#变量张量" class="headerlink" title="变量张量"></a>变量张量</h1><p>模型中需要被训练的参数一般被设置为变量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 常量值不可改变, 常量的重新赋值相当于创造新的内存空间.</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a = a + tf.constant([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(c)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor([1 2], shape=(2,), dtype=int32)</span><br><span class="line">140680756570256</span><br><span class="line">tf.Tensor([2 3], shape=(2,), dtype=int32)</span><br><span class="line">140680756571312</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变, 可以通过assign, assign_add等方法进行赋值.</span></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a.assign_add([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br></pre></td></tr></table></figure>
<h1 id="张量的结构操作"><a href="#张量的结构操作" class="headerlink" title="张量的结构操作"></a>张量的结构操作</h1><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的许多方法和numpy中创建array的方法很像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>, <span class="number">10</span>, delta=<span class="number">2</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 3 5 7 9]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">100</span>)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0 0.101010099 0.202020198 ... 9.79797935 9.89899 10]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.ones([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.zeros_like(a, dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 1]</span><br><span class="line"> [1 1]]</span><br><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[5 5]</span><br><span class="line"> [5 5]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[8.34453773 2.33366609 8.79651928 0.466492176 8.03496838]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-2.27333117 -1.65921044]</span><br><span class="line"> [-0.263356805 -0.809234142]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机，对两倍方差进行截断</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">2</span>, <span class="number">2</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1.57495022 -0.52902168]</span><br><span class="line"> [-0.704079211 -0.923798501]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print()</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 对角阵</span></span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 0]</span><br><span class="line"> [0 1]]</span><br><span class="line"></span><br><span class="line">[[1 0 0]</span><br><span class="line"> [0 2 0]</span><br><span class="line"> [0 0 3]]</span><br></pre></td></tr></table></figure>
<h2 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h2><p>张量的索引切片方式和numpy几乎是一样的, 切片时支持缺省参数和省略号. </p>
<p>对于<code>tf.Variable</code>可以通过索引和切片对部分元素进行修改.</p>
<p>对于提取张量的连续子区域, 也可以使用<code>tf.slice</code>.</p>
<p>此外, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p><code>tf.boolean_mask</code>功能最为强大, 它可以实现<code>tf.gather</code>, <code>tf.gather_nd</code>的功能, 并且<code>tf.boolean_mask</code>还可以实现布尔索引.</p>
<p>如果要通过修改张量的某些元素得到新的张量, 可以使用<code>tf.where</code>, <code>tf.scatter_nd</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>, <span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[4 5 7 8 4]</span><br><span class="line"> [7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]</span><br><span class="line"> [4 6 5 9 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[4 5 7 8 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[4 6 5 9 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :])</span><br><span class="line">tf.print(tf.slice(t, [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">3</span>, <span class="number">5</span>]))  <span class="comment">#tf.slice(input, begin_vector, size_vector)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :<span class="number">4</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[7 8]</span><br><span class="line"> [0 7]</span><br><span class="line"> [0 2]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对变量来说，还可以使用索引和切片修改部分元素</span></span><br><span class="line">x = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">x[<span class="number">1</span>, :].assign(tf.constant([<span class="number">0.0</span>, <span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 2]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[0 4]</span><br><span class="line">  [7 2]]</span><br><span class="line"></span><br><span class="line"> [[4 0]</span><br><span class="line">  [7 5]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 省略号可以表示多个冒号</span></span><br><span class="line">tf.print(a[..., <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[4 2]</span><br><span class="line"> [0 5]]</span><br></pre></td></tr></table></figure>
<p>以上切片方式相对规则, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p>考虑班级成绩册的例子, 有4个班级, 每个班级5个学生, 每个学生6门科目成绩. 可以用一个(4, 5, 6)的张量来表示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), minval=<span class="number">0</span>, maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.print(scores)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [22 67 12 59 33 61]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [43 94 21 25 69 53]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [77 34 29 78 64 96]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [14 79 72 81 90 86]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [5 18 37 63 36 27]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [79 62 29 13 62 3]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [19 51 84 98 81 97]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [89 69 94 84 13 54]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生, 第2个学生, 第4个学生的第1门课程, 第3门课程, 第5门课程成绩</span></span><br><span class="line">q = tf.gather(tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>), [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[84 14 4]</span><br><span class="line">  [29 94 60]</span><br><span class="line">  [1 10 58]]</span><br><span class="line"></span><br><span class="line"> [[31 1 29]</span><br><span class="line">  [74 14 45]</span><br><span class="line">  [70 41 12]]</span><br><span class="line"></span><br><span class="line"> [[24 80 70]</span><br><span class="line">  [32 36 55]</span><br><span class="line">  [22 2 10]]</span><br><span class="line"></span><br><span class="line"> [[66 16 56]</span><br><span class="line">  [87 29 71]</span><br><span class="line">  [73 41 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取第1个班级第1个学生, 第2个班级的第2个学生, 第3个班级的第3个学生的全部成绩</span></span><br><span class="line"><span class="comment"># indices的长度为采样样本的个数, 每个元素为采样位置的坐标</span></span><br><span class="line">s = tf.gather_nd(scores, indices=[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]</span><br><span class="line"> [89 69 94 84 13 54]]</span><br></pre></td></tr></table></figure>
<p>以上<code>tf.gather</code>和<code>tf.gather_nd</code>的功能也可以用<code>tf.boolean_mask</code>来实现.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores, [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取第0个班级第0个学生, 第1个班级的第1个学生, 第2个班级的第2个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(</span><br><span class="line">    scores,</span><br><span class="line">    [[<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[28 84 29 14 65 4]</span><br><span class="line"> [77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.boolean_mask可以实现布尔索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到矩阵中小于0的元素</span></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">tf.print(c, <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">tf.print(tf.boolean_mask(c, c &lt; <span class="number">0</span>), <span class="string">"\n"</span>)</span><br><span class="line">tf.print(c[c &lt; <span class="number">0</span>])  <span class="comment"># 布尔索引, 为boolean_mask的语法糖形式</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-1 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 -3 3]] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3]</span><br></pre></td></tr></table></figure>
<p>以上这些方法仅能提取张量的部分元素值, 但不能更改张量的部分元素值得到新的张量.</p>
<p>如果要通过修改张量的部分元素值得到新的张量, 可以使用<code>tf.where</code>和<code>tf.scatter_nd</code>.</p>
<p><code>tf.where</code>可以理解为<code>if</code>的张量版本, 此外它还可以用于找到满足条件的所有元素的位置坐标.</p>
<p><code>tf.scatter_nd</code>的作用和<code>tf.gather_nd</code>有些相反, <code>tf.gather_nd</code>用于收集张量的给定位置的元素, 而<code>tf.scatter_nd</code>可以将某些值插入到一个给定shape的全0的张量的指定位置处.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找到张量中小于0的元素, 将其换成np.nan得到新的张量</span></span><br><span class="line"><span class="comment"># tf.where和np.where作用类似, 可以理解为if的张量版本</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">d = tf.where(c &lt; <span class="number">0</span>, np.nan, c)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[nan 1 nan]</span><br><span class="line"> [2 2 nan]</span><br><span class="line"> [3 nan 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果where只有一个参数, 将返回所有满足条件的位置坐标</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(indices)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 2]</span><br><span class="line"> [1 2]</span><br><span class="line"> [2 1]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量</span></span><br><span class="line"></span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>]], [c[<span class="number">0</span>, <span class="number">0</span>], c[<span class="number">2</span>, <span class="number">1</span>]], c.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 0 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># scatter_nd的作用和gather_nd有些相反</span></span><br><span class="line"><span class="comment"># 可以将某些值插入到一个给定shape的全0的张量的指定位置处</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(tf.scatter_nd(indices, tf.gather_nd(c, indices), c.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-1 0 -1]</span><br><span class="line"> [0 0 -2]</span><br><span class="line"> [0 -3 0]]</span><br></pre></td></tr></table></figure>
<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>维度变换相关函数主要有 <code>tf.reshape</code>, <code>tf.squeeze</code>, <code>tf.expand_dims</code>, <code>tf.transpose</code>.</p>
<p><code>tf.reshape</code>可以改变张量的形状. 可以改变张量的形状, 但是其本质上不会改变张量元素的存储顺序, 所以该操作实际上非常迅速, 并且是可逆的.</p>
<p><code>tf.squeeze</code>可以减少维度.</p>
<p><code>tf.expand_dims</code>可以增加维度.</p>
<p><code>tf.transpose</code>可以交换维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">255</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改成(3, 6)形状的张量</span></span><br><span class="line"></span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.print(b.shape)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 6])</span><br><span class="line">[[95 119 125 231 120 219]</span><br><span class="line"> [179 202 37 124 246 167]</span><br><span class="line"> [211 93 165 94 31 189]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改回成(1,3,3,2)形状的张量</span></span><br><span class="line"></span><br><span class="line">c = tf.reshape(b, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p>如果张量在某个维度上只有一个元素, 利用<code>tf.squeeze</code>可以消除这个维度.</p>
<p>和<code>tf.reshape</code>相似, 它本质上不会改变张量元素的存储顺序. 张量的各个元素在内存中是线性存储的, 其一般规律是, 同一层级中的相邻元素的物理地址也相邻.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = tf.squeeze(a)</span><br><span class="line">tf.print(s.shape)</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 3, 2])</span><br><span class="line">[[[95 119]</span><br><span class="line">  [125 231]</span><br><span class="line">  [120 219]]</span><br><span class="line"></span><br><span class="line"> [[179 202]</span><br><span class="line">  [37 124]</span><br><span class="line">  [246 167]]</span><br><span class="line"></span><br><span class="line"> [[211 93]</span><br><span class="line">  [165 94]</span><br><span class="line">  [31 189]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.expand_dims(s, axis=<span class="number">0</span>)  <span class="comment"># 在第0维插入长度为1的一个维度</span></span><br><span class="line">tf.print(d.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p><code>tf.transpose</code>可以交换张量的维度, 与<code>tf.reshape</code>不同, 它会改变张量元素的存储顺序.</p>
<p><code>tf.transpose</code>常用于图片存储格式的变换上.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Batch, Height, Width, Channel</span></span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>, <span class="number">600</span>, <span class="number">600</span>, <span class="number">4</span>],</span><br><span class="line">                      minval=<span class="number">0</span>,</span><br><span class="line">                      maxval=<span class="number">255</span>,</span><br><span class="line">                      dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Channel, Height, Width, Batch</span></span><br><span class="line">s = tf.transpose(a, perm=[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">tf.print(s.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([100, 600, 600, 4])</span><br><span class="line">TensorShape([4, 600, 600, 100])</span><br></pre></td></tr></table></figure>
<h2 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h2><p>和numpy类似, 可以用<code>tf.concat</code>和<code>tf.stack</code>方法对多个张量进行合并, 可以用<code>tf.split</code>方法把一个张量分割成多个张量.</p>
<p><code>tf.concat</code>和<code>tf.stack</code>有略微的区别, <code>tf.concat</code>是连接, 不会增加维度, 而<code>tf.stack</code>是堆叠, 会增加维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(6, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [ 3.,  4.],</span><br><span class="line">       [ 5.,  6.],</span><br><span class="line">       [ 7.,  8.],</span><br><span class="line">       [ 9., 10.],</span><br><span class="line">       [11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.concat([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 6), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.,  5.,  6.,  9., 10.],</span><br><span class="line">       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 3.,  4.]],</span><br><span class="line"></span><br><span class="line">       [[ 5.,  6.],</span><br><span class="line">        [ 7.,  8.]],</span><br><span class="line"></span><br><span class="line">       [[ 9., 10.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 5.,  6.],</span><br><span class="line">        [ 9., 10.]],</span><br><span class="line"></span><br><span class="line">       [[ 3.,  4.],</span><br><span class="line">        [ 7.,  8.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.split</code>是<code>tf.concat</code>的逆运算, 可以指定分割份数平均分割, 也可以通过指定每份的记录数量进行分割.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf.split(value, num_or_size_splits, axis)</span></span><br><span class="line">tf.print(tf.split(c, <span class="number">3</span>, axis=<span class="number">0</span>))  <span class="comment"># 指定分割份数, 平均分割</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.split(c, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], axis=<span class="number">0</span>))  <span class="comment"># 指定每份的记录数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<h1 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h1><p>张量的数学运算符可以分为标量运算符, 向量运算符, 以及矩阵运算符.</p>
<h2 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h2><p>加减乘除乘方, 以及三角函数, 指数, 对数等常见函数, 逻辑比较运算符等都是标量运算符.</p>
<p>标量运算符的特点是对张量实施逐元素运算.</p>
<p>有些标量运算符对常用的数学运算符进行了重载. 并且支持类似numpy的广播特性.</p>
<p>许多标量运算符都在<code>tf.math</code>模块下.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">-3</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">a + b  <span class="comment"># 运算符重载</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 6.,  8.],</span><br><span class="line">       [ 4., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a - b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ -4.,  -4.],</span><br><span class="line">       [-10.,  -4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a * b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[  5.,  12.],</span><br><span class="line">       [-21.,  32.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a / b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.2       ,  0.33333334],</span><br><span class="line">       [-0.42857143,  0.5       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  4.],</span><br><span class="line">       [ 9., 16.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a**<span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1.       , 1.4142135],</span><br><span class="line">       [      nan, 2.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a % <span class="number">3</span>  <span class="comment"># mod的运算符重载, 等价于m = tf.math.mod(a, 3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [-0.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a // <span class="number">3</span>  <span class="comment"># 地板除法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.,  0.],</span><br><span class="line">       [-1.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a &gt;= <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) &amp; (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) | (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[ True,  True],</span><br><span class="line">       [ True,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a == <span class="number">5</span>  <span class="comment"># tf.equal(a, 5)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False, False],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>, <span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>, <span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.maximum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[5 8]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.minimum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 6]</span><br></pre></td></tr></table></figure>
<h2 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h2><p>向量运算符只在一个特定轴上运算, 将一个向量映射到一个标量或者另外一个向量. 许多向量运算符都以<code>reduce</code>开头.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a))</span><br><span class="line">tf.print(tf.reduce_mean(a))</span><br><span class="line">tf.print(tf.reduce_max(a))</span><br><span class="line">tf.print(tf.reduce_min(a))</span><br><span class="line">tf.print(tf.reduce_prod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">45</span><br><span class="line">5</span><br><span class="line">9</span><br><span class="line">1</span><br><span class="line">362880</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[6]</span><br><span class="line"> [15]</span><br><span class="line"> [24]]</span><br><span class="line">[[12 15 18]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p))</span><br><span class="line">tf.print(tf.reduce_any(q))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a, b: a + b, tf.range(<span class="number">10</span>))</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">45</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a))</span><br><span class="line">tf.print(tf.math.cumprod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 3 6 ... 28 36 45]</span><br><span class="line">[1 2 6 ... 5040 40320 362880]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a))</span><br><span class="line">tf.print(tf.argmin(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">8</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values, indices = tf.math.top_k(a, <span class="number">3</span>, sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values)</span><br><span class="line">tf.print(indices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[8 7 5]</span><br><span class="line">[5 2 3]</span><br></pre></td></tr></table></figure>
<h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><p>矩阵必须是二维的, 类似<code>tf.constant([1, 2, 3])</code>这样的不是矩阵.</p>
<p>矩阵运算包括: 矩阵乘法, 矩阵转置, 矩阵逆, 矩阵求迹, 矩阵范数, 矩阵行列式, 矩阵求特征值, 矩阵分解等运算.</p>
<p>除了一些常用的运算外, 大部分和矩阵有关的运算都在<code>tf.linalg</code>子包中.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">a @ b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span><br><span class="line">array([[2, 4],</span><br><span class="line">       [6, 8]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1., 3.],</span><br><span class="line">       [2., 4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵逆, 必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3.0</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[-2.0000002 ,  1.0000001 ],</span><br><span class="line">       [ 1.5000001 , -0.50000006]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.trace(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.477226&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵特征值</span></span><br><span class="line">tf.linalg.eigvalsh(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵qr分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">q, r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q @ r)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-0.316227794 -0.948683321]</span><br><span class="line"> [-0.948683321 0.316227734]]</span><br><span class="line">[[-3.1622777 -4.4271884]</span><br><span class="line"> [0 -0.632455349]]</span><br><span class="line">[[1.00000012 1.99999976]</span><br><span class="line"> [3 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵svd分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">v, s, d = tf.linalg.svd(a)</span><br><span class="line">tf.matmul(tf.matmul(s, tf.linalg.diag(v)), d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[0.9999996, 1.9999996],</span><br><span class="line">       [2.9999998, 4.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><p>TensorFlow的广播规则和numpy是一样的:</p>
<ul>
<li>如果张量的维度不同, 将维度较小的张量进行扩展, 直到两个张量的维度都一样.</li>
<li>如果两个张量在某个维度上的长度是相同的, 或者其中一个张量在该维度上的长度为1, 那么我们就说这两个张量在该维度上是相容的.</li>
<li>如果两个张量在所有维度上都是相容的, 它们就能使用广播.</li>
<li>广播之后, 每个维度的长度将取两个张量在该维度长度的较大值.</li>
<li>在任何一个维度上, 如果一个张量的长度为1, 另一个张量长度大于1, 那么在该维度上, 就好像是对第一个张量进行了复制.</li>
</ul>
<p><code>tf.broadcast_to</code>以显式的方式按照广播机制扩展张量的维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment"># 等价于 b + tf.broadcast_to(a, b.shape)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [2, 3, 4],</span><br><span class="line">       [3, 4, 5]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.broadcast_to(a, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [1, 2, 3],</span><br><span class="line">       [1, 2, 3]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 静态形状, TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 3])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 动态形状, Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c), tf.shape(d))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 广播效果</span></span><br><span class="line">c + d  <span class="comment"># 等价于 tf.broadcast_to(c, [3, 3]) + tf.broadcast_to(d, [3, 3])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[2, 3, 4],</span><br><span class="line">       [3, 4, 5],</span><br><span class="line">       [4, 5, 6]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>二分网络的部分方法与应用</title>
    <url>/2020/05/24/%E5%9B%BE%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86%E7%BD%91%E7%BB%9C%E7%9A%84%E9%83%A8%E5%88%86%E6%96%B9%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>除了社交网络这一常见的网络类型外, 二分网络也是一种经常出现的网络. 所谓二分网络, 即节点被分为了两大类, 其中的边总是存在于一类节点到另一类节点之间. 例如用户与商品的二分网络, 投资者与股票的二分网络等.</p>
<p>特别是用户与商品构成的商品偏好网络, 有着很大的研究价值, 对于构建推荐系统来说必不可少的. 推荐系统一般考虑的是, 想什么样的用户推荐什么样的商品, 会使得用户喜欢, 点击率更高一些. 比如豆瓣电影, 每部电影下, 有用户对该电影的评分, 评分的由高到低, 表示了用户对其的喜欢与不喜欢.</p>
<a id="more"></a>
<p>这样, 用户对产品的评分, 或者是否购买某产品, 构成了一个二分网络. 仍然以豆瓣电影为例, 一个简单的二分网络如下图:</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>对于该网络, 以矩阵来表示为:</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p>其中每个格点的值, 表示该用户对该电影的评分. 接下来, 就以推荐这一应用为目的, 来介绍与二分网络相关的一些方法.</p>
<h1 id="基于内容"><a href="#基于内容" class="headerlink" title="基于内容"></a>基于内容</h1><p>其实基于内容进行推荐的方法, 并没有充分利用到二分网络的特点, 在这里进行简要介绍.</p>
<p>基于内容的推荐, 核心的步骤为3个:</p>
<ul>
<li>构建商品的特征向量.</li>
<li>构建用户的特征向量.</li>
<li>计算商品-用户的相似性.</li>
</ul>
<p>对于特征向量的构造, 没有固定的方法, 根据实际情况可以很简单, 也可以很复杂.</p>
<p>对于相似性评估指标, 一般来说余弦相似度就是不错的相似度评估指标(遇事不决选余弦).</p>
<p>基于内容进行推荐有着如下的优缺点:</p>
<ul>
<li>优点:<ul>
<li>可以应用到一个新商品上.</li>
<li>不会倾向于流行商品.</li>
<li>如果特征向量构建解释性强, 则最终推荐结果解释性强.</li>
</ul>
</li>
<li>缺点:<ul>
<li>有时候构建有效特征比较困难.</li>
<li>推荐范围可能比较狭窄.</li>
<li>对新用户无效.</li>
<li>没有充分利用到二分网络的整体结构信息.</li>
</ul>
</li>
</ul>
<h1 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h1><p>在推荐领域, 协同过滤是一种有效的方法, 分为基于用户与基于物品的协同过滤. 之所以叫做协同过滤, 指的是利用其他的用户或者商品一起, 过滤出值得推荐的商品.</p>
<ul>
<li><p>基于用户的协同过滤.</p>
<p>基于用户进行协同过滤, 基本思想是首先根据二分网络, 找到与目标用户相似的那些用户, 然后再根据相似的用户会喜欢哪些商品, 推荐给目标用户.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
<li><p>基于物品的协同过滤.</p>
<p>基于物品进行协同过滤, 基本思想是首先根据二分网络, 找到与目标用户所喜欢的商品相似的商品, 然后将这些商品推荐给用户.</p>
<p><img src="fig_3.png" alt="fig_3"></p>
</li>
<li><p>相似性计算.</p>
<p>用户与物品的相似性计算是类似的, 将每个用户对每个物品的打分(没有记录则为空)整理为一个矩阵. 每个用户的向量可以使用对物品的打分构成; 每个物品的向量可以用各个用户对它的打分构成. 向量中的空值可以用0来填充.</p>
<p>得到向量后, 相似性就可以利用余弦相似度$Cosine_Similarity=\frac{AB}{|A||B|}$进行计算了.</p>
<p>然而这里存在的一个问题是, 对于用户没有评分的物品, 其实有两种可能:</p>
<ul>
<li><p>用户还没接触过.</p>
</li>
<li><p>用户提前知道自己不喜欢该物品, 不参与打分.</p>
</li>
</ul>
<p>因此直接使用0来进行填充, 是不太好的. 一种更好的方法是, 是计算出用户对每个物品打分的平均值, 再将用户对物品的打分基础上减去这个平均值.</p>
<p><img src="fig_4.png" alt="fig_4"></p>
<p>进行这样的处理后, 0可以视作用户对物品的一个平均评分, 比之前的情形要好一些.</p>
<p>使用协同过滤, 有如下优缺点:</p>
</li>
<li><p>优点:</p>
<ul>
<li>不需要去精心设计物品与用户的特征向量, 只要有二分网络即可.</li>
<li>在有一定数据积累的情况下, 效果一般比基于内容的方法好.</li>
</ul>
</li>
<li><p>缺点:</p>
<ul>
<li>冷启动问题, 无论是对新用户, 还是新物品都难以进行推荐.</li>
<li>倾向推荐比较流行的商品.</li>
<li>如果二分网络对应的矩阵非常稀疏, 那么效果将会不好.</li>
</ul>
</li>
</ul>
<h1 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h1><p>我们知道对称方阵$A$, 可以用下式的特征分解表示:</p>
<script type="math/tex; mode=display">
A=W\Sigma W^{-1}</script><p>其中$W$为酉矩阵, $\Sigma$为对角矩阵, 对角值为特征值.</p>
<p>那么对于任意的距阵$A_{m\times n}$, 可以进行类似的分解吗? 也是可以的:</p>
<script type="math/tex; mode=display">
A=U\Sigma V^{T}</script><p>这样的分解称为奇异值分解(SVD), 其中, $U$是$m\times m$的酉矩阵, $V$是$n\times n$的酉矩阵, $\Sigma$是$m\times n$的矩阵, 除了主对角线外的元素都是0, 主对角线上的每个元素称为奇异值.</p>
<p>这里暂且不细说矩阵分解的具体过程, 可以采用解析的方式, 也可以采用迭代的方式.</p>
<p>当使用奇异值分解后, 在奇异值矩阵中也是按照从大到小排列, 而且奇异值的减小特别快, 在很多情况下, 前10%甚至1%的奇异值就占了全部奇异值之和的99%以上的比例. 也就是说, 可以用最大的$k$个奇异值和对应的左右奇异向量来近似描述矩阵:</p>
<script type="math/tex; mode=display">
A_{m\times n}\thickapprox U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}</script><p>如果在原矩阵中, 一些格点上存在缺失, 经过改动的矩阵分解方法, 可以在进行分解时只考虑非缺失部分. 当得到分解后的矩阵后, 可以对原本缺失的格点进行预测.</p>
<p>将其运用到二分网络, 商品推荐中, 如下图:</p>
<p><img src="fig_5.png" alt="fig_5"></p>
<p>可以对每个物品, 每个用户, 得到他们对应的$k$维特征向量, 要计算某个用户对某个物品的打分或者评价, 最简单的方法就是将两者的特征向量点乘.</p>
<p>对于SVD, 以及改进的方法SVD++, 在推荐系统中的效果是很好的, 相比协同过滤, 在面对稀疏数据的时候, 可以有更好的表现.</p>
<h1 id="信息推测"><a href="#信息推测" class="headerlink" title="信息推测"></a>信息推测</h1><p>在上面协同过滤的方法中, 可以知道某个用户与哪些用户更加相似, 那么利用这个相似度, 可以做更多的事情, 比如可以用于推测用户的信息.</p>
<p><img src="fig_6.png" alt="fig_6"></p>
<p>同样, 由矩阵分解得到的特征, 也能够用来做类似的事情, 例如利用特征与某用户信息的标签(如年龄), 构建模型进行预测, 通常会取得不错的效果.</p>
<h1 id="Surprise"><a href="#Surprise" class="headerlink" title="Surprise"></a>Surprise</h1><p><a href="https://github.com/NicolasHug/Surprise" target="_blank" rel="noopener">Surprise</a>是Python下一个推荐系统库.</p>
<p>使用方式与scikit包相似.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> surprise <span class="keyword">import</span> SVD</span><br><span class="line"><span class="keyword">from</span> surprise <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> surprise.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the movielens-100k dataset (download it if needed).</span></span><br><span class="line">data = Dataset.load_builtin(<span class="string">'ml-100k'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the famous SVD algorithm.</span></span><br><span class="line">algo = SVD()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run 5-fold cross-validation and print results.</span></span><br><span class="line">cross_validate(algo, data, measures=[<span class="string">'RMSE'</span>, <span class="string">'MAE'</span>], cv=<span class="number">5</span>, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Evaluating RMSE, MAE of algorithm SVD on 5 split(s).</span><br><span class="line"></span><br><span class="line">            Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std</span><br><span class="line">RMSE        0.9311  0.9370  0.9320  0.9317  0.9391  0.9342  0.0032</span><br><span class="line">MAE         0.7350  0.7375  0.7341  0.7342  0.7375  0.7357  0.0015</span><br><span class="line">Fit time    6.53    7.11    7.23    7.15    3.99    6.40    1.23</span><br><span class="line">Test time   0.26    0.26    0.25    0.15    0.13    0.21    0.06</span><br></pre></td></tr></table></figure>
<p>其中包含了一些主流的应用于二分网络的推荐算法, 包括协同过滤, SVD等.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a href="http://grouplens.org/datasets/movielens/100k" target="_blank" rel="noopener">Movielens 100k</a></th>
<th>RMSE</th>
<th>MAE</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD" target="_blank" rel="noopener">SVD</a></td>
<td>0.934</td>
<td>0.737</td>
<td>0:00:11</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp" target="_blank" rel="noopener">SVD++</a></td>
<td>0.92</td>
<td>0.722</td>
<td>0:09:03</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF" target="_blank" rel="noopener">NMF</a></td>
<td>0.963</td>
<td>0.758</td>
<td>0:00:15</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/slope_one.html#surprise.prediction_algorithms.slope_one.SlopeOne" target="_blank" rel="noopener">Slope One</a></td>
<td>0.946</td>
<td>0.743</td>
<td>0:00:08</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBasic" target="_blank" rel="noopener">k-NN</a></td>
<td>0.98</td>
<td>0.774</td>
<td>0:00:10</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans" target="_blank" rel="noopener">Centered k-NN</a></td>
<td>0.951</td>
<td>0.749</td>
<td>0:00:10</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline" target="_blank" rel="noopener">k-NN Baseline</a></td>
<td>0.931</td>
<td>0.733</td>
<td>0:00:12</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/co_clustering.html#surprise.prediction_algorithms.co_clustering.CoClustering" target="_blank" rel="noopener">Co-Clustering</a></td>
<td>0.963</td>
<td>0.753</td>
<td>0:00:03</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly" target="_blank" rel="noopener">Baseline</a></td>
<td>0.944</td>
<td>0.748</td>
<td>0:00:01</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor" target="_blank" rel="noopener">Random</a></td>
<td>1.514</td>
<td>1.215</td>
<td>0:00:01</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>社交网络-组群发现</title>
    <url>/2020/05/23/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C-%E7%BB%84%E7%BE%A4%E5%8F%91%E7%8E%B0/</url>
    <content><![CDATA[<p>对于社交网络, 主要的研究与应用包括以下三个方面:</p>
<ul>
<li>信息推测.</li>
<li>组群发现.</li>
<li>消息传播.</li>
</ul>
<p>这里着重介绍一些组群发现的理论, 以及使用真实的数据进行试验.</p>
<a id="more"></a>
<h1 id="组群分类"><a href="#组群分类" class="headerlink" title="组群分类"></a>组群分类</h1><h2 id="显式组群-amp-隐式组群"><a href="#显式组群-amp-隐式组群" class="headerlink" title="显式组群 &amp; 隐式组群"></a>显式组群 &amp; 隐式组群</h2><p>在一般的社交网络中, 组群有显式与隐式之分.</p>
<p>比如在豆瓣网上的各种小组就属于<strong>显式群组</strong>.</p>
<p>而<strong>隐式群组</strong>, 是通过观察和分析节点之间的交互特征发现的群组. 比如通过一些人平时的通话记录, 来推断哪些人是朋友, 一起工作等.</p>
<p>这里所说的组群发现, 一般默认指的是隐式组群发现.</p>
<h2 id="可重叠组群-amp-不可重叠组群"><a href="#可重叠组群-amp-不可重叠组群" class="headerlink" title="可重叠组群 &amp; 不可重叠组群"></a>可重叠组群 &amp; 不可重叠组群</h2><p><img src="fig_0.png" alt="fig_0"></p>
<p>网络中的节点可以同时属于多个组群, 这样的组群称为<strong>可重叠组群</strong>.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p>网络中的每个节点只属于一个组群, 组群之间没有共同节点, 则称为<strong>不可重叠组群</strong>.</p>
<h1 id="组群发现的意义"><a href="#组群发现的意义" class="headerlink" title="组群发现的意义"></a>组群发现的意义</h1><ul>
<li><p>单个节点.</p>
<p>通过组群发现, 将节点划分到了不同的团体, 可以通过整个团体的信息, 来对单个节点进行推测, 更好地理解单个节点.</p>
</li>
<li><p>群体.</p>
<p>有时候, 一些事实很难甚至无法通过单个节点看出来, 但是却能够从整体的行为中挖掘得到. 对于划分得到的组群, 它们之间的区别与联系可以通过划分结果进行分析.</p>
</li>
</ul>
<h1 id="组群发现算法"><a href="#组群发现算法" class="headerlink" title="组群发现算法"></a>组群发现算法</h1><ul>
<li><p>核心思想.</p>
<ol>
<li>高内聚: 组群内部节点之间交互密集.</li>
<li>低耦合: 组群之间的交互稀疏.</li>
</ol>
</li>
<li><p>组群发现 vs 聚类.</p>
<ul>
<li>聚类: 各数据点之间没有交互(连边), 根据节点的属性(如人的性别, 年龄, 学历等)来进行划分.</li>
<li>组群发现: 节点之间有交互(连边), 根据其交互行为来进行划分.</li>
</ul>
</li>
<li><p>经典算法Louvain.</p>
<ul>
<li><p>模块度(Modularity).</p>
<p>用以衡量组分划分的好坏的一个非常重要的指标, 越大说明划分效果越好. 计算公式为:</p>
<script type="math/tex; mode=display">
Q=\sum_i(组群_i内部的连边之和比例-组群_i连边之和比例的平方)</script><p>对于无向图, 一般看做有向图处理, 即一条无向连边, 等价于两条有向连边.</p>
<p>一般Q值在0.3~0.7.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">C1</th>
<th style="text-align:center">C2</th>
<th style="text-align:center">C3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">C1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">C2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">6</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">C3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">8</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
Q=\frac{6+6+8}{2\times 12}-(\frac{6+1}{2\times 12})^2-(\frac{6+1}{2\times 12})^2-(\frac{8+1+1}{2\times 12})^2=0.49</script><ul>
<li><p>算法流程.</p>
<ol>
<li>将每个节点作为一个组群.</li>
<li>尝试将某个节点向邻近节点的组群合并, 计算合并后的模块度增量, 选择增加最大的组群进行移动. 若增量为负数, 则不移动. 遍历所有节点, 并循环处理, 当稳定时停止本次划分.</li>
<li>将现阶段组群看做一个节点, 组群内的边作为”新节点”的自环边, 同时边的权重为之前边的权重和. 再进行第二步.</li>
</ol>
<p>从算法可以看出, Louvain算法是一种层次型的组群发现, 可以得到一系列不同层次的组群以及对应的Q值. 最终可以选择最大的Q值划分, 最为最优划分.</p>
</li>
</ul>
<h1 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h1><p>对于Louvain算法, 在Github上有开源的包<a href="https://github.com/taynaud/python-louvain" target="_blank" rel="noopener">python-louvain</a>, 利用这个包来对网络进行组群划分.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> community  <span class="comment"># python-louvain 在调用时的名称为community</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_list_df = pd.read_csv(<span class="string">'network_links.csv'</span>)</span><br><span class="line">G = nx.from_pandas_edgelist(edge_list_df)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用best_partition将会得到最佳组群划分的一个字典, 其中key为节点, value为组群编号.</span></span><br><span class="line">node_group = community.best_partition(G)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 得到不同节点的组群编号列表.</span></span><br><span class="line">values = [node_group.get(node) <span class="keyword">for</span> node <span class="keyword">in</span> G.nodes()]</span><br><span class="line"><span class="comment">#对不同组群的节点进行不同着色.</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line">nx.draw(G, cmap=plt.cm.RdYlBu, node_color=values, with_labels=<span class="literal">True</span>, edge_color=<span class="string">'gray'</span>, node_size=<span class="number">200</span>, pos=nx.spring_layout(G, k=<span class="number">0.2</span>, iterations=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig_3"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>使用NetworkX分析网络</title>
    <url>/2020/05/23/%E5%9B%BE%E7%AE%97%E6%B3%95/%E4%BD%BF%E7%94%A8NetworkX%E5%88%86%E6%9E%90%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>NetworkX是一个Python包, 其中包含了丰富的与网络分析相关的方法, 对于中小型的网络来说, 用来进行分析是非常合适的.</p>
<p>这里将使用NetworkX包对一份网络数据进行构建, 绘图, 统计等, 以展示其基本功能.</p>
<a id="more"></a>
<h1 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">edge_list_df = pd.read_csv(<span class="string">'network_links.csv'</span>)</span><br><span class="line">edge_list_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>这是以边表形式保存的数据, 其中<code>source</code>和<code>target</code>表示节点对, <code>value</code>表示边权重, 这里暂不考虑.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建网络对象.</span></span><br><span class="line">G = nx.from_pandas_edgelist(edge_list_df)</span><br></pre></td></tr></table></figure>
<h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><p>对于NetworkX, 提供静态的可视化网络, 对于节点不是特别多的情况下, 使用可视化能够非常直观地了解网络.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>,</span><br><span class="line">        edge_color=<span class="string">'grey'</span>,</span><br><span class="line">        node_color=<span class="string">'blue'</span>,</span><br><span class="line">        node_size=<span class="number">10</span>,</span><br><span class="line">        pos=nx.spring_layout(G,k=<span class="number">0.2</span>,iterations=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<p><img src="fig_1.png" alt="fig_1"></p>
<h1 id="获取网络信息"><a href="#获取网络信息" class="headerlink" title="获取网络信息"></a>获取网络信息</h1><ul>
<li><p>各节点度数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.degree(G)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
<li><p>节点中心性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 度数占比.</span></span><br><span class="line">nx.degree_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中介中心性.</span></span><br><span class="line">nx.betweenness_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接近中心性.</span></span><br><span class="line">nx.closeness_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征向量中心性.</span></span><br><span class="line">nx.eigenvector_centrality(G)</span><br></pre></td></tr></table></figure>
</li>
<li><p>最大连通子图.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取出网络的最大子图</span></span><br><span class="line">Sub_G=G.subgraph(list(nx.connected_components(G))[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络平均聚类系数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算网络的聚类系数</span></span><br><span class="line">nx.average_clustering(G)</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络平均路径长度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算网络的平均路径</span></span><br><span class="line">nx.average_shortest_path_length(G)</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>网络生成模型</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>大多数时候, 我们可以去采集获取现实中真实的图结构的数据, 对其进行分析和研究.</p>
<p>而如果想通过一些方法, 来生成类似的数据, 可以怎么做呢? 如果一个相对简单的过程, 就能够生成与现实世界中非常相似的网络, 那么有如下好处:</p>
<ol>
<li>可以用生成的网络, 代替真实网络进行研究和实验.</li>
<li>可以更加清楚地了解真实世界网络的生成方式.</li>
</ol>
<a id="more"></a>
<h1 id="网络相似度衡量指标"><a href="#网络相似度衡量指标" class="headerlink" title="网络相似度衡量指标"></a>网络相似度衡量指标</h1><p>那么, 怎么知道生成的网络与真实世界网络之间是否相似呢? 需要一些衡量指标.</p>
<h2 id="节点度数分布"><a href="#节点度数分布" class="headerlink" title="节点度数分布"></a>节点度数分布</h2><p>首先是节点的度分布, 在现实世界中, 节点的度数分布通常服从幂律分布:</p>
<script type="math/tex; mode=display">
P_d=\alpha d^{-\beta}</script><p>其中$\alpha,\beta$均为参数, $d$为度数.</p>
<p>大多数节点的度数较小, 小部分节点的度数很大.</p>
<h2 id="聚类系数"><a href="#聚类系数" class="headerlink" title="聚类系数"></a>聚类系数</h2><p>以社交网络为例, A有两个朋友B和朋友C, 那么B和C本身可能也是朋友.</p>
<p>也就是说, 真实世界的网络一般具有不低的聚类系数.</p>
<h2 id="平均路径长度"><a href="#平均路径长度" class="headerlink" title="平均路径长度"></a>平均路径长度</h2><p>有一个著名的理论可能大家都听过, 叫做<strong>六度分隔</strong>理论, 或者也叫<strong>小世界</strong>理论. 大致就是说, 原本互不相识的人, 平均下来经过6个两两相识的人, 就能连接到一起.</p>
<p>而在真实的网络中, 平均路径长度可能比6还要小, 例如以前统计过, Facebook中用户的平均路径长度为4.7, YouTube为5.1.</p>
<h1 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h1><h2 id="随机网络"><a href="#随机网络" class="headerlink" title="随机网络"></a>随机网络</h2><p>随机网络模型, 指的是固定节点数, 对于所有可能存在的边, 以概率p决定是否出现.</p>
<p>在10个节点, 不同的p参数下, 随机网络的表现如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">概率p</th>
<th style="text-align:center">平均度数</th>
<th style="text-align:center">直径(最长距离)</th>
<th style="text-align:center">最大连通子图</th>
<th style="text-align:center">平均距离</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">0.055</td>
<td style="text-align:center">0.8</td>
<td style="text-align:center">2</td>
<td style="text-align:center">4</td>
<td style="text-align:center">1.5</td>
</tr>
<tr>
<td style="text-align:center">0.11</td>
<td style="text-align:center">1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">7</td>
<td style="text-align:center">2.66</td>
</tr>
<tr>
<td style="text-align:center">1.0</td>
<td style="text-align:center">9</td>
<td style="text-align:center">1</td>
<td style="text-align:center">10</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>随着p的增加, 平均度数到达1附近时, 网络会发生”相变”. 其最大连通子图的大小, 会突然变大, 几乎囊括所有节点; 同时, 网络的直径也会达到最大, 但后续又会降低.</p>
<p>可以总结下随机网络的性质:</p>
<ul>
<li>平均度数小于1.<ul>
<li>分散的簇.</li>
<li>直径小.</li>
<li>平均路径长度小.</li>
</ul>
</li>
<li>平均度数约等于1.<ul>
<li>最大连通子图的大小会迅速增大.</li>
<li>直径最大.</li>
<li>平均路径长度大.</li>
</ul>
</li>
<li>平均度数大于1.<ul>
<li>几乎所有节点都被连接.</li>
<li>直径减小.</li>
<li>平均路径长度减小.</li>
</ul>
</li>
</ul>
<p>最后随机网络与真实的网络有哪些差别呢?</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">随机网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">泊松分布</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较小</td>
</tr>
</tbody>
</table>
</div>
<p>也就是说, 如果用随机网络来模拟真实网络, 那么除了平均路径长度, 在度分布以及聚类系数上都不符合.</p>
<h2 id="小世界网络"><a href="#小世界网络" class="headerlink" title="小世界网络"></a>小世界网络</h2><p>小世界网络也被称为WS(Watts-Strogatz)网络, 是一种进阶的随机模型.</p>
<p>生成小世界网络的过程如下:</p>
<ol>
<li><p>生成一个符合期望度数的规则网络.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
</li>
<li><p>以概率p重连其中的某些边.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
</ol>
<p>在不同的概率p下, 平均距离与聚类系数的变化如下图:</p>
<p><img src="fig_3.png" alt="fig_3"></p>
<p>可以看到, 随着p的增大, 在中间的一段区域, 聚类系数减小缓慢, 而平均距离减小很快, 选择合适的p, 可以获得高聚类系数与低平均距离的网络.</p>
<p>小世界网络与真实的网络对比:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">小世界网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">集中在期望度数附近</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较大</td>
</tr>
</tbody>
</table>
</div>
<p>除了度分布, 小世界网络在平均路径长度与聚类系数上, 都可以较好地模拟真实网络.</p>
<h2 id="优先连接网络"><a href="#优先连接网络" class="headerlink" title="优先连接网络"></a>优先连接网络</h2><p>如何让生成的网络度分布符合幂率分布呢? 遵循富者更富的原则, 可以在生成网络时, 一个一个地加入节点, 对于新加入的节点, 更倾向于连接网络中度数较大的节点, 连接到节点$i$相对概率为:</p>
<script type="math/tex; mode=display">
p(v_i)=\frac{d_i}{\sum_j d_j}</script><p>优先连接网络与真实网络比较:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">优先连接网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">集中在期望度数附近</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较小</td>
</tr>
</tbody>
</table>
</div>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上介绍了三种经典并简单的生成网络的方法, 它们相比真实网络, 都有一些不一样, 但同样有其价值.</p>
<p>与真实网络对比, 做一个总结:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">随机网络</th>
<th style="text-align:center">小世界网络</th>
<th style="text-align:center">优先连接网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">构造方式</td>
<td style="text-align:center">以概率随机连边</td>
<td style="text-align:center">以概率在规则网络随机重连边</td>
<td style="text-align:center">新加入节点优先连接高度数节点</td>
</tr>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">符合</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">不符合</td>
</tr>
<tr>
<td style="text-align:center">平均距离</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">符合</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>节点的相似性</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E8%8A%82%E7%82%B9%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7/</url>
    <content><![CDATA[<p>如何判断一个网络中, 某两个节点的相似性呢? 一般来说有两种方法:</p>
<ul>
<li>如果节点带有属性, 那么可以根据属性是否相似来衡量.</li>
<li>根据节点在网络中所处的位置.</li>
</ul>
<p>这里主要讲第二种方法.</p>
<a id="more"></a>
<h1 id="结构等价性-Structural-Equivalence"><a href="#结构等价性-Structural-Equivalence" class="headerlink" title="结构等价性(Structural Equivalence)"></a>结构等价性(Structural Equivalence)</h1><p>这种方法非常简单, 两个节点的共有邻居节点的数目定义了两个节点之间的相似程度.</p>
<p>公式为:</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=v_i与v_j的共同邻居数量</script><p>此公式的一个问题是, 没有考虑到各个节点本身的度数, 比如同样都是10个共同的节点, 但一些节点的度数很高, 一些节点度数很低, 那么这种情况下是不能一概而论的.</p>
<p>因此需要做一些调整, 有两种进阶的计算方法:</p>
<ul>
<li><p>Jaccard Similarity</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=\frac{v_i与v_j的共同邻居数量}{v_i与v_j的所有邻居数量}</script></li>
<li><p>Cosine Similarity</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=\frac{v_i与v_j的共同邻居数量}{\sqrt{v_i邻居数量\times v_j邻居数量}}</script></li>
</ul>
<h1 id="规则等价性-Regular-Equivalence"><a href="#规则等价性-Regular-Equivalence" class="headerlink" title="规则等价性(Regular Equivalence)"></a>规则等价性(Regular Equivalence)</h1><p>在衡量等价性时, 不仅仅看相同的邻居, 对于不同的邻居也要看它们是否相似.</p>
<p>计算公式为:</p>
<script type="math/tex; mode=display">
\sigma(v_i, v_j)=\alpha\sum_{k.l}A_{i,k}A_{j,l}\sigma(v_k,v_l)</script><p><img src="fig_0.png" alt="fig_0"></p>
<p>显然这个计算方式相比结构等价性要复杂很多, 并且是利用迭代的方式进行计算的.</p>
<p>一种相对简单的近似算法如下:</p>
<script type="math/tex; mode=display">
\sigma(v_i, v_j)=\alpha\sum_{k}A_{i,k}\sigma(v_k,v_j)</script><p><img src="fig_1.png" alt="fig_1"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>网络的传递性</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%A0%E9%80%92%E6%80%A7/</url>
    <content><![CDATA[<h1 id="传递性-Transitivity"><a href="#传递性-Transitivity" class="headerlink" title="传递性(Transitivity)"></a>传递性(Transitivity)</h1><p>网络的传递性, 可以用于衡量一个网络中, 各个节点之间相互连通的程度. 在一些网络中, 节点倾向于相互孤立, 那么传递性就较差; 在一些网络中, 节点之间倾向于相互连接, 那么传递性一般就会高一些.</p>
<p>传递性的思想用一句通俗的话来说, 就是: 我朋友的朋友也是我的朋友.</p>
<a id="more"></a>
<p><img src="fig_0.png" alt="fig_0"></p>
<h1 id="聚集系数"><a href="#聚集系数" class="headerlink" title="聚集系数"></a>聚集系数</h1><p>那么, 如何定量地去刻画一个网络的传递性呢? 可以用到聚集系数, 其计算方法非常简单:</p>
<script type="math/tex; mode=display">
聚集系数=\frac{构成三角形的三元节点数量 \times 3}{连通的三元节点数量}</script><p>也就是说, 节点之间两两相互连接, 尽量构成更多的三角形, 那么就拥有更大的聚集系数, 也就拥有更高的传递性.</p>
<h1 id="结构平衡理论"><a href="#结构平衡理论" class="headerlink" title="结构平衡理论"></a>结构平衡理论</h1><p>这里再说一下与传递性相关的理论, 结构平衡理论, 同样用通俗的话来说就是:</p>
<ul>
<li>我朋友的朋友就是我朋友.</li>
<li>我朋友的敌人就是我敌人.</li>
<li>我敌人的敌人就是我朋友.</li>
<li>我敌人的朋友就是我敌人.</li>
</ul>
<p>在这样的理论下, 就会出现符合这种描述的网络结构, 称为平衡网络; 以及不符合这种描述的网络, 称为不平衡网络.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p><img src="fig_2.png" alt="fig_2"></p>
<p>使用计算的方式判断网络是否平衡的方法:</p>
<p><img src="fig_3.png" alt="fig_3"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>节点的中心性</title>
    <url>/2020/05/20/%E5%9B%BE%E7%AE%97%E6%B3%95/%E8%8A%82%E7%82%B9%E7%9A%84%E4%B8%AD%E5%BF%83%E6%80%A7/</url>
    <content><![CDATA[<p>在一个由节点和边组成的网络中, 很多时候想知道哪些节点是相对重要的, 或者进一步对所有节点的重要性进行排序.</p>
<p>根据不同的场景需求, 可以选择不同的方式来度量节点的中心性(重要性).</p>
<a id="more"></a>
<h1 id="基于度数"><a href="#基于度数" class="headerlink" title="基于度数"></a>基于度数</h1><p>基于度数是非常自然的想法, 一个节点的度数越大, 通常是越重要的节点.</p>
<h2 id="点度中心性-Degree-Centrality"><a href="#点度中心性-Degree-Centrality" class="headerlink" title="点度中心性(Degree Centrality)"></a>点度中心性(Degree Centrality)</h2><p>直接利用度数的大小对节点中心性进行排序, 简单, 应用广泛, 可解释性强.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>但该方法显然存在明显的缺点, 邻居节点的重要性没有考虑. 因为在一些场景下, 可以通过非常规方式增加度数, 如一些社交平台上买假粉.</p>
<h2 id="特征向量中心性-Eigenvector-Centrality"><a href="#特征向量中心性-Eigenvector-Centrality" class="headerlink" title="特征向量中心性(Eigenvector Centrality)"></a>特征向量中心性(Eigenvector Centrality)</h2><p>于是, 在考虑某个节点中心性时, 也同时考虑其邻居节点的中心性. 不仅需要邻居节点多, 还需要邻居节点是重要的.</p>
<p>给定一个网络, 对应一个邻接矩阵$A$, 以及一个各节点度数向量$X$.</p>
<p>利用$AX$会得到什么呢? 得到一个新的向量$X$, 其中的每个值代表原本每个节点的邻居节点的度数(重要性)和.</p>
<p>经过反复迭代, 最终向量$X$会收敛稳定, 对于这种情况, 正是线性代数中的特征向量:</p>
<script type="math/tex; mode=display">
AX=\lambda X</script><p>其中$\lambda$是邻接矩阵$A$的特征根, $X$是对应的特征向量, 其各个值也就是各节点的中心性.</p>
<h2 id="PageRank-Centrality"><a href="#PageRank-Centrality" class="headerlink" title="PageRank Centrality"></a>PageRank Centrality</h2><p>特征向量中心性对于无向网络中心性的度量效果是较好的, 但是对于有向网络却并不适用.</p>
<p>下面介绍大名鼎鼎的PageRank算法.</p>
<p>PageRank的基本思想非常简单:</p>
<ol>
<li><p>预设一个阻尼系数$\beta$, 表示在一个页面上跳转到下一个页面的概率.</p>
</li>
<li><p>首先初始化所有节点的重要性为$(1-\beta)$.</p>
</li>
<li><p>每一次迭代时, 某个节点的重要性等于:</p>
<script type="math/tex; mode=display">
PR_i=(1-\beta)+\beta(\sum_{j\in I}PR_j/C_j)</script><p>其中, $PR_i$表示节点$i$在某一次迭代中的重要性, $I$表示连接向节点$i$的邻居节点, $C_j$表示节点$j$的出度.</p>
</li>
<li><p>当迭代后, 各节点前后重要性变化很小时, 停止迭代, 得到结果.</p>
</li>
</ol>
<p>以上就是PageRank的简单版流程, 同时考虑到了邻居节点的数量和重要性, 可以很好地对节点中心性进行排序.</p>
<h1 id="中介中心性-Betweenness-Centrality"><a href="#中介中心性-Betweenness-Centrality" class="headerlink" title="中介中心性(Betweenness Centrality)"></a>中介中心性(Betweenness Centrality)</h1><p>除了可以基于节点度数来进行衡量节点中心性外, 使用betweenness在一些场景下, 也是一种很好的方法.</p>
<p>中介中心性, 强调了某个节点作为”交通枢纽”的作用, 即如果该节点堵塞, 在网络流通将会受到较大影响.</p>
<p>假设要求某个节点$i$的中心性, 计算方式如下:</p>
<ol>
<li><p>计算网络中, 除了节点$i$外, 任意两两节点之间的最短路径.</p>
</li>
<li><p>统计有多少条最短路径经过节点$i$.</p>
</li>
<li><p>节点$i$中心性为:</p>
<script type="math/tex; mode=display">
中心性_i=\frac{经过节点i的最短路径数量}{除节点i的最短路径数量}</script></li>
</ol>
<h1 id="接近中心性-Closeness-Centrality"><a href="#接近中心性-Closeness-Centrality" class="headerlink" title="接近中心性(Closeness Centrality)"></a>接近中心性(Closeness Centrality)</h1><p>想象一种场景下, 需要找到一个节点作为初始点, 想周围扩散一些事物(如观点), 那应该怎样评价各节点的中心性呢?</p>
<p>很自然地想到, 利用平均最短距离进行衡量.</p>
<p>计算方式非常简单:</p>
<script type="math/tex; mode=display">
中心性_i=\frac{1}{平均最短距离_i} \\
平均最短距离_i=\frac{1}{n-1}\sum_{j\ne i}最短路径_{i,j}</script>]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>图基本概念</title>
    <url>/2020/05/17/%E5%9B%BE%E7%AE%97%E6%B3%95/%E5%9B%BE%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>作为一个还只会一点皮毛的初学者, 在这里把自己的所学记录下来, 有问题的地方希望大家可以指点(暂时没开评论, 不过有邮箱~). 以后随着学习的深入, 可能还会对现在的文章进行编辑.</p>
<p>在我们的生活中, 各种学科中, 一些事物可以被抽象成的节点与连边的结构. 比如:</p>
<ul>
<li>人与人之间的社交, 每个人可以被看成一个节点, 两个人是否认识决定他们之间是否拥有连边, 这样可以构成一个网络, 称为社交网络.</li>
<li>互联网中, 网站可以作为节点, 超链接作为边.</li>
<li>在电商中, 用户与商品都可以作为节点, 用户是否购买商品可以作为边.</li>
</ul>
<a id="more"></a>
<p>节点与边组成的抽象网络, 学术上可以被称为<strong>复杂网络</strong>, 属于系统科学的一个重要部分. 围绕复杂网络展开的研究, 可以包括网络本身的一些静态性质, 如节点重要性等; 也可以包括网络的一些动态演变, 比如疾病传播等.</p>
<p>同时, 更进一步, 节点可以有自己的类别和属性, 边可以有方向, 也可以有属性, 这样就可以构成更加丰富的网络. 而基于这样一些丰富的网络, 可以用来做更多的分析与创造. 比如基于图结构的数据, 构建模型, 来进行预测或表示; 结合自然语言处理技术, 构建<strong>知识图谱</strong>, 也是可以大有作为的.</p>
<p>若想在一些现实问题中, 运用图算法相关的理论和工具, 有两个关键要点:</p>
<ol>
<li>如何将现实场景中的对象转化为网络结构.</li>
<li>选择合适的方法处理网络结构数据, 以获得期望的结果.</li>
</ol>
<p>谈到图, 或者图论, 都会提及<strong>柯尼斯堡七桥问题</strong>.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>问题是能否每座桥只经过一次, 遍历所有桥? 经过大神欧拉的一顿操作, 证明这是不可能的, 同时还对这一类问题做了总结和抽象: 对于连通有限图, 度数为奇数的节点, 数量只能为0或者2, 只有这样才能所有边都经过且只经过一次.</p>
<p>这里的度数, 就是节点的连边数量. 通俗地理解, 就是一个节点, 由一条边到达, 必然需要另一条边离开, 所以需要偶数的度数. 除非这个节点作为起点或者终点.</p>
<h1 id="图的基本结构"><a href="#图的基本结构" class="headerlink" title="图的基本结构"></a>图的基本结构</h1><h2 id="节点-amp-边"><a href="#节点-amp-边" class="headerlink" title="节点 &amp; 边"></a>节点 &amp; 边</h2><p>节点与边是组成图的基本单元.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<h2 id="有向图-amp-无向图"><a href="#有向图-amp-无向图" class="headerlink" title="有向图 &amp; 无向图"></a>有向图 &amp; 无向图</h2><p>根据边是否带有方向(指向性), 可以把图划分为有向图和无向图.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
<h2 id="度数"><a href="#度数" class="headerlink" title="度数"></a>度数</h2><p>节点的度数指的是其周围边的数量, 如果是有向图, 那么还可以细分为<strong>出度</strong>和<strong>入度</strong>.</p>
<p><img src="fig_3.png" alt="fig_3"></p>
<p><img src="fig_4.png" alt="fig_4"></p>
<h2 id="度数分布"><a href="#度数分布" class="headerlink" title="度数分布"></a>度数分布</h2><p>度数的分布指的是具有不同度数的节点, 其数量在整体节点中的占比.</p>
<p><img src="fig_5.png" alt="fig_5"></p>
<p>与其它一些场景下, 经常出现正态分布不一样, 在显示中的不少网络, 其度数分布服从幂律分布, 即绝大部分节点的度数较小, 极少部分节点度数很大.</p>
<h1 id="计算机储存图的方法"><a href="#计算机储存图的方法" class="headerlink" title="计算机储存图的方法"></a>计算机储存图的方法</h1><h2 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h2><p><img src="fig_6.png" alt="fig_6"></p>
<p>在一个正方矩阵中, 行列数目等于节点数目, 格点数值1表示对应两个节点相连, 或者其中一个节点连向另外一个节点.</p>
<p>如果是无向图, 那么是对称矩阵.</p>
<p>用邻接矩阵, 有一些问题:</p>
<ul>
<li><p>通常非常稀疏, 即大量格点为0.</p>
</li>
<li><p>若节点较多, 则会耗费大量储存空间.</p>
</li>
</ul>
<h2 id="邻接表"><a href="#邻接表" class="headerlink" title="邻接表"></a>邻接表</h2><p>只记录相互连接的节点与边的信息, 相比邻接矩阵, 可以节省很多空间.</p>
<p><img src="fig_7.png" alt="fig_7"></p>
<p><img src="fig_8.png" alt="fig_8"></p>
<p>并且具体的储存方式可以根据有向图, 还是无向图进行调整.</p>
<p>其优点是便于查询每个节点的连接情况, 缺点是数据仍然存在冗余.</p>
<h2 id="边表"><a href="#边表" class="headerlink" title="边表"></a>边表</h2><p>只储存边, 其占用空间最小, 是当前比较常用的储存方式.</p>
<p>边表中一条数据, 代表一条边. 在有向图中, 代表一个节点指向另一个节点.</p>
<p><img src="fig_9.png" alt="fig_9"></p>
<p><img src="fig_10.png" alt="fig_10"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/2020/05/13/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>正则表达式(regular expression)描述了一种字符串匹配的模式(pattern), 利用这种模式, 可以判断某字符串中是否包含可匹配的子字符串; 可以对匹配到的字符串进行移除, 替换等操作.</p>
<a id="more"></a>
<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="基本匹配"><a href="#基本匹配" class="headerlink" title="基本匹配"></a>基本匹配</h2><p>最简单最基本的匹配方式, 就是直接逐位进行匹配.</p>
<p>例如正则表达式为<code>the</code>, 则会去尝试匹配<code>the</code>.</p>
<p>同时, 基本匹配是大小写敏感的.</p>
<h2 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h2><p>复杂一些的正则表达式, 需要<strong>元字符</strong>来参与.</p>
<p>元字符不代表他们本身的字面意思, 都有特殊的含义. 同时一些元字符写在方括号中的时候会转变用法. 以下是元字符的介绍:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">元字符</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">句号匹配任意单个字符, 除了换行符.</td>
</tr>
<tr>
<td style="text-align:center"><code>[]</code></td>
<td style="text-align:center">匹配方括号内的任意字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>[^]</code></td>
<td style="text-align:center">匹配除了方括号内的任意字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>*</code></td>
<td style="text-align:center">匹配*前的字符0到多次.</td>
</tr>
<tr>
<td style="text-align:center"><code>+</code></td>
<td style="text-align:center">匹配+前的字符1到多次.</td>
</tr>
<tr>
<td style="text-align:center"><code>?</code></td>
<td style="text-align:center">标记?之前的字符为可选.</td>
</tr>
<tr>
<td style="text-align:center"><code>{m, n}</code></td>
<td style="text-align:center">匹配<code>num</code>个大括号之前的字符或者字符集<code>(m &lt;= num &lt;= n)</code>.</td>
</tr>
<tr>
<td style="text-align:center"><code>(xyz)</code></td>
<td style="text-align:center">字符集, 匹配与<code>xyz</code>完全相同的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>竖线</code></td>
<td style="text-align:center">或运算符, 匹配符合前或后的字符(由于显示问题, 以中文示意).</td>
</tr>
<tr>
<td style="text-align:center"><code>\</code></td>
<td style="text-align:center">转义字符, 将字符特殊用法消除或者赋予特殊用法.</td>
</tr>
<tr>
<td style="text-align:center"><code>^</code></td>
<td style="text-align:center">从开始进行匹配.</td>
</tr>
<tr>
<td style="text-align:center"><code>$</code></td>
<td style="text-align:center">从末端进行匹配.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="简写字符集"><a href="#简写字符集" class="headerlink" title="简写字符集"></a>简写字符集</h2><p>正则表达式提供一些常用的字符集简写, 如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">简写</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">除换行符外的所有字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\w</code></td>
<td style="text-align:center">匹配所有字母数字, 等价于<code>[a-zA-Z0-9_]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\W</code></td>
<td style="text-align:center">匹配所有非字母数字, 即符号, 等价于<code>[^\w]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\d</code></td>
<td style="text-align:center">匹配数字, 等价于<code>[0-9]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\D</code></td>
<td style="text-align:center">匹配非数字, 等价于<code>[^\d]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\s</code></td>
<td style="text-align:center">匹配所有空格字符, 等价于<code>[\t\n\f\r]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\f</code></td>
<td style="text-align:center">匹配一个换页符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\n</code></td>
<td style="text-align:center">匹配一个换行符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\r</code></td>
<td style="text-align:center">匹配一个回车符</td>
</tr>
<tr>
<td style="text-align:center"><code>\t</code></td>
<td style="text-align:center">匹配一个制表符</td>
</tr>
</tbody>
</table>
</div>
<h2 id="前后预查"><a href="#前后预查" class="headerlink" title="前后预查"></a>前后预查</h2><p>在进行匹配的时候, 还要检查匹配字符串前或后的字符串是否满足指定的模式.</p>
<p>一般配合<code>()</code>使用.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>?=</code></td>
<td style="text-align:center">后面存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?!</code></td>
<td style="text-align:center">后面不存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?&lt;=</code></td>
<td style="text-align:center">前面存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?&lt;!</code></td>
<td style="text-align:center">前面不存在</td>
</tr>
</tbody>
</table>
</div>
<h2 id="标志"><a href="#标志" class="headerlink" title="标志"></a>标志</h2><p>标志也可以称作修正符, 用以控制搜索及返回结果的模式.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标志</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>i</code></td>
<td style="text-align:center">忽略大小写.</td>
</tr>
<tr>
<td style="text-align:center"><code>g</code></td>
<td style="text-align:center">全局搜索.</td>
</tr>
<tr>
<td style="text-align:center"><code>m</code></td>
<td style="text-align:center">多行修饰符, 让<code>^</code>与<code>$</code>工作范围在每行的起始.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="贪婪匹配-amp-惰性匹配"><a href="#贪婪匹配-amp-惰性匹配" class="headerlink" title="贪婪匹配 &amp; 惰性匹配"></a>贪婪匹配 &amp; 惰性匹配</h2><p>正则表达式默认采用的贪婪匹配模式, 在该模式下会匹配尽可能长的字符串.</p>
<p>使用<code>?</code>将贪婪匹配转换为惰性匹配, 常配合<code>*</code>, <code>+</code>等元字符使用.</p>
<h1 id="re模块"><a href="#re模块" class="headerlink" title="re模块"></a>re模块</h1><p>在Python中的re模块是用来专门处理正则表达式的, 下面介绍一下基础的常用方法.</p>
<h2 id="match-amp-search"><a href="#match-amp-search" class="headerlink" title="match &amp; search"></a>match &amp; search</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.match(pattern, string, flags=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">re.search(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>match</code>可用于从一开始进行匹配, 若没有匹配到结果, 则返回<code>None</code>.</p>
<p><code>search</code>与<code>match</code>非常相似, 只是它可以从任意位置开始匹配.</p>
<p>参数说明:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>pattern</code></td>
<td style="text-align:center">正则表达式.</td>
</tr>
<tr>
<td style="text-align:center"><code>string</code></td>
<td style="text-align:center">要匹配的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>flag</code></td>
<td style="text-align:center">标志位, 用于控制正则表达式的匹配方式.</td>
</tr>
</tbody>
</table>
</div>
<p>可选标志位:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">修饰符</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>re.I</code></td>
<td style="text-align:center">忽略大小写.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.M</code></td>
<td style="text-align:center">多行匹配, 影响<code>^</code>和<code>$</code>.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.S</code></td>
<td style="text-align:center">使<code>.</code>匹配包含换行符在内的所有字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.L</code></td>
<td style="text-align:center">由当前环境解析字符, 影响<code>\w, \W, \B, \b</code>, 不推荐使用.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.U</code></td>
<td style="text-align:center">由Unicode字符集解析字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.X</code></td>
<td style="text-align:center">这个标记允许你编写更具可读性更友好的正则表达式.</td>
</tr>
</tbody>
</table>
</div>
<p>常用的是忽略大小写以及多行匹配.</p>
<p>返回结果为匹配对象:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">匹配对象方法</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>group(num=0)</code></td>
<td style="text-align:center">返回匹配到的整个字符串. 可以通过输入1及以上的单个或多个数字, 来获取对应模式的小组的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>groups()</code></td>
<td style="text-align:center">返回一个包含所有小组字符串的元组.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.sub(pattern, repl, string, count=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>sub</code>可用于替换给定字符串中匹配项.</p>
<p>参数说明:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>pattern</code></td>
<td style="text-align:center">正则表达式.</td>
</tr>
<tr>
<td style="text-align:center"><code>repl</code></td>
<td style="text-align:center">用于替换的字符串, 也可以是函数.</td>
</tr>
<tr>
<td style="text-align:center"><code>string</code></td>
<td style="text-align:center">要匹配的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>count</code></td>
<td style="text-align:center">匹配替换的最大次数, 默认0表示全部替换.</td>
</tr>
<tr>
<td style="text-align:center"><code>flag</code></td>
<td style="text-align:center">标志位, 用于控制正则表达式的匹配方式.</td>
</tr>
</tbody>
</table>
</div>
<p>举个栗子来说明<code>repl</code>为函数时的情形:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将匹配的数字乘于2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(s)</span>:</span></span><br><span class="line">    value = int(s.group())</span><br><span class="line">    <span class="keyword">return</span> str(value * <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">string_ = <span class="string">'aaa123bbb234'</span></span><br><span class="line">print(re.sub(<span class="string">'\d+'</span>, func, string_))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aaa246bbb468</span><br></pre></td></tr></table></figure>
<h2 id="findall-amp-finditer"><a href="#findall-amp-finditer" class="headerlink" title="findall &amp; finditer"></a>findall &amp; finditer</h2><p>利用<code>match</code>与<code>search</code>进行匹配时, 只能匹配一个结果.</p>
<p>而<code>findall</code>与<code>finditer</code>可以返回所有匹配项, 根据不同情况选择使用. 相比<code>findall</code>一次性返回所有结果, <code>finditer</code>返回一个迭代器.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.findall(pattern, string, flags=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">re.finditer(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>其中的参数, 与前面方法相同.</p>
<h2 id="split"><a href="#split" class="headerlink" title="split"></a>split</h2><p>与字符串的<code>split</code>方法类似, 将匹配项作为分割边界, 返回分割后的结果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.split(pattern, string, maxsplit=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>其中的<code>maxsplit</code>参数表示最大分割次数, 其余参数与前面方法相同.</p>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>编程基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac配置优化</title>
    <url>/2020/03/27/%E5%85%B6%E5%AE%83/Mac%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>话说为啥会用到Mac Pro呢, 原本我用的小黑, 原因是近期为了搭梯子, 使得连接公司的VPN损坏Σ(っ °Д °;)っ</p>
<p>然后经过一番原因排查, 知道了是因为这个梯子和Windows下的EC会起冲突.</p>
<p>其实如果排除玩游戏, Office, 以及一些必要的社交工具如微信, 钉钉(这排除的有点多啊喂), 我个人是更喜欢Ubuntu的, WIndows系统实在一言难尽つ﹏⊂</p>
<p>于是我将目光投向了Mac OS, 买Mac Pro, 是的我看中的是苹果系统. 但在人生第一次开箱后, 我被震惊了, 这一眼看过去就很高级的做工, 这低调内敛, 精益求精的态度, 我甚至想下一部手机也换Iphone好了(｡･∀･)ﾉﾞ</p>
<a id="more"></a>
<h1 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h1><p>有一说一, 对于习惯了常规键盘布局的我来说, 已经形成了肌肉记忆, 在编辑文字或者写代码的时候, 效率还行. 而Mac的键盘布局发生了变化, 一开始用得真的不习惯, 希望后面慢慢适应吧.</p>
<p>同时, Mac上有些常规的键没有, 比如<code>Home</code>, <code>End</code>, <code>PageUp</code>, <code>PageDown</code>等等, 这些可以由组合键完成.</p>
<ul>
<li><p>Home: fn left or cmd left</p>
</li>
<li><p>End: fn right or cmd right</p>
</li>
<li><p>PageUp: fn up</p>
</li>
<li><p>PageDown: fn down</p>
</li>
<li><p>Delete: fn backspace</p>
</li>
<li><p>按单词移动: option left or option right</p>
</li>
<li><p>iTerm2快捷键设置:</p>
<p>总体思路是查询原有快捷键, 将其配置赋予自定义的快捷键.</p>
<ul>
<li><p>按单词移动:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>option left Send Escape Sequence b</p>
<p>option right Send Escape Sequence f</p>
</li>
<li><p>按单词删除:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>Left Option: Normal &gt; Esc+</p>
</li>
<li><p>整行删除:</p>
<p>Preferences &gt; Keys</p>
<p>cmd backspace Send Hex Codes 0x15</p>
</li>
<li><p>移动到句首/尾:</p>
<p>Preferences &gt; Keys</p>
<p>cmd left/right Send Hex Codes 0x01/0x05</p>
</li>
<li><p>撤销:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>cmd z Send Hex Codes 0x01/0x1f</p>
</li>
</ul>
</li>
</ul>
<h1 id="触控板"><a href="#触控板" class="headerlink" title="触控板"></a>触控板</h1><p>系统偏好设置 &gt; 触控板</p>
<p><img src="chukongban_0.png" alt="触控板0"></p>
<p><img src="chukongban_1.png" alt="触控板1"></p>
<p>系统偏好设置 &gt; 辅助功能 &gt; 鼠标与触控板 &gt; 触控板选项</p>
<p><img src="chukongban_2.png" alt="触控板2"></p>
<h1 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h1><p>原本Mac自带的输入法有如下一些问题:</p>
<ul>
<li>智能输入体验差.</li>
<li>中英文切换是原本的大写锁定键. 按逗号, 句号键无法查看更多候选.</li>
<li>虽然设定里面有设定半角模式, 但实际上没用. 这一点对于俺这样的小码农来说是最难受的.</li>
</ul>
<p>所以尝试了一下<strong>搜狗输入法Mac版</strong>, 不得不说, 真滴好用, 不愧是<strong>中文输入法界永远滴神!</strong></p>
<p>需要进行一些设置:</p>
<ul>
<li>全局半角.</li>
<li>关闭TouchBar功能, 关闭小图标(语音, 截图这些功能对我来说多余了).</li>
</ul>
<p>相比自带的输入法, 上面提到的问题都不再是问题, 而且如果愿意可以自定义更加适合自己的东西, 比如斗图, 颜表情…用搜狗输入法写到这里, 我还没有翻过提示页, 我这么说, 你懂吧.</p>
<h1 id="文本编辑器"><a href="#文本编辑器" class="headerlink" title="文本编辑器"></a>文本编辑器</h1><p>Mac自带的文本编辑器, 很难使, 只能说可以正常打字吧, 嗯.</p>
<p>所以需要一款功能多一些, 但仍然轻量级的文本编辑器, 这里我选择的是<strong>Sublime Text</strong>.</p>
<p>安装方法是直接到官网下载安装.</p>
<p>配置命令行方法:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ln -sv &quot;/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl&quot; /usr/local/bin/subl</span><br></pre></td></tr></table></figure>
<h1 id="基础APP"><a href="#基础APP" class="headerlink" title="基础APP"></a>基础APP</h1><p>包含但不限于:</p>
<ul>
<li>钉钉</li>
<li>微信</li>
<li>Chrome</li>
<li>EeasyConnect</li>
<li>ClashX</li>
<li>Tencent lemon</li>
<li>The Unarchiver</li>
</ul>
<h1 id="生产工具"><a href="#生产工具" class="headerlink" title="生产工具"></a>生产工具</h1><ul>
<li><p>Anaconda3</p>
<p>可在清华镜像下载. 安装完成后, 需要将路径写入环境变量<code>~/.zsh_profile</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Anaconda3.</span><br><span class="line">export PATH=&quot;/Users/opt/anaconda3/bin:$PATH&quot;</span><br></pre></td></tr></table></figure>
<p>安装Jupyter插件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  # 安装nbextensions.</span><br><span class="line">  $ pip install jupyter_contrib_nbextensions</span><br><span class="line">  $ jupyter contrib nbextension install --user</span><br><span class="line">  # 安装nbextensions_configurator.</span><br><span class="line">  $ pip install jupyter_nbextensions_configurator</span><br><span class="line">  $ jupyter nbextensions_configurator enable --user</span><br><span class="line">  # 安装yapf, 使用代码格式标准化工具时需要.</span><br><span class="line">$ pip install yapf</span><br></pre></td></tr></table></figure>
<p>安装完成后, 勾选如下配置:</p>
<ul>
<li><code>Codefolding</code> &amp; <code>Codefolding in Editor</code>: 代码折叠.</li>
<li><code>Move selected cells</code>: 按住<code>option</code>可上下移动<code>cells</code>.</li>
<li><code>Toggle all line numbers</code>: 显示行数.</li>
<li><code>AutoSaveTime</code>: 自动按时保存.</li>
<li><code>Hide Header</code>: <code>control H</code>隐藏/展现工具栏.</li>
<li><code>Highlight selected word</code>: 变量高亮.</li>
<li><code>Table of Contents</code>: 目录.</li>
<li><code>Code prettify</code>: 代码格式美化.</li>
<li><code>ExecuteTime</code>: 显示代码执行时间.</li>
<li><code>ScrollDown</code>: 输出自动下滑.</li>
</ul>
</li>
<li><p>PyCharm</p>
<p>下载安装, 并配置解释器路径和主题.</p>
</li>
<li><p>Clion</p>
<p>下载安装, 并破解.</p>
</li>
<li><p>Office</p>
<p>下载官网最新版, 然后寻找激活方法. 可选择装部分应用.</p>
</li>
<li><p>Git</p>
<p>一般自带. 需要创建公钥私钥, GitHub关联.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure>
</li>
<li><p>Node.js</p>
<p>官网下载安装.</p>
</li>
<li><p>typora</p>
<p>官网下载安装.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在~/.zsh_profile中添加如下, 使其可在终端快速启动.</span><br><span class="line">alias typora=&quot;open -a typora&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>java8</p>
<p>为了配合<code>Spark</code>的使用.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 更新brew库.</span><br><span class="line">$ brew tap homebrew/cask-versions</span><br><span class="line"># 制定版本安装.</span><br><span class="line">$ brew cask install adoptopenjdk8</span><br><span class="line"># 查看是否安装成功.</span><br><span class="line">$ java -version</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="终端"><a href="#终端" class="headerlink" title="终端"></a>终端</h1><ul>
<li><p>homebrew</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure>
<p>在执行<code>brew update</code>时, 基本卡死, 解决方法为更换国内镜像源.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 切换brew</span><br><span class="line">$ git -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-core</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-cask</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.git</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ brew config</span><br><span class="line">$ brew update -verbose</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 恢复源</span><br><span class="line"># 恢复 brew</span><br><span class="line">$ git -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-core</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-cask</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>iTerm2</p>
<p>官网下载安装.</p>
<p>设置为默认终端.</p>
<p>调整字体大小15, 光标闪动, 色彩对比度.</p>
<p>preference =&gt; Keys =&gt; Hotkey =&gt; Show/hide iTerm2 with a system-wide hotkey, 设置为command ..</p>
<p>preference  =&gt; profiles =&gt; colors =&gt; Color Presets =&gt; Solarized Light.</p>
</li>
<li><p>oh-my-zsh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看安装的shell.</span><br><span class="line">$ cat /etc/shells</span><br><span class="line"># 查看当前shell.</span><br><span class="line">$ echo $SHELL</span><br><span class="line"># 切换shell.</span><br><span class="line">$ chsh -s /bin/zsh</span><br><span class="line"># 安装oh-my-zsh</span><br><span class="line">$ sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置oh-my-zsh.</span><br><span class="line">$ open -t .zshrc</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主题列表: https://github.com/ohmyzsh/ohmyzsh/wiki/themes</span><br><span class="line">ZSH_THEME=avit</span><br><span class="line"># 插件.</span><br><span class="line">plugins=(git osx zsh-autosuggestions zsh-syntax-highlighting)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 插件中的zsh-autosuggestions 和 zsh-syntax-highlighting 是自定义安装的插件, 需要用 git 将插件 clone 到指定插件目录下：</span><br><span class="line"># 自动提示插件</span><br><span class="line">git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions</span><br><span class="line"># 语法高亮插件</span><br><span class="line">git clone git://github.com/zsh-users/zsh-syntax-highlighting $ZSH_CUSTOM/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="DS-Store"><a href="#DS-Store" class="headerlink" title=".DS_Store"></a>.DS_Store</h1><p><code>.DS_Store</code>这个文件没什么用, 但是却会在很多传输文件的时候带来不必要的问题. 比如在使用<code>Git</code>的时候.</p>
<p>有没有什么办法能够完全禁止呢? 暂时没找到.</p>
<p>删除所有目录下的<code>.DS_Store</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo find / -name &quot;.DS_Store&quot; -depth -exec rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<p>这条命令可以临时删除, 但后续只要文件夹有一些改动, 又回自动生成.</p>
<p>针对<code>Git</code>, 可以设定全局的<code>.gitignore_global</code>进行一定程度上的帮助. 创建<code>~/.gitignore_global</code>, 其中写入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># .gitignore_global</span><br><span class="line">####################################</span><br><span class="line">######## OS generated files ########</span><br><span class="line">####################################</span><br><span class="line">.DS_Store</span><br><span class="line">.DS_Store?</span><br></pre></td></tr></table></figure>
<p>同时在<code>~./gitconfig</code>中引入设置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[user]</span><br><span class="line">	name = xxx</span><br><span class="line">	email = xxx</span><br><span class="line">[core]</span><br><span class="line">	excludesfile = /Users/shy/.gitignore_global</span><br></pre></td></tr></table></figure>
<p>或者输入如下命令(推荐):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global core.excludesfile ~/.gitignore_global</span><br></pre></td></tr></table></figure>
<h1 id="TouchBar"><a href="#TouchBar" class="headerlink" title="TouchBar"></a>TouchBar</h1><p>最开始我觉得Mac的TouchBar是鸡肋, 花里胡哨的, 还我<code>F1~F12</code>(ﾟДﾟ*)ﾉ</p>
<p>比如Pycharm里面的<code>运行</code>快捷键是<code>shift F10</code>, 你这都没有这个键怎么办啊? 哦…变成<code>cmd shift R</code>了啊, 那没事了…</p>
<p>TouchBar的存在, 可以在系统层面, 或者一些APP上, 将一些功能直接放上去, 只要设置合理, 在熟练使用以后, 我相信效率是会提高的.</p>
<p>系统层面的TouchBar设置, 这个根据自己喜好就行. 我是将<code>调度管理</code>,<code>截屏</code>, <code>静音</code>设置成默认, 同时将<code>调度管理</code>, <code>截屏</code>, <code>音量调节</code>, <code>亮度调节</code>, <code>锁屏</code>, <code>睡眠</code>放到详细页.</p>
<p>一些APP, 比如PyCharm是支持定制TouchBar的. 对PyCharm来说, 可以在<code>Preferences &gt; Appearance &amp; Behavior &gt; Menus and Tollbars &gt; Touch Bar</code>中进行设置.</p>
<p>但是关于显示图标, 需要自己指定图案. 我在谷歌上搜索<code>touchbar icon download</code>这个关键词, 找到了<a href="https://community.folivora.ai/t/v3-update-native-apple-touch-bar-icon-pack-for-btt/3310/18" target="_blank" rel="noopener">这个网站</a>, 提供了TouchBar图标的下载地址.</p>
<p>然鹅, 直接使用里面的图标, 仍然会觉得有些蛋疼(* ￣︿￣), 因为你看自带的TouchBar图标, 都是亮亮的, 咱这图标是黑黑的, 白天用着还行, 光线不好的时候用着一片黑啊…</p>
<p>于是, 我想到了利用Python来修改图标颜色, 经过一番尝试, 成功了（＃￣～￣＃）</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">path=<span class="string">'image.png'</span></span><br><span class="line">im = Image.open(path)</span><br><span class="line">im.show()</span><br><span class="line"></span><br><span class="line">print(im.format)</span><br><span class="line">print(im.mode)</span><br><span class="line">print(im.size)</span><br><span class="line"></span><br><span class="line">source = im.split()</span><br><span class="line">source = (source[<span class="number">0</span>].point(<span class="keyword">lambda</span> i: <span class="number">255</span>), source[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">im = Image.merge(im.mode, source)</span><br><span class="line"></span><br><span class="line">im.show()</span><br><span class="line">im.save(<span class="string">'new_image.png'</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码的思路, 是咱们手里的原始图片, 是<code>LA</code>格式的, 众所周知<code>RGB</code>格式有三个通道, 而这种格式呢有两个通道. 第一个通道控制颜色, 0 ~ 255, 其中0是黑色, 255是白色; 第二个通道控制是否透明, 0 ~ 255, 0表示透明, 255表示不透明. 所以我们只要将第一个通道的数值, 全部改为255即可o(≧口≦)o</p>
<p>由此, 我在PyCharm的TouchBar上, 配置了<code>Home</code>, <code>End</code>等功能, 某种程度上加快了编辑效率.</p>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>删除一些无用APP</p>
<p>菜单栏 &gt; 关于本机 &gt; 储存空间 &gt; 管理</p>
<p>在其中可以看到一些可删除的系统自带的APP, 删除后可以节省几个G的空间.</p>
</li>
<li><p>设置访达的侧边栏</p>
<p>访达 &gt; 偏好设置</p>
<p>设置开启新窗口为用户根目录. 并将其显示到边栏中.</p>
</li>
</ul>
<h1 id="键盘-1"><a href="#键盘-1" class="headerlink" title="键盘"></a>键盘</h1><p>为了拥有更好的体验, 我买了一块外接无线键盘. 主要就是为了<code>Hone</code>, <code>End</code>, <code>PageUp</code>, <code>PageDown</code>, <code>Delete</code>键. 然后当我买入后, 发现, 这些键是有了, 但是在一些APP上的功能却不是我所想的. 比如想要移动光标到当前行第一列或者最后一列, 使用<code>cmd left/right</code>肯定是可以的, 但是<code>Home</code>和<code>End</code>却不一定行…还好与代码编辑相关的PyCharm与Jupyter可以与预期一致.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(三)</title>
    <url>/2020/03/20/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%B8%89/</url>
    <content><![CDATA[<p>这一篇是本系列的终篇, 主要介绍一个方法.</p>
<p>问题是这样的, 我在笔记本A上搭建博客, 并进行文章的编辑和推送部署. 现在我换了一台笔记本B, 那如果我想在B上面进行继续创作应该怎么办呢? 以后某一天由需要用A进行创作呢?</p>
<p>相信很多同学已经想到了, 没错, 还是基于<code>Github</code>来进行操作.</p>
<a id="more"></a>
<h1 id="创建一个用于管理博客的仓库"><a href="#创建一个用于管理博客的仓库" class="headerlink" title="创建一个用于管理博客的仓库"></a>创建一个用于管理博客的仓库</h1><p>在篇章一中, 我们创建了一个名为<code>xiaoming.github.io</code>的仓库, 这是我们用来管理博客的仓库吗?</p>
<p>当然不是啦, 对比一下咱们本地和它的文件布局就知道, 这个仓库存放的, 是我们<code>$ hexo d</code>时, 推送的博客相关文件的仓库, 并不包含其它的文件.</p>
<p>于是我们还需要创建一个仓库, 比如命名为<code>my_blog</code>.</p>
<h1 id="本地远端关联"><a href="#本地远端关联" class="headerlink" title="本地远端关联"></a>本地远端关联</h1><p>创建好<code>my_blog</code>以后, 进入到<code>笔记本A</code>本地博客的根目录(假设也叫<code>my_blog</code>), 执行一波如下操作, 经常使用<code>Github</code>的同学应该很熟悉:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd my_blog/</span><br><span class="line">$ git init</span><br><span class="line">$ git remote add origin git@github.com:xiaoming/my_blog.git  # 根据自己实际地址进行修改.</span><br></pre></td></tr></table></figure>
<p>那这时候可能有同学就说说了, 啊我知道了, 接下来<code>add commit push</code>就完事了.</p>
<p>但是此时存在一个问题,  在<code>my_blog</code>中, 存在几个<code>Git子模块</code>, 就是一个大的Git仓库里面, 还有小的Git仓库.</p>
<p>还记得篇章一中, 咱们在<code>theme/</code>中利用clone的<code>Next</code>主题吗, 这是一个子模块; 后来我们在执行<code>$ hexo d</code>时, 是将<code>my_blog/.deploy_git/</code>这个子模块push到了Github.</p>
<p>子模块的存在, 在一些开发项目中可以带来很多便利, 但是在咱们这里并不需要, 鉴于本宝宝不怎么使用Git子模块(对, 我就不会​​), 所以需要一些额外的操作, 消除子模块的存在, 以此来保证后续<code>add commit push</code>的顺利进行.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以下操作只用执行一次, 除非以后跟新主题.</span><br><span class="line">$ cd my_blog/theme/</span><br><span class="line">$ rm -rf .git/</span><br></pre></td></tr></table></figure>
<p>在根目录添加<code>.gitignore</code>文件, 并将<code>.deploy_git/</code>写入.</p>
<p>Ok, 现在假设已经执行<code>$ hexo d</code>了, 就可以执行如下代码, 推送到远端的<code>my_blog</code>仓库了:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git add -A</span><br><span class="line">$ git commit -m &quot;happy day&quot;</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>
<h1 id="在另外一台电脑上"><a href="#在另外一台电脑上" class="headerlink" title="在另外一台电脑上"></a>在另外一台电脑上</h1><p>经过上面的一顿操作, 接下来就是要在<code>笔记本B</code>上写博客了.</p>
<p>首先我们要保证<code>笔记本B</code>上也有相应的轮子<code>Git &amp; Node.js</code>, 照着篇章一就ok.</p>
<p>然后, 假设我们也在<code>笔记本B</code>上创建了一个<code>my_blog</code>的文件夹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd my_blog/</span><br><span class="line">$ git init</span><br><span class="line">$ git remote add origin git@github.com:xiaoming/my_blog.git</span><br><span class="line">$ git pull origin master</span><br></pre></td></tr></table></figure>
<p>为了验证是否移植成功, 可以在本地看一下效果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
<p>如果在<code>localhost:4000</code>中看到了自己的博客, 说明一切尽在掌握.</p>
<p>那接下来, 就可以在<code>笔记本B</code>写博客, 推送部署了.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(二)</title>
    <url>/2020/03/20/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 记录了使用<code>Hexo</code>来搭建博客的基础操作, 这一篇紧接上一篇, 来介绍更多的东西.</p>
<p>因为后续可能为博客添加更多的功能, 所以本篇章后续还会更新.</p>
<a id="more"></a>
<h1 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h1><p>一般来说都会保留一个页面, 来展示博主的一些信息, <code>about</code>或者<code>关于</code>页面就是用来做这个的.</p>
<p>其文件路径在<code>source/about/index.md</code>, 按照自己想法, 利用<code>Markdown</code>的语法进行编辑就好.</p>
<h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><p>尽管一个良好的博客, 有<code>分类</code>或者<code>标签</code>, 来帮助大家快速找到自己想要看的文章, 但是当文章数量日渐增长, 同时没有找到对应的<code>分类</code>时, 怎么办? </p>
<p>此时就需要<code>本地搜索</code>.</p>
<p>感恩大神, 如果没有额外的需求, 我们只需要做如下操作, 就能拥有<code>本地搜索</code>的功能.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-generator-searchdb</span><br></pre></td></tr></table></figure>
<p>在根目录下的<code>_config.yml</code>中添加如下配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  format: html</span><br></pre></td></tr></table></figure>
<p>重新编译博客后, 会发现多了一个<code>搜索</code>的功能, 敲好用!</p>
<h1 id="流量统计"><a href="#流量统计" class="headerlink" title="流量统计"></a>流量统计</h1><p>我想大部分同学还是想知道, 自己的博客/文章被多少人看过, 被看过多少次这样的信息.</p>
<p>统计这样的信息有不少方法和途径, 这里我使用的是一款叫做<code>不蒜子</code>的插件.</p>
<p>使用方法非常简单, 再次感恩大神.</p>
<p>只需要将主题文件中的<code>_config.yml</code>修改一下就行, 就这么简单.​</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Show Views / Visitors of the website / page with busuanzi.</span><br><span class="line"># Get more information on http://ibruce.info/2015/04/04/busuanzi</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br><span class="line">  total_visitors: true</span><br><span class="line">  total_visitors_icon: user</span><br><span class="line">  total_views: true</span><br><span class="line">  total_views_icon: eye</span><br><span class="line">  post_views: true</span><br><span class="line">  post_views_icon: eye</span><br></pre></td></tr></table></figure>
<p>然后在网页的底部会显示整个博客的流量, 文章的开头会显示本篇文章的流量.</p>
<p>这里再说一句, 在本地测试<code>$ hexo s</code>时看到的流量数据是虚假的.</p>
<h1 id="插图"><a href="#插图" class="headerlink" title="插图"></a>插图</h1><p>想要在文章中插图, 方式主要有两种: </p>
<ul>
<li>将图片放置与网络图床, 然后直接引用就行.</li>
<li>将图片放置于博客根目录下的某目录中, 然后再用相对路径/绝对路径引用.</li>
</ul>
<p>这里介绍相对路径的方法, 这样我认为更加方便管理一些. 方法非常简单, 即在文章存在的文件夹中, 创建一个同文章名的文件夹, 用于存放该文章的图片.</p>
<p>如文章路径为<code>source/_post/happy_day.md</code>, 放置图片的文件夹路径为<code>source/_post/happy_day/</code></p>
<p>而在文章中插入图片(假设图片路径为<code>source/_post/happy_day/happy.png</code>)时, 语法为<code>![我是图](happy.png)</code></p>
<p>如果觉得每次同时创建文章和放置图片的同名文件夹麻烦, 可以修改<code>_config.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
<p>以后每次创建文章<code>$ hexo new xxx</code>时, 就会同时出现<code>xxx.md</code>和<code>xxx/</code>.</p>
<h1 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h1><p>使用默认的渲染引擎<code>hexo-renderer-marked</code>时, 会和Latex各种冲突, 公式换行为<code>\\</code>, 下标<code>_</code>等.</p>
<p>这个问题比较蛋疼, 试了好些办法, 最后选择的一种办法是更换引擎, 更换为<code>hexo-renderer-kramed</code>这个渲染引擎, 该引擎其实就是在<code>hexo-renderer-marked</code>基础上, 针对<code>mathjax</code>进行了改进, 使其优先支持Latex公式语法.</p>
<p>更换引擎代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm uninstall hexo-renderer-marked --save</span><br><span class="line">$ npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>若更换过程中报错, 则根据错误信息提示, 进行相应操作即可.</p>
<p>但是<code>hexo-renderer-kramed</code>只能够解决单行的渲染问题, 行内的仍然会出问题, 需要手工矫正一下, 修改<code>node_modules\kramed\lib\rules\inline.js</code>:</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改第 11 行</span></span><br><span class="line"><span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 修改第 20 行</span></span><br><span class="line"><span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> 重启 hexo</span><br><span class="line">hexo clean </span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<p>最后, 若发现浏览器中仍然无法正常显示换行, 尝试更换浏览器. 在Windows系统下, 火狐浏览器即使更换了引擎也不行, 但谷歌浏览器可以.</p>
<p>Chrome大法好啊(｡･∀･)ﾉﾞ</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(一)</title>
    <url>/2020/03/17/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%B8%80/</url>
    <content><![CDATA[<p>这里开始记录自己构建博客的流程, 帮助别人也是帮助自己.</p>
<p>当然, 每个人的需求是不一样的. 有的人不需要一个自己独有的博客, 因为现在有各种很好的流量平台, 知乎, 微信公众号等. 同时, 不同人的审美是不一样的, 也许我觉得简约明了就很漂亮, 一些人觉得充满各种图案颜色更好看. 对于博客的一些功能, 如社交链接, 打赏, 广告位等, 我暂时是没有啥兴趣的, 而可能一部分人希望加上这些功能, 能够更加充分地展示自己~</p>
<a id="more"></a>
<p>所以, 我这里会将重要的, 我自己需要的一些步骤和流程做记录. 我个人更喜欢Linux系统, 但是工作需要与他人交互, 所以现在使用更多的反而是Windows和Mac OS. 后面的操作都是基于Windows的, 其它的系统如Mac OS, Linux也是类似的.</p>
<p>如果以后有一天, 你看了我的流程, 发现自己在用的时候出错了, 或者你想添加不一样的更多的东西, 建议谷歌​​.</p>
<h1 id="借助别人的轮子"><a href="#借助别人的轮子" class="headerlink" title="借助别人的轮子"></a>借助别人的轮子</h1><p>说起来, 搭建博客, 写网页, 最专业的应该是偏开发, 前端的程序员. 而咱们如果从头去学, 成本太高, 也没必要.</p>
<p>很多大神程序员已经帮我们造好了各种轮子, 我们只要好好利用起来就行了.</p>
<blockquote>
<p>合理使用别人的轮子, 是对造轮子的人的一种尊重.</p>
</blockquote>
<p>而在搭建博客这块, 有几个(我使用)关键元素:</p>
<ul>
<li><strong>Hexo</strong>: 什么是Hexo? Hexo是一个快速, 简洁且高效的博客框架. Hexo使用Markdown(或其它渲染引擎)解析文章, 在几秒内, 即可利用靓丽的主题生成静态页面. <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo官网</a>, 中文的哟.</li>
<li><strong>Next</strong>: 什么是Next? Next是一款可以与Hexo搭配使用的主题风格. <a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">Next主题Github地址</a>.</li>
<li><strong>Github</strong>: 什么是Github? 啊…这…就不解释了哈哈.</li>
</ul>
<p>那么用举栗子的说法, 来解释一下这三个元素, 就是比如你想做一件衣服, 一件能够去参加艺术品展示的衣服. 那么Hexo就是裁缝, 负责将布料做成衣服; Next是设计师, 告诉裁缝怎么做漂亮一些; Github是模特, 将衣服穿上向大家展示.</p>
<p>需要说的是, 与上述的三个轮子, 具有相似功能的其它轮子也有一些, 不一定非要使用这几个轮子.</p>
<p>同时需要说明的是我使用的<code>Hexo</code>版本是<code>v3.9.0</code>, <code>Next</code>版本是<code>v7.2.0</code>.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><ul>
<li><a href="https://git-scm.com/download" target="_blank" rel="noopener">安装地址</a>.</li>
</ul>
<h2 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h2><ul>
<li><a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">安装地址</a>.</li>
</ul>
<h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>在Windows下使用PowerShell或者GitBash(更加推荐GitBash), 输入如下代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h1 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h1><h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>安装 Hexo 完成后, 执行下列命令, Hexo 将会在指定文件夹中新建所需要的文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ cd &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
<p>新建完成后, 指定文件夹的目录如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>_config.yml</strong></p>
<p>网站的配置信息, 可以在此配置大部分的参数. 后续再做详细说明.</p>
</li>
<li><p><strong>scaffold</strong></p>
<p>模板文件夹, 文章的布局会根据模板来进行建立.</p>
</li>
<li><p><strong>source</strong></p>
<p>资源文件夹是存放用户资源(文章)的地方. </p>
<p>除 <code>_posts</code> 文件夹之外, 开头命名为 <code>_</code> (下划线)的文件 / 文件夹和隐藏的文件将会被忽略.</p>
<p>Markdown 和 HTML 文件会被解析并放到 <code>public</code> 文件夹, 而其他文件会被拷贝过去.</p>
</li>
<li><p><strong>themes</strong></p>
<p>主题文件夹, Hexo根据选定的主题来生成静态页面.</p>
</li>
</ul>
<h2 id="远端"><a href="#远端" class="headerlink" title="远端"></a>远端</h2><p>上文说到, 需要借助Github来帮助我们展示博客, 因此需要在Github上创建对应的仓库.</p>
<p>用于网站的仓库, 相比普通仓库多了一些限制和操作.</p>
<ul>
<li><p><strong>仓库名称</strong></p>
<p>必须满足如下格式: Github用户名.github.io, 如名字叫xiaoming, 就是xiaoming.github.io.</p>
</li>
<li><p><strong>仓库公开</strong></p>
<p>要给大家伙看的嘛, 自然不能是private的, 必须是public的.</p>
</li>
<li><p><strong>Github Pages</strong></p>
<p>进入到仓库的settings中, 往下面拉到Github Pages的版块, 进行相应配置.</p>
<p>可以填入自定义域名, 这个后续会加以说明.</p>
<p>建议勾选<code>Enforce HTTPS</code>, 这样Github会免费把你的网站升级成HTTPS, 更加安全.</p>
</li>
</ul>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="添加主题"><a href="#添加主题" class="headerlink" title="添加主题"></a>添加主题</h2><p>我有点忘了是否安装Hexo并建站以后, <code>themes</code>文件夹下会自带<code>Next</code>主题.</p>
<p>若没有, 需要自己安装主题. 以<code>Next</code>主题为例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd themes/</span><br><span class="line">$ git init</span><br><span class="line">$ git clone https://github.com/theme-next/hexo-theme-next.git</span><br></pre></td></tr></table></figure>
<p>主题文件夹的一般布局:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── languages</span><br><span class="line">├── layout</span><br><span class="line">├── scripts</span><br><span class="line">└── source</span><br></pre></td></tr></table></figure>
<h2 id="Hexo-config-yml"><a href="#Hexo-config-yml" class="headerlink" title="Hexo_config.yml"></a>Hexo_config.yml</h2><p>一下对部分我认为较为重要, 以及基础的参数进行介绍.</p>
<p><strong>网站相关</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>title</code></td>
<td style="text-align:center">网站标题</td>
</tr>
<tr>
<td style="text-align:center"><code>subtitle</code></td>
<td style="text-align:center">网站副标题</td>
</tr>
<tr>
<td style="text-align:center"><code>description</code></td>
<td style="text-align:center">网站描述</td>
</tr>
<tr>
<td style="text-align:center"><code>keywords</code></td>
<td style="text-align:center">网站的关键词。使用半角逗号 <code>,</code> 分隔多个关键词。</td>
</tr>
<tr>
<td style="text-align:center"><code>author</code></td>
<td style="text-align:center">您的名字</td>
</tr>
<tr>
<td style="text-align:center"><code>language</code></td>
<td style="text-align:center">网站使用的语言, 我使用的zh-Hans</td>
</tr>
<tr>
<td style="text-align:center"><code>timezone</code></td>
<td style="text-align:center">网站时区。Hexo 默认使用您电脑的时区。请参考 <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" target="_blank" rel="noopener">时区列表</a> 进行设置，如 <code>America/New_York</code>, <code>Japan</code>, 和 <code>UTC</code> 。一般的，对于中国大陆地区可以使用 <code>Asia/Shanghai</code>。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>网址相关</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>url</code></td>
<td style="text-align:center">网址, 修改为Github仓库地址, 如<a href="https://xiaoming.github.io/" target="_blank" rel="noopener">https://xiaoming.github.io/</a></td>
</tr>
<tr>
<td style="text-align:center"><code>root</code></td>
<td style="text-align:center">网站根目录, 默认为/</td>
</tr>
</tbody>
</table>
</div>
<p><strong>主题相关</strong></p>
<p>指定使用的主题. 如上文中我们在<code>theme</code>文件中安装了<code>Next</code>主题, 则可将其对应文件名进行设置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theme: hexo-theme-next</span><br></pre></td></tr></table></figure>
<p><strong>部署相关</strong></p>
<p>这一部分的配置, 是告诉Hexo你要用什么方法来将博客部署到服务器上. 当然我是用的Github.</p>
<p>首先安装hexo-deployer-git:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>然后对应修改配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:xiaoming/xiaoming.github.io.git</span><br></pre></td></tr></table></figure>
<p>到这里, 基础的配置就完成了, 接下来进行主题的相关配置.</p>
<h2 id="主题-config-yml"><a href="#主题-config-yml" class="headerlink" title="主题 _config.yml"></a>主题 _config.yml</h2><p>相比Hexo的_config.yml, 主题文件中的_config.yml有更多的内容, 下面针对部分内容进行修改.</p>
<ul>
<li><p><strong>菜单设置</strong></p>
<p>这里设置你的博客有哪些主要的版块, 我的设置保留了主页, 关于, 分类, 归档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home</span><br><span class="line">  about: /about/ || user</span><br><span class="line">  tags: /tags/ || tags</span><br><span class="line">  categories: /categories/ || th</span><br><span class="line">  archives: /archives/ || archive</span><br><span class="line">  #schedule: /schedule/ || calendar</span><br><span class="line">  #sitemap: /sitemap.xml || sitemap</span><br><span class="line">  #commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo new page tags</span><br><span class="line">$ hexo new page categories</span><br><span class="line">$ hexo new page about</span><br></pre></td></tr></table></figure>
<p>编辑对应的index.md文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tags</span><br><span class="line">date: 2020-09-20 15:41:00</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comment: false</span><br><span class="line"></span><br><span class="line"># categories</span><br><span class="line">date: 2019-07-23 22:11:08</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line"></span><br><span class="line"># about</span><br><span class="line">date: 2019-07-23 01:01:43</span><br><span class="line">comment: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>子主题设置</strong></p>
<p>可以认为<code>Next</code>主题下包含了4个子主题: <code>Muse</code>, <code>Mist</code>, <code>Pisces</code>, <code>Gemini</code>.</p>
<p>根据个人喜好进行选择, 不满意可更换. 我选择的<code>Mist</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>侧边栏设置</strong></p>
<p>打开侧边栏:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">site_state: true</span><br></pre></td></tr></table></figure>
<p>目录显示相关:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  # Automatically add list number to toc.</span><br><span class="line">  number: true</span><br><span class="line">  # If true, all words will placed on next lines if header width longer then sidebar width.</span><br><span class="line">  wrap: false</span><br><span class="line">  # If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span><br><span class="line">  expand_all: false</span><br><span class="line">  # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` in Front-matter.</span><br><span class="line">  max_depth: 6</span><br></pre></td></tr></table></figure>
<p>在左边还是右边 , 宽度, 是否自动显示:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  # Sidebar Position.</span><br><span class="line">  #position: left</span><br><span class="line">  position: right</span><br><span class="line">  </span><br><span class="line">  # Manual define the sidebar width. If commented, will be default for:</span><br><span class="line">  # Muse | Mist: 320</span><br><span class="line">  # Pisces | Gemini: 240</span><br><span class="line">  #width: 300</span><br><span class="line"></span><br><span class="line">  # Sidebar Display (only for Muse | Mist), available values:</span><br><span class="line">  #  - post    expand on posts automatically. Default.</span><br><span class="line">  #  - always  expand for all pages automatically.</span><br><span class="line">  #  - hide    expand only when click on the sidebar toggle icon.</span><br><span class="line">  #  - remove  totally remove sidebar including sidebar toggle.</span><br><span class="line">  display: hide</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>代码版块设置</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">codeblock:</span><br><span class="line">  # Code Highlight theme</span><br><span class="line">  # Available values: normal | night | night eighties | night blue | night bright</span><br><span class="line">  # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line">  highlight_theme: normal</span><br><span class="line">  # Add copy button on codeblock</span><br><span class="line">  copy_button:</span><br><span class="line">    enable: true</span><br><span class="line">    # Show text copy result.</span><br><span class="line">    show_result: true</span><br><span class="line">    # Available values: default | flat | mac</span><br><span class="line">    style:</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>字体设置</strong></p>
<p>暂时没有进行相关设置, 感觉默认的就还好.</p>
<p>如果要设置的话, 根据自带的帮助说明应该也能设置.</p>
<p>设置过多的字体的话, 博客的加载速度会变慢.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">font:</span><br><span class="line">  enable: false</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>数学公式</strong></p>
<p>如果文章里面包含了数学公式, 需要利用进行如下配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line">  # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">  # That is it only render those page which has `mathjax: true` in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: true</span><br><span class="line"></span><br><span class="line">  engine: mathjax</span><br><span class="line">  #engine: katex</span><br></pre></td></tr></table></figure>
<p>同时, 在需要解析数学公式的文章前面, 添加<code>mathjax: true</code>.</p>
</li>
</ul>
<p>更多的配置, 以后会进行补充说明.</p>
<h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>现在, 安装了需要的轮子, 并完成了基本的配置, 可以开始写博客了.</p>
<p>下面介绍几个基础但是够用的操作, 需要说的是以下操作均需要在博客文件夹中进行, 就是<code>hexo init</code>的那个.</p>
<ul>
<li><p><strong>新建文章</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>
<p>默认会在<code>source/_post/</code>中生成对应的md文件. 我们可以将文章写在里面.</p>
</li>
<li><p><strong>生成静态网页</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>本地查看</strong></p>
<p>在部署到Github之前, 可以先看看自己的博客, 文章是啥样的, 需不需要修改, 是否满足自己预期.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>部署</strong></p>
<p>觉得一切OK后, 将其部署到Github上.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>清空历史信息</strong></p>
<p>类似于删除Git仓库的历史commit, 不会影响当前现有的内容.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>等成功部署之后, 就可以在对应网址下, 看到自己的博客了. </p>
<p>需要说的是部署是由延时的, 有时候需要十来分钟的时间. 所以如果你是第一次部署, 打开自己的链接, 发现404, 不要慌, 起来喝一杯热水.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>写在最前面的</title>
    <url>/2019/07/29/%E5%85%B6%E5%AE%83/%E5%86%99%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%E7%9A%84/</url>
    <content><![CDATA[<h1 id="写博客的目的"><a href="#写博客的目的" class="headerlink" title="写博客的目的"></a>写博客的目的</h1><p>写博客这件事情, 是我从很久以前就想要做的, 但是一直拖到现在, 我想一来是我有点拖延症, 二来是没有找到合适的网站/网站外观.</p>
<p>现在国内一般可以用来写博客的并不多, 比较好的有简书, 博客园, 剩下的一些比如CSDN, 知乎…虽然它们在其它一些方面很有用, 但是就写博客来说叭太行. 而简书和博客园的外观以及功能虽然可以一定程度上地定制, 但是终究不是很喜欢, 还有一点是有广告…</p>
<a id="more"></a>
<p>所以, 有一个自己相对独立的博客就有必要了, 但是完全独立对我来说并不那么需要, 于是依托于github的免费服务器, 再利用Hexo的定制功能就是比较理想的选择.</p>
<p>关于怎么用上面那个方法搭建博客以后也许我会写一下, 步骤正确以后非常简单. 而对于博客的定制我也还在慢慢摸索, 现在我的博客很简洁, 以后会逐渐加入搜索, 评论等功能, 这样能更好地方便读者, 以及和读者进行互动.</p>
<p>Ok, 有了写博客的地方, 那我接下来要写一些什么样的文章呢?</p>
<p>首先我想写一写一些类似于聊天一样的文章, 我把这类文章划分为”摆龙门阵”, 嘿嘿, 因为我是巴渝的人.</p>
<p>然后我的工作是需要用到统计分析, 机器学习这些东西的, 所以也会写一些关于这方面的文章. 如果写我特别熟悉的部分, 算是一种对知识结构的梳理; 如果写我也刚接触的知识, 那能够让我尽可能去搞明白其前因后果, 自己大概懂了和写出来想让别人也懂是两码事, 至少…能够顺利地import, 做一个快乐的调包侠对吧~</p>
<p>还有一些东西比如图算法, 自然语言处理, 这些我暂时用不到但是以后很可能会用到的技能, 我会一边学习, 一边总结分享.</p>
<p>最后还有数据结构和算法题, 我估计我会两三天做一道. 刷题可以同时锻炼思维能力和代码能力, 无论是一个合格的程序员, 还是以后有一天不靠码代码维生了, 我认为都是很有用的. 而且, 当在LeetCode上提交代码, 得到一个大大的绿色的Accept的时候, 大脑会分泌多巴胺!</p>
<p>此外, 写博客还有一个目的, 大概是在两年以前, 我接触并逐渐开始理解”开源”这个词, 一开始我并不理解它的意思, 而作为一个一路自学过来的人, 现在可以说如果没有”开源”, 就不会有现在的我. </p>
<blockquote>
<p>我相信, 一定程度上的开源, 可以让世界变得更加美好!</p>
</blockquote>
<p>当你要主动去学习一些东西的时候, 那么就去找资料, 可以谷歌(大伙懂的…), 可以去一些社区寻求答案. ApacheCN, Github, 知乎, Kaggle, YouTube等等, 很多的人在分享他们的代码, 他们的经验, 无数的知识躺在那里, 等着想要学习的人去学习. 而现在, 我受益于开源, 也想要回报开源, 这也是我写博客的一大原因之一.</p>
<p>总结下就是:</p>
<ul>
<li>觉得写博客很酷!</li>
<li>写博客可以帮我更好地理解知识(自己都不懂还来教别人???)</li>
<li>可以和世界各地的人(理论上)的人交流问题.</li>
<li>回报开源.</li>
</ul>
<h1 id="我的黄金三年"><a href="#我的黄金三年" class="headerlink" title="我的黄金三年"></a>我的黄金三年</h1><p>趁我刚毕业没多久, 一些东西还没有忘记, 把一些对我人生改变很大的事情记录下来.</p>
<p>一直一来很多事情都能对一个人的人生产生影响, 有好有坏, 有大有小, 而我之所以要写下了, 如小标题所言, 我认为这些事情是有意义的. 它们也许改变了我的世界线, 也许世界线会收束, 它们的影响其实很小, 但都是我的黄金三年.</p>
<h2 id="放弃了物理"><a href="#放弃了物理" class="headerlink" title="放弃了物理."></a>放弃了物理.</h2><p>说出来可能你不信, 我当初是想成为一名物流学家的, 可以拿诺贝尔奖的那种, 爱因斯坦是我的偶像.</p>
<p>我以前物理学得应该还算好吧, 其次我对这个世界感兴趣, 但这些并不是让我学物理的全部原因, 还有一个很重要的原因, 我想要时光机, 我想要长生不老. 而我认为达成这两样, 或者就时光机来说, 正确的路径, 就是物理.</p>
<p>我想要时光机, 回到过去, 改变一些东西.</p>
<p>而后的一段时间, 我没有好好学习, 是的, 和大多数大学生一样, 翘课, 考试临时抱佛脚, 打游戏. 还好我打游戏比较厉害, 相比玩得菜的能获得更多的快乐哈哈哈. 本来没好好学习想甩锅的, 想想还是算了, 脑子长自己头上, 别人把你关起来不让你学了?</p>
<p>再到后来, 经过大概一个假期的纠结, 我决定放弃当初那个要成为物理学家的想法. </p>
<p>当时我哭了吗? 应该没有吧.</p>
<blockquote>
<p>这个世界, 所有世界线, 都不可能存在时光机!</p>
</blockquote>
<h2 id="谈了场恋爱"><a href="#谈了场恋爱" class="headerlink" title="谈了场恋爱?"></a>谈了场恋爱?</h2><p><code>自我A</code>: 痛苦吗?</p>
<p><code>自我B</code>: 很痛苦!</p>
<p><code>自我A</code>: 问心无愧吗?</p>
<p><code>自我B</code>: 问心无愧!</p>
<p><code>自我A</code>: 浪费的时间可惜吗?</p>
<p><code>自我B</code>: 可惜! 但也许是我必须经历的, 我对世界的认知还太少, 我的自控能力还太差.</p>
<p><code>自我A</code>: 熬夜, 暴饮暴食, 不锻炼, 现在你的身体素质不太好啊.</p>
<p><code>自我B</code>: 是的, 不过一切都在慢慢好转. 我现在没有暴饮暴食, 不会过度熬夜, 偶尔会进行室内运动. 等春暖花开, 我会去跑步的.</p>
<p><code>自我A</code>: 有总结过经验吗?</p>
<p><code>自我B</code>: 哈哈哈, 非要说的话, “不做舔狗”吧.</p>
<p><code>自我A</code>: 你还记得多少?</p>
<p><code>自我B</code>: 时间自然会帮我忘记.</p>
<p><code>自我A</code>: 那个小男孩还活着吗?</p>
<p><code>自我B</code>: 这你直接问<code>自我C</code>了啊…小C很好, 现在睡得很香, 还说睡醒了一起开黑呢.</p>
<h2 id="交了群朋友"><a href="#交了群朋友" class="headerlink" title="交了群朋友!"></a>交了群朋友!</h2><p>在研究生期间, 室友, 同学, 师兄师姐对我都挺好的, 不知道从哪说起, 就想到啥说啥吧.</p>
<p>先说室友A, 我的Linux大门是他帮忙打开的, 还记得当时我的小黑在win7上卡的快成PPT了, 他向我安利了Ubuntu, 并帮我用U盘进行了安装, 从此过后(直到毕业)再没用win系统做过正事(除了玩游戏).</p>
<p>再说室友B, 在我放弃物理后, 都是能混就混了, 没错我就是个混子嗷. 室友B学习比较扎实, 关键是他乐于传教, 有不懂的知识可以问, 不会的题可以抄…某种意义上, 要是没他帮我, 我可能毕不了业.</p>
<p>还有室友C, 接下来就亲切的称呼他为宝子哥好了, 宝子哥乃教育学某大师关门弟子, 造诣自不必多言. 精通心理学, 心理暗示, 传销…啊不没传销, 反正就是很厉害的一个人. 我还记得他对我的谆谆教诲:</p>
<blockquote>
<p>没人对你说过, 你很帅吗?</p>
<p>诶, 你真的觉得我帅吗?</p>
<p>当年本科的时候, 大伙来的时候就我一个有女朋友, 毕业的时候基本上都有女朋友了.</p>
<p>老哥, 你废了.</p>
<p>你有没有听说过…PXX(少儿不宜, 手动屏蔽).</p>
</blockquote>
<p>除了室友, 好几个同学人都很好, 能遇到这么好的一群人实属幸事!</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
</search>

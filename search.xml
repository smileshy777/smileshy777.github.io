<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Linux常用命令</title>
    <url>/2021/03/24/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>掌握一些基础的Linux命令是非常必要的事情嗷~</p>
<a id="more"></a>
<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>显示指定工作目录下之内容, list的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示当前路径下内容.</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示指定路径下内容.</span></span><br><span class="line">ls path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示所有文件及目录, 包含隐藏文件.</span></span><br><span class="line">ls -a</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示详细信息.</span></span><br><span class="line">ls -lh</span><br></pre></td></tr></table></figure>
<h3 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h3><p>改变目标文件夹, change directory的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入目标路径.</span></span><br><span class="line">cd path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入上一级路径.</span></span><br><span class="line">cd ..</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入家目录.</span></span><br><span class="line">cd ~</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入上一次路径.</span></span><br><span class="line">cd -</span><br></pre></td></tr></table></figure>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>复制文件或文件夹, copy的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 复制文件.</span></span><br><span class="line">cp source dest</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 复制文件夹.</span></span><br><span class="line">cp -r source dest</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 复制文件到文件夹.</span></span><br><span class="line">cp source directory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制覆盖.</span></span><br><span class="line">cp -f source dest</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 覆盖前询问.</span></span><br><span class="line">cp -i source dest</span><br></pre></td></tr></table></figure>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>移动文件或文件夹, move的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移动文件.</span></span><br><span class="line">mv source dest</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制覆盖.</span></span><br><span class="line">mv -f source dest</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 覆盖前询问.</span></span><br><span class="line">mv -i source dest</span><br></pre></td></tr></table></figure>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>删除文件或者文件夹, remove的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除文件.</span></span><br><span class="line">rm file</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除前逐一确认.</span></span><br><span class="line">rm -i files</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制删除.</span></span><br><span class="line">rm -f file</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归删除, 包含文件夹.</span></span><br><span class="line">rm -r dirName</span><br></pre></td></tr></table></figure>
<h3 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h3><p>若文件存在, 则修改文件的时间; 若不存在, 则创建文件.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改时间或者创建.</span></span><br><span class="line">touch file</span><br></pre></td></tr></table></figure>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>创建文件夹, make directories的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建文件夹.</span></span><br><span class="line">mkdir dirName</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若文件夹父目录不存在, 则一起创建.</span></span><br><span class="line">mkdir -p dirName0/dirName1</span><br></pre></td></tr></table></figure>
<h3 id="tree"><a href="#tree" class="headerlink" title="tree"></a>tree</h3><p>以树状图列出文件结构.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示指定目录的文件结构.</span></span><br><span class="line">tree path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制显示最大级别.</span></span><br><span class="line">tree -L n path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 只显示文件夹.</span></span><br><span class="line">tree -d path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 能正常显示中文.</span></span><br><span class="line">tree -N path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示所有文件.</span></span><br><span class="line">tree -a path</span><br></pre></td></tr></table></figure>
<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>查看文件内容, 创建文件, 合并文件, 追加文件内容等, concatenate的缩写.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接显示文件全部内容.</span></span><br><span class="line">cat file</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加行号.</span></span><br><span class="line">cat -n file</span><br></pre></td></tr></table></figure>
<h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><p>分屏显示文件内容.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示文件内容.</span></span><br><span class="line">less file</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按行上下移动, 使用上下方向键.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按页前后翻滚, 使用F和B键.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 跳转到第一行, 输入g. 跳转到最后一行, 输入G.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 跳转到第n行, 输入ng.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> /xxx搜索, 回车后使用n/N继续搜索.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出按Q键.</span></span><br></pre></td></tr></table></figure>
<h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p>搜索文件内容, Globally search a Regular Expression and Print的缩写.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示文件中匹配模式的行.</span></span><br><span class="line">grep pattern file</span><br><span class="line"></span><br><span class="line"><span class="comment"># -n显示匹配行及行号.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -v显示不匹配的行.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -忽略大小写.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ^xxx, 以xxx开头.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># xxx$, 以xxx结尾.</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>颜文字</title>
    <url>/2021/02/16/%E5%85%B6%E5%AE%83/%E9%A2%9C%E6%96%87%E5%AD%97/</url>
    <content><![CDATA[<p>下面收集一些个人感觉比较有意思的颜文字.</p>
<a id="more"></a>
<h1 id="高兴"><a href="#高兴" class="headerlink" title="高兴"></a>高兴</h1><p>(๑•̀ㅂ•́)و✧</p>
<p>ヾ(≧▽≦*)o</p>
<p>o(*≧▽≦)ツ</p>
<p>ヽ(✿ﾟ▽ﾟ)ノ</p>
<p>(´▽`ʃ♡ƪ)</p>
<p>╰(°▽°)╯</p>
<p>(๑´ㅂ`๑)</p>
<p>ε(*´･∀･｀)зﾞ</p>
<p>(　ﾟ∀ﾟ) ﾉ♡</p>
<p>(｡･∀･)ﾉﾞ</p>
<p>(✧◡✧)</p>
<h1 id="卖萌"><a href="#卖萌" class="headerlink" title="卖萌"></a>卖萌</h1><p>(&gt;▽&lt;)</p>
<p>（○｀ 3′○）</p>
<p>(っ*´Д`)っ</p>
<p>(≧∇≦)ﾉ</p>
<p>ヾ(´･ω･｀)ﾉ</p>
<p>o(〃’▽’〃)o</p>
<p>（〃｀ 3′〃）</p>
<h1 id="震惊"><a href="#震惊" class="headerlink" title="震惊"></a>震惊</h1><p>Σ(っ °Д °;)っ</p>
<p>Ｏ(≧口≦)Ｏ</p>
<p>(￣△￣；)</p>
<p>(○´･д･)ﾉ</p>
<h1 id="生气"><a href="#生气" class="headerlink" title="生气"></a>生气</h1><p>(* ￣︿￣)</p>
<p>( ｀д′)</p>
<p>(σ｀д′)σ</p>
<h1 id="无奈"><a href="#无奈" class="headerlink" title="无奈"></a>无奈</h1><p>╮(╯▽╰)╭</p>
<p>┑(￣Д ￣)┍</p>
<h1 id="害羞"><a href="#害羞" class="headerlink" title="害羞"></a>害羞</h1><p>(/▽＼)</p>
<p>(<em>/ω＼</em>)</p>
<p>つ﹏⊂</p>
<h1 id="哭哭"><a href="#哭哭" class="headerlink" title="哭哭"></a>哭哭</h1><p>(ノへ￣、)</p>
<p>o(TヘTo)</p>
<p>(＞﹏＜)</p>
<h1 id="亲亲"><a href="#亲亲" class="headerlink" title="亲亲"></a>亲亲</h1><p>(*￣3￣)╭</p>
<p>（づ￣3￣）づ╭❤～</p>
<p>o(*￣3￣)o</p>
<h1 id="睡了"><a href="#睡了" class="headerlink" title="睡了"></a>睡了</h1><p>(￣o￣) . z Z</p>
<p>(つω｀)～</p>
<h1 id="再见"><a href="#再见" class="headerlink" title="再见"></a>再见</h1><p>ヾ(￣▽￣)Bye~Bye~</p>
<p>(。・_・)/~~~</p>
<p>(o´ω`o)ﾉ</p>
<p>（＾∀＾●）ﾉｼ</p>
<h1 id="摸头"><a href="#摸头" class="headerlink" title="摸头"></a>摸头</h1><p>( ´･･)ﾉ(._.`)</p>
<h1 id="恰饭"><a href="#恰饭" class="headerlink" title="恰饭"></a>恰饭</h1><p>ψ(｀∇´)ψ</p>
<p>Ψ(￣∀￣)Ψ</p>
<p>(￣～￣) 嚼！</p>
<h1 id="害怕"><a href="#害怕" class="headerlink" title="害怕"></a>害怕</h1><p>┐(T.T ) ( T.T) ノ</p>
<p>ヽ(*。&gt;Д&lt;)o゜</p>
<h1 id="囧囧"><a href="#囧囧" class="headerlink" title="囧囧"></a>囧囧</h1><p>○|￣|_</p>
<p>ε=ε=ε=(~￣▽￣)~</p>
<p>━━∑(￣□￣*|||━━</p>
<p>ε=ε=ε=┏(゜ロ゜;)┛</p>
<p>(*￣rǒ￣)</p>
<h1 id="加油"><a href="#加油" class="headerlink" title="加油"></a>加油</h1><p>d=====(￣▽￣*)b</p>
<p>o(￣▽￣)ｄ</p>
<p>(o゜▽゜)o☆</p>
<p>(ง •_•)ง</p>
<p>ヾ(●゜ⅴ゜)ﾉ</p>
<p>ㄟ(≧◇≦)ㄏ</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>FTRL优化算法</title>
    <url>/2020/10/22/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/FTRL%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>总所周知, L1正则可以较好地让模型参数具有稀疏性, 但是单纯的L1正则, 在一些时候并不是万能的.</p>
<p>这一篇介绍一种可以带来稀疏性的优化算法, FTRL.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>首先说说为啥需要模型稀疏性, 或者说稀疏性的好处有哪些.</p>
<p>比如对于线性模型来说, 可能原本在训练的时候, 输入的特征维度是十分巨大的, 上万甚至上亿都有可能. 而如果在线上使用模型时, 每次都要传输这么多的特征, 还是会带来一些麻烦的. 所以如果将模型中的一些, 甚至大量参数变成0, 并还能够保证模型的效果, 那么对于参数为0的特征, 就没必要输入, 从而简化了模型, 方便了部署与在线调用.</p>
<p>同时, 模型参数的稀疏性, 还能一定程度上提高模型的可解释性, 使人知道哪些特征是与任务最相关的.</p>
<p>此外, 使用能够让模型参数具有稀疏性的方法(如L1正则), 来训练模型时, 相比其它一些方法(如L2), 可以在保持模型效果的同时, 对模型参数带来更大的改变, 以适应新的数据变化, 这在Online Learning(在线学习)中, 也是比较重要的.</p>
<p>既然模型参数稀疏性有这么多好处, 那么使用L1正则不就行了吗, 为什么还要扯感觉挺复杂的FTRL呢?</p>
<p>原因是L1产生模型参数稀疏性是有一定的条件的, 比如当我们在一些小数据集上训练线性模型时, 通常是用整个数据集一起拿来训练, 这时候使用L1正则效果是比较明显的. 而当数据变得很大时, 无法全部一次性训练, 那么就分成多个batch, 每次训练一部分小数据. 或者在线学习时, 每次只用最近的一点数据来更新参数. 问题这时候就出现了, 当使用全量数据一起进行训练时, 模型参数会朝着一个方向稳定地移动, 此时一些不显著的特征, 对应的参数由于L1正则的作用, 就会变成0; 但是当每次只使用一小部分数据时, 梯度是相对不稳定的, 模型参数可能也不会朝着一个方向稳定移动, 想要使其趋于0, 就更加困难.</p>
<p>模型参数的稀疏性很重要, 同时在小批量训练模型时, 单纯的L1正则并不能获得很好的稀疏性, 那么能不能考虑从其它方面入手, 比如…优化算法.</p>
<p>FTRL: 没错, 正是在下♪(^∇^*)</p>
<p>FTRL的全称是Follow the Regularized Leader, 是一种优化算法(如SGD), 并且其侧重点是让模型保持原有效果的同时, 参数具有更好的稀疏性.</p>
<p>下面就来介绍FTRL的原理.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>在具体讲解FTRL之前, 先来说一下它的前辈们.</p>
<p>这里会用尽量少的公式(才不是懒OvO), 来把算法的思想讲明白.</p>
<h2 id="TG"><a href="#TG" class="headerlink" title="TG"></a>TG</h2><p>Truncated Gradient, 即梯度截断法, 思想是比较简单直接的.</p>
<p>不就是想要让模型参数变得稀疏吗, 直接设定一个阈值, 让小于阈值的模型参数等于0即可.</p>
<p>但是这样做似乎太粗暴了, 可能有些模型参数还正在更新, 只是现在比较小而已, 这样直接使其等于0, 会比较显著地影响到模型效果.</p>
<p>所以做了一些改进, 假设模型的某一个参数为$w$, 迭代期数为$t$, 迭代周期$k$, 模型参数阈值$\tau$, 则:</p>
<ul>
<li><p>$t\%k\ne 0$时, 进行正常的梯度更新:</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t-\eta g_t</script></li>
<li><p>$t\%k= 0$时, 根据阈值$\tau$比较来进行更新:</p>
<p>首先仍然得到一个临时的更新参数$w’_{t+1}=w_t-\eta g_t$.</p>
<p>然后与$\tau$比较, 如果$|w’_{t+1}|&gt;\tau$, 则$w_{t+1}=w’_{t+1}$.</p>
<p>如果$|w’_{t+1}|&lt;\tau$, 则将$w’_{t+1}$向0移动移动一段距离$\alpha$后的值作为$w_{t+1}$. 并且如果移动$\alpha$后反向(越过了0), 则令$w_{t-1}=0$.</p>
</li>
</ul>
<p>整体上来说, TG就是让数值较小的参数向0靠近, 当距离0很近时, 直接令其等于0.</p>
<h2 id="FOBOS"><a href="#FOBOS" class="headerlink" title="FOBOS"></a>FOBOS</h2><p>FOBOS的全称为Forward-Backward Splitting(是不是发现缩写和全称对应不上哈哈), 前向后向切分.</p>
<p>其参数更新分为两个步骤:</p>
<script type="math/tex; mode=display">
w'_{t+1}=w_t-\eta g_t \\[7mm]
w_{t+1}=\underbrace{\arg\min}_w \bigg(\frac{1}{2}(w'_{t+1}-w)^2+\eta'\Omega(w)\bigg)</script><p>其中第一个步骤, 就是正常的使用梯度来对参数进行更新.</p>
<p>而第二步中, 观察式子的第一项, 其含义是最终更新参数$w$需要在临时更新参数$w’$的附近; 式子的第二项, 是关于$w$的正则项.</p>
<p>所以这样结合起来看, FOBOS的目的就是在原本得到的$w’$的基础上, 再进行一遍正则化. 如果原本模型的损失函数中, 就包含了关于模型参数的正则项, 那么可以理解成做了两遍正则, 妙啊OvO</p>
<p>将第一步带入第二步, 并对$w$求导等于0, 可得:</p>
<script type="math/tex; mode=display">
w'_{t+1}-w+\eta'\nabla_{w}\Omega(w)=0 \\[7mm]
w_{t+1}=w'_{t+1}+\eta'\nabla_{w}\Omega(w_{t+1})</script><p>上式也正是FOBOS名称的由来, 即$w_{t+1}$可以分解成关于$t$和$t+1$参数的组合.</p>
<p>具体的, $\Omega$也是L1正则, 即$\Omega(w)=\lambda|w|$, 令$\lambda’=\eta’\lambda$则:</p>
<script type="math/tex; mode=display">
w_{t+1}=\underbrace{\arg\min}_w \bigg(\frac{1}{2}(w'_{t+1}-w)^2+\lambda'\ |w|\bigg)</script><p>来分析上式, $|w|$是对称函数, $(w’_{t+1}-w)$是中心在$w’_{t+1}$的二次函数, 根据它们的性质, 在极小值点, 最优值$w^*$必满足如下不等式:</p>
<script type="math/tex; mode=display">
w^*\times w'_{t+1}\ge 0</script><p>所以, 当$w’_{t+1}\ge0$时, 则$w^*\ge 0$, 可得:</p>
<script type="math/tex; mode=display">
w_{t+1}=\underbrace{\arg\min}_w \bigg(\frac{1}{2}(w'_{t+1}-w)^2+\lambda'\ w\bigg) \\[7mm]
s.t.\quad w\ge 0 \\[7mm]
\Rightarrow\quad w_{t+1}=\max\bigg(0,w'_{t+1}-\lambda'\bigg)</script><p>当$w’_{t+1}&lt;0$时, 则$w^*&lt; 0$, 可得:</p>
<script type="math/tex; mode=display">
w_{t+1}=\underbrace{\arg\min}_w \bigg(\frac{1}{2}(w'_{t+1}-w)^2-\lambda'\ w\bigg) \\[7mm]
s.t.\quad w< 0 \\[7mm]
\Rightarrow\quad w_{t+1}=\min\bigg(0,w'_{t+1}+\lambda'\bigg)</script><p>整体来看, FOBOS在每次更新参数时, 先根据当前参数和梯度, 得到一个临时参数, 然后进一步让临时参数向0靠近, 当越过0时则令其等于0, 与前面讲到的TG方法是比较相似的.</p>
<h2 id="RDA"><a href="#RDA" class="headerlink" title="RDA"></a>RDA</h2><p>RDA的全称是Regularized Dual Averaging, 正则对偶平均, 从另外一个角度来对参数进行稀疏化.</p>
<p>RDA的参数优化公式为:</p>
<script type="math/tex; mode=display">
w_{t+1}=\underbrace{\arg\min}_w\bigg(\frac{1}{t}\sum_{i=1}^tg_iw+\Omega(w)+\frac{\beta_t}{t}f(w)\bigg)</script><p>其中, $g_i$表示之前第$i$次迭代时的参数梯度, $\Omega$为正则项, $\beta_t$为一个非负非递减序列, $f$为一个严格凸函数.</p>
<p>一般可以假设$\Omega(w)=\lambda|w|$, $f(w)=1/2\times w^2$, $\beta_t=\gamma\sqrt{t}$, 则:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(\frac{1}{t}\sum_{i=1}^tg_iw+\lambda|w|+\frac{\gamma}{2\sqrt{t}}w^2\bigg) \\
&=\underbrace{\arg\min}_w\bigg(\overline{g}w+\lambda|w|+\frac{\gamma}{2\sqrt{t}}w^2\bigg)
\end{align*}</script><p>当$w\le0$时:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(\overline{g}w-\lambda w+\frac{\gamma}{2\sqrt{t}}w^2\bigg) \\
&=\min\bigg(0,\frac{\sqrt{t}}{\gamma}(\lambda-\overline{g})\bigg)
\end{align*}</script><p>当$w&gt;0$时:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(\overline{g}w+\lambda w+\frac{\gamma}{2\sqrt{t}}w^2\bigg) \\
&=\max\bigg(0,\frac{\sqrt{t}}{\gamma}(-\lambda-\overline{g})\bigg)
\end{align*}</script><p>用RDA于FOBOS做比较:</p>
<ul>
<li>对于FOBOS, $\eta’$通常会随着时间下降, 因此FOBOS的截断阈值(令参数等于0的阈值), 会随着时间逐渐变小. RDA的截断阈值$\lambda$是固定的常数, 更加激进, 也更容易产生稀疏性.</li>
<li>FOBOS基于单次的梯度$g_t$来更新当前参数, RDA基于累计的梯度均值$\overline{g}$来更新当前参数, 更加稳定.</li>
<li>在FOBOS更新过程中, $w_{t+1}$由$w_{t}$与$g_t$共同决定. 而RDA与前一次的$w_t$无关.</li>
</ul>
<h2 id="FTRL"><a href="#FTRL" class="headerlink" title="FTRL"></a>FTRL</h2><p>上面介绍了FOBOS和RDA的原理, 从其原理本身, 以及一些实验可以发现, FOBOS由于仍然属于基于梯度下降的方法, 所以具有较高的精度(模型效果); 而RDA具有更好的稀疏性.</p>
<p>在前辈的基础上, FTRL结合了两者的优点: 即可以有较好的稀疏性, 又能保证较高的精度.</p>
<p>FTRL的参数优化公式为:</p>
<script type="math/tex; mode=display">
w_{t+1}=\underbrace{\arg\min}_w\bigg(\sum_{i=1}^tg_iw+\lambda_1|w|+\frac{1}{2}\lambda_2w^2+\frac{1}{2}\sum_{i=1}^t\sigma_i(w_i-w)^2\bigg) \tag{1}</script><p>其中$\lambda_1,\lambda_2$为超参数, $\sigma_i$可以看成加权求和系数, 等价于学习率, $w_i$为每次迭代后的模型参数.</p>
<p>现在调整公式进行优化:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(\sum_{i=1}^tg_iw+\lambda_1|w|+\frac{1}{2}\lambda_2w^2+\frac{1}{2}\sum_{i=1}^t\sigma_i(w_i-w)^2\bigg) \\
&=\underbrace{\arg\min}_w\bigg(\sum_{i=1}^t(g_i-\sigma_iw_i)w+\lambda_1|w|+\frac{1}{2}(\lambda_2+\sum_{i=1}^t\sigma_i)w^2\bigg) \\
\end{align*}</script><p>令$a_t=\sum_{i=1}^t(g_i-\sigma_iw_i)$, $b_t=\lambda_2+\sum_{i=1}^t\sigma_i$则:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(\sum_{i=1}^t(g_i-\sigma_iw_i)w+\lambda_1|w|+\frac{1}{2}(\lambda_2+\sum_{i=1}^t\sigma_i)w^2\bigg) \\
&=\underbrace{\arg\min}_w\bigg(a_tw+\lambda_1|w|+\frac{1}{2}b_tw^2\bigg) \\
\end{align*}</script><p>当$w\le0$时:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(a_tw-\lambda_1w+\frac{1}{2}b_tw^2\bigg) \\
&=\min\bigg(0,\frac{1}{b_t}(\lambda_1-a_t)\bigg)
\end{align*}</script><p>当$w&gt;0$时:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1}&=\underbrace{\arg\min}_w\bigg(a_tw+\lambda_1w+\frac{1}{2}b_tw^2\bigg) \\
&=\max\bigg(0,\frac{1}{b_t}(-\lambda_1-a_t)\bigg)
\end{align*}</script><p>上面说到$\sigma_i$类似于学习率, 假设$\eta_i$随迭代次数变化, 且每个参数变化不同, 如同Adagrad, 则有:</p>
<script type="math/tex; mode=display">
\eta_i=\frac{\eta_0}{\sqrt{\sum_{j=1}^ig_j^2}+\epsilon} \\[7mm]
\sigma_i=\frac{1}{\eta_i}-\frac{1}{\eta_{i-1}} \\[7mm]
\sum_{i=1}^t\sigma_i=\frac{1}{\eta_t}</script><p>那为什么FTRL技能够产生较好的稀疏性, 还能保证模型效果呢, 现在结合上面的公式来进行分析.</p>
<p>在(1)式中, 由4项组成:</p>
<ul>
<li><p>第一项: $\sum_{i=1}^tg_iw$</p>
<p>这一项有两个作用, 一个是可以让$w$沿负梯度方向变化, 还有一个是累计历史梯度, 使得变化更加平稳.</p>
</li>
<li><p>第二项: $\lambda_1|w|$</p>
<p>L1正则项, 使模型参数稀疏化.</p>
</li>
<li><p>第三项: $\frac{1}{2}\lambda_2w^2$</p>
<p>L2正则项, 使模型参数更加趋于0.</p>
</li>
<li><p>第四项: $\frac{1}{2}\sum_{i=1}^t\sigma_i(w_i-w)^2$</p>
<p>这一项同样有两个作用, 一个是让$w$靠近之前的参数$w_i$, 即让$w$在历史参数基础上进行变化; 另一个是考虑了历史的每次迭代的参数, 而不仅仅只是上一次的模型参数.</p>
</li>
</ul>
<p>也就是说, 在FTRL中, 同时有FOBOS的特点, 即让参数在迭代时, 是同时基于模型参数和梯度, 保证模型效果; 也有RDA的特点, 即综合考虑历史的模型参数与梯度, 使得参数变化平稳, 且阈值$\lambda_1$固定, 增强参数稀疏性.</p>
<p>进一步来看$\sigma$参数, 一般来说, 随着迭代次数的增加, $\eta$会减小. 而由于$\sigma_i=\frac{1}{\eta_i}-\frac{1}{\eta_{i-1}}$, 当某两次迭代之间$\eta$差距较大时(对应梯度$g$较大), $\sigma$就会比较大. 这样在第四项$\frac{1}{2}\sum_{i=1}^t\sigma_i(w_i-w)^2$中, $\sigma$可以看成每次迭代的权重系数, 即当两次迭代之间有较大差别时, 会重点考虑, 而随着训练的进行, 学习率趋于0, 参数趋于收敛后, 就不重点考虑.</p>
<p>我认为之所以这样做, 是考虑到了真实场景下, 数据的稀疏性带来的困难. 因为对于稀疏数据来说, 某个特征对应的模型参数, 只要在其不为0的情况下, 其梯度$g$才不为0, 才能得到得到更新. 而当某一次迭代梯度$g$为0时, 学习率$\eta$不变, 对应$\sigma$也就为0, 非常合理.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 通过阐述模型参数稀疏性的好处, 到单纯的L1正则在一些情况下无法获得较好的稀疏性, 引出了利用优化算法来增强参数稀疏性的理念.</p>
<p>从最简单的TG, 到FOBOS, 再到RDA, 最后到FTRL, 讲解了如何在使用优化算法, 来让模型参数更加稀疏, 给出了公式推导, 以及对公式的分析和理解.</p>
<p>在面对海量数据, 高维稀疏特征时, 使用线性模型或者将线性模型作为模型一部分时, 可以使用FTRL来作为模型的优化算法.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>FTRL</tag>
        <tag>L1正则</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统的实时性</title>
    <url>/2020/10/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E6%97%B6%E6%80%A7/</url>
    <content><![CDATA[<p>世间的大部分事物, 都是随着时间在逐渐变化的, 在推荐系统中用户的偏好亦是如此. 能够快速地对用户偏好的转变做出快速的响应, 即推荐系统的实时性, 是非常重要的.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一般来说, 用户有一些长期的, 稳定的偏好, 同时在有时候, 也会存在临时的偏好. 如果我们的推荐系统是比较”刻板”的, 只是记住了用户的长期偏好, 而忽略了用户的短期偏好, 那么这时候对于黏性比较高, 或者有硬性需求的用户, 会选择使用搜索的方式; 而对于一些黏性相对较差的, 没有硬性需求的用户, 可能就会减少停留时长, 关闭客户端离开了.</p>
<p>能够及时的把握住用户偏好的转变, 从而提升用户体验, 给到用户一种”惊喜”, “懂我”的感觉, 这就是推荐系统的实时性的任务.</p>
<p>而要做好实时性, 可以从多个角度, 来进行优化, 经过本宝的学习与思考, 认为可以有三个方面, 来增强推荐系统的实时性, 分别是从特征的角度, 模型的角度, 以及策略的角度, 下面就来进行逐一阐述.</p>
<h1 id="特征实时性"><a href="#特征实时性" class="headerlink" title="特征实时性"></a>特征实时性</h1><p>一般在推荐系统的模型中, 尤其是在排序阶段的模型, 其特征中一般会有用户的历史行为(如点击序列), 这是一个非常重要的特征, 能够有效地反映用户近期的偏好.</p>
<p>而如果能够将用户的行为, 及时地更新到这个特征里面, 比如用户上一刷(滑动屏幕, 更新列表)中点击的物品, 在下一刷中能够加入行为特征, 那么理论上是可以更好地捕捉到用户临时的偏好的. 模型看到了用户近期点击了哪些物品, 会猜测会有相对更大的概率点击同类或者相关物品, 给出更加符合用户预期的推送.</p>
<p>这里其实感觉不是一些算法, 原理上的问题, 而主要是工程上的一些问题.</p>
<p>用户在客户端上的行为, 收集传回服务器, 再经过特征的处理, 输入模型, 进行预测. 这个过程看似简单, 但是实际上有些时候难以做到秒级的响应, 只能做到分钟级的响应, 也就是说难以做到真正的”实时”, 不过可以做到”准实时”, 这在不少场景下, 其实也是能够接受的.</p>
<h1 id="模型实时性"><a href="#模型实时性" class="headerlink" title="模型实时性"></a>模型实时性</h1><p>可以统计模型更新完一次, 上线以后, 一些模型指标(如GAUC)随时间的变化, 通常会随着时间有一个变差的趋势.</p>
<p>为什么会这样呢, 其实这是比较正常的情况, 模型在训练时, 学习到的是过去数据中的模式, 而当数据的分布(特征本身, 或者特征与目标变量的关系)发生变化以后, 模型的预测效果必然会变差.</p>
<p>那现在有一些可选的方法来进行应对, 一种是增加更新模型的频率, 比如过去一周全量更新一次模型, 现在三天更新一次. 还有一种方法, 是使用增量更新的方式, 即不使用全量数据, 而是在原有模型的基础上, 仅使用近期的数据来更新模型.</p>
<p>对于增量更新的方式, 其进阶版应该就是在线学习(Online Learning), 即在线上更新模型, 每次只使用很少的样本来优化模型. 具体的更新方法, 一般会采用<a href="whitemoonlight.top/2020/10/22/传统机器学习/FTRL优化算法/">FTRL</a>这样能够保证参数稀疏性的优化算法. 在线学习的方式一来工程上会困难一些, 二来是否有效还要看具体的实际情况.</p>
<h1 id="策略实时性"><a href="#策略实时性" class="headerlink" title="策略实时性"></a>策略实时性</h1><p>前面两种方式, 从特征, 模型的角度, 其实都可以归结到模型的范畴. 模型的优点是什么, 是准, 但不一定快, 那么有没有不采用模型, 同时比较快的方式来实现推荐系统的实时性呢?</p>
<p>在<a href="whitemoonlight.top/2020/10/10/推荐系统/用户画像-一/">用户画像</a>中, 提到了一般会根据用户不同时间跨度的兴趣偏好, 来设置长期, 中期, 短期的画像. 而其中的短期画像, 就可以是小时, 甚至分钟级的. 那么如果通过一些算法或者规则, 将用户分钟级的画像进行刻画, 并利用这个画像来直接进行召回推送, 或许能比仅仅调整模型更快一些, 并且可能效果更好一些.</p>
<p>使用(排序)模型进行推送的时候, 即使模型能够得到用户上一刻的行为特征, 或者随着时间更新参数, 但召回模块也能进行及时的更新调整吗, 可能不太行. 假如用户临时对某个之前没有点击过的类型的物品感兴趣, 而这类物品可能都没怎么出现在召回出的候选集中, 那排序模型再怎么厉害, 也推不出来用户现在想看到的啊.</p>
<p>而如果使用分钟级的画像来直接进行快速召回, 并推送, 这其实某种意义上, 可以算作一条比较特殊的不走模型的召回通道, 能够对用户的行为作出快速响应. 那进一步, 这个召回通道出来的物品, 是否也可以走(排序)模型呢, 我认为这要看模型或者模型的特征是否能跟得上调整速度, 如果能跟得上, 那这样应该可以达到最好的效果. 同时, 可以考虑不与其它召回通道进行混合后排序, 而是单独排序, 以免被用户过往感兴趣的物品比下去, 达不到实时性的效果.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是关于推荐系统实时性的内容了.</p>
<p>分别从三个角度来对实时性的方法进行了阐述, 分别是特征的角度, 模型的角度, 以及策略的角度.</p>
<p>这三个角度, 其实不是完全分离的, 而是关联在一起的. 模型想要有较好的预测效果, 具有实时性的特征, 以及模型本身参数随时间的优化都是不能少的. 同时通过一些实时性策略, 快速召回出的物品, 在经过模型排序后, 可能会更加精准.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统的冷启动</title>
    <url>/2020/10/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%B7%E5%90%AF%E5%8A%A8/</url>
    <content><![CDATA[<p>冷启动是推荐系统中比较困难, 而又必须要面对的一项任务, 这一篇就来说一下推荐系统在冷启动时可以做的一些事情.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>推荐系统, 之所以叫做”系统”, 而不是”算法”, 就是因为算法只是推荐系统中的一部分, 有些地方难以运用算法来处理, 需要运用一些规则策略, 甚至人工.</p>
<p>冷启动的意思, 广义来说就是缺少信息, 或者信息不充足, 狭义来说, 就是面对新物品, 新用户时, 难以进行准确地推荐.</p>
<p>但是呢, 又不能不好好做冷启动, 因为在商业场景中, 一般比较注重增长, 其中一个主要的组成成分, 就是新来的用户, 未来还是否能够继续来. 如果不停地有新用户来, 并且能保证一定的留存率, 那么这就像一潭有源泉的水, 是活的; 反之, 新用户来了没待多久就跑了, 以后再也不来了, 而获客本身就是需要成本的, 久而久之可能就如同死水一般了.</p>
<p>所以, 推荐系统中的冷启动, 是有必要尽力做好的. 不同于一些经典的推荐算法, 冷启动的方法在各种环境下, 可能需要不同的处理方式, 不过仍然存在一些共同的思路与范式, 下面来对其中一些可用的方法进行阐述.</p>
<h1 id="物品冷启动"><a href="#物品冷启动" class="headerlink" title="物品冷启动"></a>物品冷启动</h1><p>所谓物品冷启动, 就是面对新物品时应该如何处理, 相比用户冷启动, 物品冷启动整体来说要简单一些.</p>
<p>首先, 拿到一个新物品, 会有物品的ID, 一些基础信息(如类别, 关键词等), 那么是否可以直接根据物品的基础信息, 再结合用户画像进行大量推送呢? 一般来说是不行的, 在之前一篇关于<a href="whitemoonlight.top/2020/10/10/推荐系统/物品画像/">物品画像</a>中, 就提到过, 对物品来说, 一个关键的特征就是其”质量”, 无论是文章, 视频, 还是商品, 总是有质量好坏之分的, 对于用户而言, 大概率是希望看到一些质量较好的物品, 所以对于新物品, 需要事先衡量其质量的好坏.</p>
<p>那么如何衡量一个物品的好坏呢, 一些时候可以人工审核, 这个方法通常比较稳, 可以把一些质量差的物品识别出来, 但缺点是, 一来要耗费不少人力, 二来被人工判定为质量好的, 给予了较高的评级, 但却不一定是受用户欢迎的. 所以, 相比用人工来判断物品有多好(类似回归问题), 也许不如用人工来判断物品是否很差以至于不应该推送(二分类问题).</p>
<p>另一个方法, 就是通过曝光点击率或一些其它用户行为, 来衡量物品的质量. 具体的做法, 可以将每个新物品, 随机地推送固定的次数, 然后统计用户行为. 还有一种更细化的方法, 是按照物品基础信息, 结合用户画像, 再来定向地进行随机推送. 相比完全地随机推送, 后者会更有效率一些. 因为也许一些用户, 对某个物品的类别压根不感兴趣, 那么无论质量好坏, 人家就是不点, 某种程度上, 浪费了曝光次数. 通过这种方式来进行衡量物品的质量, 相比人工更加便捷也更加能够反映用户的喜好.</p>
<p>在获取到了物品的质量(是否受欢迎)后, 就可以将一些质量好的, 优先结合用户画像进行推送. 并在积累了一定与用户交互的数据后, 可以用更多的算法, 来提升推荐效果.</p>
<h1 id="用户冷启动"><a href="#用户冷启动" class="headerlink" title="用户冷启动"></a>用户冷启动</h1><p>用户的冷启动, 相比物品的冷启动, 是要困难一些的, 它困难就困难在, 如果一股脑瞎推送, 可能用户没多久就走了, 卸载APP, 顺便给个一星差评QAQ</p>
<p>所以, 用户的冷启动, 可能要花更多的功夫去优化与改进.</p>
<h2 id="扩充原始信息"><a href="#扩充原始信息" class="headerlink" title="扩充原始信息"></a>扩充原始信息</h2><p>尽管冷启动缺少有效信息, 但是没有信息不代表不可以想办法获取信息.</p>
<p>那么如何获取用户更多的信息呢, 一般来说, 可以有以下一些途径:</p>
<ul>
<li><p>用户注册基本信息.</p>
<p>比如昵称, 性别, 年龄, 职业… 这些一般都是可以让用户在注册时填写的, 也能反映用户的一些偏好(概率上).</p>
<p>但是有时候用户并不会填写真实信息, 同时在注册时要求用户填写过多信息, 会使得用户反感.</p>
</li>
<li><p>引导用户选择偏好.</p>
<p>在用户注册完成后, 可以明确的告诉用户, 选择自己喜爱的物品类型, 将会获得更好的体验. 用一些比较有代表性的标签, 来让用户进行选择, 可以形成一个初始的用户画像.</p>
<p>一般来说用户会真实地进行选择, 但有时候用户主观选择的偏好类型, 并不能完全反映其真实偏好.</p>
</li>
<li><p>第三方数据.</p>
<p>一些时候, 可以通过一些方法, 合法地获取用户在其它场景下的数据, 包括基础信息, 以及行为数据. </p>
<p>充分利用这些信息, 可以在一开始给到新用户更好的推送.</p>
</li>
</ul>
<p>如果原始信息足够丰富, 那么即便这个用户算是这个场景下名义上的”新用户”, 但实际上也可按”老用户”处理了. 但更多的时候, 一开始在没有用户行为时, 获取到的信息是非常有限的, 仍然要重点考虑如何有效地通过用户行为, 来提升推荐效果.</p>
<h2 id="探索与利用"><a href="#探索与利用" class="headerlink" title="探索与利用"></a>探索与利用</h2><p>在物品的冷启动那里, 可以将物品按一定的量随机地推送出去, 但是在用户的冷启动这里, 仍然随机地把各种类型的物品都推送一定的量, 可以吗?</p>
<p>应该是可以的, 但是有可能推送的物品还没在新用户那里曝光完, 就没机会曝光了OvO</p>
<p>随机推送物品, 可以算作是一个”探索”的过程, 而在这个过程中, 用户会与物品产生行为, 会对物品进行点击, 这个信息其实是可以想办法及时地利用起来的, 而不是等到整个随机推送结束, 才对用户行为进行统计, 调整推送策略.</p>
<p>根据用户近几次, 或者近几十次的行为, 来有目的地推荐用户可能喜欢的物品, 这就算作是一个”利用”的过程.</p>
<p>因为不知道用户的偏好, 所以要进行探索(Explore); 在有了用户行为以后, 就要进行利用(Exploit). 这应该是推荐系统用户冷启动这里, 比较重要的思想, 而围绕如何进行探索和利用, 可以有多种方法.</p>
<p>比如可以将物品划分为一些大类, 通过探索与利用, 逐渐掌握用户对这几个大类物品的喜好. 这个问题, 其实可以抽象为一个多臂老虎机(Multi-Armed Bandit problem, MAB). 那么在推荐系统这里, 就可以将物品的几个大类, 看做是多臂老虎机的”臂”, 每次推送某个大类的一个物品; 相当于摇对应的臂, 是否点击相当于对应的收益; 随机推送某个大类的物品, 相当于探索; 根据用户点击(收益), 来有目的地推送, 就是利用.</p>
<p>而针对多臂老虎机问题, 有一些经典的方法:</p>
<ul>
<li><p>$\epsilon$-Greedy 算法.</p>
<p>$\epsilon$-Greedy 算法是比较简单粗暴的, 设定一个$\epsilon$作为探索的概率, $(1-\epsilon)$作为利用的概率.</p>
<p>优点: 简单.</p>
<p>缺点: $\epsilon$如果设置为一个固定的值, 是不合适的. 一开始没有信息, 可能需要多探索, 而后在有了一定积累后, 应该减小探索, 着重利用.</p>
</li>
<li><p>UCB 算法.</p>
<p>直接上公式:</p>
<script type="math/tex; mode=display">
{\rm UCB}(i)=\overline{x}_i+\sqrt{\frac{2\log t}{n_i}}</script><p>其中, ${\rm UCB}(i)$表示当前第$i$个臂的UCB值, 每次选择值最大的臂来摇. $\overline{x}_i$表示到目前为止第$i$个臂的平均收益, 表示利用. 第二项中, $t$表示整体到目前摇了多少次臂, $n_i$表示第$i$个臂摇了多少次, 如果$n_i$较小, 那么探索的必要性就越大.</p>
<p>这个公式平衡了探索与利用, 即会优先去摇当前收益大的, 也会给一些没怎么被摇的臂机会.</p>
<p>那么这个公式怎么来的呢, 这个涉及到霍夫丁不等式, 有兴趣的同学可以进一步查阅.</p>
<p>此外, 还可以人为地给这个公式的两项, 加上权重系数, 以根据具体场景进行调整.</p>
<p>优点: 能够根据臂的当前收益, 以及被摇的次数, 平衡探索与利用.</p>
<p>缺点: 变化可能有时候比较缓慢, 即某些臂可能连续带来收益(用户连续点击某个类的物品), 但是仍然有不少的次数去摇其它的臂(推送其它大类的物品). 这个感觉可以通过一些规则, 动态地调整探索与利用的比重(权重系数)来缓解.</p>
</li>
<li><p>Thompson Sampling 算法.</p>
<p>摇臂后是否带来收益(是否点击), 可以看成一个伯努利分布, 那么每个臂都对应一个伯努利参数$p$. 从贝叶斯学派的角度出发, $p$不能完全又现有数据完全确定, 服从一个分布, 即贝塔分布.</p>
<p>那么在摇臂的过程中, 每次通过是否有收益, 来调整对应臂的贝塔分布. 一开始贝塔分布比较宽, 随着摇臂次数增加, 分布变窄, 即$p$更加确定.</p>
<p>而每次选择摇哪个臂, 是从每个臂的贝塔分布中, 按分布随机出一个$p$, 然后选择最大的摇.</p>
<p>这个方法比较优雅的使用了统计学理论, 与UCB类似, 对于不确定的部分, 给予更多的探索, 对确定的部分, 则采取更多的利用.</p>
</li>
</ul>
<p>在探索与利用时, 是从某个大类那里随机挑选, 或者按物品的质量进行挑选, 来进行推送. 而当积累了一点数据以后, 其实还可以尝试在这一步使用一些模型, 做一些简单地排序后, 再将高分值的物品进行推送. 那么这个模型怎么来的呢, 可以考虑用更早的时, 用户冷启动的数据来进行建模, 只是这其中进行建模的特征, 要具有适用性, 像用户ID特征这样的, 就可以不用加了.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是本篇文章关于推荐系统冷启动问题的阐述了. 主要分为物品的冷启动, 以及用户的冷启动. 其中物品冷启动相对简单, 而用户冷启动需要注意更多的东西.</p>
<p>在用户冷启动中, 提到了要想办法获取更多信息来做初始推荐, 以及探索与利用的一些具体方法.</p>
<p>实际的场景中, 要灵活地进行冷启动, 可以一开始有一个简单可行的方案, 然后再逐步进行迭代和优化, 以求在更少的曝光中, 给到用户更好的体验, 增加用户留存.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>多路召回+排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%8F%AC%E5%9B%9E-%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>在现在的推荐系统中, 使用多中方法(通道)来进行召回, 然后将召回物品集合再通过排序模型, 作为最终的推送物品. 下面就来对多路召回+排序进行介绍与实践.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前面的一些文章中, 介绍了好一些个召回方法, 包括协同过滤(物品/用户), 矩阵分解,基于用户画像, DSSM, YouTubeNet, item2vec等.</p>
<p>同时也实践了多种排序模型, 包括LR, FM, DNN(Embedding+MLP), DeepFM等.</p>
<p>那么这么多的召回方法, 以及排序模型, 是怎么在推荐系统中进行使用的呢? 通常是采用多种召回方法, 分别召回出每个用户可能偏好的物品, 然后将它们的结果整合到一起, 再送入到一个排序模型, 将预测得到的用户最可能喜欢的物品进行推送.</p>
<p>为什么要这样设置, 这样设置有神马好处呢, 下面来分析一下.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p><img src="fig_0.png" alt="fig"></p>
<p>上图是YouTubeNet论文中的图, 其实把图中的视频源看成广义上的物品, 其整体架构差不多就是一个典型的多路召回+排序.</p>
<p>之所以使用多路召回+排序的架构, 我认为是推荐系统的工程实现以及推送效果的一种平衡.</p>
<p>首先是从工程实现上, 大多数的召回模型或者算法, 用到的特征(信息)是很少的, 例如协同过滤类的算法, 就用到了用户和物品的ID特征. 而与之对应的排序模型, 则几乎是能有多少特征, 就用多少特征. 众所周知, 有效的特征越多, 模型效果越好, 那么问题来了, 既然排序模型效果比召回模型好, 为啥不直接全部用排序模型呢?</p>
<p>假如只使用排序模型, 那么当给一个用户进行推荐的时候, 需要联合这个用户对所有的物品打分, 然后排序, 然后推送头部的物品. 想像一下当物品数量超过10万, 100万, 甚至上亿的时候, 这消耗的时间就不是毫秒, 秒级的了, 而是分钟, 小时级. 等你排序完, 用户早走了 QAQ 而且这还只是一个用户, 要知道同时会有多个用户进行请求, 用这种方法(一个排序模型)是明显不行的.</p>
<p>那么只使用召回模型呢, 也是有问题的. 召回模型一般离线训练好以后, 可以进行快速匹配, 比如给出一个用户对应的Embedding, 可以在毫秒级的时间里, 返回与之相似的一些物品, 不过推荐效果嘛可能相对会差一点~ </p>
<p>一个召回模型不行, 那么就可以考虑上多个, 因为它们之间是可以并行的. 每个召回方法, 都有自己的一些特点, 从不同的角度, 去猜测用户的偏好, 在将它们的结果融合以后, 理论上的确是可以获得更好的效果的.</p>
<p>但现在仍然存在一个问题, 多个子模型可以融合成一个更强的融合模型, 但是来自不同召回通道的不同物品集, 它们最终会融合成一个更大的物品集. 假设现在有10个召回通道, 想为用户推送100个物品, 那么一种方法是每个通道来大概10个, 对于每个召回通道来说, 一次召回10个物品, 且不说查准率(Precision)如何, 查全率(Recall)一定很感人, 运气不好可能为零. 就是说, 用户最感兴趣的一些物品, 并不完全集中在召回模型选出的头部物品中, 需要扩大召回物品的数量, 才能保证查全率.</p>
<p>另一种方法, 就是每个通道多来一些物品, 假设50个, 加起来约500个(考虑去重), 这500个物品当中, 应该就包含了用户比较感兴趣的物品, 但可能也存在一些不怎么感兴趣的物品, 怎么进一步挑出100个呢?</p>
<p>排序模型这时候站了出来: 没错, 正是在下 ♪(^∇^*)</p>
<p>举个栗子, 班上有一群同学, 假设有$N$个, 现在经过了一场考试, 考试成绩下来了, 想统计出成绩排名前3(托普思瑞)的同学, 可以怎么做呢? 一种方法是进行全排序, 再取托普思瑞, 这样的时间复杂度, 大概是$O(N\log N)$. 还有一种方法, 就是假设已经知道了托普思瑞的同学成绩必然在90分以上, 并且90分以上的同学并不多, 那么先将90分的同学找出来, 再在这部分同学当中进行排序, 整体时间复杂度就趋于了$O(N)$, 当$N$较大时, 能剩下不少时间.</p>
<p>通过上面的栗子, 可以感觉到, 先用一些快速的召回方法, 来去掉用户大概率不感兴趣的物品, 保留少部分可能感兴趣的, 然后再使用排序模型来进行预测, 优中选优. 这样即能够节省计算时间, 减轻工程鸭梨, 又能够保证一定的推荐效果</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>现在, 就用实际的数据与算法来实践一下.</p>
<p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>按时间, 即将时间靠后的样本划分为测试集. 同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<p>对于多路召回部分, 将会使用矩阵分解, 基于用户画像, DSSM, DeepWalk; 对于排序模型, 将会使用DeepFM.</p>
<p>关于评估指标, 将会采用查准率(Precision), 查全率(Recall), F1值.</p>
<p>这里再多说一下关于评估指标的事情, 对于推荐系统来说, 真正能衡量其好坏的一些指标, 应该是一些偏向业务的指标, 如点击率, 停留时长, 留存率等, 而这些指标通常只有在线上才能进行准确评估.</p>
<p>在线下迭代模型的时候, 通常会采用一些与模型相关的指标, 如AUC(模型能够输出分数), 查准率, 查全率等. 其中一些模型, 如排序模型, 是可以在同一批样本上, 与上一版模型进行比较的, 并且指标越好, 通常上线后也会有较好的效果.</p>
<p>但还有一些算法或者模型, 如协同过滤, 是难以在线下进行较好有效评估的. 比如上一版协同过滤在使用时, 是召回了一堆物品, 再经过排序模型筛选后曝光, 然后用户选择其中一些物品进行点击. 而在迭代算法(如协同过滤)的时候, 在训练集上进行训练, 目的是召回用户可能感兴趣的物品, 但在测试集上评估时, 其实相当于是在猜用户之前(在上一版模型下)点击了哪些物品, 这是有区别的. 所以要进行真正有效的比较与评估, 还是得进行线上AB Test.</p>
<p>不过毕竟不能做线上测试 OvO 而查准率, 查全率和F1值在测试集上也能反映一些整体的效果, 所以仍采用这些指标. 同时, 考虑到在测试集上, 有的用户点击了较多的物品, 有的只点击了很少的物品, 在评估时, 如果统一推送相同数量(如200), 是有一些不合理的. 我认为更好一些的方式, 是根据用户的点击(或曝光)数量, 来决定推送数量, 这里采取的方式为推送数量为用户在测试集点击物品数量的3倍.</p>
<p>在排序时, 根据几种召回方法各自的表现, 联合其结果进行排序并推送.</p>
<p>这里考虑到篇幅问题, 只列出部分关键代码, 或者可以移步到我对每种算法单独介绍的篇章中查看, 每个部分将会给出对应的链接.</p>
<h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><p>在我的<a href="whitemoonlight.top/2020/10/10/推荐系统/矩阵分解-一/">这篇文章</a>中介绍了矩阵分解算法的原理, 并在<a href="whitemoonlight/2020/10/10/推荐系统/矩阵分解-二/">这篇文章</a>中进行了代码实践. 当时是作为一个回归问题, 即用向量之间的内积, 来拟合用户对电影的评分(1-5), 而这里则是作为一个二分类问题来处理. 同时对模型代码做了一些调整, 将原本的固定的整体均值<code>mean</code>换成了可以学习的<code>bias</code>, 并去掉了每个用户与物品的偏置项, 这是因为后续在使用Faiss对Embedding的内积进行搜索时可以更加方便 OvO</p>
<p>模型及训练代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SVD</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_user, num_item, embedding_dim, reg=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_user = num_user</span><br><span class="line">        self.num_item = num_item</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.user_embedding = keras.layers.Embedding(self.num_user, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                     embeddings_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line">        self.item_embedding = keras.layers.Embedding(self.num_item, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                     embeddings_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line"></span><br><span class="line">        self.bias = tf.Variable(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, 2)</span></span><br><span class="line">        user_ids = inputs[:, <span class="number">0</span>]</span><br><span class="line">        item_ids = inputs[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        user_embedding = tf.reshape(self.user_embedding(user_ids), (<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">        item_embedding = tf.reshape(self.item_embedding(item_ids), (<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line"></span><br><span class="line">        logits = self.bias + keras.layers.dot([user_embedding, item_embedding], [<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = SVD(<span class="number">3756</span>, <span class="number">2805</span>, <span class="number">32</span>, reg=<span class="number">1e-6</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8691, val: 0.7902, test: 0.7643</span><br></pre></td></tr></table></figure>
<p>这里数据预处理的方式, 和之前排序模型那里的方式是一样的, 在这种情况下, 看起来简单的矩阵分解模型, 却可以达到还不错的效果. 这一方面说明了内积这种特征交叉形式的强大, 另一方面当然也说明了在这份数据中能起到主要作用的, 就是ID类的特征.</p>
<p>然后提取模型中用户与物品的Embedding, 并使用Faiss进行召回, 在测试集上查看评估指标.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户已曝光过的物品</span></span><br><span class="line">user_exposed_item_set = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>].iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_exposed_item_set:</span><br><span class="line">        user_exposed_item_set[user] = set([])</span><br><span class="line">    user_exposed_item_set[user].add(item)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计测试集上每个用户点击的物品</span></span><br><span class="line"></span><br><span class="line">user_clicked_item_set_on_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>].iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    is_clicked = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> is_clicked == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">            user_clicked_item_set_on_test[user] = set([])</span><br><span class="line">        user_clicked_item_set_on_test[user].add(item)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取向量</span></span><br><span class="line">user_embeddings = model.user_embedding.get_weights()[<span class="number">0</span>]</span><br><span class="line">item_embeddings = model.item_embedding.get_weights()[<span class="number">0</span>][<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为每个用户寻找感兴趣的物品(去除已曝光)</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">32</span>)  <span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line">index.add(item_embeddings)  <span class="comment"># 添加向量</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">1000</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line">D, I = index.search(user_embeddings, k)  <span class="comment"># 用查询向量进行搜索</span></span><br><span class="line">I += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">user_match_item_set = &#123;i: I[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3756</span>)&#125;</span><br><span class="line"></span><br><span class="line">user_match_item_set_filter = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个用户的召回数量, 为其点击物品的3倍</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    user_match_item_set_filter[user] = [x <span class="keyword">for</span> x <span class="keyword">in</span> user_match_item_set[user] <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> user_exposed_item_set[user]][:<span class="number">3</span> * len(user_clicked_item_set_on_test[user])]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1300, Recall: 0.3850, F1: 0.1943</span><br></pre></td></tr></table></figure>
<h2 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h2><p>之前分别介绍了<a href="whitemoonlight.top/2020/10/10/推荐系统/物品画像/">物品画像</a>, <a href="whitemoonlight.top/2020/10/10/推荐系统/用户画像-一/">用户画像</a>的相关内容, 并进行了<a href="whitemoonlight.top/2020/10/10/推荐系统/用户画像-二/">代码实践</a>, 这里的做的方式仍然是一致的.</p>
<p>在物品画像上, 主要使用贝叶斯平滑来处理点击率, 用以衡量各个物品的质量(受欢迎度), 并按照各个物品的类别, 来制作倒排表.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 贝叶斯平滑, 挑选出记录大于50条的电影的点击率, 用来估计贝塔分布参数</span></span><br><span class="line">avg_ctr = item_ctr.loc[item_ctr[<span class="string">'exposure'</span>] &gt; <span class="number">50</span>, <span class="string">'item_ctr'</span>].mean()</span><br><span class="line">std_ctr = item_ctr.loc[item_ctr[<span class="string">'exposure'</span>] &gt; <span class="number">50</span>, <span class="string">'item_ctr'</span>].std()</span><br><span class="line">print(avg_ctr, std_ctr)</span><br><span class="line">var_ctr = std_ctr**<span class="number">2</span></span><br><span class="line">alpha = avg_ctr * (avg_ctr * (<span class="number">1</span> - avg_ctr) / var_ctr - <span class="number">1</span>)</span><br><span class="line">beta = (<span class="number">1</span> - avg_ctr) * (avg_ctr * (<span class="number">1</span> - avg_ctr) / var_ctr - <span class="number">1</span>)</span><br><span class="line">print(alpha, beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个电影经过贝叶斯平滑后的点击率</span></span><br><span class="line">item_ctr[<span class="string">'bayes_ctr'</span>] = item_ctr.apply(<span class="keyword">lambda</span> x: (x[<span class="string">'click'</span>] + alpha) / (x[<span class="string">'exposure'</span>] + alpha + beta), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.48379310645264734 0.20019408317307302</span><br><span class="line">2.530883122115303 2.7004504549021298</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 制作倒排表</span></span><br><span class="line"></span><br><span class="line">item_df = df_train[[<span class="string">'item_id'</span>, <span class="string">'category_list'</span>, <span class="string">'bayes_ctr'</span>]].copy()</span><br><span class="line">item_df[<span class="string">'idx'</span>] = range(item_df.shape[<span class="number">0</span>])</span><br><span class="line">item_df = item_df.loc[item_df[<span class="string">'idx'</span>].isin(item_df.groupby(<span class="string">'item_id'</span>)[<span class="string">'idx'</span>].min().tolist())]</span><br><span class="line"></span><br><span class="line">tag_item_dict = &#123;&#125;</span><br><span class="line">tag_set = set([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">        tag_set.add(j)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tag_set:</span><br><span class="line">    tag_item_dict[i] = []</span><br><span class="line">item_bayes_dict = dict(zip(item_ctr.index.tolist(), item_ctr[<span class="string">'bayes_ctr'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_df.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    category_list = tmp[<span class="string">'category_list'</span>]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        tag_item_dict[j].append((tmp[<span class="string">'item_id'</span>], item_bayes_dict[tmp[<span class="string">'item_id'</span>]]))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tag_item_dict:</span><br><span class="line">    tag_item_dict[i] = list(sorted(tag_item_dict[i], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>而对于用户画像, 则根据用户过往点击过的物品, 统计对于物品的类别分布, 估计用户的偏好, 用标签来进行刻画.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 遍历数据进行统计, 包括各标签整体的曝光/点击, 以及每个用户的曝光点击</span></span><br><span class="line">tag_list = list(tag_set)</span><br><span class="line"></span><br><span class="line">all_info_dict = &#123;&#125;</span><br><span class="line">user_info_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_train.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    category_list = tmp[<span class="string">'category_list'</span>]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> all_info_dict:</span><br><span class="line">            all_info_dict[j] = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        all_info_dict[j][<span class="number">0</span>] += tmp[<span class="string">'label'</span>]</span><br><span class="line">        all_info_dict[j][<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    user_id = tmp[<span class="string">'user_id'</span>]</span><br><span class="line">    <span class="keyword">if</span> user_id <span class="keyword">not</span> <span class="keyword">in</span> user_info_dict:</span><br><span class="line">        user_info_dict[user_id] =  &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> user_info_dict[user_id]:</span><br><span class="line">            user_info_dict[user_id][j] = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        user_info_dict[user_id][j][<span class="number">0</span>] += tmp[<span class="string">'label'</span>]</span><br><span class="line">        user_info_dict[user_id][j][<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个用户各个标签的分值, 并排序</span></span><br><span class="line">user_portrait = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_info_dict:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> user_info_dict[user]:</span><br><span class="line">        score = (<span class="number">0.0002</span> * all_info_dict[tag][<span class="number">0</span>] + <span class="number">0.9998</span> * user_info_dict[user][tag][<span class="number">0</span>]) / (<span class="number">0.0002</span> * all_info_dict[tag][<span class="number">1</span>] + <span class="number">0.9998</span> * user_info_dict[user][tag][<span class="number">1</span>])</span><br><span class="line">        tmp_list.append((tag, score))</span><br><span class="line">    tmp_list = list(sorted(tmp_list, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">    user_portrait[user] = tmp_list</span><br></pre></td></tr></table></figure>
<p>通过用户画像中, 用户对各个类别物品的偏好程度, 来决定各个类别物品的召回数量, 并再从倒排表中获取对应的头部物品, 从而进行推送.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = match_dict[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1053, Recall: 0.3344, F1: 0.1602</span><br></pre></td></tr></table></figure>
<p>用户画像这里, 从测试集上的结果来看, 相比上面的矩阵分解方法(F1值0.1943), 要差了一些. 这显示了隐式召回的优势, 但基于用户画像的显式召回, 有着不少隐式召回所没有的优点, 比如可控性, 可解释性等, 因此也是要尽量做好的.</p>
<h2 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h2><p>我的<a href="whitemoonlight.top/2020/10/10/推荐系统/DSSM召回/">这篇文章</a>中, 介绍了DSSM模型的原理, 也进行了实践.</p>
<p>个人感觉DSSM在召回模型中, 是比较像排序模型的, 因为理论上可以把已有的特征, 都加入到模型当中, 包括用户行为序列特征.</p>
<p>在离线训练好了DSSM模型以后, 物品的Embedding一般就固定了, 而用户的Embedding, 要看原本的特征中, 是否包含了一些动态的特征(如点击行为序列), 若包含, 那么可能每次申请召回时, 需要单独过一遍用户侧的网络, 来获取用户Embedding.</p>
<p>下面是DSSM模型的相关代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># DSSM</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DSSM</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, user_feature_info_dict, item_feature_info_dict, reg_embedding=<span class="number">1e-4</span>, reg_dnn=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param user_feature_info_dict: 用户侧特征信息字典.</span></span><br><span class="line"><span class="string">        :param item_feature_info_dict: 物品侧特征信息字典.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.user_feature_info_dict = user_feature_info_dict</span><br><span class="line">        self.item_feature_info_dict = item_feature_info_dict</span><br><span class="line">        self.reg_embedding = reg_embedding</span><br><span class="line">        self.reg_dnn = reg_dnn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.user_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.user_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>, name=name, embeddings_regularizer=keras.regularizers.l2(self.reg_embedding))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>, name=name, embeddings_regularizer=keras.regularizers.l2(self.reg_embedding))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.user_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.item_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.item_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>, name=name, embeddings_regularizer=keras.regularizers.l2(self.reg_embedding))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>, name=name, embeddings_regularizer=keras.regularizers.l2(self.reg_embedding))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.item_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.user_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'user_layer_0'</span>, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        self.user_layer_1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'user_layer_1'</span>, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line"><span class="comment">#         self.user_layer_2 = keras.layers.Dense(32, activation='relu', name='user_layer_2', kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span></span><br><span class="line"></span><br><span class="line">        self.item_layer_0 = keras.layers.Dense(<span class="number">32</span>, name=<span class="string">'item_layer_0'</span>)</span><br><span class="line">        <span class="comment"># self.item_layer_1 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        <span class="comment"># self.item_layer_2 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature)</span></span><br><span class="line">        user_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.user_embedding_dict.items():</span><br><span class="line">            type_ = self.user_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">        user_ft_concat = tf.concat(user_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        user_x = self.user_layer_0(user_ft_concat)</span><br><span class="line">        user_x = self.user_layer_1(user_x)</span><br><span class="line"><span class="comment">#         user_x = self.user_layer_2(user_x)</span></span><br><span class="line"></span><br><span class="line">        item_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.item_embedding_dict.items():</span><br><span class="line">            type_ = self.item_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">        item_ft_concat = tf.concat(item_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        item_x = self.item_layer_0(item_ft_concat)</span><br><span class="line">        <span class="comment"># item_x = self.item_layer_1(item_x)</span></span><br><span class="line">        <span class="comment"># item_x = self.item_layer_2(item_x)</span></span><br><span class="line"></span><br><span class="line">        output = keras.layers.dot([user_x, item_x], axes=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># output = output / tf.reshape(tf.norm(user_x, axis=1) * tf.norm(item_x, axis=1), shape=(-1, 1))</span></span><br><span class="line">        output = tf.sigmoid(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = DSSM(user_feature_info_dict,</span><br><span class="line">             item_feature_info_dict,</span><br><span class="line">             reg_embedding=<span class="number">1e-4</span>,</span><br><span class="line">             reg_dnn=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_set,</span><br><span class="line">          df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train: <span class="number">0.8398</span>, val: <span class="number">0.7949</span>, test: <span class="number">0.7692</span></span><br></pre></td></tr></table></figure>
<p>对比前面的矩阵分解模型, 在测试集上DSSM比矩阵分解模型的AUC高了约5个千分点, 这河里吗, 这河里呀 OvO 毕竟DSSM用到了更多的特征, 更加灵活的网络结构.</p>
<p>然后分别使用用户侧与物品侧的网络, 来获取Embedding:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户侧网络模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, user_feature_info_dict)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param user_feature_info_dict: 用户侧特征信息字典.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.user_feature_info_dict = user_feature_info_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.user_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.user_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>, name=name)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>, name=name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.user_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.user_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'user_layer_0'</span>)</span><br><span class="line">        self.user_layer_1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'user_layer_1'</span>)</span><br><span class="line"><span class="comment">#         self.user_layer_2 = keras.layers.Dense(32, activation='relu', name='user_layer_2')</span></span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature)</span></span><br><span class="line">        user_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.user_embedding_dict.items():</span><br><span class="line">            type_ = self.user_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">        user_ft_concat = tf.concat(user_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        user_x = self.user_layer_0(user_ft_concat)</span><br><span class="line">        output = self.user_layer_1(user_x)</span><br><span class="line"><span class="comment">#         output = self.user_layer_2(user_x)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 物品侧网络模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Item</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, item_feature_info_dict)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param item_feature_info_dict: 物品侧特征信息字典.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.item_feature_info_dict = item_feature_info_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.item_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.item_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>, name=name)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>, name=name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.item_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.item_layer_0 = keras.layers.Dense(<span class="number">32</span>, name=<span class="string">'item_layer_0'</span>)</span><br><span class="line">        <span class="comment"># self.item_layer_1 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        <span class="comment"># self.item_layer_2 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature)</span></span><br><span class="line"></span><br><span class="line">        item_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.item_embedding_dict.items():</span><br><span class="line">            type_ = self.item_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">        item_ft_concat = tf.concat(item_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        output = self.item_layer_0(item_ft_concat)</span><br><span class="line">        <span class="comment"># item_x = self.item_layer_1(item_x)</span></span><br><span class="line">        <span class="comment"># item_x = self.item_layer_2(item_x)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制模型参数权重</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">user_model = User(user_feature_info_dict)</span><br><span class="line">user_model.build(())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(user_model.layers):</span><br><span class="line">    weights = model.get_layer(layer.name).get_weights()</span><br><span class="line">    layer.build(weights[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    layer.set_weights(weights)</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">item_model = Item(item_feature_info_dict)</span><br><span class="line">item_model.build(())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(item_model.layers):</span><br><span class="line">    weights = model.get_layer(layer.name).get_weights()</span><br><span class="line">    print(i, layer.name, weights[<span class="number">0</span>].shape)</span><br><span class="line">    layer.build(weights[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    layer.set_weights(weights)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 获取每个用户的Embedding</span></span><br><span class="line">df_user = df_train.drop_duplicates(<span class="string">'user_id'</span>)</span><br><span class="line">df_user = df_user.sort_values(<span class="string">'user_id'</span>)</span><br><span class="line">user_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_user[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_user[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_user[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_user[<span class="string">'job'</span>].values,</span><br><span class="line">&#125;</span><br><span class="line">user_embeddings = user_model(user_set).numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每个物品的Embedding</span></span><br><span class="line">df_item = df_train.drop_duplicates(<span class="string">'item_id'</span>)</span><br><span class="line">df_item = df_item.sort_values(<span class="string">'item_id'</span>)</span><br><span class="line">item_set = &#123;</span><br><span class="line">    <span class="string">'item_id'</span>: df_item[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_item[<span class="string">'category_list'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line">item_embeddings = item_model(item_set).numpy()</span><br></pre></td></tr></table></figure>
<p>最后使用Faiss进行搜索召回, 再来看召回物品在测试集上的其它评估指标:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1278, Recall: 0.3785, F1: 0.1911</span><br></pre></td></tr></table></figure>
<p>对比前面矩阵分解的结果(F1值0.1943), 要差了一些. 诶明明AUC都要高一些, 为啥F1值却下降了呢 QAQ 其实一开始也提到了, 这里用AUC, 以及查准率, 查全率, F1值来进行评估, 并不是评估的同一件事情, 前者评估的是整体的排序能力, 后者评估的是召回的物品对用户原本点击的物品”猜”得对不对, 所以两者表现不一致也是可以解释的~</p>
<h2 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h2><p>在我的<a href="whitemoonlight.top/2020/10/10/图算法/DeepWalk/">这篇文章</a>中, 对DeepWalk的原理进行了讲解, 并还进行了代码实践.</p>
<p>而在这里呢, 相比之前有一点区别, 是把原始数据中的部分高分电影当成”曝光后点击”, 部分低分电影当成”曝光后未点击”, 并只使用曝光后点击的电影来形成序列.</p>
<p>首先是用原始数据构建图:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_edge_file</span><span class="params">(data, file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    生成包含连边的文件.</span></span><br><span class="line"><span class="string">    :param data: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq) - <span class="number">1</span>):</span><br><span class="line">                tmp = str(seq[i]) + <span class="string">' '</span> + str(seq[i + <span class="number">1</span>]) + <span class="string">'\n'</span></span><br><span class="line">                f.write(tmp)</span><br><span class="line">        f.close()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    构建图.</span></span><br><span class="line"><span class="string">    :param file: 节点连边文件路径.</span></span><br><span class="line"><span class="string">    :return: dict, 图.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    graph = defaultdict(set)</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            node_0, node_1 = line.strip().split()</span><br><span class="line">            <span class="keyword">if</span> node_0 != node_1:</span><br><span class="line">                graph[node_0].add(node_1)</span><br><span class="line">                graph[node_1].add(node_0)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        graph[node] = list(graph[node])</span><br><span class="line">    <span class="keyword">return</span> graph</span><br><span class="line"></span><br><span class="line">create_edge_file(train_data, <span class="string">'tmp.txt'</span>)</span><br><span class="line">graph = create_graph(<span class="string">'tmp.txt'</span>)</span><br></pre></td></tr></table></figure>
<p>然后是用随机游走的方式, 在图中抽取序列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行随机游走构建序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_walk</span><span class="params">(graph, epoch=<span class="number">2</span>, max_len=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在图中随机游走, 返回生成的序列数据.</span></span><br><span class="line"><span class="string">    :param graph: dict, 图.</span></span><br><span class="line"><span class="string">    :param epoch: 遍历多少次所以节点.</span></span><br><span class="line"><span class="string">    :param max_len: 序列最大长度.</span></span><br><span class="line"><span class="string">    :return: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    nodes = list(graph.keys())</span><br><span class="line">    res_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        random.shuffle(nodes)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            seq = [node]</span><br><span class="line">            cur_node = node</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(max_len - <span class="number">1</span>):</span><br><span class="line">                cur_node = random.choice(graph[cur_node])</span><br><span class="line">                seq.append(cur_node)</span><br><span class="line">            res_data.append(seq)</span><br><span class="line">    random.shuffle(res_data)</span><br><span class="line">    <span class="keyword">return</span> res_data</span><br><span class="line"></span><br><span class="line">train_data_deep_walk = random_walk(graph, epoch=<span class="number">1</span>, max_len=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>接着用word2vec来训练模型, 这里使用gensim中的word2vec, 简单方便:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">dim = <span class="number">32</span>  <span class="comment"># 向量维度</span></span><br><span class="line"></span><br><span class="line">model = word2vec.Word2Vec(sentences=train_data_deep_walk,</span><br><span class="line">                          size=dim,  <span class="comment"># 向量维度</span></span><br><span class="line">                          alpha=<span class="number">0.025</span>,  <span class="comment"># 学习率</span></span><br><span class="line">                          window=<span class="number">1</span>,  <span class="comment"># 窗口大小</span></span><br><span class="line">                          min_count=<span class="number">5</span>,</span><br><span class="line">                          sample=<span class="number">0.001</span>,</span><br><span class="line">                          seed=<span class="number">7</span>,</span><br><span class="line">                          workers=<span class="number">12</span>,</span><br><span class="line">                          min_alpha=<span class="number">0.0001</span>,</span><br><span class="line">                          sg=<span class="number">1</span>,  <span class="comment"># 使用skip-gram</span></span><br><span class="line">                          hs=<span class="number">0</span>,  <span class="comment"># 使用neg-sample</span></span><br><span class="line">                          negative=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取词表, 以及每个词汇的词向量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">vocab = model.wv.index2word</span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line">item_embedding = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> vocab:</span><br><span class="line">    item_embedding[int(k)] = model[k]</span><br><span class="line">    </span><br><span class="line">item_embedding_mat = np.zeros((<span class="number">2804</span>, dim))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_embedding:</span><br><span class="line">    item_embedding_mat[i - <span class="number">1</span>] = item_embedding[i]</span><br><span class="line">item_embedding_mat = item_embedding_mat.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为2738</span><br></pre></td></tr></table></figure>
<p>在用物品的Embedding来表示用户的Embedding时, 采取的方法是取用户之前点击过的100个(不满则全取)物品的Embedding的池化平均:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将用户近期观看的100部影片的Embedding进行平均池化</span></span><br><span class="line"></span><br><span class="line">user_embedding_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    tmp_item_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user][:<span class="number">100</span>]]</span><br><span class="line">    tmp_embedding = np.zeros(dim)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tmp_item_list:</span><br><span class="line">        tmp_embedding += item_embedding.get(item, np.zeros(dim))</span><br><span class="line">    user_embedding_dict[user] = tmp_embedding / len(tmp_item_list)</span><br></pre></td></tr></table></figure>
<p>最后用Faiss进行搜索召回, 并在测试集上进行评估:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1574, Recall: 0.4650, F1: 0.2352</span><br></pre></td></tr></table></figure>
<p>啊… 这就是… 这就是传说中图算法的力量吗!</p>
<p>相比之前最好的矩阵分解的结果(F1值0.1943), 这里直接拉到了0.2352, 提升了4个百分点. 必须说的是这个结果是经过我调参得到的, 但即便如此, 一个无监督算法能够比监督算法取得更好的结果(至少在这份数据集上), 仍然是值得我们思考的.</p>
<p>其实一开始我对DeepWalk的期望是能够达到接近用户画像召回的水平就行了, 可是为什么最后效果却这么好呢? 想一下, 在矩阵分解模型中, 需要同时学习用户以及物品的Embedding, 而这两者一开始都是随机的, 如果其中一个没有学习得好, 那么另外一个可能也不会学习得那么好. 而在DeepWalk这里, 一开始是专注于学习物品的Embedding, 在学习好了以后, 再用物品的Embedding来对用户进行表示, 某种意义上说, 只要物品Embedding学习得足够好, 那么用户Embedding也会是相对准确的. 此外, 在矩阵分解这样的模型中, 如果某些用户行为记录较少, 那么其Embedding的学习就不充分, 但DeepWalk面对这种问题应该会好一些.</p>
<p>类比NLP中的词向量预训练, 除了可以直接将词向量运用到下游任务(比如进行文本分类), 还可以用预训练的词向量, 来初始化其它模型的参数, 以获得更快的收敛和更好的效果. 在推荐系统这里可能也有类似的用法, 即使用图算法(如DeepWalk)得到的Embedding, 直接使用到其它的召回或者排序模型中, 或者进行初始化, 应该也可以取得更好的表现♪(^∇^*)</p>
<h2 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h2><p>前面分别使用了矩阵分解, 用户画像, DSSM, DeepWalk来进行召回, 现在综合它们的结果并使用DeepFM来进行排序.</p>
<p>在我的<a href="whitemoonlight.top/2020/10/10/推荐系统/DeepFM排序/">这篇文章</a>中, 介绍了DeepFM的原理, 并进行了实践.</p>
<p>下面是训练模型的部分代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br><span class="line"></span><br><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># DeepFM</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepFM</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, embedding_dim=<span class="number">16</span>, reg_linear=<span class="number">1e-3</span>, reg_fm=<span class="number">1e-5</span>, reg_dnn=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param embedding_dim: 隐向量维度.</span></span><br><span class="line"><span class="string">        :param reg_linear: 线性部分正则系数.</span></span><br><span class="line"><span class="string">        :param reg_fm: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        :param reg_dnn: DNN正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg_linear = reg_linear</span><br><span class="line">        self.reg_fm = reg_fm</span><br><span class="line">        self.reg_dnn = reg_dnn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.linear_dict = OrderedDict()</span><br><span class="line">        self.embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.linear_dict[name] = keras.layers.Dense(<span class="number">1</span>, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                                            kernel_regularizer=keras.regularizers.l2(self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Dense(self.embedding_dim, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                                               kernel_regularizer=keras.regularizers.l2(self.reg_fm))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.linear_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                    self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_fm))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.linear_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=max_len, mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                    self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=max_len,</span><br><span class="line">                                                                   mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_fm))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf.print(<span class="string">'Feature type error.'</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.dnn_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        <span class="comment"># self.dnn_layer_1 = keras.layers.Dense(64, activation='relu',</span></span><br><span class="line">        <span class="comment">#                                       kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span></span><br><span class="line">        <span class="comment"># self.dnn_layer_2 = keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span></span><br><span class="line">        self.dnn_layer_1 = keras.layers.Dense(<span class="number">1</span>, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.bias = tf.Variable(0, dtype=tf.float32)</span></span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        linear_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, w_layer <span class="keyword">in</span> self.linear_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span> <span class="keyword">or</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>,))</span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                <span class="comment"># real_len = tf.reshape(tf.reduce_sum(mask, axis=1), (-1, 1))</span></span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>]))</span><br><span class="line">                ft = ft * mask</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># ft = ft / real_len</span></span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">        linear_logits = tf.reduce_sum(linear_res_list, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># fm</span></span><br><span class="line">        fm_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, v_layer <span class="keyword">in</span> self.embedding_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">        sum_square = tf.pow(tf.reduce_sum(fm_res_list, axis=<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        square_sum = tf.reduce_sum(tf.pow(fm_res_list, <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">        fm_logits = tf.reduce_sum(<span class="number">0.5</span> * (sum_square - square_sum), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dnn</span></span><br><span class="line">        dnn_inputs = tf.concat(fm_res_list, axis=<span class="number">1</span>)</span><br><span class="line">        dnn_ft = self.dnn_layer_0(dnn_inputs)</span><br><span class="line">        <span class="comment"># dnn_ft = self.dnn_layer_1(dnn_ft)</span></span><br><span class="line">        <span class="comment"># dnn_logits = self.dnn_layer_2(dnn_ft)</span></span><br><span class="line"></span><br><span class="line">        dnn_logits = tf.reshape(self.dnn_layer_1(dnn_ft), shape=(<span class="number">-1</span>,))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output</span></span><br><span class="line">        logits = linear_logits + fm_logits + dnn_logits</span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = DeepFM(feature_info_dict,</span><br><span class="line">               embedding_dim=<span class="number">16</span>,</span><br><span class="line">               reg_linear=<span class="number">5e-4</span>,</span><br><span class="line">               reg_fm=<span class="number">5e-5</span>,</span><br><span class="line">               reg_dnn=<span class="number">1e-4</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8382, val: 0.7961, test: 0.7705</span><br></pre></td></tr></table></figure>
<p>在使用DeepFM进行排序前, 对几种召回方法得到的结果进行整合.</p>
<p>这里可以将几个召回物品的集合直接取并集, 然后交由DeepFM排序. 而在这里由于DeepWalk的”异军突起”, 同时DeepFM相比矩阵分解, DSSM并没有大的提升, 这就会导致最后排序得到的结果, 看起来还没有单路召回(DeepWalk)的效果好Σ( ° △ °|||)</p>
<p>类似的情况在真实环境中应该能够得到避免, 排序模型会尽可能用到所有的有效信息(特征), 来使其预测能力相对最好, 毕竟如果单路召回模型效果优于排序模型了, 那这个多路召回+排序这个架构就没有意义了OvO</p>
<p>考虑到各路召回方法效果的差异, 这里根据其各自表现来进行数量(送入排序模型)的控制, DeepWalk保留全部, 矩阵分解和DSSM保留三分之一, 用户画像保留六分之一. 在使用DeepFM排序时, 用户侧的特征是每个用户在训练集上最新的特征, 物品侧则包含对应的召回物品集合. 在经过排序后, 取预测分值相对最高的, 数量为3倍于用户在测试集上的点击数量的物品来进行评估:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = final_match_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1459, Recall: 0.4369, F1: 0.2188</span><br></pre></td></tr></table></figure>
<p>如前面所述, 这里多路召回+排序的效果还没有单路DeepWalk的效果好哈哈(都怪DeepWalk效果太好了QAQ).</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>首先对比一下各路召回和排序后的表现:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">矩阵分解</td>
<td style="text-align:center">0.1300</td>
<td style="text-align:center">0.3850</td>
<td style="text-align:center">0.1943</td>
</tr>
<tr>
<td style="text-align:center">用户画像</td>
<td style="text-align:center">0.1053</td>
<td style="text-align:center">0.3344</td>
<td style="text-align:center">0.1602</td>
</tr>
<tr>
<td style="text-align:center">DSSM</td>
<td style="text-align:center">0.1278</td>
<td style="text-align:center">0.3785</td>
<td style="text-align:center">0.1911</td>
</tr>
<tr>
<td style="text-align:center">DeepWalk</td>
<td style="text-align:center">0.1574</td>
<td style="text-align:center">0.4650</td>
<td style="text-align:center">0.2352</td>
</tr>
<tr>
<td style="text-align:center">多路召回+排序(DeepFM)</td>
<td style="text-align:center">0.1459</td>
<td style="text-align:center">0.4369</td>
<td style="text-align:center">0.2188</td>
</tr>
</tbody>
</table>
</div>
<p>从结果来看, 矩阵分解与DSSM的表现是非常相似的, 基于用户画像的召回效果相对偏差, DeepWalk的召回效果最好. 同时, 由于DeepWalk表现过好, 甚至比排序模型DeepFM还好, 也导致了多路召回+排序的表现还不如单路召回, 当然了这在实际生产环境中一般能够得到避免OvO</p>
<p>多路召回+排序的架构, 是平衡推荐系统中工程与性能的结果, 使用多路并行的召回通道, 快速地返回物品候选集, 然后再使用排序模型进行精准预测. 这样的做法, 我认为是具有一定的泛用性的, 即在我们生活或者工作中的其它地方, 也许同样能够运用类似的思想来更好地解决问题.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>DIN排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/DIN%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>在前面的排序模型中, 更多是关于特征的不同的交叉方式, 而在这篇中, 将会介绍一种将Attention思想引入进来的经典排序模型, DIN.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>DIN的全称为Deep Interest Network, 即深度兴趣网络, 之所以叫这个名字, 应该是阿里的开发者一开始就是将这个模型用在CTR领域, 用以更好地估计用户的兴趣. 原始的论文在<a href="https://arxiv.org/pdf/1706.06978.pdf" target="_blank" rel="noopener">这里</a>.</p>
<p>在前面的一些排序模型中, 如GBDT+LR, FM, DNN等, 大多都是将关注点, 放在特征不同的交叉方式上, 而DIN这里与它们不同, 将重心放在了对用户历史行为的进一步挖掘上.</p>
<p>在排序模型中, 用户的历史行为(如点击序列), 是一个非常重要的特征, 能够反映用户近期的一些兴趣偏好或者变化. 通常的做法, 是将物品进行Embedding, 然后再将序列中的物品对应的Embedding做(平均)池化, 来作为一个特征.</p>
<p>然鹅有些时候这样不太好, 将多个不同物品的Embedding进行池化后, 可能整体难以突出地反映用户对某些物品的偏好. 比如用户过去买了一个鼠标, 还买了一些其它生活用品, 现在要估计用户是否会买键盘. 直接平均池化后, 买了一个鼠标这个事件也许就被掩盖了, 其实从用户买过鼠标这件事, 就应该给出一个较大的概率来预测用户会买键盘.</p>
<p>现在有用户的行为序列特征, 以及待曝光物品, 如何能够让模型学会抓住”重点”, 对行为进行深度挖掘呢? Attention! 是的, DIN正是将Attention的方法引入了排序模型. 关于Attention的介绍, 可以看一下我的<a href>这篇文章</a>.</p>
<p>下面对DIN的结构进行阐述.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p><img src="fig_0.png" alt="fig"></p>
<p>模型的结构如上图, 其中左边的Base Model, 就是常规的DNN.</p>
<p>其中的一些特征, 包括用户的画像信息(User Profile Features); 中间的用户历史行为特征, 其中每个物品的Embedding由物品本身ID, 商铺ID, 分类ID组成, 然后进行池化作为User Behavior Feature; 再往右则是对应的待曝光物品(Candidate Ad)的Embedding; 最后是一些上下文特征(Context Features).</p>
<p>在右边的DIN中, 相比Base Model, 只多了一个Actication Unit, 其具体结构在右上角. 对于Actication Unit来说, 其输入是用户行为序列中的某个物品, 以及待曝光物品的Embedding, 然后将两者原始Embedding和外积得到的向量一起拼接, 再经过一个隐藏层, 输出一个数值.</p>
<p>Actication Unit是干嘛用的呢, 前面说到DIN是加入了Attention, 那么就体现在了这里. 通过将待曝光物品分别和行为序列中每个物品输入Actication Unit, 得到一组数值, 再将这一组数值转换为权重系数, 将行为序列中每个物品的Embedding加权相加, 来替换原始的直接平均池化.</p>
<p>是的, DIN模型的结构其实也比较简单, 并不复杂. 其中的Actication Unit, 在论文中使用了外积的方式, 其实其它一些方式, 如元素差, 哈达玛积等, 也未尝不可.</p>
<p>除了DIN模型本身的结果, 在DIN的论文中, 还有一些值得注意的点:</p>
<ul>
<li><p>自适应正则(Adaptive Regularization).</p>
<p>在大规模的ID特征上(想想阿里的用户和物品量), 可以说是稀疏中的稀疏. 在这种情况下, 使用平时的一些优化算法, 会导致一个问题, 比如某个特征对应的参数, 由于这个特征长期为0, 但是又因为正则损失一直存在, 就会让这些参数不停地向零靠拢, 可能导致学习效果不好.</p>
<p>但是为了防止过拟合, 正则是必须要加的, 怎么办呢, 就想了一个办法, 即对于某个特征对应的参数来说, 如果这个特征在这次的batch中出现(为1)了, 那么就正常更新参数, 如果没有, 那么就不更新参数. 具体算法可查看原论文.</p>
<p>这就是自适应正则的主要思想, 除了可以在极度稀疏的数据上提升模型表现, 还可以减少一些计算(没出现的特征对应的参数不更新).</p>
</li>
<li><p>GAUC.</p>
<p>大家应该都知道在二分类排序任务中, AUC是一个经典好用的评估指标, 能够反映整体的模型效果. 然鹅在CTR的场景下, 是否一定很好呢, 不一定.</p>
<p>在CTR场景中, 一般来说是”以人为本”的, 就是相比整体的排序表现, 更希望关注到个人上的排序表现. 而GAUC就考虑到了这一点, G是group的缩写, 意思是按用户来对样本进行分组, 对每组样本计算一个AUC, 然后再按每组的样本数量, 进行加权平均, 得到GAUC.</p>
<p>GAUC相比AUC, 在CTR中更加贴近业务, 更加能够反映真实的排序性能.</p>
</li>
<li><p>Data Adaptive Activation Function (Dice)</p>
<p>对于常见的PRelu激活函数来说, 其转折点是固定的(0点). 而Dice做了改进, 即将通过当前batch对应数据的均值与方差, 来对激活函数的转折点进行调整, 并将整体变得平滑. 将激活函数的转折点对应到均值处, 并用方差来进行伸缩.</p>
<script type="math/tex; mode=display">
Function=p(s)\cdot s+\big(1-p(s)\big)\cdot\alpha s \\[7mm]</script><p><img src="fig_1.png" alt="fig"></p>
<p>对于PRelu来说, $p(s)$是阶跃且固定的, 而对于Dice来说, $p(s)$是变化且连续的. 通过对激活函数的改进, 在DIN中也取得了更好一些的效果.</p>
</li>
</ul>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>对入模特征来说, 会分为用户侧特征和物品侧特征. 在这里, 用户侧的特征可以包括用户ID, 用户的基本信息(性别, 年龄, 职业), 用户的历史行为(近期10部点击的电影). 物品侧的特征包括电影ID, 电影的风格流派.</p>
<p>按时间, 即将时间靠后的样本划分为测试集. 同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># DIN</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskSoftmax</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask)</span>:</span></span><br><span class="line">        tmp = tf.math.exp(inputs) * mask</span><br><span class="line">        <span class="keyword">return</span> tmp / tf.reshape(tf.reduce_sum(tmp, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, reg=<span class="number">1e-7</span>, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.layer_0 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                          kernel_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line">        self.layer_1 = keras.layers.Dense(<span class="number">1</span>, kernel_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: ((batch_size, vec_dim), (batch_size, vec_dim))</span></span><br><span class="line"></span><br><span class="line">        vec_0 = inputs[<span class="number">0</span>]</span><br><span class="line">        vec_1 = inputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        x_list = [vec_0, vec_1]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算向量元素差</span></span><br><span class="line">        vec_sub = vec_0 - vec_1</span><br><span class="line">        x_list.append(vec_sub)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算向量哈达玛积</span></span><br><span class="line">        vec_hadama = vec_0 * vec_1</span><br><span class="line">        x_list.append(vec_hadama)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拼接</span></span><br><span class="line">        vec = tf.concat(x_list, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算</span></span><br><span class="line">        vec = self.layer_0(vec)</span><br><span class="line">        output = self.layer_1(vec)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DIN</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, user_behavior_ft=None, item_ft=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_dim=<span class="number">16</span>, reg_embedding=<span class="number">1e-5</span>, reg_dnn=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param user_behavior_ft: 用户行为特征名称.</span></span><br><span class="line"><span class="string">        :param item_ft: 物品特征名称.</span></span><br><span class="line"><span class="string">        :param embedding_dim: 隐向量维度.</span></span><br><span class="line"><span class="string">        :param reg_embedding: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        :param reg_dnn: DNN正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.user_behavior_ft = user_behavior_ft</span><br><span class="line">        self.item_ft = item_ft</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg_embedding = reg_embedding</span><br><span class="line">        self.reg_dnn = reg_dnn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.embedding_dict[name] = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_embedding))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=max_len,</span><br><span class="line">                                                                   mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_embedding))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf.print(<span class="string">'Feature type error.'</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.attention_layer = Attention(reg=self.reg_dnn)</span><br><span class="line"></span><br><span class="line">        self.mask_softmax_layer = MaskSoftmax()</span><br><span class="line"></span><br><span class="line">        self.dnn_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        self.dnn_layer_1 = keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        self.dnn_layer_2 = keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dnn</span></span><br><span class="line">        item_ft = inputs[self.item_ft]</span><br><span class="line">        item_ft = self.embedding_dict[self.item_ft](item_ft)</span><br><span class="line">        item_ft = tf.reshape(item_ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line"></span><br><span class="line">        dnn_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, v_layer <span class="keyword">in</span> self.embedding_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span> <span class="keyword">and</span> name != self.user_behavior_ft:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 用户行为</span></span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                weight_list = []</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(self.feature_info_dict[name][<span class="string">'max_len'</span>]):</span><br><span class="line">                    tmp_ft = ft[:, i]</span><br><span class="line">                    weight = self.attention_layer((item_ft, tmp_ft))</span><br><span class="line">                    weight_list.append(weight)</span><br><span class="line">                weight_list = tf.concat(weight_list, axis=<span class="number">1</span>)</span><br><span class="line">                weight_list = self.mask_softmax_layer(weight_list, mask)</span><br><span class="line">                ft = tf.reduce_sum(ft * tf.reshape(weight_list, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>)),</span><br><span class="line">                                   axis=<span class="number">1</span>)</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">        dnn_inputs = tf.concat(dnn_res_list, axis=<span class="number">1</span>)</span><br><span class="line">        dnn_ft = self.dnn_layer_0(dnn_inputs)</span><br><span class="line">        dnn_ft = self.dnn_layer_1(dnn_ft)</span><br><span class="line">        output = self.dnn_layer_2(dnn_ft)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = DIN(feature_info_dict,</span><br><span class="line">            user_behavior_ft=<span class="string">'recent_click'</span>,</span><br><span class="line">            item_ft=<span class="string">'item_id'</span>,</span><br><span class="line">            embedding_dim=<span class="number">16</span>,</span><br><span class="line">            reg_embedding=<span class="number">1e-3</span>,</span><br><span class="line">            reg_dnn=<span class="number">1e-5</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8346, val: 0.7941, test: 0.7693</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>对比到目前各排序模型的表现:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LR</td>
<td style="text-align:center">0.7616</td>
<td style="text-align:center">0.7348</td>
</tr>
<tr>
<td style="text-align:center">GBDT+LR</td>
<td style="text-align:center">0.7628</td>
<td style="text-align:center">0.7352</td>
</tr>
<tr>
<td style="text-align:center">FM</td>
<td style="text-align:center">0.7834</td>
<td style="text-align:center">0.7540</td>
</tr>
<tr>
<td style="text-align:center">Embedding+MLP</td>
<td style="text-align:center">0.7937</td>
<td style="text-align:center">0.7690</td>
</tr>
<tr>
<td style="text-align:center">DeepFM</td>
<td style="text-align:center">0.7961</td>
<td style="text-align:center">0.7705</td>
</tr>
<tr>
<td style="text-align:center">DIN</td>
<td style="text-align:center">0.7941</td>
<td style="text-align:center">0.7693</td>
</tr>
</tbody>
</table>
</div>
<p>从结果来看, 与Embedding+MLP的表现几乎一致, 在验证集和测试集上都只要几个万分点的差距, 这说明在这份数据集上, DIN可能并没有发挥其特点.</p>
<p>DIN通过运用Attention的方式, 深度挖掘用户的行为序列特征, 可以更好地感知到用户的一些偏好, 尽管在这里与Embedding+MLP效果相当, 但仍是一个值得一试的模型.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>Attention</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepFM排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/DeepFM%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>DeepFM也算是CTR中比较经典的模型了, 下面来对其结构进行说明, 并用代码进行实践.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面的几篇关于排序模型的文章中, 有介绍到LR, FM, DNN. 如果从特征交叉的角度来看待这几种模型的话, LR是没有特征交叉(可以由特征工程来处理); FM用内积的方式, 实现了二阶的特征交叉; DNN可以看做是多个特征进行复杂的高阶特征交叉.</p>
<p>那么对于某个场景下的数据来说, 其中蕴含的模式, 可能与原始特征(不交叉)有关, 也可能与二阶或者高阶的交叉特征相关. 那么这时候, 想要模型获得更好的效果, 就需要将几种不同的方式组合起来, 而这样的模型, 就叫做DeepFM, 在理解前面几种模型的情况下, DeepFM就比较简单了, 下面来对其结构进行说明.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>这里是<a href="https://www.ijcai.org/Proceedings/2017/0239.pdf" target="_blank" rel="noopener">DeepFM</a>的论文, 有兴趣的同学可以看一下.</p>
<p>DeepFM的整体结构如下:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>乍一看还挺复杂的, 下面来对其进行分解, 首先是左边FM的部分:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>最下面是一些离散的稀疏特征, 这些特征通过加权相加, 组成了模型的LR部分, 对应图中FM Layer最左边的那个Addition操作.</p>
<p>上面一层是Embedding层, 将每个离散特征映射到一个稠密的向量, 然后由这些向量的内积, 来表示特征之间的两两交叉, 然后与前面LR的结果一起加和输出.</p>
<p>接下来是右边DNN的部分:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>这个就是把上面得到的每个特征(域)的Embedding拼接, 作为DNN的输入, 经过一些隐藏层后, 进行输出.</p>
<p>最后, 就是将LR, FM(不包含LR), DNN的logits(原始的数值)相加, 再经过sigmoid函数最为最终的模型结果.</p>
<p>感觉好像没有太多需要说的♪(^∇^*)</p>
<p>哦对了, 可以默认让所有的特征, 都经过LR, FM(不包含LR), DNN, 也可以根据实际情况与尝试, 让指定的特征分别经过LR, FM和DNN.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>对入模特征来说, 会分为用户侧特征和物品侧特征. 在这里, 用户侧的特征可以包括用户ID, 用户的基本信息(性别, 年龄, 职业), 用户的历史行为(近期10部点击的电影). 物品侧的特征包括电影ID, 电影的风格流派.</p>
<p>按时间, 即将时间靠后的样本划分为测试集. 同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># DeepFM</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepFM</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, embedding_dim=<span class="number">16</span>, reg_linear=<span class="number">1e-3</span>, reg_fm=<span class="number">1e-5</span>, reg_dnn=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param embedding_dim: 隐向量维度.</span></span><br><span class="line"><span class="string">        :param reg_linear: 线性部分正则系数.</span></span><br><span class="line"><span class="string">        :param reg_fm: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        :param reg_dnn: DNN正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg_linear = reg_linear</span><br><span class="line">        self.reg_fm = reg_fm</span><br><span class="line">        self.reg_dnn = reg_dnn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.linear_dict = OrderedDict()</span><br><span class="line">        self.embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.linear_dict[name] = keras.layers.Dense(<span class="number">1</span>, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                                            kernel_regularizer=keras.regularizers.l2(self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Dense(self.embedding_dim, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>,</span><br><span class="line">                                                               kernel_regularizer=keras.regularizers.l2(self.reg_fm))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.linear_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                    self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_fm))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.linear_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=max_len, mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                    self.reg_linear))</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=max_len,</span><br><span class="line">                                                                   mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_fm))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf.print(<span class="string">'Feature type error.'</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.dnn_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        <span class="comment"># self.dnn_layer_1 = keras.layers.Dense(64, activation='relu',</span></span><br><span class="line">        <span class="comment">#                                       kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span></span><br><span class="line">        <span class="comment"># self.dnn_layer_2 = keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span></span><br><span class="line">        self.dnn_layer_1 = keras.layers.Dense(<span class="number">1</span>, kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.bias = tf.Variable(0, dtype=tf.float32)</span></span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        linear_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, w_layer <span class="keyword">in</span> self.linear_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span> <span class="keyword">or</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>,))</span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                <span class="comment"># real_len = tf.reshape(tf.reduce_sum(mask, axis=1), (-1, 1))</span></span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>]))</span><br><span class="line">                ft = ft * mask</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># ft = ft / real_len</span></span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">        linear_logits = tf.reduce_sum(linear_res_list, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># fm</span></span><br><span class="line">        fm_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, v_layer <span class="keyword">in</span> self.embedding_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">        sum_square = tf.pow(tf.reduce_sum(fm_res_list, axis=<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        square_sum = tf.reduce_sum(tf.pow(fm_res_list, <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">        fm_logits = tf.reduce_sum(<span class="number">0.5</span> * (sum_square - square_sum), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dnn</span></span><br><span class="line">        dnn_inputs = tf.concat(fm_res_list, axis=<span class="number">1</span>)</span><br><span class="line">        dnn_ft = self.dnn_layer_0(dnn_inputs)</span><br><span class="line">        <span class="comment"># dnn_ft = self.dnn_layer_1(dnn_ft)</span></span><br><span class="line">        <span class="comment"># dnn_logits = self.dnn_layer_2(dnn_ft)</span></span><br><span class="line"></span><br><span class="line">        dnn_logits = tf.reshape(self.dnn_layer_1(dnn_ft), shape=(<span class="number">-1</span>,))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output</span></span><br><span class="line">        logits = linear_logits + fm_logits + dnn_logits</span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = DeepFM(feature_info_dict,</span><br><span class="line">               embedding_dim=<span class="number">16</span>,</span><br><span class="line">               reg_linear=<span class="number">5e-4</span>,</span><br><span class="line">               reg_fm=<span class="number">5e-5</span>,</span><br><span class="line">               reg_dnn=<span class="number">1e-4</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8382, val: 0.7961, test: 0.7705</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>对比到目前各排序模型的表现:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LR</td>
<td style="text-align:center">0.7616</td>
<td style="text-align:center">0.7348</td>
</tr>
<tr>
<td style="text-align:center">GBDT+LR</td>
<td style="text-align:center">0.7628</td>
<td style="text-align:center">0.7352</td>
</tr>
<tr>
<td style="text-align:center">FM</td>
<td style="text-align:center">0.7834</td>
<td style="text-align:center">0.7540</td>
</tr>
<tr>
<td style="text-align:center">Embedding+MLP</td>
<td style="text-align:center">0.7937</td>
<td style="text-align:center">0.7690</td>
</tr>
<tr>
<td style="text-align:center">DeepFM</td>
<td style="text-align:center">0.7961</td>
<td style="text-align:center">0.7705</td>
</tr>
</tbody>
</table>
</div>
<p>从结果来看, 在验证集上比Embedding+MLP高$2.4$个千分点, 在测试集上高出了$1.5$个千分点.</p>
<p>DeepFM通过结合LR, FM, DNN的结构, 使其同时具备了多种特征交叉的模式, 可以更好地应对各种情况下的数据.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>FM</tag>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>Embedding+MLP排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/Embedding-MLP%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>这里来试一下用MLP(多层感知机)来作为排序模型, 看一下效果如何.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>MLP就是最原始的神经网络模型, 原理的话我想应该不比我多说了♪(^∇^*)</p>
<p>在前面, 已经试过了FM, 其在LR的基础上, 通过对特征进行Embedding, 并使用Embedding的内积, 来获取特征之间的交叉信息, 相比LR有着明显的提升.</p>
<p>但是特征交叉的方式, 并不止一种, 让我捋一捋我知道的几种, 比如树模型里面同一路径下不同特征用来划分节点; 比如支持向量机里面的核函数; 比如神经网络中的全连接方式; 比如在给出特征Embedding时, 可以对Embedding采取的交叉计算, 如内积, 外积, 哈达玛积…</p>
<p>所以, 从某种角度, 可以将MLP也看做另一种特征的交叉方式.</p>
<p>而在这里, 不仅仅使用MLP, 而是使用Embedding+MLP, 这是为什么呢?</p>
<p>一般来说, 如果原本是一些稠密的连续数值型特征, 那么就可以直接使用MLP. 而当特征当中包含了大量离散稀疏特征的时候, 这时候再直接使用全连接的MLP, 那么在保证隐藏层神经元数量的情况下, 输入层的参数可能会很大. 假如离散特征有10000个, 第一层隐藏层有100个神经元, 那么直接MLP的参数量为一百万. 而在先对特征进行Embedding, 再拼接输入后, 可以减少参数量, 比如这10000个离散特征, 原本是由100个特征独热编码得到的, Embedding向量的维度设为10, 那么输入层的参数量为$10000\times 10+100\times 10\times 100=200000$, 为原本的$1/5$.</p>
<p>使用Embedding+MLP还有一个好处, 就是如果后续想对MLP进行改造, 添加一些其它的功能或者计算方式, 那么在基于Embedding的情况下, 会更加方便和更具有多样性.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>对入模特征来说, 会分为用户侧特征和物品侧特征. 在这里, 用户侧的特征可以包括用户ID, 用户的基本信息(性别, 年龄, 职业), 用户的历史行为(近期10部点击的电影). 物品侧的特征包括电影ID, 电影的风格流派.</p>
<p>按时间, 即将时间靠后的样本划分为测试集. 同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Embedding+MLP</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNN</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, embedding_dim=<span class="number">16</span>, reg_embedding=<span class="number">1e-5</span>, reg_dnn=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param embedding_dim: 隐向量维度.</span></span><br><span class="line"><span class="string">        :param reg_embedding: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        :param reg_dnn: DNN正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg_embedding = reg_embedding</span><br><span class="line">        self.reg_dnn = reg_dnn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.embedding_dict[name] = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_embedding))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.embedding_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=max_len,</span><br><span class="line">                                                                   mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                   embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                       self.reg_embedding))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf.print(<span class="string">'Feature type error.'</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.dnn_layer_0 = keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        self.dnn_layer_1 = keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line">        self.dnn_layer_2 = keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">                                              kernel_regularizer=keras.regularizers.l2(self.reg_dnn))</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dnn</span></span><br><span class="line">        dnn_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, v_layer <span class="keyword">in</span> self.embedding_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                dnn_res_list.append(ft)</span><br><span class="line"></span><br><span class="line">        dnn_inputs = tf.concat(dnn_res_list, axis=<span class="number">1</span>)</span><br><span class="line">        dnn_ft = self.dnn_layer_0(dnn_inputs)</span><br><span class="line">        dnn_ft = self.dnn_layer_1(dnn_ft)</span><br><span class="line">        output = self.dnn_layer_2(dnn_ft)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = DNN(feature_info_dict,</span><br><span class="line">            embedding_dim=<span class="number">16</span>,</span><br><span class="line">            reg_embedding=<span class="number">1e-3</span>,</span><br><span class="line">            reg_dnn=<span class="number">1e-05</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8336, val: 0.7937, test: 0.7690</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>对比到目前各排序模型的表现:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LR</td>
<td style="text-align:center">0.7616</td>
<td style="text-align:center">0.7348</td>
</tr>
<tr>
<td style="text-align:center">GBDT+LR</td>
<td style="text-align:center">0.7628</td>
<td style="text-align:center">0.7352</td>
</tr>
<tr>
<td style="text-align:center">FM</td>
<td style="text-align:center">0.7834</td>
<td style="text-align:center">0.7540</td>
</tr>
<tr>
<td style="text-align:center">Embedding+MLP</td>
<td style="text-align:center">0.7937</td>
<td style="text-align:center">0.7690</td>
</tr>
</tbody>
</table>
</div>
<p>从结果来看, 比之前相比LR有较大提升的FM, 还要好一些, 在验证集上提升了约1个百分点, 在测试集上提升了约$1.5$个百分点.</p>
<p>这说明Embedding+MLP在这份数据集上, 是要比FM要更适合的, 但不能说明Embedding+MLP就在其它一些数据集上, 也会比FM表现好.</p>
<p>在FM中, 对最终结果造成影响的, 是各个原始(一阶)的特征, 以及二阶交叉特征. 而MLP则可以看做是多个特征共同的高阶复杂交叉, 来对目标进行预测. 两者在不同场景下, 可能各有优劣.</p>
<p>那么, 有没有可能把两者结合起来呢, 有的嗷, 在下一篇中, 会介绍FM和MLP的结合, 即DeepFM ♪(^∇^*)</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>FM排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/FM%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>在<a href="whitemoonlight.top/2020/10/10/传统机器学习/FM/">之前一篇</a>文章中, 讲解了FM算法的原理, 在这一篇中将使用TensorFlow实现FM算法, 并用于排序.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>某种程度上, 我认为可以把FM(因子分解机)看成LR(逻辑回归)的升级版, 在其原本的基础上, 实现了特征的自动交叉, 并使其具有一定的泛化性.</p>
<p>一般来说, 会将FM特征交叉的阶数限制到二阶, 这又有点类似于树深度为二的GBDT, 但GBDT对于稀疏数据的处理实属不行(其它方面有优点).</p>
<p>相比之下, 我认为是拿到稀疏数据后, 如果想迅速地构建一个比较强的baseline模型, 那么FM可以优先尝试.</p>
<p>下面, 就用TensorFlow实现一个FM算法, 并在一份数据上查看表现.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>对入模特征来说, 会分为用户侧特征和物品侧特征. 在这里, 用户侧的特征可以包括用户ID, 用户的基本信息(性别, 年龄, 职业), 用户的历史行为(近期10部点击的电影). 物品侧的特征包括电影ID, 电影的风格流派.</p>
<p>按时间, 即将时间靠后的样本划分为测试集. 同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练集/验证集/测试集</span></span><br><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># FM model</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FM</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, embedding_dim=<span class="number">16</span>, reg_w=<span class="number">1e-3</span>, reg_v=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param embedding_dim: 隐向量维度.</span></span><br><span class="line"><span class="string">        :param reg_w: 线性部分正则系数.</span></span><br><span class="line"><span class="string">        :param reg_v: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.reg_w = reg_w</span><br><span class="line">        self.reg_v = reg_v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.w_dict = OrderedDict()</span><br><span class="line">        self.v_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.w_dict[name] = keras.layers.Dense(<span class="number">1</span>, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>, kernel_regularizer=keras.regularizers.l2(self.reg_w))</span><br><span class="line">                self.v_dict[name] = keras.layers.Dense(self.embedding_dim, input_dim=<span class="number">1</span>, use_bias=<span class="literal">False</span>, kernel_regularizer=keras.regularizers.l2(self.reg_v))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.w_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=<span class="number">1</span>, embeddings_regularizer=keras.regularizers.l2(self.reg_w))</span><br><span class="line">                self.v_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=<span class="number">1</span>, embeddings_regularizer=keras.regularizers.l2(self.reg_v))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.w_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=max_len, mask_zero=<span class="literal">True</span>, embeddings_regularizer=keras.regularizers.l2(self.reg_w))</span><br><span class="line">                self.v_dict[name] = keras.layers.Embedding(num, self.embedding_dim, input_length=max_len, mask_zero=<span class="literal">True</span>, embeddings_regularizer=keras.regularizers.l2(self.reg_v))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf.print(<span class="string">'Feature type error.'</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.bias = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        linear_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, w_layer <span class="keyword">in</span> self.w_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span> <span class="keyword">or</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>,))</span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                <span class="comment"># real_len = tf.reshape(tf.reduce_sum(mask, axis=1), (-1, 1))</span></span><br><span class="line">                ft = w_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>]))</span><br><span class="line">                ft = ft * mask</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># ft = ft / real_len</span></span><br><span class="line">                linear_res_list.append(ft)</span><br><span class="line">        linear_logits = tf.reduce_sum(linear_res_list, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># fm</span></span><br><span class="line">        fm_res_list = []</span><br><span class="line">        <span class="keyword">for</span> name, v_layer <span class="keyword">in</span> self.v_dict.items():</span><br><span class="line">            type_ = self.feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'dense'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = v_layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                fm_res_list.append(ft)</span><br><span class="line">        sum_square = tf.pow(tf.reduce_sum(fm_res_list, axis=<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        square_sum = tf.reduce_sum(tf.pow(fm_res_list, <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">        fm_logits = tf.reduce_sum(<span class="number">0.5</span> * (sum_square - square_sum), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output</span></span><br><span class="line">        logits = self.bias + linear_logits + fm_logits</span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf.random.set_seed(7)</span></span><br><span class="line">reset_random()</span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = FM(feature_info_dict, embedding_dim=<span class="number">16</span>, reg_w=<span class="number">1e-2</span>, reg_v=<span class="number">1e-5</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(</span><br><span class="line">    x=train_set,</span><br><span class="line">    y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">    validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment">#           verbose=0,</span></span><br><span class="line">    callbacks=[</span><br><span class="line">        keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                      patience=<span class="number">5</span>,</span><br><span class="line">                                      restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                      mode=<span class="string">'max'</span>,</span><br><span class="line">                                      min_delta=<span class="number">0.0003</span>),</span><br><span class="line">        keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                          factor=<span class="number">0.2</span>,</span><br><span class="line">                                          patience=<span class="number">1</span>,</span><br><span class="line">                                          mode=<span class="string">'max'</span>,</span><br><span class="line">                                          min_delta=<span class="number">0.0003</span>),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.8325, val: 0.7834, test: 0.7540</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这里对比一下之前排序模型的表现:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LR</td>
<td style="text-align:center">0.7616</td>
<td style="text-align:center">0.7348</td>
</tr>
<tr>
<td style="text-align:center">GBDT+LR</td>
<td style="text-align:center">0.7628</td>
<td style="text-align:center">0.7352</td>
</tr>
<tr>
<td style="text-align:center">FM</td>
<td style="text-align:center">0.7834</td>
<td style="text-align:center">0.7540</td>
</tr>
</tbody>
</table>
</div>
<p>从结果上看, FM模型相比LR以及GBDT+LR模型, 在验证集和测试集上均提升了约2个百分点, 可以说提升是非常明显的, 这样印证了特征交叉的重要性与有效性.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>FM</tag>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT+LR排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/GBDT-LR%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>好的, 今天使用GBDT+LR来实现推荐系统中的排序模型.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对本宝而言, GBDT和LR可以说是用得最多的模型, 但是由于任务场景与推荐系统不同, 所以数据的类型, 以及模型的使用方式和推荐系统中的有一些差别.</p>
<p>在推荐系统中, 数据的一大特点就是稀疏性, 这个稀疏性通常是由于一些ID类特征, 高基数类别特征导致的, 也可以由一些特征交叉而形成稀疏性.</p>
<p>众所周知, GBDT在面对结构型数据时, 是一种使用方便, 且效果很好的算法, 这一点从许多相关的比赛中就可以看得出来.</p>
<p>而对于稀疏数据, GBDT就会呈现出其无力的一面, 关于这一点的讨论, 可以看一下我这篇介绍<a href="whitemoonlight.top/2020/10/10/传统机器学习/GBDT-一/">GBDT</a>的文章.</p>
<p>那么, 既然如此, 那就用线性模型(如逻辑回归)就好了呀. 虽然线性模型在面对稀疏数据时, 不容易过拟合, 但是仍然存在一个问题, 就是模型过于简单, 没有特指之间的交叉, 它学不好啊QAQ 关于这一点, 可以查看我在介绍<a href="/2020/10/10/传统机器学习/FM/">FM</a>时, 对特征交叉重要性的讨论.</p>
<p>现在, 如果将GBDT与LR结合起来, 是否可以获取两者的优点, 以取得更好的效果呢?</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><img src="fig_0.jpg" alt="fig"></p>
<p>首先, 将一些不那么稀疏的特征, 放入GBDT进行学习,  并将GBDT当成一种特征转换器, 可以把GBDT的所有叶子节点看做一系列转换后的特征组成的向量. 当某个样本落在某个叶子节点上时, 其值为1, 否则为0.</p>
<p>这样得到的特征, 再与其余的稀疏特征拼接在一起, 送入到LR进行学习, 作为最终的模型.</p>
<p>是的, GBDT+LR的原理就是这么简单, 或者说这都算不上是啥原理, 顶多就是一个流程.</p>
<p>利用GBDT, 可以看做自动化地做了一些特征的交叉与转换, 而LR则可以进一步融入稀疏数据的信息.</p>
<h2 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h2><p>从GBDT+LR的出发点, 以及思路上来说, 好像挺好的, 但是…好像还是有一些问题.</p>
<p>用GBDT做自动特征工程, 获取特征之间的非线性交叉信息, 而GBDT本身对稀疏数据难以处理, 所以仅处理一些非稀疏数据. 但是, 如果原本的数据中, 大部分都是稀疏特征呢?</p>
<p>再来看看LR这里, 经过GBDT转换后的特征, 以及原本的稀疏特征一起作为LR的输入特征… 这里的稀疏特征是没有交叉的!</p>
<p>不慌, 手动对这些原本的稀疏特征做交叉就好, 然鹅现在又面临一些问题:</p>
<ul>
<li>直接交叉以后维度过大.</li>
<li>其中一些交叉特征是不具备泛化性的.</li>
</ul>
<p>关于上面第一个问题, 比如原本1万的用户, 和1万的物品, 交叉后多少维, 1亿啊QAQ</p>
<p>第二个问题, 在训练集中, 有用户A点击过物品B, 那么就可以学习到用户A-物品B的交互特征对应的权重系数. 而在测试集中, 每个用户和每个物品的交互, 都是在训练集中没有出现过的, 也就意味着这个看似强力的特征, 其实在LR这里完全没用!</p>
<p>不过在LR中添加交叉特征时, 可以采用另外一些方式, 使其既不会维度过大, 也能具有一定的泛化性. 比如用户ID和物品ID的交叉, 不是用他们独热编码进行简单的相乘, 而是用一些其它算法, 比如矩阵分解, DSSM得到的向量, 做内积来进行交叉. 或者还可以由一些算法, 如item2vec, 得到物品的向量, 然后用户向量可以由其行为序列, 基于物品向量得到.</p>
<p>就是说, GBDT本身在推荐系统这样的场景下, 可能作用非常有限. 而LR一来并不能完全依赖GBDT, 二来对于特征的交叉与构造, 需要花费较大的人力, 才能获得相对不错的效果. 这好像就有点违背初衷了啊, 原本想的是用了GBDT+LR的组合, 即方便又好用, 但是实际上仍然避免不了对数据的深入挖掘和特征工程, 啊这…</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>由于是将GBDT+LR用于推荐系统中的排序任务, 所以为了达到更好的效果, 会尝试使用尽可能多的特征.</p>
<p>具体说来, 会将特征分为用户侧特征和物品侧特征. 在这里, 用户侧的特征可以包括用户ID, 用户的基本信息(性别, 年龄, 职业), 用户的历史行为(近期10部点击的电影). 物品侧的特征包括电影ID, 电影的风格流派.</p>
<p>其中, 将并不是那么稀疏的特征, 包括性别, 年龄, 职业, 电影分类, 加入GBDT进行学习. 然后将GBDT的输出(叶子节点), 结合原始的特征(包括用户/物品ID)一起作为LR的入模特征.</p>
<p>同样的, 也要在预处理时对活跃用户进行下采样OvO</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((776916, 9), (70916, 9))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备GBDT的输入数据</span></span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df_gbdt = df[[<span class="string">'is_test'</span>, <span class="string">'label'</span>]]</span><br><span class="line">df_gbdt = pd.concat([df_gbdt, pd.DataFrame(df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: &#123;i: <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i != <span class="number">0</span>&#125;).tolist())], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    df_gbdt = pd.concat([df_gbdt, pd.get_dummies(df[ft], prefix=ft)], axis=<span class="number">1</span>)</span><br><span class="line">df_gbdt = df_gbdt.fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ft_list = df_gbdt.columns.tolist()</span><br><span class="line">ft_list.remove(<span class="string">'is_test'</span>)</span><br><span class="line">ft_list.remove(<span class="string">'label'</span>)</span><br><span class="line"></span><br><span class="line">df_train = df_gbdt.loc[df_gbdt[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df_gbdt.loc[df_gbdt[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 50), (70000, 50), (65866, 50))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_sample_weight</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, log_loss</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">weight = compute_sample_weight(class_weight=<span class="string">'balanced'</span>, y=df_train[<span class="string">'label'</span>])</span><br><span class="line">train_set = lgb.Dataset(data=df_train[ft_list],</span><br><span class="line">                        label=df_train[<span class="string">'label'</span>],</span><br><span class="line">                        free_raw_data=<span class="literal">False</span>,</span><br><span class="line">                        weight=weight)</span><br><span class="line"></span><br><span class="line">weight = compute_sample_weight(class_weight=<span class="string">'balanced'</span>, y=df_val[<span class="string">'label'</span>])</span><br><span class="line">val_set = lgb.Dataset(data=df_val[ft_list],</span><br><span class="line">                      label=df_val[<span class="string">'label'</span>],</span><br><span class="line">                      free_raw_data=<span class="literal">False</span>,</span><br><span class="line">                      weight=weight)</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">    <span class="string">'num_tree'</span>: <span class="number">50</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'num_leaf'</span>: <span class="number">256</span>,</span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'min_data_in_leaf'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'min_sum_hessian_in_leaf'</span>: <span class="number">0.</span>,</span><br><span class="line">    <span class="string">'lambda_l1'</span>: <span class="number">0.</span>,</span><br><span class="line">    <span class="string">'lambda_l2'</span>: <span class="number">0.</span>,</span><br><span class="line">    <span class="string">'min_split_gain'</span>: <span class="number">0.</span>,</span><br><span class="line">    <span class="string">'metrics'</span>: <span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">'num_thread'</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = lgb.train(</span><br><span class="line">    params=params,</span><br><span class="line">    train_set=train_set,</span><br><span class="line">    valid_sets=val_set,</span><br><span class="line">    valid_names=[<span class="string">'val'</span>],</span><br><span class="line">    verbose_eval=<span class="literal">False</span>,</span><br><span class="line">    callbacks=[lgb.early_stopping(<span class="number">10</span>, first_metric_only=<span class="literal">True</span>, verbose=<span class="literal">False</span>)])</span><br><span class="line"></span><br><span class="line">train_auc = roc_auc_score(df_train[<span class="string">'label'</span>], model.predict(df_train[ft_list]))</span><br><span class="line">val_auc = roc_auc_score(df_val[<span class="string">'label'</span>], model.predict(df_val[ft_list]))</span><br><span class="line">test_auc = roc_auc_score(df_test[<span class="string">'label'</span>], model.predict(df_test[ft_list]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.6341, val: 0.6246, test: 0.6133</span><br></pre></td></tr></table></figure>
<p>少了用户与电影的ID类特征, 可以看到即使是用GBDT, 也不能获得比较好的效果.</p>
<h2 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用GBDT将数据进行转换</span></span><br><span class="line"></span><br><span class="line">df_gbdt_leaf = model.predict(df_gbdt[ft_list], pred_leaf=<span class="literal">True</span>)</span><br><span class="line">num_leaf = df_gbdt_leaf.shape[<span class="number">1</span>]</span><br><span class="line">df_gbdt_leaf = pd.DataFrame(df_gbdt_leaf, columns=[<span class="string">'leaf_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_leaf)])</span><br><span class="line">gbdt_leaf_list = df_gbdt_leaf.columns.tolist()</span><br><span class="line">len(gbdt_leaf_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">49</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> gbdt_leaf_list:</span><br><span class="line">    num = df_gbdt_leaf[ft].max() + <span class="number">1</span></span><br><span class="line">    feature_info_dict[ft] = sparse_feature(num)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练集/验证集/测试集</span></span><br><span class="line">df = pd.concat([df, df_gbdt_leaf], axis=<span class="number">1</span>)</span><br><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 59), (70000, 59), (65866, 59))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> gbdt_leaf_list:</span><br><span class="line">    train_set[ft] = df_train[ft].values</span><br><span class="line">    val_set[ft] = df_val[ft].values    </span><br><span class="line">    test_set[ft] = df_test[ft].values</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LR模型</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, reg=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param reg: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.dense_ft_list = []</span><br><span class="line">        self.sparse_ft_list = []</span><br><span class="line">        self.varlen_sparse_ft_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.dense_ft_list.append(name)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                self.sparse_ft_list.append(name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.varlen_sparse_ft_list.append(name)</span><br><span class="line"></span><br><span class="line">        self.sparse_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> self.sparse_ft_list:</span><br><span class="line">            num = self.feature_info_dict[name][<span class="string">'num'</span>]</span><br><span class="line">            self.sparse_embedding_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                      embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                          self.reg))</span><br><span class="line">        self.varlen_sparse_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> self.varlen_sparse_ft_list:</span><br><span class="line">            num = self.feature_info_dict[name][<span class="string">'num'</span>]</span><br><span class="line">            max_len = self.feature_info_dict[name][<span class="string">'max_len'</span>]</span><br><span class="line"></span><br><span class="line">            self.varlen_sparse_embedding_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=max_len,</span><br><span class="line">                                                                             mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                             embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                                 self.reg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(self.dense_ft_list) &gt; <span class="number">0</span>:</span><br><span class="line">            self.dense_layer = keras.layers.Dense(<span class="number">1</span>, use_bias=<span class="literal">False</span>, kernel_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line"></span><br><span class="line">        self.bias = tf.Variable(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dense</span></span><br><span class="line">        <span class="keyword">if</span> len(self.dense_ft_list) &gt; <span class="number">0</span>:</span><br><span class="line">            dense_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.dense_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                dense_ft_list.append(ft)</span><br><span class="line">            dense_ft_list = tf.concat(dense_ft_list, axis=<span class="number">1</span>)</span><br><span class="line">            dense_logits = self.dense_layer(dense_ft_list)</span><br><span class="line">            dense_logits = tf.reshape(dense_logits, shape=(<span class="number">-1</span>,))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dense_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># sparse</span></span><br><span class="line">        <span class="keyword">if</span> len(self.sparse_embedding_dict) &gt; <span class="number">0</span>:</span><br><span class="line">            sparse_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.sparse_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                layer = self.sparse_embedding_dict[name]</span><br><span class="line">                ft = layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                sparse_ft_list.append(ft)</span><br><span class="line">            sparse_logits = tf.reduce_sum(tf.concat(sparse_ft_list, axis=<span class="number">1</span>), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sparse_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># varlen_sparse</span></span><br><span class="line">        <span class="keyword">if</span> len(self.varlen_sparse_embedding_dict) &gt; <span class="number">0</span>:</span><br><span class="line">            varlen_sparse_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.varlen_sparse_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                layer = self.varlen_sparse_embedding_dict[name]</span><br><span class="line">                ft = layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reshape(tf.reduce_sum(ft, axis=<span class="number">1</span>), shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                varlen_sparse_ft_list.append(ft)</span><br><span class="line">            varlen_sparse_logits = tf.reduce_sum(tf.concat(varlen_sparse_ft_list, axis=<span class="number">1</span>), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            varlen_sparse_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        logits = self.bias + dense_logits + sparse_logits + varlen_sparse_logits</span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = LR(feature_info_dict, reg=<span class="number">1e-6</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.7822, val: 0.7628, test: 0.7352</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>对比之前单独使用的LR模型:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LR</td>
<td style="text-align:center">0.7616</td>
<td style="text-align:center">0.7348</td>
</tr>
<tr>
<td style="text-align:center">GBDT</td>
<td style="text-align:center">0.6246</td>
<td style="text-align:center">0.6133</td>
</tr>
<tr>
<td style="text-align:center">GBDT+LR</td>
<td style="text-align:center">0.7628</td>
<td style="text-align:center">0.7352</td>
</tr>
</tbody>
</table>
</div>
<p>可以发现, 在这里GBDT+LR, 相比单独的LR在验证集上仅高1个千分点,  在测试集上几乎没有区别, 原因应该是非稀疏特征较少, 且交叉后相比没有交叉, 并没有提供更多有效信息.</p>
<p>其实, 本宝是顶着鸭梨, 尝试了一波把用户ID和电影ID进行独热编码后, 加入GBDT一起进行训练的, 得到的结果是, 并不比不加更好(验证集和测试集上差不多), 而且会带来更大的方差(训练集与验证集之间的差距增大). 所以在数据比较稀疏的时候, 感觉GBDT确实不是一个好的选择QAQ</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
        <tag>逻辑回归</tag>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>LR排序</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/LR%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>尽管LR(逻辑回归)是非常简单的模型, 但是由于其诸多的优点, 使其在一些时候, 仍然可以作为推荐系统中的排序模型, 或者作为一个baseline.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在我的<a href="whitemoonlight.top/2020/10/10/传统机器学习/逻辑回归/">逻辑回归</a>这篇博客中, 比较详细地讲解了LR算法的原理.</p>
<p>总体来说, LR有不少优点, 包括可解释性, 训练与部署模型相对便捷, 可以应对稀疏数据, 比较容易控制过拟合等. 而其最为明显的缺点, 我认为应该就是模型本身不能够学习到特征之间的交互信息.</p>
<p>在推荐系统的召回阶段, 面对的是海量的物品, 需要用一些相对轻量级的, 快速的方法, 从中找出用户可能感兴趣的物品候选集. 在排序阶段, 面对的是由召回阶段返回的少量的物品, 但是相对的, 需要尽可能多的信息(特征), 去进一步对用户对这些物品的兴趣度进行估计和排序.</p>
<p>所以, 在排序阶段, 相比召回阶段用到的特征更多, 而且其中可能包含一些ID类的特征, 会使得对应的独热编码维度很高, 不过这在逻辑回归这里并不是难以处理的地方.</p>
<p>前面也说到了, 对LR来说, 其短板在于不能自动学到特征之间的交互, 关于特征的交互的重要性, 在我的<a href="whitemoonlight.top/2020/10/10/传统机器学习/FM/">FM</a>这篇文章中有过讨论, 总之就是非常重要啦OvO</p>
<p>因此, 想要用LR模型获得更好的效果, 就要花更多的功夫来对数据进行预处理, 进行更多的特征工程. 比如通过对特征的分析, 发现一些特征交叉后能够得到比较强力的特征(简称强特征), 那么就把这样的强特征也一起加入到LR中来进行学习.</p>
<p>不过有时候一些特征的交叉是有困难的, 比如ID类(如用户ID, 物品ID)的特征, 如果直接交叉, 维度爆炸QAQ 而且即便不看维度, 这样直接交叉也没有用, 比如要预测用户A是否对物品B感兴趣, 那么它们的交叉特征在训练集中, 永远都是0, 是无法学习的. 这样的情况也可以称为记忆性, 即可以把看到的东西学得较好, 但是难以对没有看过的进行泛化. 有一些解决的方法, 就是用其它一些算法(如矩阵分解)得到的向量, 来进行替换, 这样进行交叉, 即解决了维度问题, 也能有一定的泛化性.</p>
<p>总之, LR仍然可以一试, 不过可能相比其它一些模型, 需要耗费更多的人力来提升模型效果.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里使用的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟点击预估的情形, 统计了每个用户的平均打分, 将高于平均打分的看做正样本(点击, 1), 低于平均分的看做负样本(曝光未点击, 0).</p>
<p>在入模特征的选择上, 尽可能包含原始的用户侧以及物品侧的特征, 并且还加入了用户历史行为特征, 即近期用户点击过的10部影片的ID序列.</p>
<p>不过这里并不包含交叉特征, 嗯OvO</p>
<p>并且下采样活跃用户, 是因为推荐系统是服务于整个用户群体的, 当样本中存在一些非常活跃的用户时, 其对损失函数的影响会更多, 进而影响模型的训练. 但是这些活跃用户就一定比其他用户重要很多吗, 当然不是的, 所以从整体考虑, 需要对活跃用户进行下采样.</p>
<p>这时候可能有同学会问了, 既然对活跃用户下采样了, 那么为什么没有对热门物品(样本中出现较多)下采样呢? 从业务的角度考虑, 物品与用户是不同的, 每个用户都比较重要, 都希望使其获得较好的体验. 而物品是确实有差别的, 有的物品质量就是好, 大家都喜欢, 有的物品就是差, 大家都不喜欢, 当一些物品经常出现在正样本中时, 会让模型知道, 这样的物品是受欢迎的.</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户-电影-打分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户</span></span><br><span class="line">user_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_df, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>, <span class="string">'zip_code'</span>])</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip_code'</span>]</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line">item_category_dict = dict(zip(item_df[<span class="string">'item_id'</span>], item_df[<span class="string">'category_list'</span>]))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接数据</span></span><br><span class="line">df = pd.merge(left=df, right=user_df, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df = pd.merge(left=df, right=item_df, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'score'</span>]</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'avg_score'</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">time</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">job</th>
<th style="text-align:right">category_list</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Musical]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Musical, Romance]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Drama]</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">[Animation, Children’s, Comedy]</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户每次点击/未点击之前的点击记录</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">df = df.sort_values([<span class="string">'user_id'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line">    time = line[<span class="string">'time'</span>]</span><br><span class="line">    label = line[<span class="string">'label'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = []</span><br><span class="line">    user_items_dict[user].append((item, time, label))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_list:</span><br><span class="line">        user_list.append(user)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> user_list[<span class="number">-1</span>] != user:</span><br><span class="line">            user_list.append(user)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">df_list = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    tmp_item_list = user_items_dict[user]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(user_items_dict[user])):</span><br><span class="line">        tmp_data = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list[: i] <span class="keyword">if</span> x[<span class="number">2</span>] == <span class="number">1</span>] <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">else</span> []</span><br><span class="line">        tmp_list.append(tmp_data)</span><br><span class="line">    df_list += tmp_list</span><br><span class="line">    </span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">'recent_click'</span>] = df_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> df_list</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1000209, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下采样活跃用户</span></span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &gt; <span class="number">800</span>]</span><br><span class="line">tmp_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = tmp_df.append(df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">800</span>, random_state=<span class="number">7</span>))</span><br><span class="line">df = tmp_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(976564, 9)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">800000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">800000</span>:]</span><br><span class="line">df_train[<span class="string">'is_test'</span>] = <span class="number">0</span></span><br><span class="line">df_test[<span class="string">'is_test'</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去除训练集记录过少的用户和电影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_train_len = <span class="number">0</span></span><br><span class="line">n = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> df_train_len != df_train.shape[<span class="number">0</span>]:</span><br><span class="line">    df_train_len = df_train.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'循环第%d次'</span> % n)</span><br><span class="line">    <span class="comment"># 将行为序列少于10的样本去除</span></span><br><span class="line">    df_train = df_train.loc[df_train[<span class="string">'recent_click'</span>].apply(len) &gt;= <span class="number">10</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户出现次数少于30的用户去除</span></span><br><span class="line">    sample_user = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'user_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">30</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'user_id'</span>].isin(sample_user)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将电影出现次数少于20的电影去除</span></span><br><span class="line">    sample_item = [x <span class="keyword">for</span> x, y <span class="keyword">in</span> df_train.groupby(<span class="string">'item_id'</span>)[<span class="string">'label'</span>].count().to_dict().items() <span class="keyword">if</span> y &lt; <span class="number">20</span>]</span><br><span class="line">    df_train = df_train.loc[~df_train[<span class="string">'item_id'</span>].isin(sample_item)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将行为序列中对应的电影也去除</span></span><br><span class="line">    user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">    item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line"></span><br><span class="line">    df_train[<span class="string">'recent_click'</span>] = df_train[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_set = set(df_train[<span class="string">'user_id'</span>].unique().tolist())</span><br><span class="line">item_set = set(df_train[<span class="string">'item_id'</span>].unique().tolist())</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line">df_test[<span class="string">'recent_click'</span>] = df_test[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [i <span class="keyword">for</span> i <span class="keyword">in</span> x <span class="keyword">if</span> i <span class="keyword">in</span> item_set])</span><br><span class="line"></span><br><span class="line">df_train.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">循环第1次</span><br><span class="line">循环第2次</span><br><span class="line">循环第3次</span><br><span class="line">循环第4次</span><br><span class="line">((672717, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据编码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line">df = df_train.append(df_test)</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'job'</span>]:</span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    df[ft] = le.fit_transform(df[ft])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影类别编码</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        category_set.add(i)</span><br><span class="line">category_set = list(sorted(category_set))</span><br><span class="line"></span><br><span class="line">category_ids_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'category_list'</span>] = df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x] + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">df[<span class="string">'category_list'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">922815     [5, 14, 0, 0, 0, 0]</span><br><span class="line">922816    [3, 4, 5, 12, 14, 0]</span><br><span class="line">922817    [1, 14, 16, 0, 0, 0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 电影编码</span></span><br><span class="line">item_ids_dict = dict(zip(df[<span class="string">'item_id'</span>].unique(), range(<span class="number">1</span>, len(df[<span class="string">'item_id'</span>].unique()) + <span class="number">1</span>)))</span><br><span class="line">df[<span class="string">'item_id'</span>] = df[<span class="string">'item_id'</span>].map(item_ids_dict)</span><br><span class="line"></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">-10</span>:])  <span class="comment"># 截取10部电影</span></span><br><span class="line">df[<span class="string">'recent_click'</span>] = df[<span class="string">'recent_click'</span>].apply(<span class="keyword">lambda</span> x: [item_ids_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Int64Index: 738583 entries, 922815 to 120526</span><br><span class="line">Data columns (total 10 columns):</span><br><span class="line">user_id          738583 non-null int64</span><br><span class="line">item_id          738583 non-null int64</span><br><span class="line">time             738583 non-null object</span><br><span class="line">gender           738583 non-null int64</span><br><span class="line">age              738583 non-null int64</span><br><span class="line">job              738583 non-null int64</span><br><span class="line">category_list    738583 non-null object</span><br><span class="line">label            738583 non-null int64</span><br><span class="line">recent_click     738583 non-null object</span><br><span class="line">is_test          738583 non-null int64</span><br><span class="line">dtypes: int64(7), object(3)</span><br><span class="line">memory usage: 62.0+ MB</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">3756</span>),</span><br><span class="line">                    <span class="string">'item_id'</span>: sparse_feature(<span class="number">2805</span>),</span><br><span class="line">                    <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>),</span><br><span class="line">                    <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>),</span><br><span class="line">                    <span class="string">'job'</span>: sparse_feature(<span class="number">21</span>),</span><br><span class="line">                    <span class="string">'category_list'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>),</span><br><span class="line">                    <span class="string">'recent_click'</span>: varlen_sparse_feature(<span class="number">2805</span>, <span class="number">10</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练集/验证集/测试集</span></span><br><span class="line">df_train = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">0</span>]</span><br><span class="line">df_test = df.loc[df[<span class="string">'is_test'</span>] == <span class="number">1</span>]</span><br><span class="line">df_val = df_train.sample(n=<span class="number">70000</span>, random_state=<span class="number">7</span>)</span><br><span class="line">df_train = df_train.loc[~df_train.index.isin(df_val.index.tolist())]</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">((602717, 10), (70000, 10), (65866, 10))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换为字典格式</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_train[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_train[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_train[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_train[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_train[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_train[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_train[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_val[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_val[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_val[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_val[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_val[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_val[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_val[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_set = &#123;</span><br><span class="line">    <span class="string">'user_id'</span>: df_test[<span class="string">'user_id'</span>].values,</span><br><span class="line">    <span class="string">'item_id'</span>: df_test[<span class="string">'item_id'</span>].values,</span><br><span class="line">    <span class="string">'gender'</span>: df_test[<span class="string">'gender'</span>].values,</span><br><span class="line">    <span class="string">'age'</span>: df_test[<span class="string">'age'</span>].values,</span><br><span class="line">    <span class="string">'job'</span>: df_test[<span class="string">'job'</span>].values,</span><br><span class="line">    <span class="string">'category_list'</span>: np.array(df_test[<span class="string">'category_list'</span>].tolist()),</span><br><span class="line">    <span class="string">'recent_click'</span>: np.array(df_test[<span class="string">'recent_click'</span>].tolist())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TensorFlow实现的LR模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_dict, reg=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_dict: 特征信息字典.</span></span><br><span class="line"><span class="string">        :param reg: 隐向量正则系数.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_dict = feature_info_dict</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.dense_ft_list = []</span><br><span class="line">        self.sparse_ft_list = []</span><br><span class="line">        self.varlen_sparse_ft_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.dense_ft_list.append(name)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                self.sparse_ft_list.append(name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.varlen_sparse_ft_list.append(name)</span><br><span class="line"></span><br><span class="line">        self.sparse_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> self.sparse_ft_list:</span><br><span class="line">            num = self.feature_info_dict[name][<span class="string">'num'</span>]</span><br><span class="line">            self.sparse_embedding_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=<span class="number">1</span>,</span><br><span class="line">                                                                      embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                          self.reg))</span><br><span class="line">        self.varlen_sparse_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> self.varlen_sparse_ft_list:</span><br><span class="line">            num = self.feature_info_dict[name][<span class="string">'num'</span>]</span><br><span class="line">            max_len = self.feature_info_dict[name][<span class="string">'max_len'</span>]</span><br><span class="line"></span><br><span class="line">            self.varlen_sparse_embedding_dict[name] = keras.layers.Embedding(num, <span class="number">1</span>, input_length=max_len,</span><br><span class="line">                                                                             mask_zero=<span class="literal">True</span>,</span><br><span class="line">                                                                             embeddings_regularizer=keras.regularizers.l2(</span><br><span class="line">                                                                                 self.reg))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(self.dense_ft_list) &gt; <span class="number">0</span>:</span><br><span class="line">            self.dense_layer = keras.layers.Dense(<span class="number">1</span>, use_bias=<span class="literal">False</span>, kernel_regularizer=keras.regularizers.l2(self.reg))</span><br><span class="line"></span><br><span class="line">        self.bias = tf.Variable(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature_dict)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dense</span></span><br><span class="line">        <span class="keyword">if</span> len(self.dense_ft_list) &gt; <span class="number">0</span>:</span><br><span class="line">            dense_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.dense_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                dense_ft_list.append(ft)</span><br><span class="line">            dense_ft_list = tf.concat(dense_ft_list, axis=<span class="number">1</span>)</span><br><span class="line">            dense_logits = self.dense_layer(dense_ft_list)</span><br><span class="line">            dense_logits = tf.reshape(dense_logits, shape=(<span class="number">-1</span>,))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dense_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># sparse</span></span><br><span class="line">        <span class="keyword">if</span> len(self.sparse_embedding_dict) &gt; <span class="number">0</span>:</span><br><span class="line">            sparse_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.sparse_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                layer = self.sparse_embedding_dict[name]</span><br><span class="line">                ft = layer(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                sparse_ft_list.append(ft)</span><br><span class="line">            sparse_logits = tf.reduce_sum(tf.concat(sparse_ft_list, axis=<span class="number">1</span>), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sparse_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># varlen_sparse</span></span><br><span class="line">        <span class="keyword">if</span> len(self.varlen_sparse_embedding_dict) &gt; <span class="number">0</span>:</span><br><span class="line">            varlen_sparse_ft_list = []</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> self.varlen_sparse_ft_list:</span><br><span class="line">                ft = inputs[name]</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                layer = self.varlen_sparse_embedding_dict[name]</span><br><span class="line">                ft = layer(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reshape(tf.reduce_sum(ft, axis=<span class="number">1</span>), shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                varlen_sparse_ft_list.append(ft)</span><br><span class="line">            varlen_sparse_logits = tf.reduce_sum(tf.concat(varlen_sparse_ft_list, axis=<span class="number">1</span>), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            varlen_sparse_logits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        logits = self.bias + dense_logits + sparse_logits + varlen_sparse_logits</span><br><span class="line">        output = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = LR(feature_info_dict, reg=<span class="number">1e-6</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line">model.fit(x=train_set,</span><br><span class="line">          y=df_train[<span class="string">'label'</span>].values,</span><br><span class="line">          validation_data=(val_set, df_val[<span class="string">'label'</span>].values),</span><br><span class="line">          batch_size=<span class="number">256</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line">train_auc = model.evaluate(train_set, df_train[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">val_auc = model.evaluate(val_set, df_val[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">test_auc = model.evaluate(test_set, df_test[<span class="string">'label'</span>].values, verbose=<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'train: %.4f, val: %.4f, test: %.4f'</span> % (train_auc, val_auc, test_auc))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train: 0.7783, val: 0.7616, test: 0.7348</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是用LR来作为排序模型的内容了.</p>
<p>在前面说到过, LR模型可以处理稀疏数据, 但是却无法自动完成对特征的交叉, 需要精心的特征工程来进行辅助.</p>
<p>在下一篇中, 将会尝试用GBDT, 结合LR来进行排序, 看是否能带来提升♪(^∇^*)</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>推荐系统</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>FM</title>
    <url>/2020/10/10/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/FM/</url>
    <content><![CDATA[<p>这里来讲一下FM(Factorization Machines), 也称作分解机算法.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面的<a href="whitemoonlight.top/2020/10/10/传统机器学习/逻辑回归/">文章</a>中, 介绍了线性模型(逻辑回归), 当时提到线性模型的一个明显的缺点, 就是作为线性模型, 其表示能力不行.</p>
<p>那么具体哪里不行呢?</p>
<p>举两个栗子. </p>
<p><strong>栗子一</strong>:</p>
<p>以线性回归来说, 假如特征只有一个, 那么此时线性回归模型形式为:</p>
<script type="math/tex; mode=display">
y=ax+b</script><p>其中$a$和$b$为待学习模型参数.</p>
<p>如果$y=x^2+\epsilon$, 那么用一个线性模型怎么去拟合这样的非线性数据呢?</p>
<p>直接拟合肯定不行, 如果做了一些EDA工作观察了数据的关系, 尝试增加特征$x’=x^2$, 进而线性模型变为:</p>
<script type="math/tex; mode=display">
y=ax+bx^2+c</script><p>就可以拟合, 但问题是对于更加复杂的, 多特征的非线性模式, 这样的方法仍然不可行.</p>
<p>还有一种方法是对特征进行离散化, 这样可以让线性模型学习到非线性模式, 也就是说对于非线性问题而言, 其实也还好, 只是需要细致的特征预处理来帮助模型进行学习.</p>
<p><strong>栗子二</strong>:</p>
<p>假如有两个布尔型特征$x_1$和$x_2$, 其不同特征组合, 与目标变量$y$之间的对应关系如下:</p>
<script type="math/tex; mode=display">
\begin{array}{c}
&(x_1,x_2)&\Rightarrow &y \\
&(0,0)&\Rightarrow &0 \\
&(1,0)&\Rightarrow &1 \\
&(0,1)&\Rightarrow &1 \\
&(1,1)&\Rightarrow &0 \\
\end{array}</script><p>此时如果直接使用线性模型$y=w_1x_1+w_2x_2+b$, 是无法拟合这种模式的.</p>
<p>而如果添加了特征之间的交互特征:</p>
<script type="math/tex; mode=display">
y=w_1x_1+w_2x_2+w_{1,2}x_1x_2+b</script><p>是可以拟合这种模式的:</p>
<script type="math/tex; mode=display">
y=x_1+x_2-2x_1x_2</script><p>从上面的栗子可以看出, 特征之间的交互, 可以让线性模型原本无法学习到的模式变成可能.</p>
<p>此外, 仍然是上面两个特征, 一种更加细致的的交互方式, $x_1$是否等于1和$x_2$是否等于1, 可以得到4个新的布尔型特征.</p>
<p>其实无论是对于线性模型来说, 还是其它模型, 特征交互都是非常重要的, 它体现了特征之间的联系, 而并非独立对结果造成影响.</p>
<p>而对于不同模型, 有不同的特征交互方式, 比如对于线性模型来说, 可以对特征进行两两相乘; 对于树模型而言, 一条从根节点到叶子节点的路径上, 使用不同特征进行节点划分, 也是一种特征交互; 在深度学习中, 也可以通过尝试更多的交互方式, 在一些场景下提升模型表现.</p>
<h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><p>在知道了特征交互的重要性后, 那以后咱们用线性模型, 都把特征做交叉就好了是吧? 且慢, 这还不够, 简单地增加交互特征的线性模型还不够打, 还需要更加强力的模型来进行实战.</p>
<p>FM: 没错, 正是在下! ♪(^∇^*)</p>
<p>一开始听到分解机的名字, 可能会想, 这是什么奇怪的模型, 一看具体的模型形式, 哦, 这不就是个带特征交互的线性模型嘛.</p>
<p>在实际场景的数据中, 会经常面临稀疏数据的问题, 就是一份样本中, 或者一个特征中, 多数位置上是0. 是的, 线性模型面对稀疏数据其实还好, 但是如果再进行特征交叉, 那就是稀疏上再稀疏, 稀疏到家了, 线性模型是难以从某个特别特别稀疏的特征中学到什么有效信息的, 为什么?</p>
<p>假设线性模型中, 某个特征对应的系数为$w$, 训练前进行随机初始化, 然后在训练中逐渐进行优化. 比如有10000个样本, 而由于稀疏性假设只有10个样本该特征为1. 采用SGD每次训练一个样本, 10000个样本训练下来, 参数$w$更新了10次…而即便多次训练, 这么小的数据量, 其本身所携带的信息也难以让人信服.</p>
<p>而FM通过借鉴矩阵分解的思想(这应该是分解机名称的由来), 使其能够在特别稀疏数据中, 也能学到较好的模型, 从而大放异彩.</p>
<p>下面进行数学原理的讲解.</p>
<h2 id="模型形式"><a href="#模型形式" class="headerlink" title="模型形式"></a>模型形式</h2><p>假设特征向量为$(x_1,x_2,\dots,x_n)^T$, 则对应线性模型为:</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{y}(x)&=w_0+w_1x_1+w_2x_2+\cdots+w_nx_n \\
&=w_0+\sum_iw_ix_i
\end{align*}</script><p>添加特征交互后的形式为:</p>
<script type="math/tex; mode=display">
\hat{y}(x)=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^nw_{ij}x_ix_j</script><p>前面说到了, 如果直接表示为这样的形式并进行学习, 那么效果不会好, 于是这里借鉴矩阵分解的思想, 将交互特征的权重系数集合看成一个对称矩阵$W_{n\times n}$, 而这个矩阵可以被分解成的两个句子近似表示. 并且由于$W_{n\times n}$是对称方阵, 所以可以分解成如下形式:</p>
<script type="math/tex; mode=display">
W_{n\times n}=VV^T \\</script><p>其中的$V_{n\times k}$为每个特征的辅助向量(隐向量)矩阵, $k$为隐向量维度.</p>
<p>从数学上来说, 当$k$足够大时, 对于任意正定的实矩阵$W$, 均存在实矩阵$V$, 使上式成立.</p>
<p>而模型中交互特征的权重系数, 对应于矩阵$W$的上半角或者下半角(不包含对角线).</p>
<p>即:</p>
<script type="math/tex; mode=display">
w_ij=\vec v_i^T\vec v_j=\sum_l^kv_{il}v_{jl}</script><p>所以模型可以表示为:</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{y}(x)&=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^nw_{ij}x_ix_j \\
&=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\vec v_i^T\vec v_j)x_ix_j
\end{align*}</script><p>那在借鉴矩阵分解的方法, 进行了这样的表示以后, 相比原方法, 为什么就能够更好地避免数据稀疏带来的问题了呢?</p>
<p>假如现在某个样本, 有多个布尔型特征, 然后由这些原始特征两两交互, 又得到了了一大堆交互特征, 也是布尔型的, 对于某个交互特征来说, 只要其原始特征都为1时, 其值才为1. 如果是原本的没有采用隐向量的做法, 那么在训练时, 对于某个交互特征的权重系数而言, 只有当两个原始布尔型特征全为1时, 交互特征的权重系数才会得到更新, 极端情况下, 若训练数据里该交互特征全为0, 那么如果测试集出现了1, 将难以预测; 而FM这里, 一个交互特征的权重系数对应两个原始特征的隐向量, 即便该交互特征为0, 只要原始特征与其它原始特征存在值为1的交互特征, 那么这个隐向量就可以得到训练, 不会出现上面的困境.</p>
<p>上面的问题, 可以归结为记忆性和泛化性的问题, 一些模型结构(如原始线性模型, 树模型)倾向于学习数据固有的模式, “记住”数据中发生过的事情; 而一些模型(如FM的特征交叉部分), 则可以学习到泛化的模式, 能够从已有的发生过的事情, 去推测未发生过的事情.</p>
<p>再从另外一个角度考虑, 假设有$n$个原始特征, 那么如果不采用隐向量的方式, 需要学习的参数量约为$n^2$. 而在FM里面, 需要学习的参数量约为$kn$, 在实际场景中一般特征也是海量的, 即$k\ll n$, 那么这样看, 也是FM更容易学习一些.</p>
<h2 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h2><p>上一节给出了FM模型的形式, 其计算的时间复杂度是多少呢, 其实估算就行了, 因为交互特征占用的复杂度是大头. 假设原本有$n$个特征, 那么就大约有$(1/2)n^2$个交互特征, 再假设隐向量的维度为$k$, 这样FM的时间复杂度为$O(kn^2)$.</p>
<p>这个复杂度其实还蛮高的, 但是机智的前人们通过一些变化, 找到了减少时间复杂度的方法.</p>
<p>数学, 永远滴神!</p>
<script type="math/tex; mode=display">
\begin{align*}
&\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\vec v_i^T\vec v_j)x_ix_j \\
=&\frac{1}{2}\bigg(\sum_{i=1}^n\sum_{j=1}^n(\vec v_i^T\vec v_j)x_ix_j-\sum_{i=1}^n(\vec v_i^T\vec v_i)x_ix_i \bigg) \\
=&\frac{1}{2}\bigg(\sum_{i=1}^n\sum_{j=1}^n\sum_{l=1}^kv_{il} v_{jl}x_ix_j-\sum_{i=1}^n\sum_{l=1}^kv_{il}^2x_i^2 \bigg) \\
=&\frac{1}{2}\sum_{l=1}^k\bigg(\sum_{i=1}^n(v_{il}x_i)\sum_{j=1}^n (v_{jl}x_j)-\sum_{i=1}^nv_{il}^2x_i^2 \bigg) \\
=&\frac{1}{2}\sum_{l=1}^k\bigg[\bigg(\sum_{i=1}^n(v_{il}x_i)\bigg)^2-\sum_{i=1}^nv_{il}^2x_i^2 \bigg] \\
\end{align*}</script><p>很明显, 经过上面的变化, 原本$O(kn^2)$的时间复杂度, 变为了$O(kn)$!</p>
<h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>下面来介绍如何优化FM, 其实没啥特别的, 方法不止一种, SGD够用♪(^∇^*)</p>
<script type="math/tex; mode=display">
\frac{\partial\hat{y}}{\partial \theta}=\left\{
\begin{array}{lcl}
1 &    & \theta=w_0  \\
x_i &    & \theta=w_i,\ i\in\{1,2,\dots,n\} \\
x_i\sum_{j=1,j\ne i}^nv_{jl}x_j &    & \theta=v_{il},\ i\in\{1,2,\dots,n\},\ l\in\{1,2,\dots,k\}

\end{array}
\right.</script><p>上面就是模型对各参数求偏导的结果, 接下来面对具体的损失函数, 来进行优化.</p>
<p>FM整体的损失函数可以表示为:</p>
<script type="math/tex; mode=display">
L=\sum_{m}loss(y_i,\hat{y}_i)</script><p>当面对回归问题, 使用平方误差函数时:</p>
<script type="math/tex; mode=display">
loss=\frac{1}{2}(y-\hat{y})^2 \\[7mm]
\frac{\partial loss(y-\hat{y})}{\partial \theta}=(y-\hat{y})\frac{\partial\hat{y}}{\partial \theta}</script><p>当面对(二)分类问题, 使用对数损失函数时:</p>
<script type="math/tex; mode=display">
FM=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\vec v_i^T\vec v_j)x_ix_j \\[7mm]
\hat{y}=\frac{1}{1+\exp(-FM)}  \\[7mm]
loss=-y\ln\hat{y}-(1-y)\ln(1-\hat{y}) \\[7mm]
\frac{\partial loss}{\partial \theta}=\frac{\partial loss}{\partial FM}\frac{\partial FM}{\partial \theta}=(\hat{y}-y)\frac{\partial FM}{\partial \theta}</script><p>在选择具体的优化算法(优化器)时, 可以使用常规的SGD, 但是可能会对学习率的调整比较敏感, 因为对于一些相对稠密的特征参数会进行更多优化, 而稀疏的特征可能难以得到有效优化. 所以可以尝试使用能够随着学习的进行, 调整学习率的算法, 如Adagrad, 或者Adam.</p>
<script type="math/tex; mode=display">
\begin{align*}
SGD:&\ w_{t+1,i}=w_{t,i}-\eta \cdot g_{t,i} \\[7mm]
Adagrad:&\ w_{t+1,i}=w_{t,i}-\frac{\eta}{\sqrt{\sum_jg_{j,i}^2}+\epsilon}\cdot g_{t,i}
\end{align*}</script><h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><p>下面再说一下FM算法的升级版, FFM(Field-aware Factorization Machine).</p>
<p>FFM的核心思想是, 不同的域(filed)的特征空间分布是不一样的, 当使用不同域的特征进行交叉时, 应该对应使用不同的隐向量.</p>
<p>比如用户ID和物品ID进行交叉, 会分别用到$\vec v_{用户,物品}$和$\vec v_{物品,用户}$. 而用户ID和日期进行交叉, 会分别用到$\vec v_{用户,日期}$和$\vec v_{日期,用户}$.</p>
<p>也就是说, 原本在FM中, 每个特征只有一个隐向量, 而在FFM中, 每个特征有$f$个隐向量, 当域的数量增多时, 参数数量也会随之增加. 假设FM交叉部分对应的参数量为$kn$, 那么FFM的参数量为$nfk$.</p>
<p>FFM的模型表示如下:</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{y}(x)&=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^nw_{ij}x_ix_j \\
&=w_0+\sum_iw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\vec v_{i,j}^T\vec v_{j,i})x_ix_j
\end{align*}</script><p>FFM不仅参数量增多, 而且由于涉及到不同域选择不同隐向量, 使得计算的时间复杂度也不能得到优化, 仍然是$O(kn^2)$.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>FM在海量数据, 且数据具有稀疏性时, 是一种强力有效的算法, 其升级版算法FFM, 在原本基础上增加了更多的参数, 不过在一些时候提升效果并不明显.</p>
<p>在整体模型形式上, FM与二阶多项式核的SVM, MF模型比较相似. 相比SVM模型, FM在数据稀疏情况下优势明显, 且FM模型时间复杂度远小于SVM. 相比MF模型, FM更加灵活, 不局限于用户/物品两类特征信息.</p>
<p>在使用FM进行建模时, 对于类别特征, 可以进行独热编码; 对于连续特征, 可以进行归一化处理, 不过更好一些的方法可能是对连续型特征进行离散化后也做度热编码. 此外, 还会遇到一种类别特征, 一个特征下同时包含多个属性, 比如”兴趣”, 一个样本的”兴趣”特征是”篮球, 足球”, 另一个样本的”兴趣”特征是”游戏, 娱乐, 唱歌”, 对于这种情况, 一种较好的方法是取到各个特征的Embedding, 然后做pooling; 不过这样实际操作时代码会复杂一些, 简单的一种做法可以把这个特征”兴趣”拆分为多个布尔型特征.</p>
<p>在推荐系统中, FM由于其本身近乎线性模型, 计算相对较快, 可以使用一些较少的特征, 来作为一个召回模型. 同时, 又由于其在线性模型基础上, 添加了具有泛化性的特征交互, 所以也可以使用更多的特征, 作为一个排序模型.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>FM</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT(二)</title>
    <url>/2020/10/10/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/GBDT-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 讲了GBDT, 嗯, 这一篇接着讲♪(^∇^*)</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>其实, 当现在提到GBDT的时候, 一般不是指原本的GBDT算法, 而是指的它的升级版XGBoost算法, 或者不同的工程实现(如LightGBM, CatBoost), 所以这里仍然采用GBDT作为文章的标题. 而为了在本文中进行区分, 分别采用GBDT和XGBoost(简称XGB)的名称.</p>
<p>前面有说到, 在boosting集成方法下, AdaBoost通过改变样本权重进行更新学习, GBDT通过改变学习目标进行更新学习, 而在它们之上, 还有一种更强的算法, 那就是XGB, 下面就来详细地康康XGB的数学原理.</p>
<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><h2 id="泰勒公式"><a href="#泰勒公式" class="headerlink" title="泰勒公式"></a>泰勒公式</h2><p>还记得以前在上大一的数学分析课程的时候, 在为数不多有在听(才不是逃课玩游戏呢)的课程中, 记得讲台上的老师在讲到泰勒公式时, 用满怀欣喜的语气向我们说到: 同学们, 现在你们见到的, 是微积分理论的巅峰, 泰勒展开!</p>
<p>不过有一说一, 我那时年少无知, 其实没听太懂, 怎么就巅峰了啊? 差不多记住公式, 考试套用.</p>
<p>后来逐渐在不少地方见过泰勒公式的身影, 夜深人静时, 我脑海里又浮现出当年老师的教诲, 我感觉我悟了, 我觉得我比以前懂了. 虽然可能理解还是不到位, 但是总归是好一些了.</p>
<p>扯了这么多, 下面来看一下泰勒公式的形式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
f(x)&=\sum_{n=0}^\infty\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \\
&=f(x_0)+f^1(x_0)(x-x_0)+\frac{f^2(x_0)}{2}(x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\end{aligned}</script><p>泰勒公式的目的是什么, 假设现在已知一个函数的形式$f(x)$, 并且这个函数在某个点$x_0$处可导, 有对应的各阶导数$f^{(n)}(x_0)$, 那么其邻近的一点的值$f(x_0+\Delta x)$可以通过多项式的形式近似得到, 并且多项式的阶数越多, 那么就可以更加准确.</p>
<p>这里有几个要点:</p>
<ul>
<li><p>阶数越高越精准.</p>
<p>当$\Delta x$确定时, 阶数越多, 可以近似得更加准确.</p>
</li>
<li><p>只能近似表示邻近的点.</p>
<p>当$\Delta x$越小的时候, 可能用到二阶, 甚至一阶就可以较好地近似. 但是当$\Delta x$较大的时候, 可能二阶以上也不能很好地表示.</p>
</li>
<li><p>为什么要用多项式近似.</p>
<p>既然已经知道了原函数的形式, 为什么还要费劲地求导表示成泰勒展开的形式呢?</p>
<p>因为多项式这种形式, 在计算上存在很多便利的地方.</p>
<p>比如在找一个函数极小值的时候, 对原函数求导等于零很难解, 但是求导本身可能并不难. 这时候从某一个初始点开始, 用泰勒展开来估计邻居点的值,  逐步地向更小的方向前进, 也许就能够找到一个解.</p>
<p>其实, 梯度下降法本质上就是用到了泰勒一阶展开, 牛顿法就是用到了泰勒二阶展开.</p>
</li>
</ul>
<p>上面是泰勒展开在单元函数的形式, 多元函数会复杂一些, 但是本质是不变的.</p>
<p>在机器学习领域, 对于可导的损失函数, 可以采用泰勒展开来进行优化.</p>
<p>对于梯度下降法, 可以理解为知道了当前朝哪个方向前进, 可以进行逐步地有效优化, 但是视野是非常局部的, 并且在一些”平坦”的loss landscape上, 优化起来非常缓慢.</p>
<p>而对于牛顿法, 基于泰勒展开的二阶形式(二次函数), 可以得到其极值点, 来进行优化. 相比梯度下降法, 可以说”看得更远”, 可以”一步到位”. 这样牛顿法在优化效率上高于梯度下降法, 效果也可能好一些.</p>
<p>但实际的情况是大多数机器学习算法, 几乎都使用梯度下降法, 而很少使用牛顿法. 原因是一些模型, 如神经网络, 在使用牛顿法时, 需要计算其二阶导数矩阵(海森矩阵), 以及逆矩阵, 当参数一多, 开销非常大, 所以实际使用体验远不如梯度下降法.</p>
<p>那么, 就不能想想办法吗?</p>
<p>类似神经网络的模型, 是一个模型, 优化其中的所有参数; 而原始的GBDT模型, 是多个串联的子模型, 每次优化一棵树, 把牛顿法运用到这里, 或许可以.</p>
<h2 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h2><p>下面一步一步来对XGB的原理进行解析.</p>
<p><strong>任务</strong>:</p>
<p>XGB构建的是一个基于boosting的通用框架, 可以用于各种监督学习的任务.</p>
<p>也就是说XGB可以适应回归, 分类等多种任务, 同时可以自定义损失函数, 只要给出对应的一阶导数和二阶导数即可.</p>
<p><strong>模型</strong>:</p>
<p>子模型为决策树(回归树), 模型的参数, 为决策树的划分结构, 以及叶子节点的值.</p>
<script type="math/tex; mode=display">
f_{K}=\sum_kT_k=f_{K-1}+T_K</script><p>其中的$f_K$表示前$K$棵树的输出结果, $T_k$表示第$k$棵决策树.</p>
<p><strong>损失函数</strong>:</p>
<script type="math/tex; mode=display">
Loss=\sum_iL(y_i,f_{K,i})+\sum_k\Omega(T_k)</script><p>损失函数包含两项, 其中第一项为模型效果的损失函数, 比如回归任务下的MSE, 分类问题下的对数损失函数; 第二项为正则项, 是关于每棵子决策树的函数.</p>
<p><strong>优化方法</strong>:</p>
<p>考虑到模型为加法模型, 所以采用前向分步算法, 逐步进行优化.</p>
<p><strong>推导流程</strong>:</p>
<p>在有了上面对任务, 模型, 损失函数和优化方法的列举后, 下面进行具体的推导, 看如何利用泰勒展开来获得XGB算法.</p>
<p>首先分离出最新的一棵树:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Loss&=\sum_iL(y_i,f_{K,i})+\sum_k\Omega(T_k) \\
&=\sum_iL(y_i,f_{K-1,i}+T_K)+\Omega(T_K)+Constant  \\
&=\sum_iL(y_i,f_{K-1,i}+T_K)+\Omega(T_K)
\end{aligned}</script><p>然后利用泰勒二阶展开:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Loss=&=\sum_iL(y_i,f_{K-1,i}+T_K(x_i))+\Omega(T_K) \\
&=\sum_i\bigg[L(y_i,f_{K-1,i})+g_iT_K(x_i)+\frac{1}{2}h_iT_K^2(x_i)\bigg]+\Omega(T_K) \\
\end{aligned}</script><p>其中:</p>
<script type="math/tex; mode=display">
g_i=\frac{\partial L(y_i,f_{i})}{\partial f_i}\bigg|_{f_i=f_{K-1,i}} \quad h_i=\frac{\partial^2 L(y_i,f_{i})}{\partial f_i^2}\bigg|_{f_i=f_{K-1,i}}</script><p>除去常数项, 简化损失函数:</p>
<script type="math/tex; mode=display">
Loss=\sum_i\bigg[g_iT_K(x_i)+\frac{1}{2}h_iT_K^2(x_i)\bigg]+\Omega(T_K)</script><p>现在定义正则项具体形式:</p>
<script type="math/tex; mode=display">
\Omega(T_K)=\gamma J+\frac{1}{2}\lambda \sum_jw_j^2</script><p>其中的$\gamma$和$\lambda$为正则系数, $J$表示叶子节点数量, $w$表示叶子节点输出值.</p>
<p>现在注意啊, 要开始变形了, 定义一个函数$q$, 表示将输入$x_i$映射到某个叶子节点上, 即$T_K(x_i)=w_{q(x_i)}$.</p>
<p>然后在定义叶子节点$j$上面的样本集合为$p_j=\{i|q(x_i)=j\}$.</p>
<p>可得:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Loss&=\sum_i\bigg[g_iT_K(x_i)+\frac{1}{2}h_iT_K^2(x_i)\bigg]+\Omega(T_K) \\
&=\sum_i\bigg[g_iT_K(x_i)+\frac{1}{2}h_iT_K^2(x_i)\bigg]+\gamma J+\frac{1}{2}\lambda \sum_jw_j^2 \\
&=\sum_i\bigg[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2\bigg]+\gamma J+\frac{1}{2}\lambda \sum_jw_j^2 \\
&=\sum_j\bigg[\sum_{i\in p_j}g_iw_j+\frac{1}{2}\sum_{i\in p_j}h_iw_j^2\bigg]+\gamma J+\frac{1}{2}\lambda \sum_jw_j^2 \\
&=\sum_j\bigg[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\bigg]+\gamma J
\end{aligned}</script><p>其中$G_j=\sum_{i\in p_j}g_i,\ H_j=\sum_{i\in p_j}h_i$.</p>
<p>前面说到了, 对于XGB模型来说, 需要确定的模型参数有两个, 一个是决策树的划分结构, 一个是划分以后叶子节点是值.</p>
<p>假设已经划分好了树的结构, 则可以比较容易(对$w$求偏导等于零)地得到叶子的最优取值的解析式:</p>
<script type="math/tex; mode=display">
w_j=-\frac{G_j}{H_j+\lambda}</script><p>带入损失函数:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Loss&=\sum_j\bigg[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\bigg]+\gamma J \\
&=\sum_j\bigg[-\frac{G_j^2}{H_j+\lambda}+\frac{1}{2}\frac{G_j^2}{H_j+\lambda}\bigg]+\gamma J \\
&=-\frac{1}{2}\sum_j\bigg[\frac{G_j^2}{H_j+\lambda}\bigg]+\gamma J
\end{aligned}</script><p>现在给定任何划分结构的决策树, 就能够通过上式来评判其好坏程度.</p>
<p>但是枚举法复杂度太高, 仍然是采用贪心法, 每次尝试分裂一个节点, 计算分裂后的增益:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Gain&=Loss_{before\ split}-Loss_{after\ split} \\
&=\frac{1}{2}\bigg[\frac{G_{left}^2}{H_{left}+\lambda}+\frac{G_{right}^2}{H_{right}+\lambda}-\frac{(G_{left}+G_{right})^2}{H_{left}+H_{right}+\lambda}\bigg]+\gamma
\end{aligned}</script><p>与一般决策树一样, 仍可以每次遍历所有特征, 所有可能的切分点, 寻找最优切分, 根据增益判断是否分裂.</p>
<h2 id="另一种形式"><a href="#另一种形式" class="headerlink" title="另一种形式"></a>另一种形式</h2><p>上面的推断, 主要是面向如何进行优化学习的, 如果看完了数学推导, 仍然感觉云里雾里的话, 还可以看一下关于XGB的另外一种表达方式.</p>
<p>基于数学等式:</p>
<script type="math/tex; mode=display">
(a+b)^2=a^2+2ab+b^2</script><p>可推导得:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underbrace{\arg\min}_{T_K}\ Loss&=\sum_i\bigg[g_iT_K(x_i)+\frac{1}{2}h_iT_K^2(x_i)\bigg]+\Omega(T_K) \\
&=\sum_i\frac{1}{2}h_i\bigg[T_K^2(x_i)+2\frac{g_i}{h_i}T_K(x_i)\bigg]+\Omega(T_K) \\
&=\sum_i\frac{1}{2}h_i\bigg[T_K^2(x_i)+2\frac{g_i}{h_i}T_K(x_i)+\frac{g_i^2}{h_i^2}\bigg]+\Omega(T_K) \\
&=\sum_i\frac{1}{2}h_i\bigg[T_K(x_i)-(-\frac{g_i}{h_i})\bigg]^2+\Omega(T_K) \\
\end{aligned}</script><p>上面这个式子有木有觉得很眼熟. 是的, 这形式上就是回归树的损失函数(平方损失函数), 其中每个样本的样本权重为$h_i$, 每个样本的学习目标为$-g_i/h_i$. 在树处于某种结构时, 每个叶子节点的最优取值(不考虑正则项), 即为各个样本的学习目标加权平均, 为$-G/H$.</p>
<p>是的, GBDT引入了一阶梯度来提高模型效果, 而XGB这里通过引入二阶梯度, 不仅能够随着学习的进行(决策树的增加), 改变样本的学习目标, 并且还能够改变样本的权重, 进一步提升模型效果.</p>
<p>所以, 根据具体的任务, 定义损失函数, 然后得到关于模型一阶导数和二阶导数的信息, 交给回归树学习即可.</p>
<h1 id="不同框架特点"><a href="#不同框架特点" class="headerlink" title="不同框架特点"></a>不同框架特点</h1><p>对于XGB算法来说, 有一些比较常用的成熟的包, 它们的算法原理都是一样的, 只是在一些工程实现上有一些不同, 这里不做详细阐述, 只说一下自己的使用感受.</p>
<p>首先是XGBoost, 这个是伴随算法而出现的, 支持精确算法, 即在对特征进行分割时, 会尝试所有可能的分割点. 后来增加了近似算法, 即先对特征进行分桶, 再进行分割以加速训练. 整体说来XGBoost中规中矩吧.</p>
<p>然后是LightGBM, 从名字就可以看得出来, 就是想体现一个快. 在加速算法这块, 使用了直方图算法(可以看做XGBoost近似算法升级版), 通常在CPU上进行训练时, 可以达到XGBoost的2-10倍的速度, 我自己一般也是优先使用LightGBM.</p>
<p>同时LightGBM还对类别特征有自己的支持, 在告诉了LightGBM哪些是类别特征以后, 其处理的方式为:</p>
<ul>
<li><p>当属性较少时.</p>
<p>如类别特征属性为3个, 那么这时候LightGBM会每次分离一个属性来计算增益.</p>
</li>
<li><p>当属性较多时.</p>
<p>首先按属性分箱, 计算每箱的$G_i/H_i$, 并按其排序, 然后再进行分割.</p>
</li>
</ul>
<p>不过前面不是说了嘛, 基于决策树的模型, 对于稀疏数据容易过拟合, 所以LightGBM可以直接处理类别特征比较方便, 但是不一定是最好的选择.</p>
<p>再者是CatBoost, 前面的Cat表示category, 就是类别的意思, 这表明CatBoost在处理类别特征这块, 是下了大功夫的, 有许多超参数可以调节与尝试, 选择合适的超参数可以尽可能地减小过拟合的发生.</p>
<p>同时CatBoost还有一个有趣的设置, 即在默认超参数(即不认为设置超参数)下, 会根据输入数据的维度(样本量, 特征数量), 自己选择合适的学习率和决策树数量, 这一点是XGBoost和LightGBM没有的.</p>
<p>那么, 到底哪一个最好呢? 这个确实没有定论, 要在不同数据下进行尝试才能知道, 不过一般来说, XGBoost和LightGBM模型效果差距不大, 而CatBoost在类别特征较多的情况下, 可能会更好一些.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是关于GBDT算法的全部内容了.</p>
<p>GBDT(或者说XGB)算法, 我认为是传统机器学习的巅峰, 其数学原理感觉很优雅, 算法具有一定的泛用性, 效果也不错.</p>
<p>对于不是特别海量的结构化数据(样本量多, 特征维度大), 某种程度上可以无脑使用GBDT, 不用花很大力气去预处理数据特征, 也不用花很多精力去调参(相比神经网络, GBDT对超参数更加鲁棒), 就可以获得一个不错的效果.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT(一)</title>
    <url>/2020/10/10/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/GBDT-%E4%B8%80/</url>
    <content><![CDATA[<p>好的, 今天在讲一下GBDT, 梯度提升树模型. 虽然重点是将GBDT, 但是也会把与之有关的其它一些模型一并进行讲解.</p>
<a id="more"></a>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>首先说一下树模型, 也就是决策树, 是非常经典的模型, 有各种各样的版本, 但其核心思路, 就是从根节点开始, 每次根据某个特征来划分节点, 最终得到叶子节点. 而一个样本进入决策树以后, 从根节点开始, 进入到对应的叶子节点, 返回叶子节点对应的预测值.</p>
<p>树模型本身有哪些好处呢? 诶, 好处还很多, 不完全列举如下:</p>
<ul>
<li><p>可解释性.</p>
<p>经过怎样的路径(中间节点), 最后会得到一个怎样的结果, 都是一目了然的.</p>
</li>
<li><p>非线性.</p>
<p>树模型通过内部节点的划分, 可以获得非线性, 同时还可以获得特征之间的交互性, 这是非常重要的.</p>
</li>
<li><p>便捷性.</p>
<p>这里便捷的意思, 是说对于树模型而言, 理论上不用像线性模型那样小心翼翼地去预处理特征, 不用在意缺失值(模型可自行处理), 类别特征也可以处理. 由于这个特点, 决策树, 或者基于决策树的模型可以在一些场景下, 作为一个快速实现的base line模型.</p>
</li>
</ul>
<p>同时也列举一些树模型会面临的缺点:</p>
<ul>
<li><p>模型结构易变.</p>
<p>这个意思是说, 同样分布的数据用来训练决策树, 不同的随机数种子, 或者还采用了特征采样方法的话, 那么每次训练出来的模型可能不一样, 不仅内部模型参数不一样, 甚至模型结构(如何划分节点)也有很大差别.</p>
<p>然鹅也正是因为这个缺点, 所以在一些情况下, 可以变为优点. 比如在集成模型的bagging方法, 需要子模型具有较大地差异, 从而集成起来获得更好的效果. 而决策树正好可以满足这个需求, 通过bagging方法, 将不同参数不同结构的决策树结合起来, 形成随机森林模型.</p>
</li>
<li><p>难以增量训练.</p>
<p>所谓的增量训练, 就是在训练时不一次性使用全部数据, 而是每次使用部分数据更新参数. 一个典型的例子就是神经网络的训练, 每次只使用一个batch size的数据.</p>
<p>而增量训练所带来的好处, 我认为有两个, 一个是在大数据场景下, 可以增量训练, 则在工程上会更加容易; 二是当积累了更多的新的数据时, 不用从头开始再训练, 只要对新的数据进行训练即可, 甚至可以做到在线学习.</p>
</li>
<li><p>容易过拟合.</p>
<p>其实, 只要有足够多的叶子, 比如, 把训练集的每个样本都划分到单独的一个叶子节点, 那么就可以获得100%的准确率.</p>
<p>是的, 决策树, 永远滴神!</p>
<p>所以在建模时, 需要更加细致地控制决策树的生长, 找到合适的超参数, 尽量降低过拟合.</p>
</li>
<li><p>难以学习稀疏数据.</p>
<p>关于这一点, 其实和上面的容易过拟合也相关. 比如现在有一份有N个特征的稀疏数据, 如果用加了正则的线性模型来学, 可能得到的模型, 是一个众多特征权重系数都比较小的模型. 而如果用决策树来学习, 可能只会选取到少数的几个特征来划分节点, 同时给予对应的叶子节点较大的权重系数.</p>
<p>但是因为是稀疏数据, 本身就可能缺乏统计上的有效性, 由决策树找到的规则, 可能有较大的随机成分, 一旦那几个特征拉胯, 那就无了. 而这时候对线性模型来说, 用到了大部分特征, 并且权重系数都较小, 可能在训练集上评估指标不如决策树, 但具有更好的泛化性.</p>
</li>
</ul>
<p>上面说到的决策树一些优点和缺点, 其实在后面介绍的一些基于决策树的模型中, 也有, 不过它们通过放大决策树的优点, 并尝试减小其缺点, 来将自己在机器学习算法中的地位往前挪.</p>
<h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><p>在前面也提到了随机森林, 原理非常简单, 就是采用了bagging方法, 将多棵决策树进行集成, 来达到三个臭皮匠的效果, 嗯.</p>
<p>而要想用于集成的子决策树, 尽可能的不同, 使用了自助采样法, 标准的做法是每次对样本进行有重复采样, 比如原本有M个样本, 则有放回地采样M次, 这时每次大约有37%的样本没有进入训练集, 这部分称为OOB(out of bag), 可以作为对应子决策树的验证集.</p>
<p>随机森林相比原始的决策树, 有如下一些优点:</p>
<ul>
<li><p>准确率上升.</p>
<p>一棵决策树, 怎么打得过多棵决策树呢?</p>
</li>
<li><p>有效防止过拟合.</p>
<p>如果在某一棵子决策树上, 学到了什么奇怪的规则, 与其它子决策树相比显得格格不入, 那么就会被排挤, 被弱化[滑稽].</p>
</li>
<li><p>方便并行化.</p>
<p>每一颗子决策树的生成是独立的过程, 所以在训练时可以通过并行训练的方式, 来快速得到随机森林.</p>
</li>
</ul>
<p>但是仍然有一些缺点:</p>
<ul>
<li><p>准确率提升有限.</p>
<p>对于bagging类的集成模型来说, 要求子模型需要是强学习器, 在决策树这里可以理解为更深的层数.</p>
<p>虽然确实随机森林表现相比单棵决策树提升了, 但是作为基于bagging的集成模型, 其整体模型的效果, 严重依赖于单个子模型的效果. 也就是说, 如果子模型学得不咋地, 那么集成模型的提升也是很有限的.</p>
</li>
</ul>
<p>所以, 现在随机森林其实使用得并不多, 只是在一些时候可能还会用到, 比如想用随机森林获得特征的重要性.</p>
<p>随机森林算法还有一些推广:</p>
<ul>
<li><p>Extra Tree.</p>
<p>生成一棵树时, 不进行采样, 使用原数据集.</p>
<p>在划分节点时, 不筛选最优划分特征, 随机选取特征进行划分, 树的规模一般更大.</p>
</li>
<li><p>Isolation Forest:</p>
<p>基于随机森林的一种异常检测算法.</p>
<p>核心的思路为, 生成每棵树时, 采样的样本量很小, 然后随机选择划分特征, 随机选择划分值, 直到树深度阈值或者仅剩一个样本. 然后计算样本在每棵树上的平均高度, 平均高度越高的样本, 是异常点的概率更大.</p>
</li>
</ul>
<h1 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h1><p>上面的随机森林是一种典型的bagging集成方法, 下面开始介绍boost集成方法之AdaBoost模型.</p>
<p>先说一下什么是boost(提升)方法, 对比bagging方法, bagging就像一个袋子一样, 把多个模型装在一起, 模型之间是独立无关的, 但boost就像是一条线, 将一连串子模型连接在一起, 模型之间是有关的.</p>
<p>首先介绍boost方法下的AdaBoost模型.</p>
<p>在说一个模型或者算法的时候, 其实可以有几个重要且通用的点, 用以概括模型的特点:</p>
<ul>
<li>模型所属类别.</li>
<li>目标/损失函数.</li>
<li>学习\优化算法.</li>
</ul>
<p>那么在这里, 对于AdaBoost模型(二分类)来说:</p>
<ul>
<li><p>模型所属类别.</p>
<p>加法模型.</p>
</li>
<li><p>目标/损失函数.</p>
<p>指数函数.</p>
</li>
<li><p>学习/优化算法.</p>
<p>前向分步学习算法.</p>
</li>
</ul>
<p>下面进行具体的数学公式推导, 加法模型:</p>
<script type="math/tex; mode=display">
f_k(x)=f_{k-1}+\alpha_kG_k(x)</script><p>上面的$f_k$表示前$k$棵树组成的模型, $\alpha_k$表示第$k$棵树的权重, $G_k$表示第$k$棵树.</p>
<p>指数损失函数(二分类):</p>
<script type="math/tex; mode=display">
(\alpha_k,G_k)=\underbrace{\arg \min}_{\alpha,G}\sum_i\exp\bigg[(-y_i)\big(f_{k-1}(x_i)+\alpha G(x_i)\big)\bigg]</script><p>令$w_{k,i}’=\exp\big(-y_if_{k-1}(x_i)\big)$, 可得:</p>
<script type="math/tex; mode=display">
(\alpha_k,G_k)=\underbrace{\arg \min}_{\alpha,G}\sum_iw_{k,i}'\exp\bigg[-y_i\alpha G(x_i)\bigg]</script><p>仔细观察上面的式子, 可以发现$w_{k,i}’$是已知的, 每个样本对应一个, 并且以相乘的形式出现在损失函数这里, 就完全可以看做是每个样本的样本权重(未归一化)!</p>
<p>进一步进行推导:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_iw_{k,i}'\exp\bigg[-y_i\alpha G(x_i)\bigg]&=\sum_{y_i=G_k(x_i)}w_{k,i}'\exp^{-\alpha}+\sum_{y_i\ne G_k(x_i)}w_{k,i}'\exp^{\alpha} \\
&=(e^\alpha-e^{-\alpha})\sum_iw_{k,i}'I\big(y_i\ne G_k(x_i)\big)+e^{-\alpha}\sum_iw_{k,i}'
\end{aligned}</script><p>注意到后面的$G_k$和$\alpha$是分开的, 所以可以先求解$G_k$:</p>
<script type="math/tex; mode=display">
G_k=\underbrace{\arg \min}_{G}\sum_iw_{k,i}'I\big(y_i\ne G_k(x_i)\big)</script><p>其中$I$表示判别函数, true则为1, false则为0. 而这是什么意思呢, 这就是说要让误差最小, 就要尽可能地让$G_k$进行正确的预测, 并且优先关注样本权重更大的样本, 这本质上, 就等价于让一棵决策树去学习带权重的样本.</p>
<p>现在假设已经学好了一棵新的决策树, 那么还差$\alpha_k$, 将$G_k$带入损失函数, 来对$\alpha_k$求导等于0, 求解$\alpha_k$:</p>
<script type="math/tex; mode=display">
\begin{align*}
&\frac{\partial}{\partial \alpha}\bigg[(e^\alpha-e^{-\alpha})\sum_iw_{k,i}'I\big(y_i\ne G_k(x_i)\big)+e^{-\alpha}\sum_iw_{k,i}'\bigg] \\
&=(e^\alpha+e^{-\alpha})\sum_iw_{k,i}'I\big(y_i\ne G_k(x_i)\big)-e^{-\alpha}\sum_iw_{k,i}'=0 \tag{1}
\end{align*}</script><p>令:</p>
<script type="math/tex; mode=display">
E_k=\frac{\sum_i w_{k,i}'I\big(y_i\ne G_k(x_i)\big)}{\sum_i w_{k,i}'}</script><p>表示第$k$棵树的带权重误差率, 带入(1)式中可得:</p>
<script type="math/tex; mode=display">
(e^\alpha+e^{-\alpha})E_k-e^{-\alpha}=0 \\[3mm]
\alpha_k=\frac{1}{2}\log\frac{1-E_k}{E_k}</script><p>从$\alpha_k$的表示式可以看出, 当误差率$E_k$越小时, $\alpha_k$越大.</p>
<p>最后还有下一轮($k+1$轮)样本权重的更新, 因为:</p>
<script type="math/tex; mode=display">
f_k(x)=f_{k-1}+\alpha_kG_k(x) \\[3mm]
w_{k,i}'=\exp(-y_if_{k-1}(x_i))</script><p>可得:</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{k+1,i}'&=\exp\bigg(-y_if_{k}(x_i)\bigg) \\
&=\exp\bigg(-y_i\big(f_{k-1}(x)+\alpha_kG_k(x_i)\big)\bigg) \\
&=w_{k,i}'\exp\bigg(-y_i\alpha_kG_k(x_i)\bigg)
\end{align*}</script><p>观察上面的样本权重迭代公式, 发现如果对于某个样本来说, 当前$G_k$能够正确识别的话, 那么权重将会缩小, 反之变大.</p>
<p>以上就是AdaBoost模型的原理, 在学习的时候, 每次会根据之前学习的情况, 来调整每个样本的权重, 供下一次学习, 是一个串行的过程. 最终模型为这些子模型加权投票来觉得预测输出.</p>
<p>这里以二分类问题, 决策树子模型为例, 但AdaBoost本质上是一个算法框架, 没有限制子模型种类, 同时对于回归或者其它任务可以设置不同的损失函数, 对应不同的优化过程.</p>
<p>AdaBoost基于boost集成方法, 相比基于bagging的随机森林方法, 其优点是模型表现更上一层楼, 在学习过程中, 每次针对之前没有学好的部分进行重点学习, 这波啊, 这波叫查漏补缺. 但是会容易过拟合一些, 所以需要对训练或者学习进行控制, 在获得更好效果的同时, 减轻过拟合.</p>
<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><p>这一节来讲boost集成方法下的另一种, 也算是本文重点要讲的算法GBDT(梯度提升树).</p>
<p>其实GBDT的核心思路非常简单, 作为一个串行的加法模型, 怎样才能随着模型的学习, 或者树的增加, 让误差逐渐减小呢, 一种有效的方法是让每一棵新的树, 学习前面模型与目标变量之间的残差. 比如在回归问题下, 有一个样本的目标值为10, 第一树学习时$y=10$, 假设学习完后输出7; 第二棵树学习时$y=10-7=3$, 假设学习完后输出为2; 那么到第三棵树学习时$y=3-2=1$, 假设学习完后输出为1, 那么模型最终输出为$7+2+1=10$.</p>
<p>但是使用每次学习后的残差, 来作为新的学习目标, 并不具备泛化性, 对于回归问题比较好定义, 而对于分类或者其他一些问题怎么办呢? 于是, GBDT使用损失函数的负梯度, 来代替残差, 作为新的弱学习器的学习目标, 以适应多种任务.</p>
<p>下面按模型训练过程, 来讲解GBDT的数学原理</p>
<ul>
<li><p>初始化学习器.</p>
<script type="math/tex; mode=display">
f_0=c=\underbrace{\arg\min}_c\sum_iL(y_i,c)</script><p>上面的$c$为一个常数, $L$表示关于目标变量和树模型输出的损失函数.</p>
<p>一般在初始化第一棵树时, 如果是回归问题, 那么$c$为样本均值; 如果是二分类问题, 那么$c$为正样本占比.</p>
</li>
<li><p>计算样本负梯度.</p>
<script type="math/tex; mode=display">
-g_{k,i}=-\bigg[\frac{\partial L\big(y_i,f(x_i)\big)}{\partial f(x_i)}\bigg]_{f=f_{k-1}}</script><p>若为回归问题, 且损失函数为平方误差函数, 则负梯度为残差.</p>
<script type="math/tex; mode=display">
L(y_i,\hat{y}_i)=L\big(y_i,f(x_i)\big)=\frac{1}{2}\big(y_i-f(x_i)\big)^2 \\[3mm]
-g_{k,i}=-\bigg[\frac{\partial L\big(y_i,f(x_i)\big)}{\partial f(x_i)}\bigg]_{f=f_{k-1}}=y_i-f(x_i)</script><p>若为二分类问题, 且损失函数为对数损失函数, 则梯度为目标变量与输出概率的差.</p>
<script type="math/tex; mode=display">
L'(y_i,\hat{y}_i)=-y_i\ln \hat{y}_i-(1-y_i)\ln(1-\hat{y}_i) \\[3mm]
\hat{y}_i=\frac{1}{1+\exp\big(-f(x_i)\big)}  \\[3mm]
-g_{k,i}=-\bigg[\frac{\partial L\big(y_i,f(x_i)\big)}{\partial f(x_i)}\bigg]_{f=f_{k-1}}=y_i-\frac{1}{1+\exp\big(-f(x_i)\big)}=y_i-\hat{y}_i</script></li>
<li><p>学习新的树.</p>
<p>利用每个样本的特征和得到的负梯度作为学习目标, 生成一棵回归树.</p>
</li>
<li><p>计算每个叶子节点的最优拟合值.</p>
<p>按照常规的思路, 在学习好一棵回归树的结构以后, 叶子节点的值取分配到该节点的样本目标值的均值, 似乎是一件很正常的事情. 但是回归树学习的过程, 只是学习到了数的结构, 而叶子节点的取值, 首先应该让对应的损失函数最小, 应满足如下式子:</p>
<script type="math/tex; mode=display">
r_{k,j}=\underbrace{\arg\min}_r\sum_{x_i\in R_{k,j}}L\big(y_i,f_{k-1}(x_i)+r\big)</script><p>其中的$r_{k,j}$表示第$k$棵树的第$j$个叶子节点.</p>
<p>如果是回归问题, 平方损失函数MSE, 那么叶子节点的最优值确实就是对应样本负梯度(残差)的算术平均.</p>
<p>而如果是二分类问题, 对数损失函数, 那么需要进一步优化(可用牛顿法), 优化过程这里就不写了, 最终近似的解为:</p>
<script type="math/tex; mode=display">
r_{k,j}=\frac{\sum_{x_i\in R_{k,j}}g_{k,i}}{\sum_{x_i\in R_{k,j}}|g_{k,i}|(1-|g_{k,i}|)}</script></li>
<li><p>更新模型.</p>
<script type="math/tex; mode=display">
f_k(x)=f_{k-1}(x)+\sum_jr_{k,j}I(x\in R_{k,j})</script></li>
<li><p>重复以上过程, 直到停止条件.</p>
</li>
<li><p>最终模型:</p>
<script type="math/tex; mode=display">
f(x)=f_K(x)=f_0(x)+\sum_k\sum_jr_{k,j}I(x\in R_{k,j})</script><p>看最终模型的结构, 好像…是一个线性模型的样子, 神奇吧!</p>
</li>
</ul>
<p>GBDT的一大优点, 就是通过串行地学习一系列弱学习器(低层数回归树), 来极大提高模型拟合能力. 当然, 决策树本身的一些优点和缺点, 它也基本都具备.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文从决策树开始, 阐述了树模型整体的特点, 然后介绍了两种集成模型范式, 分别是bagging和boosting.</p>
<p>其中bagging的代表模型是随机森林, 其相比单棵决策树, 性能上有提升, 并且相对能够较好地防止过拟合.</p>
<p>而boosting的代表模型有AdaBoost, 通过改变样本权重, 来让新的子模型更加关注之前没有学好的部分; 同时还有本文的重点GBDT模型, 通过改变样本的学习目标, 即前面模型关于损失函数的负梯度, 来让新的子模型逐步提升模型效果.</p>
<p>Boosting方法相比bagging, 效果提升是明显的, 但是这就是极限了吗? 有没有什么方法, 能够结合AdaBoost和GBDT模型, 即同时修改样本权重以及学校目标呢, 将在下一篇进行讲解.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2020/10/10/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>嗯这一篇来说下经典的逻辑回归模型.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在今天看来, 似乎深度学习模型在各个领域方向都大放异彩, 但是即便如此, 线性模型, 比如逻辑回归仍然有自己的一席之地. 如果让我在机器学习领域选三个最具代表性的算法, 那逻辑回归模型一定位列其中.</p>
<p>逻辑回归本身的原理并不复杂, 算法实现起来也比较简单, 并且由于是线性模型, 在一些情况下不能去学习复杂的模式, 某种意义上来说, 逻辑回归就是神经网络中的一个神经元.</p>
<p>不过简单并不意味着作用不大, 逻辑回归模型在不同的场景下, 有着不同的使用方式. 比如可以对于特殊处理后的特征, 可以单独使用逻辑回归作为模型; 在有了一些子模型以后, 可以作为子模型的融合模型(打比赛必备); 可以与其它一些模型结合起来使用, 比如GBDT+LR.</p>
<p>除了使用场景多丰富以外, 正因为逻辑回归本身简单, 想要用简单的模型获得较好的建模效果, 对建模人员对算法的掌握和数据的处理是有一定要求的.</p>
<p>关于逻辑回归使用的一些细节, 放到后面说, 下面先梳理一下逻辑回归算法的原理. 由于逻辑回归是再经典不过的模型了, 所以关于其原理, 网上可以随便搜到, 不过其中大部分都是直接讲如何优化模型, 而关于模型的由来并没有进行阐述.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><p>首先, 我们来聊一聊广义线性模型(GLM).</p>
<p>总所周知, 一谈到线性模型, 最原始的应该就是线性回归模型了, 其结构如下:</p>
<script type="math/tex; mode=display">
y=w_1x_1+w_2x_2+\cdots+w_nx_n+b</script><p>线性回归满足四个基本假设, 它们分别是:</p>
<ul>
<li><p>存在线性关系:</p>
<script type="math/tex; mode=display">
y=W\cdot X+\epsilon</script></li>
<li><p>$X$为确定值, 满秩可逆, 即不存在共线性.</p>
</li>
<li><p>误差项均值为0, $E(\epsilon)=0$.</p>
</li>
<li><p>误差项同方差, $Var(\epsilon_1)=Var(\epsilon_2)=\cdots=\sigma^2$.</p>
</li>
<li><p>误差项不相关, $\forall i\ne j,\ Cov(\epsilon_i,\epsilon_j)=0$.</p>
</li>
</ul>
<p>这里的$y$是连续值, 且满足正态分布, 线性回归估计的, 正是这个正态分布的均值.</p>
<p>那么现在问题来了, 在一些问题当中, $y$并不是连续的, 可能是离散的, 也可能是布尔型的, 那还能用线性回归来做吗?</p>
<p>硬要做, 也不是不行[滑稽], 但是由于数据分布不满足模型成立的假设, 所以建模以后, 效果是得不到保障的.</p>
<p>如果仍然坚持用线性模型, 同时又想通过一定合理的改造, 使得改造后的模型能够一定程度上胜任特定的模式, 就需要一个指导方针.</p>
<p>广义线性模型: 对没错, 正是在下!</p>
<p>下面来详细介绍广义线性模型.</p>
<p>我认为广义线性模型有三个关键点:</p>
<ul>
<li>线性核.</li>
<li>目标期望.</li>
<li>联系函数.</li>
</ul>
<p><strong>线性核</strong>: 无论如何, 只要属于线性模型, 就具备且仅具备线性核, 形式如下:</p>
<script type="math/tex; mode=display">
w_1x_1+w_2x_2+\cdots+w_nx_n+b</script><p>如果存在非线性项, 如$x^2$这样的, 就不属于线性模型了.</p>
<p><strong>目标期望</strong>: 这一点其实是在定义我们使用广义线性模型, 来学习什么, 预测什么?</p>
<p>在真实的世界中, 随机性是无处不在的. 对于收集到的数据, 更具体的说就是目标数据$y$, 除了本身与自变量$x$之间满足一定关系外, 是存在一定误差$\epsilon$的. 在线性回归中, $y$服从正态分布, 那么就对$y$的均值(期望)和方差进行了定义.</p>
<p>而建立统计模型的目的, 是研究自变量$x$与因变量$y$, 在随机误差下, 存在的定量关系. 根据假设, 误差是必然存在且随机的(理想情况下), 那么通过(线性)模型我们必然不能准确地去预测目标数据$y$(用其它模型在过拟合的情况下可以[滑稽]), 我们能做的, 或者说我们希望能做到的, 是去拟合/预测目标数据$y$的期望$E(y)$.</p>
<p>而这个期望如何表示呢, 是通过目标数据$y$的分布得到的, 比如线性回归中$y$服从正态分布, 那么自然期望就是正态分布的均值$E(y)=\mu$.</p>
<p><strong>联系函数</strong>: 前面提到了, 最原始的线性模型, 只能处理连续型的$y$, 而对其他形式的$y$很吃力, 那么是否能够通过一些转换方法, 建立起$y$(更准确的说是$E(y)$)和线性核$W\cdot X$之间的桥梁或者联系呢?</p>
<p>联系函数(Link Function): 没错, 正是在下!</p>
<p>对于线性核来说, 理论上其能够表示的区间范围为整个实数域, 那么现在保持线性核不变, 根据具体情况, 对$E(y)$施加一个转换, 将其从原本的空间转变到实数域, 就可以继续配合线性核表演了.</p>
<p>如果是线性回归, 这个联系函数就是$f(E(y))=E(y)$, 那么如果是布尔型的呢, 又应该怎么转换呢?</p>
<p>前面说到了, 对于一个特定场景下的$y$, 需要事先给定一个分布, 这个分布的形式中, 包含了线性核, 那么通过分布得到期望$E(y)$的表达式, 就是$E(y)$与线性核的关联, 对应就可以得到联系函数.</p>
<p>现在还有一个问题, 如何指定$y$的分布? 如果能找到一种比较通用的分布形式, 能够适应多种问题, 并且在计算上还比较方便就好了, 下面介绍指数族分布.</p>
<h2 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h2><p>其实我自己也想问, 为什么是指数分布族, 其它分布行不行?</p>
<p>我自己认为, 在一些情况下, 其它类型的分布可能也行, 而之所以优先采用指数族分布, 一是本身确实包含了不少常用的分布, 如正态分布, 伯努利分布, 泊松分布, 指数分布等, 即这些分布都能够转换成一个形式; 而且具有指数族分布形式的分布, 具有一些良好的性质.</p>
<p>指数族分布的形式如下:</p>
<script type="math/tex; mode=display">
P(y;\eta)=b(y)\exp (\eta T(y)-a(\eta))</script><p>其中, $\eta$表示自然参数, 等价于线性核; $b(y)$和$T(y)$可以看成关于$y$的表达式; $a(\eta)$在这里充当归一化的角色, 以保证整个结果为一个概率(和等于一).</p>
<p>具体来康康一些具体的分布, 如何转变为指数族分布的形式吧.</p>
<p><strong>正态分布</strong>:</p>
<p>在线性回归中, 标准差$\sigma$对模型参数$w$的选择没有影响, 为方便推导, 假设其为1.</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y;\mu)&=\frac{1}{\sqrt{2\pi}}\exp\bigg(-\frac{1}{2}(y-\mu)^2\bigg) \\
&=\frac{1}{\sqrt{2\pi}}\exp\bigg(-\frac{1}{2}y^2\bigg)\exp\bigg(\mu y-\frac{1}{2}\mu^2\bigg)
\end{aligned}</script><p>对应的参数:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\eta&=\mu \\[3mm]
T(y)&=y \\[3mm]
a(\eta)&=\mu^2/2=\eta^2/2 \\[3mm]
b(y)&=(1/\sqrt{2\pi})\exp(-y^2/2)
\end{aligned}</script><p><strong>伯努利分布</strong>:</p>
<script type="math/tex; mode=display">
P(y=1;\phi)=\phi;\ p(y=0;\phi)=1-\phi \\</script><p>由上两式可以得到:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y;\phi)&=\phi^y(1-\phi)^{1-y} \\
&=\exp\bigg(y\log\phi+(1-y)\log(1-\phi)\bigg) \\
&=\exp\bigg(\big(\log(\frac{\phi}{1-\phi})\big)y+\log(1-\phi)\bigg)
\end{aligned}</script><p>对应的参数:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\eta&=\log\frac{\phi}{1-\phi} \\[3mm]
T(y)&=y \\[3mm]
a(\eta)&=-\log(1-\phi) \\[3mm]
b(y)&=1
\end{aligned}</script><p>由上面的$\eta$, 可以推出下面的表达式:</p>
<script type="math/tex; mode=display">
\phi=\frac{1}{1+\exp(-\eta)}</script><p>前面还说到指数族分布有一些良好的性质, 其中一个性质是其期望与方差求解的形式非常简单:</p>
<script type="math/tex; mode=display">
E(y;\eta)=\frac{\rm d}{ {\rm d}\eta}a(\eta) \\[3mm]
Var(y;\eta)=\frac{ {\rm d}^2}{ {\rm d}\eta^2}a(\eta)</script><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>上面先介绍了广义线性模型, 然后再介绍了指数族分布, 那么现在来讲解逻辑回归.</p>
<p>绕了这么大一圈终于轮到主角了吗喂♪(^∇^*)</p>
<p>首先将广义线性模型与指数族分布结合起来, 阐述利用其进行建模想通用步骤:</p>
<ul>
<li><p>确定指数族分布形式.</p>
<p>根据特定场景, 特定数据形式, 指定目标变量$y$的分布.</p>
</li>
<li><p>转换分布形式, 并得到期望.</p>
<p>将分布转换为指数族分布的标准形式, 得到各参数的对应关系, 并求解出分布的期望.</p>
</li>
<li><p>根据期望与线性核的关系, 进行学习.</p>
<p>将期望表示为线性核$\eta=W\cdot X$的函数, 并利用优化方法进行学习.</p>
</li>
</ul>
<p>下面就按照这个步骤来进行具体操作.</p>
<p><strong>线性回归</strong>:</p>
<ul>
<li><p>确定指数族分布形式.</p>
<p>正态分布, $y\sim N(\mu, 1)$, 这里假设标准差为1.</p>
</li>
<li><p>转换分布形式, 并得到期望.</p>
<p>在上一节中已进行推导, 正态分布的期望为:</p>
<script type="math/tex; mode=display">
E(y)=\mu=\eta</script></li>
<li><p>根据期望与线性核的关系, 进行学习.</p>
<script type="math/tex; mode=display">
h_w(x)=E(y)=\eta=W\cdot X</script></li>
</ul>
<p><strong>逻辑回归</strong>:</p>
<ul>
<li><p>确定指数族分布形式.</p>
<p>对于布尔型(0-1)的$y$, 很自然的可以使用伯努利分布:</p>
<script type="math/tex; mode=display">
y\sim Bernoulli(\phi)</script></li>
<li><p>转换分布形式, 并得到期望.</p>
<p>在上一节中已进行推导, 伯努利分布的期望为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
E(y)&=1\times P(y=1;\phi)+0\times P(y=0;\phi) \\
&=P(y=1;\phi) \\
&=\phi
\end{aligned}</script></li>
<li><p>根据期望与线性核的关系, 进行学习.</p>
<script type="math/tex; mode=display">
h_w(x)=E(y)=\phi=\frac{1}{1+\exp(-\eta)}=\frac{1}{1+\exp(-W\cdot X)}</script></li>
</ul>
<p>是的, 上面最后那个式子, 就是我们平时看到的逻辑回归的形式. 再联系到上面讲到的联系函数, 对应的形式如下:</p>
<script type="math/tex; mode=display">
\log\bigg(\frac{E(y)}{1-E(y)}\bigg)=W\cdot X</script><p>即逻辑回归的联系函数为:</p>
<script type="math/tex; mode=display">
f(E(y))=\log\bigg(\frac{E(y)}{1-E(y)}\bigg)</script><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>通过前面的一顿绕, 终于是得到了逻辑回归模型的形式, 那么接下来就是如何进行优化或者说学习了, 对于了解机器学习常用优化方法的同学来说, 应该是轻车熟路了.</p>
<p>对于这样具有统计意义的模型, 一般来说就是写出其似然函数, 然后再进行极大似然函数(MLE)就可以了.</p>
<p>对于逻辑回归来说, 模型本身表达的含义为:</p>
<script type="math/tex; mode=display">
h_w(x)=P(y=1|x;w)</script><p>那么可以得到:</p>
<script type="math/tex; mode=display">
P(y=1|x;w)=h_w(x) \\[3mm]
P(y=0|x;w)=1-h_w(x) \\[3mm]
P(y|x;w)=h_w(x)^y\cdot \big(1-h_w(x)\big)^{1-y}</script><p>似然函数为:</p>
<script type="math/tex; mode=display">
L(w)=\prod_i\big(h_w(x^{(i)})\big)^{y^{(i)}}\big(1-h_w(x^{(i)})\big)^{1-y^{(i)}}</script><p>其中的$i$表示样本.</p>
<p>对似然函数取负对数, 得到损失函数:</p>
<script type="math/tex; mode=display">
J(w)=-\ln L(w)=-\sum_i\bigg(y^{(i)}\ln \big(h_w(x^{(i)})\big)+(1-y^{(i)})\ln\big(1-h_w(x^{(i)})\big)^{}\bigg)</script><p>对应的矩阵表示:</p>
<script type="math/tex; mode=display">
J(w)=-Y^T\ln h_w(X)-(E-Y)^T\ln (E-h_w(X))</script><p>其中的$E$表示全1向量.</p>
<p>利用梯度下降法进行优化, 迭代公式如下:</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial w}J(w)=X^T(h_w(X)-Y) \\[3mm]
w=w-\alpha X^T(h_w(X)-Y)</script><p>其中的$\alpha$为梯度下降法的步长, 或者说学习率.</p>
<p>此外, 在损失函数中, 还可以加入L1正则项($\alpha||w||_1$), L2正则项($(1/2)\alpha||w||_2^2$). 关于正则项, 当然可以直接加, 在神经网络中, GBDT中就是这么干的, 但是在线性模型这里, 是可以师出有名的. 这里在优化时, 使用的是极大似然估计, 但是如果在贝叶斯学派的眼里, 参数$w$本身, 也是具有分布的, 如果加入模型参数的先验分布, 那么将会得到参数的后验分布(条件概率分布), 此时可以用最大后验估计(MAP)来做. 然后在经过一顿推导, 就可以得到正则项. 如果先验分布设置为正态分布, 那么将会得到L2正则项, 如果先验分布设置为拉普拉斯分布, 那么将会得到L1正则. 而具体的论证和推导过程, 其实又会扯上一大堆, 在这里就不再多说了(懒…</p>
<h1 id="使用注意事项"><a href="#使用注意事项" class="headerlink" title="使用注意事项"></a>使用注意事项</h1><p>现在已经讲述了逻辑回归的由来, 以及优化的方法, 而最开始也提到了, 逻辑回归模型简单, 可也正因为它简单, 所以在使用的时候也有不少需要注意的地方, 下面进行简要的讲解.</p>
<p><strong>关于数据</strong>:</p>
<p>首先, 逻辑回归只接收数值型的数据(啊好像挺废话的).</p>
<p>然后数据的预处理对于逻辑回归来说是非常重要的, 特别是把它直接用在一些原始数据(非子模型)上时. 原始数据的分布可能是多种多样的, 连续的, 离散的; 数值范围也可能相差较大, 比如有的趋于0, 有的数值很大…对于逻辑回归来说, 最基础的数值预处理工作, 就是对数据范围进行标准化, 比如转换到0-1区间, 比如减去均值再除以标准差等. 为什么要这么做, 有什么原因吗, 我认为有两方面好处:</p>
<ul>
<li><p>优化算法.</p>
<p>常用的优化算法, 比如SGD, 当不同的特征处于相似的数值区间时, 可以收敛地更快. 具体原因可以查阅优化算法原理, 这里不详细展开.</p>
</li>
<li><p>解释性.</p>
<p>当各特征都处在相似的数值范围时, 最终得到的模型, 在不过拟合的情况下, 其各特征系数是具有可解释性的, 即正相关还是负相关, 相关性大小等.</p>
</li>
</ul>
<p>对于类别特征来说, 最直接的方法就是独热编码, 或者可以标签编码(Label Encoding), 具体做法可以参考我的<a href="whitemoonlight.top/2020/06/06/传统机器学习/数据建模-特征工程/">这篇文章</a>, 或者自行查阅.</p>
<p>同时, 由于逻辑回归是线性模型, 对于非线性特征来说, 是难以进行有效学习的. 什么叫非线性特征呢, 就是特征本身与标签$y$不呈现有效的相关性. 那么这时候如果仍然想用逻辑回归, 就需要对原始特征进行进一步处理, 处理方法有很多, 这里说两个相对常用的.</p>
<ul>
<li><p>WOE</p>
<p>这个方法总体思路就是对原始特征进行分箱(分箱方式自定), 然后再统计每一箱中, 标签$y$的分布, 根据其分布, 给予该分箱一个WOE数值.</p>
<p>这个方法可以比较有效地将非线性特征转变为线性特征, 还能顺便进行数据标准化, 不过其实要做好还是比较麻烦的.</p>
</li>
<li><p>离散化</p>
<p>即将原特征进行分箱离散, 然后再做独热编码, 突出一个简单实用!</p>
</li>
</ul>
<p>此外, 关于逻辑回归特征的预处理, 其实远不止这些, 这些只是一些比较基础的处理方法, 在一些场景下, 一些独特的处理方式可以有效提升模型表现, 这比较考验建模人员的经验与积累.</p>
<p><strong>关于模型</strong>:</p>
<p>逻辑回归其实也是需要调参的, 一般来说如果是小数据, 使用sklearn这样的包的话, 一般只需要调节正则系数即可, 而如果是大一些的数据, 使用TensorFlow来做, 可能还需要调节学习率, batch size这些参数.</p>
<p>下面重点说一下关于正则项的调节.</p>
<p>关于正则项是选择L1还是L2, 或者L1与L2的混合, 可以根据实际情况而定, 而关于L1与L2的区别, 这里一句话说来, 就是L1相比L2更加倾向于得到稀疏的权重系数. 在调节参数时, 一般可以看不同正则参数下, 模型在验证集上面的表现, 选择对应评估指标最后的正则参数. 此外, 由于线性模型的可解释性, 还可以进一步对得到的结果进行检验, 比如对于某个正相关的特征, 如果得到的权重系数却是负的, 那么说明正则系数小了, 应该往大了调节.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这里从广义线性模型, 到指数族分布, 再到逻辑回归的优化与一些使用上的注意点, 比较全面地对逻辑回归模型进行了讲解.</p>
<p>不完全总结一下逻辑回归模型的优点和缺点.</p>
<p><strong>优点</strong>:</p>
<ul>
<li>可解释性强.</li>
<li>不容易过拟合.</li>
<li>训练, 部署容易.</li>
<li>在处理得当的情况下, 也能获得不错的模型表现.</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li>对数据预处理要求相对较高.</li>
<li>在一些具有复杂的, 非线性的场景下, 难以有较好的表现.</li>
</ul>
<p>逻辑回归模型看似简单, 但也有其不简单的地方, 需要用到真实的数据去进行建模, 发掘不同的建模方法对逻辑回归模型的影响, 感受逻辑回归模型的优点与缺点, 加深对模型的认识与理解.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>node2vec</title>
    <url>/2020/10/10/%E5%9B%BE%E7%AE%97%E6%B3%95/node2vec/</url>
    <content><![CDATA[<p>接着上一篇DeepWalk, 这里再来介绍一种图表示学习中的方法, node2vec.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果还不了解DeepWalk算法的同学, 可以看一下我的<a href="whitemoonlight.top/2020/10/10/图算法/DeepWalk/">这篇文章</a>.</p>
<p>这里的node2vec, 就可以算作其升级版, 也是一种图表示学习方法, 即基于图中节点的一些共现关系, 学习出每个节点的Embedding.</p>
<p>核心过程, 就是首先从图中抽取序列, 然后再利用word2vec算法进行学习. 下面就对node2vec的算法原理进行详细介绍.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h2><p>熟悉树这种数据结构的同学应该都知道, 在遍历树的时候, 可以有两种策略, 广度优先搜索(BFS), 以及深度优先搜索(DFS).</p>
<p><img src="fig_0.png" alt="fig"></p>
<p><img src="fig_1.png" alt="fig"></p>
<p>BFS会从根节点开始, 优先探索该节点的子(邻近)节点, 然后再向更远的节点探索. DFS会从根节点开始, 优先探索该节点更远的节点, 探索到底后, 再回过头来继续从该节点的其它邻近节点进一步探索.</p>
<p>对于图来说, 也有这两种遍历方式.</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>由这两种方式得到的序列, 在节点相似性的呈现上, 具有不同的性质. 由BFS得到的序列, 会有一种结构相似性(structural equivalence), 结构相似性是衡量两个节点在网络中所在的位置和结构的相似性. 而由DFS得到的序列, 在经过word2vec学习后, 会展现出一种同质性(homophily), 即相邻的, 经常一起出现的节点会更加相似.</p>
<p>通俗来讲, 在图中, 从某个初始节点开始, 给定一个最大序列长度, 利用DFS得到的序列, 倾向于一条长链; 而利用BFS得到的序列, 更倾向于团簇的形式.</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>在DeepWalk中, 从图中抽取序列的方式, 就是随机游走, 虽然同时可以有BFS和DFS的效果, 但是并不可控. 于是, 在node2vec中, 通过加入两个超参数, 来形成一个带权重的网络, 以控制BFS和DFS的组成.</p>
<p>对于其论文中的一些公式, 这里就不多说了, 感兴趣的同学可以看原论文, 这里直接介绍其实现方式.</p>
<p>要想控制是BFS多一些还是DFS方式多一些, 一个办法就是通过改变当前节点向下一个节点的转移概率.</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>如上图, 其中的$t$节点表示上一个节点, $v$节点表示当前节点. 如果想要DFS多一些, 那么从$v$节点向$x_2$和$x_3$转移的概率应该大一些; 如果想要BFS多一些, 那么对应的返回上一个节点$t$的概率应该大一些. 同两个参数$p$和$q$来控制这种概率(未归一化):</p>
<script type="math/tex; mode=display">
\alpha_{pq}(t,x)=\left\{
\begin{array}{lcl}
\frac{1}{p} &    & {\rm if}\ d_{tx}=0  \\
1 &    & {\rm if}\ d_{tx}=1 \\
\frac{1}{q} &    & {\rm if}\ d_{tx}=2
\end{array}
\right.</script><p>其中的$d_{tx}$表示在图中, 由$t$节点到达$x$节点的最短距离, $d_{tx}=0$表示返回上一个节点, 对应参返回概率参数(Return parameter $p$); $d_{tx}=2$表示转移到更远的节点, 对应由内到外概率参数(In-out parameter $q$).</p>
<p>如果原始的图本身带有权重$w$, 那么在加入了$\alpha$后, 对应的未归一化权重为两者的乘积$\pi=w\cdot \alpha$</p>
<p>node2vec整体的算法流程为:</p>
<p><img src="fig_5.jpg" alt="fig"></p>
<p>首先, 与DeepWalk一样, 要先用原始数据构成一个图, 设定一些超参数, 包括整体迭代次数$r$, 序列长度$l$, 窗口大小$k$, 向量维度$k$, 两个概率参数$p$和$q$.</p>
<p>然后一个重点部分, 是需要根据原本网络的权重, 以及参数$p$和$q$计算整个网络的新的权重. 这里考虑实际计算时, 对于每个序列的初始节点, 是没有上一个节点的, 所以可以对其各个相邻节点的权重进行归一化, 看做其作为初始节点的转移概率. 而当不是初始节点时, 对于每个节点, 需要计算基于各个相邻节点作为上一个节点时, 对应的转移概率. 或者换一个角度来说, 需要对每一条边, 来计算两组(分别作为当前节点)概率转移分布.</p>
<p>在有了一整套概率转移分布后, 就可以依照转移概率, 来进行游走以获得序列了. 最后用word2vec来对序列进行学习, 就大功告成了.</p>
<p>这里node2vec算法在构建图, 计算概率转移时, 是比较费时间的, 时间复杂度正比于图中边的数量. 但是还不算完, 当面对一个计算好的概率转移分布, 决定如何转移时, 怎么做呢? 一种简单的方法就是产生一个0-1的随机数, 然后看这个数值位于哪一个概率区间. 这样做的时间复杂度为$O(n)$, 其中$n$为当前节点相邻节点数量. 而如果使用二分法查找, 可以使时间复杂度降低到$O(\log n)$. 如果用一个很长(假设长度为$N$)的向量, 将各个概率按相对大小分布在向量上, 并在对应位置记录对应转移节点编号, 然后产生一个最大为$N$的随机数, 来决定应该向哪个节点转移, 这样做的时间复杂度为$O(1)$, 但是空间复杂度就比较高了… 在原始的word2vec负采样中, 就采取的这种方法, 但word2vec只需要维护一个这样的向量, 而node2vec这里的数量与连边数量成正比的.</p>
<p>那么, 有没有一种时间复杂度为$O(1)$, 空间复杂度也不高的算法呢?</p>
<h2 id="Alias-Method"><a href="#Alias-Method" class="headerlink" title="Alias Method"></a>Alias Method</h2><p>Alias Method: 是的, 正是在下.</p>
<p>Alias Method是一种通用的概率采样方法, 主要针对对应离散概率分布, 下面通过举例来说明其运作流程.</p>
<p>假设现在有一个离散的概率分布为:</p>
<script type="math/tex; mode=display">
p_1:\frac{1}{2}\quad p_2:\frac{1}{3}\quad p_3:\frac{1}{12}\quad p_4:\frac{1}{12}</script><p>现在想让这些概率填充在一个$1\times4$的矩形中, 概率与面积成正比, 那么首先将它们都乘以4, 这样总面积等于4:</p>
<script type="math/tex; mode=display">
p_1:2\quad p_2:\frac{4}{3}\quad p_3:\frac{1}{3}\quad p_4:\frac{1}{3}</script><p>可以看到, 现在长为4了, 但是高并不是都为1, 所以现在要”截长补短”, 即将大于1的截取一部分, 填充到小于1的位置上. 同时规定, 一个位置上, 同时最多有两种概率.</p>
<p>现在先将第1个概率截取一部分填补到第4个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_1:\frac{2}{3} \\[7mm]
p_1:\frac{4}{3}\quad p_2:\frac{4}{3}\quad p_3:\frac{1}{3}\quad &p_4:\frac{1}{3}
\end{align*}</script><p>再将第1个概率截取一部分填补到第3个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_1:\frac{2}{3}\quad p_1:\frac{2}{3} \\[7mm]
p_1:\frac{2}{3}\quad p_2:\frac{4}{3}\quad &p_3:\frac{1}{3}\quad p_4:\frac{1}{3}
\end{align*}</script><p>现在还有第1个位置上的小于1, 同时第2个概率大于1, 因此将第2个概率截取一部分填补到第1个位置上:</p>
<script type="math/tex; mode=display">
\begin{align*}
p_2:\frac{1}{3}\quad\quad\quad\quad\  p_1:\frac{2}{3}\quad p_1:\frac{2}{3} \\[7mm]
p_1:\frac{2}{3}\quad p_2:1\quad p_3:\frac{1}{3}\quad p_4:\frac{1}{3}
\end{align*}</script><p>到了这一步, 就有了两个向量, 在每次采样时, 只需要两步:</p>
<ul>
<li>随机生成一个1-N(N为离散变量数)的数, 用以确定向量中的位置.</li>
<li>随机生成一个0-1的数, 用以确定产生哪个离散变量.</li>
</ul>
<p>比如先随机得到3, 然后再随机得到$1/2$, 由$1/2&lt;2/3$, 可采样得到变量1.</p>
<p>这样的采样方式, 在生成了上面的概率向量以后, 采样时的时间复杂度为$O(1)$. 这里也解释为什么上面要规定每个位置上最多只能有两个概率, 因为如果有多个概率, 那么在随机生成一个0-1的数后, 就不是$O(1)$的时间复杂度了.</p>
<p>进一步, 给定一个离散概率分布, 这样的向量一定可以构建出来吗, 答案是肯定的, 因为: 整体的面积为$N$; 如果当前某个位置上概率小于1, 必有另一个位置上概率大于1.</p>
<p>使用Alias Method, 构建这个概率向量的时间复杂度为$O(n^2)$, $n$为离散变量数, 而通过队列来进行优化, 可以缩减到$O(n)$.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是node2vec的全部内容了, 由DeepWalk, BFS, DFS出发, 介绍了node2vec的原理和算法流程, 并对其中的一些细节, 比如概率转移时的采样方法也进行了讲解.</p>
<p>node2vec通过调整两个参数$p$和$q$, 来调整从图中抽取序列的方式, 在具体的任务和场景中, 可以尝试不同的组合. 一般来说, 合适的参数下, 相比DeepWalk会有更好一点的效果.</p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepWalk</title>
    <url>/2020/10/10/%E5%9B%BE%E7%AE%97%E6%B3%95/DeepWalk/</url>
    <content><![CDATA[<p>今天来介绍一种比较重要和实用的图算法, DeepWalk.</p>
<p>一看这名称中带有deep, 那么八九不离十和深度学习会有关, 是的, DeepWalk可以看做是深度学习算法在图算法领域的延伸. 下面就来介绍DeepWalk算法的原理, 以及代码实现.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面介绍图相关的文章中, 有一些方法, 可以衡量节点的重要性(如PageRank), 还有一些方法可以衡量两两节点之间的相似性. 这些方法得到的结果, 在一些特定的领域, 是有效果的, 但是并不具备泛用性, 这里泛用性的意思是, 能否像NLP中的词向量一样, 也用一些Embedding来表示图中的每个节点, 然后利用这个Embedding, 可以用来做各种下游任务呢? 可以的, 这就是图表示学习.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>众所周知, word2vec是一种经典且好用的Embedding方法, 可以用在NLP中的单词上. 也可以用在其它一些数据呈现离散序列的场景, 比如item2vec方法, 我的<a href="whitemoonlight.top/2020/10/10/推荐系统/item2vec召回/">这篇文章</a>中有介绍.</p>
<p>那么, 如果把word2vec也应用在图中, 来使得图中的每个节点学习到对应的Embedding, 好像是一个不错的点子. 而word2vec的输入, 是离散的序列, 所以现在问题就变成了, 如何在一个图中, 产生出一些可供word2vec学习的序列.</p>
<p>此时, 随机游走第一个站了出来: 没错, 正是在下! ♪(^∇^*)</p>
<p>所谓随机游走, 通俗地来说, 就是在当前的一个节点上, 没次随机选择一个相邻的节点, 进行移动. 具体说来, 在一个图中, 可以先随机地选取一个节点, 作为初始节点, 然后进行随机游走, 得到一个路径, 当走到设定的最大步数时停止, 而这个路径就是可以用word2vec来学习的序列.</p>
<p>DeepWalk具体的算法流程为:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>其中的$\gamma$表示整体的过程重复次数, 类似于训练神经网络时的epoch参数. 每次对所有节点形成的列表进行乱序, 然后依次选取节点作为初始节点, 开始随机行走, 得到训练用的序列数据.</p>
<p>DeepWalk简单来说, 等价于随机游走, 加word2vec, 随机游走从图中抽取序列, word2vec再从序列中学习Embedding. 假设相连的节点之间, 存在着更高的相似性, 而这种共现关系, 就可以体现在学习到的Embedding上.</p>
<p>是不是非常简单呢, 如果明白了word2vec的原理, 那么确实挺简单的. 但是我觉得其实还有一些值得注意或者讨论的地方.</p>
<h2 id="序列-图-序列"><a href="#序列-图-序列" class="headerlink" title="序列-图-序列"></a>序列-图-序列</h2><p>首先, 如果的数据, 本身就呈现一个图的形式, 比如社交网络, 那么就可以直接使用DeepWalk来进行图表示学习, 这是很自然的. 但如果初始数据不是天然的图, 还能够使用DeepWalk来进行学习吗?</p>
<p>在问这个问题之前, 其实应该问一下, 都不是图结构的数据了, 硬要用图算法来做, 图个啥? 炫技吗♪(^∇^*) 当然不是, 这里举个栗子, 在item2vec那篇文章中, 是将用户的行为序列, 当成word2vec直接的输入, 通过序列中物品的共现关系, 来学习对应向量, 最后用向量来做召回. 然鹅这里有个小问题, 即每一个序列, 都是仅仅局限于一个用户得到的, 这可能会带来一些局限性.</p>
<p>考虑一个场景, 比如两个人要去菜市场买菜做饭, 就做…番茄炒鸡蛋吧, 那么原材料肯定是需要番茄和鸡蛋了, 假设这时候番茄分为国产番茄和进口番茄, 这时候一个人就习惯买鸡蛋加国产番茄, 另外一个人就喜欢买鸡蛋加进口番茄. 即形成了(鸡蛋, 国产番茄)和(鸡蛋, 进口番茄)两个序列, 我们知道同属番茄, 它们的Embedding应该非常相似, 如果这时候将他们的行为序列使用item2vec来计算, 那么由于属于两个不同的序列, 最终结果理论上仍然具有一定的相似度, 毕竟已知A与B相似, B与C相似, 可得A与C也可能相似, 但是相似度可能不够高.</p>
<p>而如果采用了图表示学习会怎么样呢? 那就先想办法把原本的序列变成图, 再使用DeepWalk来学习. 还是上面那个栗子, 把原本的(鸡蛋, 国产番茄)和(鸡蛋, 进口番茄)两个序列转变成图, 序列中相邻的两个物品, 在图中对应节点上有连边, 那么在图中, 国产番茄和进口番茄, 就通过鸡蛋这个节点得到了连接, 而在使用随机游走后, 它们就可以同属一个序列中且距离较近, 就可以获得更加相似的Embedding向量.</p>
<p><img src="fig_1.jpeg" alt="fig"></p>
<p>如上图, 将原本的序列转变为图, 再从图中抽取出新的序列, 来使用word2vec进行学习, 可以在原本序列呈现的共现关系之上, 挖掘出更深层次的共现关系, 这通常是更有益的.</p>
<p>这里还有一个小问题, 由于从序列到图再到序列, 可以获取到更多的共现关系, 那么如果原始序列比较长, 同时一条序列中如果有大部分物品, 那么在形成图以后, 可能这个图会接近一个完全图(两两节点之间都存在连接). 这时候进行图表示学习, 得到的结果可能不尽人意. 如何处理这个问题呢? 我个人认为在原始序列那里, 就要将序列的切分做好, 即通过一些规则, 将原本相关的一些物品/行为, 划到一个子序列中, 这样相当于会使得基于原始序列构建的图中的连边被切断一些, 最终学习效果会更好.</p>
<h2 id="完全随机-带权重"><a href="#完全随机-带权重" class="headerlink" title="完全随机/带权重"></a>完全随机/带权重</h2><p>原本的DeepWalk, 节点之间的连边是不带权重的, 即只要与当前节点相连的节点, 都有相同的概率成为下一个节点.</p>
<p>但是在一些情况下, 带权重的可能会更好一些. 比如现在这个图是有一些原始序列得到的, 那么可能在原始序列中, 节点A经常与节点B出现在一起, 和其它一些节点可能只是一起出现一次, 这说明节点A与节点B是有很大关联的. 而如果在图中使用完全随机的方法, 可能就会削弱这种关联性, 使得节点B对于A来说和其它节点没有明显区别.</p>
<p>而在对连边加入权重后, 可以对类似这种情况进行调整. 比如让连边的值等于两个节点在原始序列中, 相邻出现的次数, 在形成图以后, 对于每个节点而言, 对其相邻节点的连边权重做归一化, 归一化后的值可以看做跳转概率.</p>
<p>当然这只是简单的一种加权重的方法, 真正是否加权重, 怎么样加权重, 需要由具体的数据与任务(下游任务)来决定.</p>
<h2 id="无向-有向"><a href="#无向-有向" class="headerlink" title="无向/有向"></a>无向/有向</h2><p>图属于无向图还是有向图, 或者说在构造图的时候, 选择构造成怎样的图, 对使用DeepWalk这样的图表示学习算法, 也是有一定的影响的.</p>
<p>个人认为, 构造为有向或者无向, 可以根据业务以及数据本身的性质来决定. 比如社交网络中, 以好友关系来构建, 无向网络会自然一些, 比较朋友是相互的; 而已关注来构建, 那么有向网络可能会好一些, 因为关注有时候是单方面的.</p>
<p>此外, 如果把关注点放在图表示学习后的一些下游任务上, 那么可以对无向/有向这两种形式的图都进行尝试, 比较它们的差别, 选择其中一种.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里对比item2vec那里, 仍然使用了相同的MovieLens 1M Dataset.</p>
<p>在构建图的时候, 使用了最简单的无向, 完全随机(不带权重)的方式.</p>
<p>在图上进行随机游走, 抽取序列时, 为了使与原始序列在数据量上尽量相等, 即总序列长度(各序列长度之和)相当, 设置的$\gamma$为2, 序列最大长度为100.</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">700000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">700000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整合每个用户的观看序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_items_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = list(zip(x[<span class="string">'item_id'</span>], x[<span class="string">'time'</span>]))</span><br><span class="line">    x = sorted(x, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> list(x)</span><br><span class="line"></span><br><span class="line">user_items = df_train.groupby(<span class="string">'user_id'</span>).apply(user_items_func)</span><br><span class="line">user_items = dict(zip(user_items.index, user_items.values))</span><br><span class="line">user_items[<span class="string">'1980'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;85&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;2671&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;1680&apos;, &apos;974691498&apos;),</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 序列长度分布</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对长于200的序列进行截断</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items[k] = user_items[k][: <span class="number">200</span>]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对小于5的序列进行剔除</span></span><br><span class="line">drop_list = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    <span class="keyword">if</span> len(user_items[k]) &lt; <span class="number">5</span>:</span><br><span class="line">        drop_list.append(k)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> drop_list:</span><br><span class="line">    <span class="keyword">del</span> user_items[k]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_4.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_list = user_items.keys()</span><br><span class="line">item_list = list(set([y[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items.values() <span class="keyword">for</span> y <span class="keyword">in</span> x ]))</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(user_list)]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(item_list)]</span><br><span class="line">df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(129815, 4)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据序列</span></span><br><span class="line">train_data = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    train_data.append([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[k]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_edge_file</span><span class="params">(data, file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    生成包含连边的文件.</span></span><br><span class="line"><span class="string">    :param data: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq) - <span class="number">1</span>):</span><br><span class="line">                tmp = seq[i] + <span class="string">' '</span> + seq[i + <span class="number">1</span>] + <span class="string">'\n'</span></span><br><span class="line">                f.write(tmp)</span><br><span class="line">        f.close()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    构建图.</span></span><br><span class="line"><span class="string">    :param file: 节点连边文件路径.</span></span><br><span class="line"><span class="string">    :return: dict, 图.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    graph = defaultdict(set)</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            node_0, node_1 = line.strip().split()</span><br><span class="line">            <span class="keyword">if</span> node_0 != node_1:</span><br><span class="line">                graph[node_0].add(node_1)</span><br><span class="line">                graph[node_1].add(node_0)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        graph[node] = list(graph[node])</span><br><span class="line">    <span class="keyword">return</span> graph</span><br><span class="line"></span><br><span class="line">create_edge_file(train_data, <span class="string">'tmp.txt'</span>)</span><br><span class="line">graph = create_graph(<span class="string">'tmp.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行随机游走构建序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_walk</span><span class="params">(graph, epoch=<span class="number">2</span>, max_len=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在图中随机游走, 返回生成的序列数据.</span></span><br><span class="line"><span class="string">    :param graph: dict, 图.</span></span><br><span class="line"><span class="string">    :param epoch: 遍历多少次所以节点.</span></span><br><span class="line"><span class="string">    :param max_len: 序列最大长度.</span></span><br><span class="line"><span class="string">    :return: list[list[seq...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    nodes = list(graph.keys())</span><br><span class="line">    res_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        random.shuffle(nodes)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            seq = [node]</span><br><span class="line">            cur_node = node</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(max_len - <span class="number">1</span>):</span><br><span class="line">                cur_node = random.choice(graph[cur_node])</span><br><span class="line">                seq.append(cur_node)</span><br><span class="line">            res_data.append(seq)</span><br><span class="line">    random.shuffle(res_data)</span><br><span class="line">    <span class="keyword">return</span> res_data</span><br><span class="line"></span><br><span class="line">train_data_deep_walk = random_walk(graph, epoch=<span class="number">1</span>, max_len=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>这里使用gensim中的word2vec来进行计算, 简单方便~</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.system("taskset -p 0xff %d" % os.getpid())</span></span><br><span class="line">dim = <span class="number">32</span></span><br><span class="line">model = word2vec.Word2Vec(sentences=train_data_deep_walk,</span><br><span class="line">                          size=dim,  <span class="comment"># 向量维度</span></span><br><span class="line">                          alpha=<span class="number">0.025</span>,  <span class="comment"># 学习率</span></span><br><span class="line">                          window=<span class="number">1</span>,  <span class="comment"># 窗口大小</span></span><br><span class="line">                          min_count=<span class="number">5</span>,</span><br><span class="line">                          sample=<span class="number">0.001</span>,</span><br><span class="line">                          seed=<span class="number">7</span>,</span><br><span class="line">                          workers=<span class="number">12</span>,</span><br><span class="line">                          min_alpha=<span class="number">0.0001</span>,</span><br><span class="line">                          sg=<span class="number">1</span>,  <span class="comment"># 使用skip-gram</span></span><br><span class="line">                          hs=<span class="number">0</span>,  <span class="comment"># 使用neg-sample</span></span><br><span class="line">                          negative=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取词表, 以及每个词汇的词向量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">vocab = model.wv.index2word</span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line">item_embedding = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> vocab:</span><br><span class="line">    item_embedding[k] = model[k]</span><br><span class="line">item_id_2_ids_dict = &#123;k:i  <span class="keyword">for</span> i, k <span class="keyword">in</span> enumerate(item_embedding)&#125;</span><br><span class="line">ids_2_item_id_dict = dict(zip(item_id_2_ids_dict.values(), item_id_2_ids_dict.keys()))</span><br><span class="line"></span><br><span class="line">item_embedding_mat = np.zeros((len(vocab), dim))</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> item_embedding:</span><br><span class="line">    item_embedding_mat[item_id_2_ids_dict[k]] = item_embedding[k]</span><br><span class="line">item_embedding_mat = item_embedding_mat.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为3154</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户统计已经看过的电影</span></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items_dict[user] = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计测试集上每个用户点击的物品</span></span><br><span class="line"></span><br><span class="line">user_clicked_item_set_on_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df_test.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">        user_clicked_item_set_on_test[user] = set([])</span><br><span class="line">    user_clicked_item_set_on_test[user].add(item)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将用户近期观看的100部影片的Embedding进行平均池化</span></span><br><span class="line"></span><br><span class="line">user_embedding_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    tmp_item_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user][:<span class="number">100</span>]]</span><br><span class="line">    tmp_embedding = np.zeros(dim)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tmp_item_list:</span><br><span class="line">        tmp_embedding += item_embedding.get(item, np.zeros(dim))</span><br><span class="line">    user_embedding_dict[user] = tmp_embedding / len(tmp_item_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为每个用户寻找感兴趣的200电影(去除已观看)</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line"><span class="comment"># index = faiss.IndexFlatL2(32)  # 创建索引</span></span><br><span class="line">index = faiss.IndexFlatIP(dim)  <span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line">index.add(item_embedding_mat)  <span class="comment"># 添加向量</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">1000</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line"></span><br><span class="line">user_match_item_set_filter = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    D, I = index.search(user_embedding_dict[user].reshape((<span class="number">1</span>, dim)).astype(<span class="string">'float32'</span>), k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">    user_match_item_set_filter[user] = [ids_2_item_id_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> I[<span class="number">0</span>] <span class="keyword">if</span> ids_2_item_id_dict[x] <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict[user]][:<span class="number">200</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1463, Recall: 0.2488, F1: 0.1843</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 通过对DeepWalk原理的介绍, 知道了这是一种相对简单而有效的图表示学习方法, 即可以将图中的节点, 利用其共现关系, 学习得到Embedding.</p>
<p>然后用代码来对算法流程进行实现, 并在相近的参数下, 对比item2vec在召回中的效果:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">item2vec</th>
<th style="text-align:center">DeepWalk</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Precision</td>
<td style="text-align:center">0.1213</td>
<td style="text-align:center">0.1463</td>
</tr>
<tr>
<td style="text-align:center">Recall</td>
<td style="text-align:center">0.2064</td>
<td style="text-align:center">0.2488</td>
</tr>
<tr>
<td style="text-align:center">F1</td>
<td style="text-align:center">0.1528</td>
<td style="text-align:center">0.1843</td>
</tr>
</tbody>
</table>
</div>
<p>从结果上对比, DeepWalk是要比item2vec效果更好的, 这表明在使用图表示学习后, 确实挖掘出了更多潜藏的共现关系.</p>
<p>同时, 对于item2vec来说, 其主要能够调节的超参数有:</p>
<ul>
<li><p>原始序列的长度.</p>
<p>可以对一些较短的序列进行过滤, 对一些较长的序列进行截断或者分成多个序列.</p>
</li>
<li><p>word2vec算法.</p>
<p>包括Embedding向量的维度, 邻近词的窗口大小, 负采样数量等.</p>
</li>
<li><p>用户Embedding的表示.</p>
<p>如何利用物品的Embedding来表示用户的Embedding. 可以取近期点击物品的Embedding平均, 或者加权平均等.</p>
</li>
</ul>
<p>而对于DeepWalk来, 在item2vec的基础上, 增加了更多可调节超参数:</p>
<ul>
<li><p>用于构建图的样本.</p>
<p>包括原始序列的长度以及数量, 构建出来的图的不同, 会影响到采样的序列及后续的训练.</p>
</li>
<li><p>图中序列的采样.</p>
<p>采样多少样本序列, 每个序列长度为多少, 也是需要调节的.</p>
</li>
</ul>
<p>综上, DeepWalk是一种值得使用的算法, 同时在使用时, 需要根据一些下游任务, 来对其超参数进行调节, 以达到更好的效果.</p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
        <tag>word2vec</tag>
        <tag>召回</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>item2vec召回</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/item2vec%E5%8F%AC%E5%9B%9E/</url>
    <content><![CDATA[<p>自从NLP中有了word2vec算法过后, 基于它的思想, 出现了一些衍生的算法, 有改进原理的, 也有跨领域运用的.</p>
<p>item2vec就是将word2vec的使用范围进行扩展, 从原本的单词, 字符, 变成了任何离散序列的item, 本篇主要讲解item2vec在推荐系统中的使用.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如果还有对word2vec原理不太了解的同学, 可以参看我的<a href="whitemoonlight.top/2020/08/16/自然语言处理/word2vec-一/">这篇文章</a>.</p>
<p>word2vec本质上, 就是通过在序列中, 寻找一种共现模式, 即将相邻的词看做具有更高的相关性, 并将这种关系, 在向量空间中进行表达.</p>
<p>而这种算法本身其实并不限定只能用在NLP领域, 理论上只要是离散序列, 就可以使用word2vec进行Embedding.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>在明白了word2vec的原理以后, 其实对于item2vec来说, 就没有什么原理好说的了, 这里可以举一些栗子, 来说明item2vec的使用方式.</p>
<p>首先, 这里的item是什么呢? 在推荐系统中, 通常用户是随着时间有一系列的行为的, 对典型的, 就是与物品之间的交互行为, 比如对物品的点击, 收藏, 评价等. 如果将某个用户在一段时间内交互的物品串起来, 那么不就是构成了一个序列了吗, 而这里的物品, 就等价于word2vec中的token, 这就是item2vec.</p>
<p>然后, 通过item2vec算法得到了每个物品的Embedding, 怎么使用呢? 在前面的一些推荐系统的文章中, 几乎都是属于U2I(user2item), 意思是使用能够表征用户的向量, 去匹配物品, 那么在item2vec这里, 就可以算作I2I(item2item), 即可以通过用户喜欢的物品的向量, 去匹配其它物品.</p>
<p>同时, item2vec不仅仅可以用来做I2I, 还可以像NLP中的预训练词向量一样, 服务于其它算法模型. 比如在一些排序模型中, 要使用用户近期有过交互的物品的平均向量表示, 而如果随机初始化训练, 可能会由于要同时学习向量表示和其它模型参数, 会比较困难. 而使用item2vec的向量来进行初始化, 随后或者固定不训练, 或者微调, 大概率来说会取得更好的效果, 收敛得更快.</p>
<p>此外, 我个人认为在做比较细致的处理时, 需要对用户原始的行为序列做划分, 划分为一些子序列, 然后再输入word2vec算法. 因为即使是一个用户的行为序列, 但是在不同时期, 不同场景下, 可能其目的和行为有着一定的偏向性, 比如以看视频为例, 也许用户在白天的时候喜欢看一些学习类的视频, 晚上喜欢看一些娱乐类的视频, 此时将它们划分开, 学习到的Embedding可能会更好一些</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这里主要使用item2vec算法来获取物品的Embedding, 然后利用I2I来进行召回.</p>
<p>对于训练item2vec, 会对比自己写的一个简单算法, 和成熟的word2vec算法.</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>这里使用的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了防止一些过于活跃的用户占主导影响, 会截取活跃用户近期的观看记录, 同时对于一些过短的观看记录进行过滤.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">700000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">700000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整合每个用户的观看序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_items_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = list(zip(x[<span class="string">'item_id'</span>], x[<span class="string">'time'</span>]))</span><br><span class="line">    x = sorted(x, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> list(x)</span><br><span class="line"></span><br><span class="line">user_items = df_train.groupby(<span class="string">'user_id'</span>).apply(user_items_func)</span><br><span class="line">user_items = dict(zip(user_items.index, user_items.values))</span><br><span class="line">user_items[<span class="string">'1980'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;85&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;2671&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;1680&apos;, &apos;974691498&apos;),</span><br><span class="line"> (&apos;2331&apos;, &apos;974691337&apos;),</span><br><span class="line"> (&apos;1569&apos;, &apos;974691337&apos;),</span><br><span class="line"> </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 序列长度分布</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对长于200的序列进行截断</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items[k] = user_items[k][: <span class="number">200</span>]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_1.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对小于5的序列进行剔除</span></span><br><span class="line">drop_list = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    <span class="keyword">if</span> len(user_items[k]) &lt; <span class="number">5</span>:</span><br><span class="line">        drop_list.append(k)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> drop_list:</span><br><span class="line">    <span class="keyword">del</span> user_items[k]</span><br><span class="line">sns.distplot([len(user_items[x]) <span class="keyword">for</span> x <span class="keyword">in</span> user_items], bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_list = user_items.keys()</span><br><span class="line">item_list = list(set([y[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items.values() <span class="keyword">for</span> y <span class="keyword">in</span> x ]))</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(user_list)]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(item_list)]</span><br><span class="line">df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(129815, 4)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据序列</span></span><br><span class="line">train_data = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> user_items:</span><br><span class="line">    train_data.append([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[k]])</span><br></pre></td></tr></table></figure>
<h2 id="模型训练-gensim"><a href="#模型训练-gensim" class="headerlink" title="模型训练(gensim)"></a>模型训练(gensim)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.system("taskset -p 0xff %d" % os.getpid())</span></span><br><span class="line">dim = <span class="number">32</span></span><br><span class="line">model = word2vec.Word2Vec(sentences=train_data,</span><br><span class="line">                          size=dim,  <span class="comment"># 向量维度</span></span><br><span class="line">                          alpha=<span class="number">0.025</span>,  <span class="comment"># 学习率</span></span><br><span class="line">                          window=<span class="number">1</span>,  <span class="comment"># 窗口大小</span></span><br><span class="line">                          min_count=<span class="number">5</span>,</span><br><span class="line">                          sample=<span class="number">0.001</span>,</span><br><span class="line">                          seed=<span class="number">7</span>,</span><br><span class="line">                          workers=<span class="number">12</span>,</span><br><span class="line">                          min_alpha=<span class="number">0.0001</span>,</span><br><span class="line">                          sg=<span class="number">1</span>,  <span class="comment"># 使用skip-gram</span></span><br><span class="line">                          hs=<span class="number">0</span>,  <span class="comment"># 使用neg-sample</span></span><br><span class="line">                          negative=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取词表, 以及每个词汇的词向量</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">vocab = model.wv.index2word</span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line">item_embedding = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> vocab:</span><br><span class="line">    item_embedding[k] = model[k]</span><br><span class="line">item_id_2_ids_dict = &#123;k:i  <span class="keyword">for</span> i, k <span class="keyword">in</span> enumerate(item_embedding)&#125;</span><br><span class="line">ids_2_item_id_dict = dict(zip(item_id_2_ids_dict.values(), item_id_2_ids_dict.keys()))</span><br><span class="line"></span><br><span class="line">item_embedding_mat = np.zeros((<span class="number">3202</span>, dim))</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> item_embedding:</span><br><span class="line">    item_embedding_mat[item_id_2_ids_dict[k]] = item_embedding[k]</span><br><span class="line">item_embedding_mat = item_embedding_mat.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为3202</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计测试集上每个用户点击的物品</span></span><br><span class="line"></span><br><span class="line">user_clicked_item_set_on_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df_test.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">        user_clicked_item_set_on_test[user] = set([])</span><br><span class="line">    user_clicked_item_set_on_test[user].add(item)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将用户近期观看的100部影片的Embedding进行平均池化</span></span><br><span class="line"></span><br><span class="line">user_embedding_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    tmp_item_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user][:<span class="number">100</span>]]</span><br><span class="line">    tmp_embedding = np.zeros(dim)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tmp_item_list:</span><br><span class="line">        tmp_embedding += item_embedding.get(item, np.zeros(dim))</span><br><span class="line">    user_embedding_dict[user] = tmp_embedding / len(tmp_item_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为每个用户寻找感兴趣的200电影(去除已观看)</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line"><span class="comment"># index = faiss.IndexFlatL2(32)  # 创建索引</span></span><br><span class="line">index = faiss.IndexFlatIP(dim)  <span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line">index.add(item_embedding_mat)  <span class="comment"># 添加向量</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">1000</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line"></span><br><span class="line">user_match_item_set_filter = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    D, I = index.search(user_embedding_dict[user].reshape((<span class="number">1</span>, dim)).astype(<span class="string">'float32'</span>), k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">    user_match_item_set_filter[user] = [ids_2_item_id_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> I[<span class="number">0</span>] <span class="keyword">if</span> ids_2_item_id_dict[x] <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict[user]][:<span class="number">200</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1213, Recall: 0.2064, F1: 0.1528</span><br></pre></td></tr></table></figure>
<h2 id="模型训练-TF"><a href="#模型训练-TF" class="headerlink" title="模型训练(TF)"></a>模型训练(TF)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Item2Vec</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_0 = keras.layers.Embedding(len(self.vocab), self.embedding_size, input_length=<span class="number">1</span>)</span><br><span class="line">        self.embedding_1 = keras.layers.Embedding(len(self.vocab), self.embedding_size, input_length=<span class="number">1</span>)</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x_0 = inputs[:, <span class="number">0</span>]</span><br><span class="line">        x_1 = inputs[:, <span class="number">1</span>]</span><br><span class="line">        x_0 = tf.reshape(self.embedding_0(x_0), (<span class="number">-1</span>, self.embedding_size))</span><br><span class="line">        x_1 = tf.reshape(self.embedding_1(x_1), (<span class="number">-1</span>, self.embedding_size))</span><br><span class="line">        logits = keras.layers.dot([x_0, x_1], axes=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.sigmoid(logits)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_vocab</span><span class="params">(data, min_count=<span class="number">5</span>)</span>:</span></span><br><span class="line">    count_dict = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> list_:</span><br><span class="line">            count_dict[w] += <span class="number">1</span></span><br><span class="line">    vocab = &#123;<span class="string">'UNK'</span>: <span class="number">0</span>&#125;</span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> count_dict:</span><br><span class="line">        <span class="keyword">if</span> count_dict[w] &lt; min_count:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab[w] = n</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = create_vocab(train_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_dataset_generator</span><span class="params">()</span>:</span></span><br><span class="line">    windows = <span class="number">1</span></span><br><span class="line">    negtive = <span class="number">2</span></span><br><span class="line">    num_id = len(vocab)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> train_data:</span><br><span class="line">            list_ = [vocab.get(x, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> list_]</span><br><span class="line">            len_list = len(list_)</span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(list_):</span><br><span class="line">                start = max(<span class="number">0</span>, i - windows)</span><br><span class="line">                end = min(len_list - <span class="number">1</span>, i + windows)</span><br><span class="line">                pos_ids = list_[start:i] + list_[i + <span class="number">1</span>:end + <span class="number">1</span>]</span><br><span class="line">                <span class="comment"># 负采样</span></span><br><span class="line">                neg_ids = []</span><br><span class="line">                <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                    <span class="keyword">if</span> len(neg_ids) == negtive:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    tmp_id = np.random.randint(<span class="number">0</span>, num_id)</span><br><span class="line">                    <span class="keyword">if</span> tmp_id <span class="keyword">not</span> <span class="keyword">in</span> pos_ids <span class="keyword">and</span> tmp_id != c:</span><br><span class="line">                        neg_ids.append(tmp_id)</span><br><span class="line">                id_list = pos_ids + neg_ids</span><br><span class="line">                label_list = [<span class="number">1</span>] * len(pos_ids) + [<span class="number">0</span>] * len(neg_ids)</span><br><span class="line">                shuffle = np.array(range(len(id_list)))</span><br><span class="line">                np.random.shuffle(shuffle)</span><br><span class="line">                id_list = np.array(id_list)[shuffle]</span><br><span class="line">                label_list = np.array(label_list)[shuffle]</span><br><span class="line">                <span class="keyword">for</span> j, id_ <span class="keyword">in</span> enumerate(id_list):</span><br><span class="line">                    <span class="keyword">yield</span> np.array([c, id_]), label_list[j]</span><br><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset \</span><br><span class="line">    .from_generator(train_dataset_generator, output_types=(tf.int64, tf.int64), output_shapes=(tf.TensorShape([<span class="number">2</span>]), tf.TensorShape([]))) \</span><br><span class="line">    .shuffle(<span class="number">10000</span>, seed=<span class="number">7</span>) \</span><br><span class="line">    .batch(<span class="number">2048</span>) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = Item2Vec(vocab, embedding_size=<span class="number">32</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          steps_per_epoch=<span class="number">966</span>,</span><br><span class="line">          epochs=<span class="number">1</span></span><br><span class="line">         )</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">966/966 [==============================] - 613s 635ms/step - loss: 0.5914</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取Embedding</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line">embeddings = model.embedding_0.get_weights()[<span class="number">0</span>]</span><br><span class="line">item_embedding = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> vocab:</span><br><span class="line">    item_embedding[k] = embeddings[vocab[k]]</span><br><span class="line">item_id_2_ids_dict = vocab</span><br><span class="line">ids_2_item_id_dict = dict(zip(item_id_2_ids_dict.values(), item_id_2_ids_dict.keys()))</span><br><span class="line"></span><br><span class="line">item_embedding_mat = np.zeros((len(vocab), dim))</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> item_embedding:</span><br><span class="line">    item_embedding_mat[item_id_2_ids_dict[k]] = item_embedding[k]</span><br><span class="line">item_embedding_mat = item_embedding_mat.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为3203</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户统计已经看过的电影</span></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_items:</span><br><span class="line">    user_items_dict[user] = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计测试集上每个用户点击的物品</span></span><br><span class="line"></span><br><span class="line">user_clicked_item_set_on_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> df_test.iterrows():</span><br><span class="line">    line = line[<span class="number">1</span>]</span><br><span class="line">    user = line[<span class="string">'user_id'</span>]</span><br><span class="line">    item = line[<span class="string">'item_id'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">        user_clicked_item_set_on_test[user] = set([])</span><br><span class="line">    user_clicked_item_set_on_test[user].add(item)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将用户近期观看的100部影片的Embedding进行平均池化</span></span><br><span class="line"></span><br><span class="line">user_embedding_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    tmp_item_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_items[user][:<span class="number">100</span>]]</span><br><span class="line">    tmp_embedding = np.zeros(dim)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tmp_item_list:</span><br><span class="line">        tmp_embedding += item_embedding.get(item, np.zeros(dim))</span><br><span class="line">    user_embedding_dict[user] = tmp_embedding / len(tmp_item_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为每个用户寻找感兴趣的物品(去除已观看)</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line"><span class="comment"># index = faiss.IndexFlatL2(32)  # 创建索引</span></span><br><span class="line">index = faiss.IndexFlatIP(dim)  <span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line">index.add(item_embedding_mat)  <span class="comment"># 添加向量</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">1000</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line"></span><br><span class="line">user_match_item_set_filter = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    D, I = index.search(user_embedding_dict[user].reshape((<span class="number">1</span>, dim)).astype(<span class="string">'float32'</span>), k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">    user_match_item_set_filter[user] = [ids_2_item_id_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> I[<span class="number">0</span>] <span class="keyword">if</span> ids_2_item_id_dict[x] <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict[user] <span class="keyword">and</span> x != <span class="number">0</span>][:<span class="number">200</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算查准率, 查全率, F1</span></span><br><span class="line"></span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_recommend = <span class="number">0</span></span><br><span class="line">total_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_clicked_item_set_on_test:</span><br><span class="line">    clicked_items = user_clicked_item_set_on_test[user]</span><br><span class="line">    recommend_items = user_match_item_set_filter[user]</span><br><span class="line">    true_pos += len(clicked_items.intersection(recommend_items))</span><br><span class="line">    total_recommend += len(recommend_items)</span><br><span class="line">    total_pos += len(clicked_items)</span><br><span class="line">precision = true_pos / total_recommend</span><br><span class="line">recall = true_pos / total_pos</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'Precision: %.4f, Recall: %.4f, F1: %.4f'</span> % (precision, recall, f1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Precision: 0.1398, Recall: 0.2378, F1: 0.1761</span><br></pre></td></tr></table></figure>
<p>相似超参数下, 相比使用gensim中的word2vec算法来做item2vec, 这里自己实现的在这份数据集上表现还要好一些, 哈哈 ♪(^∇^*)</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 对item2vec的原理进行了介绍, 然后在一份数据集上, 整理用户观看影片的行为序列, 基于word2vec算法来计算物品Embedding. 同时自己这边也使用TensorFlow实现了item2vec算法, 对比成熟的gensim中的word2vec算法, 效果并没有差.</p>
<p>Item2vec某种意义上, 是直接效仿word2vec, 对物品序列进行建模, 那么有没有这样方法的升级版呢? 有的哦 ♪(^∇^*) 比如<a href="whitemoonlight.top/2020/10/10/图算法/DeepWalk/">DeepWalk</a>.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>召回</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>DSSM召回</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/DSSM%E5%8F%AC%E5%9B%9E/</url>
    <content><![CDATA[<p>嗯这次来说一说DSSM在推荐系统中的应用.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在这一篇文章之前, 在推荐系统板块中, 有说到过协同过滤, 矩阵分解, YouTubeNet等算法用于召回, 这里将会再介绍一种新的用于推荐系统召回的算法.</p>
<p>自然语言处理板块的<a href="whitemoonlight.top/2020/10/07/自然语言处理/文本表示/">文本表示</a>中, 就对DSSM的原理有过阐述, DSSM的全称为Deep Structured Semantic Model, 可以翻译为深度语义匹配模型, 论文<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">在这里</a>.</p>
<p>让我先大声喊一句: NLP, 永远滴神♪(^∇^*)</p>
<p>所以, 在推荐系统这里, 把原本多用于搜索, 相似度匹配的DSSM用到了召回系统里面, 在保留了原本算法主要思想下, 根据实际情况进行了一些修改. 推荐系统中, 虽然仍然使用DSSM, 然实际上是指的广义上的双塔模型, 就好像用GBDT指代XGB一类的算法一样.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>先说一下原本的DSSM的模型, 考虑一个搜索的场景, 在输入一个Query之后, 要在众多的Doc中寻找最为匹配的项, 那么这时候如果将Query经过某种模型, 得到其向量表示, 再与Doc对应的向量进行相似度计算, 将相似度高的进行返回, 即可完成匹配.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>上图就是DSSM的结构, 其中的几个关键部分:</p>
<ul>
<li><p>输入层:</p>
<p>对于输入层, 常规的DSSM采用的是词袋模型.</p>
<p>具体说来, 比如是英文, 那么采用Letter-Trigram, 即用三个英文字母的组合来表示一个token. 这样做以后, 词表(或者说向量)的大小约为两万. 而中文则采用字作为token, 常用的子大概也是两万左右.</p>
<p>将一段原始文本, 切分为token后, 表示为词袋向量, 就是输入了.</p>
</li>
<li><p>中间层:</p>
<p>中间层其实就是一些全连接网络, 用到的激活函数为$\tanh$. 可以取中间层的最后一层的输出, 作为文本的表示向量.</p>
</li>
<li><p>匹配层:</p>
<p>最后的匹配层, 会首先计算Query与各个Doc的余弦相似度, 然后再经过一个softmax层, 来进行学习. 在训练的时候, 一般需要进行采样, 即针对一个Query, 需要有一个真正匹配的Doc, 还要有一些不匹配的Doc作为负例, 类似负采样.</p>
</li>
</ul>
<p>而将NLP中的DSSM移植到推荐系统中, 也是非常简单的.</p>
<p>一般可以将用户信息与物品信息分开, 分别在两个塔(神经网络)中输入, 在得到Embedding向量后, 根据实际情况进行操作. 比如标签数据是用户是否与某个物品产生行为, 那么这就是一个二分类问题, 首先可以通过负采样, 得到一些标签为0的负样本; 然后可以对用户与物品的Embedding做内积(或者余弦相似度), 然后再经过一个sigmoid函数, 进行预测. 在训练好模型后, 可以保存每个用户与每个物品的Embedding向量, 在召回服务阶段, 对于一个用户的Embedding, 可以用最近邻检索来获得Top-N物品, 作为排序阶段候选集.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>这里用到的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">data_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        data_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;1&apos;, &apos;1193&apos;, &apos;5&apos;, &apos;978300760&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;661&apos;, &apos;3&apos;, &apos;978302109&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;914&apos;, &apos;3&apos;, &apos;978301968&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;3408&apos;, &apos;4&apos;, &apos;978300275&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;2355&apos;, &apos;5&apos;, &apos;978824291&apos;]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6040, 3706)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的用户观看影片数量分布</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对观看影片多于500的用户进行随机抽样, 避免样本占比过多影响模型学习</span></span><br><span class="line">df = pd.DataFrame(data_list, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x <span class="keyword">in</span> c_user <span class="keyword">if</span> c_user[x] &gt; <span class="number">500</span>]</span><br><span class="line"></span><br><span class="line">res_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">500</span>, random_state=<span class="number">7</span>)</span><br><span class="line">    res_df = res_df.append(tmp_df)</span><br><span class="line">    </span><br><span class="line">res_df = res_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line"></span><br><span class="line">data_list = res_df.values.tolist()</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的影片被观看次数分布</span></span><br><span class="line"></span><br><span class="line">c_item = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_item.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_4.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对被观看多于500的电影进行随机抽样, 避免样本占比过多影响模型学习</span></span><br><span class="line">df = pd.DataFrame(data_list, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">sample_item = [x <span class="keyword">for</span> x <span class="keyword">in</span> c_item <span class="keyword">if</span> c_item[x] &gt; <span class="number">500</span>]</span><br><span class="line"></span><br><span class="line">res_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sample_item:</span><br><span class="line">    tmp_df = df.loc[df[<span class="string">'item_id'</span>] == item].sample(n=<span class="number">500</span>, random_state=<span class="number">7</span>)</span><br><span class="line">    res_df = res_df.append(tmp_df)</span><br><span class="line"></span><br><span class="line">res_df = res_df.append(df.loc[~df[<span class="string">'item_id'</span>].isin(sample_item)])</span><br><span class="line"></span><br><span class="line">data_list = res_df.values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计分析数据集中的影片被观看次数分布</span></span><br><span class="line"></span><br><span class="line">c_item = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_item.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_5.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加用户其它信息</span></span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_list, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'occupation'</span>, <span class="string">'zip'</span>])</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">occupation</th>
<th style="text-align:right">zip</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">48067</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
<td style="text-align:right">70072</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
<td style="text-align:right">55117</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
<td style="text-align:right">02460</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
<td style="text-align:right">55455</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">user_df = user_df.loc[user_df[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'occupation'</span>]:</span><br><span class="line">    led = LabelEncoder()</span><br><span class="line">    user_df[ft] = led.fit_transform(user_df[ft])</span><br><span class="line">    </span><br><span class="line">user_list = user_df.values</span><br><span class="line">user_dict = dict(zip(user_list[:, <span class="number">0</span>], user_list[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">data_list = [x + list(user_dict[x[<span class="number">0</span>]]) <span class="keyword">for</span> x <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;4959&apos;, &apos;2394&apos;, &apos;4&apos;, &apos;962634004&apos;, 1, 2, 9],</span><br><span class="line"> [&apos;1606&apos;, &apos;2394&apos;, &apos;3&apos;, &apos;974736208&apos;, 0, 3, 14],</span><br><span class="line"> [&apos;2308&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974487994&apos;, 0, 4, 8],</span><br><span class="line"> [&apos;3285&apos;, &apos;2394&apos;, &apos;2&apos;, &apos;968118037&apos;, 1, 2, 15],</span><br><span class="line"> [&apos;2325&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974435017&apos;, 0, 1, 15]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加电影其它信息</span></span><br><span class="line"></span><br><span class="line">item_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_list, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="fig_6.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对电影分类进行编码, 0为padding</span></span><br><span class="line">category_set = set([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">        category_set.add(j)</span><br><span class="line">category_map_dict = dict(zip(category_set, range(<span class="number">1</span>, len(category_set) + <span class="number">1</span>)))</span><br><span class="line">item_df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: len(x)).max()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_df[<span class="string">'category_list_ecd'</span>] = item_df[<span class="string">'category_list'</span>].apply(<span class="keyword">lambda</span> x: [category_map_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">item_df[<span class="string">'category_list_ecd'</span>] = item_df[<span class="string">'category_list_ecd'</span>].apply(<span class="keyword">lambda</span> x: x + [<span class="number">0</span>] * (<span class="number">6</span> - len(x)))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="fig_7.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">item_df = item_df.loc[item_df[<span class="string">'item_id'</span>].isin(list(item_set))]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'title'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category'</span>]</span><br><span class="line"><span class="keyword">del</span> item_df[<span class="string">'category_list'</span>]</span><br><span class="line">    </span><br><span class="line">item_list = item_df.values</span><br><span class="line">item_dict = dict(zip(item_list[:, <span class="number">0</span>], item_list[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">data_list = [x + list(item_dict[x[<span class="number">1</span>]]) <span class="keyword">for</span> x <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;4959&apos;, &apos;2394&apos;, &apos;4&apos;, &apos;962634004&apos;, 1, 2, 9, [3, 17, 0, 0, 0, 0]],</span><br><span class="line"> [&apos;1606&apos;, &apos;2394&apos;, &apos;3&apos;, &apos;974736208&apos;, 0, 3, 14, [3, 17, 0, 0, 0, 0]],</span><br><span class="line"> [&apos;2308&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974487994&apos;, 0, 4, 8, [3, 17, 0, 0, 0, 0]],</span><br><span class="line"> [&apos;3285&apos;, &apos;2394&apos;, &apos;2&apos;, &apos;968118037&apos;, 1, 2, 15, [3, 17, 0, 0, 0, 0]],</span><br><span class="line"> [&apos;2325&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974435017&apos;, 0, 1, 15, [3, 17, 0, 0, 0, 0]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据每个用户的打分, 把样本分为正/负样本</span></span><br><span class="line">user_score_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> data_list:</span><br><span class="line">    user = list_[<span class="number">0</span>]</span><br><span class="line">    score = int(list_[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_score_dict:</span><br><span class="line">        user_score_dict[user] = [score, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        user_score_dict[user][<span class="number">0</span>] += score</span><br><span class="line">        user_score_dict[user][<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">user_mean_score_dict = dict([(x, user_score_dict[x][<span class="number">0</span>] / user_score_dict[x][<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> user_score_dict])</span><br><span class="line">data_list = [x + [int(int(x[<span class="number">2</span>]) &gt; user_mean_score_dict[x[<span class="number">0</span>]])] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间戳排序, 并按排序划分训练集/测试集</span></span><br><span class="line"></span><br><span class="line">data_list = list(sorted(data_list, key=<span class="keyword">lambda</span> x: x[<span class="number">3</span>]))</span><br><span class="line">train_list = data_list[:<span class="number">463656</span>]</span><br><span class="line">test_list = data_list[<span class="number">463656</span>:]</span><br><span class="line">len(train_list), len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(463656, 198710)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 影片被观看次数少于20的剔除</span></span><br><span class="line">c_item_train = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line"></span><br><span class="line">drop_item_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_item_train <span class="keyword">if</span> c_item_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">456504</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 观看影片次数少于20的用户删除</span></span><br><span class="line">c_user_train = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">drop_user_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_user_train <span class="keyword">if</span> c_user_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">401955</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的物品剔除</span></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">in</span> train_item_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">194354</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的用户剔除</span></span><br><span class="line">train_user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">in</span> train_user_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整理用户, 电影的编号</span></span><br><span class="line">train_user_dict = dict(zip(train_user_set, range(len(train_user_set))))</span><br><span class="line"></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">train_item_dict = dict(zip(train_item_set, range(len(train_item_set))))</span><br><span class="line"></span><br><span class="line">train_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], x[<span class="number">4</span>], x[<span class="number">5</span>], x[<span class="number">6</span>], x[<span class="number">7</span>], x[<span class="number">-1</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">test_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], x[<span class="number">4</span>], x[<span class="number">5</span>], x[<span class="number">6</span>], x[<span class="number">7</span>], x[<span class="number">-1</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> test_list])</span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(train_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># user_id, gender, age, occupation, item_id, category</span></span><br><span class="line">train_dataset, train_y = train_dataset[:, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>]], train_dataset[:, <span class="number">-1</span>]</span><br><span class="line">test_dataset, test_y = test_dataset[:, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>]], test_dataset[:, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">train_dataset.max(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([4177, 1, 6, 20, 2716, list([18, 10, 0, 0, 0, 0])], dtype=object)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(num, dim=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param dim: Embedding维度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'dim'</span>: dim, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">varlen_sparse_feature</span><span class="params">(num, max_len, dim=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回变长离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param max_len: 变长序列最大长度.</span></span><br><span class="line"><span class="string">    :param dim: Embedding维度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'num'</span>: num, <span class="string">'dim'</span>: dim, <span class="string">'max_len'</span>: max_len, <span class="string">'type'</span>: <span class="string">'varlen_sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DSSM</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, user_feature_info_dict, item_feature_info_dict)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param user_feature_info_dict: 用户侧特征信息字典.</span></span><br><span class="line"><span class="string">        :param item_feature_info_dict: 物品侧特征信息字典.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.user_feature_info_dict = user_feature_info_dict</span><br><span class="line">        self.item_feature_info_dict = item_feature_info_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.user_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.user_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.user_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.user_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.item_embedding_dict = OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> name, ft_info <span class="keyword">in</span> self.item_feature_info_dict.items():</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                max_len = ft_info[<span class="string">'max_len'</span>]</span><br><span class="line">                self.item_embedding_dict[name] = keras.layers.Embedding(num, dim, input_length=max_len, mask_zero=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.item_embedding_dict[name] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.user_layer_0 = keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.user_layer_1 = keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.user_layer_2 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">        self.item_layer_0 = keras.layers.Dense(<span class="number">32</span>)</span><br><span class="line">        <span class="comment"># self.item_layer_1 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        <span class="comment"># self.item_layer_2 = keras.layers.Dense(32, activation='relu')</span></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature)</span></span><br><span class="line">        user_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.user_embedding_dict.items():</span><br><span class="line">            type_ = self.user_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.user_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                user_ft_list.append(ft)</span><br><span class="line">        user_ft_concat = tf.concat(user_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        user_x = self.user_layer_0(user_ft_concat)</span><br><span class="line">        user_x = self.user_layer_1(user_x)</span><br><span class="line">        user_x = self.user_layer_2(user_x)</span><br><span class="line"></span><br><span class="line">        item_ft_list = []</span><br><span class="line">        <span class="keyword">for</span> name, embedding <span class="keyword">in</span> self.item_embedding_dict.items():</span><br><span class="line">            type_ = self.item_feature_info_dict[name][<span class="string">'type'</span>]</span><br><span class="line">            ft = inputs[name]</span><br><span class="line">            <span class="keyword">if</span> type_ == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'dim'</span>]))</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> type_ == <span class="string">'varlen_sparse'</span>:</span><br><span class="line">                mask = tf.cast(tf.not_equal(ft, <span class="number">0</span>), dtype=tf.float32)</span><br><span class="line">                real_len = tf.reshape(tf.reduce_sum(mask, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = embedding(ft)</span><br><span class="line">                ft = ft * tf.reshape(mask, shape=(<span class="number">-1</span>, self.item_feature_info_dict[name][<span class="string">'max_len'</span>], <span class="number">1</span>))</span><br><span class="line">                ft = tf.reduce_sum(ft, axis=<span class="number">1</span>)</span><br><span class="line">                ft = ft / real_len</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft = tf.cast(ft, dtype=tf.float32)</span><br><span class="line">                item_ft_list.append(ft)</span><br><span class="line">        item_ft_concat = tf.concat(item_ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        item_x = self.item_layer_0(item_ft_concat)</span><br><span class="line">        <span class="comment"># item_x = self.item_layer_1(item_x)</span></span><br><span class="line">        <span class="comment"># item_x = self.item_layer_2(item_x)</span></span><br><span class="line"></span><br><span class="line">        output = keras.layers.dot([user_x, item_x], axes=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># output = output / tf.reshape(tf.norm(user_x, axis=1) * tf.norm(item_x, axis=1), shape=(-1, 1))</span></span><br><span class="line">        output = tf.sigmoid(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特征信息</span></span><br><span class="line">user_feature_info_dict = &#123;<span class="string">'user_id'</span>: sparse_feature(<span class="number">4178</span>, <span class="number">32</span>),</span><br><span class="line">                         <span class="string">'gender'</span>: sparse_feature(<span class="number">2</span>, <span class="number">8</span>),</span><br><span class="line">                         <span class="string">'age'</span>: sparse_feature(<span class="number">7</span>, <span class="number">16</span>),</span><br><span class="line">                         <span class="string">'occupation'</span>: sparse_feature(<span class="number">21</span>, <span class="number">16</span>)&#125;</span><br><span class="line">item_feature_info_dict = &#123;<span class="string">'item_id'</span>: sparse_feature(<span class="number">2717</span>, <span class="number">32</span>),</span><br><span class="line">                         <span class="string">'category'</span>: varlen_sparse_feature(<span class="number">19</span>, <span class="number">6</span>, <span class="number">8</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换数据格式</span></span><br><span class="line">train_dataset = &#123;<span class="string">'user_id'</span>: tf.constant(train_dataset[:, <span class="number">0</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'gender'</span>: tf.constant(train_dataset[:, <span class="number">1</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'age'</span>: tf.constant(train_dataset[:, <span class="number">2</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'occupation'</span>: tf.constant(train_dataset[:, <span class="number">3</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'item_id'</span>: tf.constant(train_dataset[:, <span class="number">4</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'category'</span>: tf.constant(list(train_dataset[:, <span class="number">5</span>]), dtype=tf.float32)&#125;</span><br><span class="line">test_dataset = &#123;<span class="string">'user_id'</span>: tf.constant(test_dataset[:, <span class="number">0</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'gender'</span>: tf.constant(test_dataset[:, <span class="number">1</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'age'</span>: tf.constant(test_dataset[:, <span class="number">2</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'occupation'</span>: tf.constant(test_dataset[:, <span class="number">3</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'item_id'</span>: tf.constant(test_dataset[:, <span class="number">4</span>], dtype=tf.float32),</span><br><span class="line">                <span class="string">'category'</span>: tf.constant(list(test_dataset[:, <span class="number">5</span>]), dtype=tf.float32)&#125;</span><br><span class="line"></span><br><span class="line">train_y = tf.constant(train_y, dtype=tf.int64)</span><br><span class="line">test_y = tf.constant(test_y, dtype=tf.int64)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = DSSM(user_feature_info_dict, item_feature_info_dict)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          train_y,</span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          batch_size=<span class="number">64</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>),</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/100</span><br><span class="line">5025/5025 [==============================] - 23s 5ms/step - loss: 0.6053 - auc: 0.7262 - val_loss: 0.5877 - val_auc: 0.7485</span><br><span class="line">Epoch 2/100</span><br><span class="line">5025/5025 [==============================] - 23s 4ms/step - loss: 0.5704 - auc: 0.7663 - val_loss: 0.5841 - val_auc: 0.7536</span><br><span class="line">Epoch 3/100</span><br><span class="line">5025/5025 [==============================] - 23s 4ms/step - loss: 0.5580 - auc: 0.7785 - val_loss: 0.5811 - val_auc: 0.7569</span><br><span class="line">Epoch 4/100</span><br><span class="line">5025/5025 [==============================] - 23s 5ms/step - loss: 0.5451 - auc: 0.7909 - val_loss: 0.5835 - val_auc: 0.7567</span><br><span class="line">Epoch 5/100</span><br><span class="line">5025/5025 [==============================] - 23s 4ms/step - loss: 0.5138 - auc: 0.8182 - val_loss: 0.5953 - val_auc: 0.7542</span><br><span class="line">Epoch 6/100</span><br><span class="line">5025/5025 [==============================] - 22s 4ms/step - loss: 0.5019 - auc: 0.8278 - val_loss: 0.6032 - val_auc: 0.7533</span><br><span class="line">Epoch 7/100</span><br><span class="line">5025/5025 [==============================] - 22s 4ms/step - loss: 0.4994 - auc: 0.8297 - val_loss: 0.6038 - val_auc: 0.7531</span><br><span class="line">Epoch 8/100</span><br><span class="line">5025/5025 [==============================] - 22s 4ms/step - loss: 0.4989 - auc: 0.8301 - val_loss: 0.6039 - val_auc: 0.7530</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate(test_dataset, test_y)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2080/2080 [==============================] - 4s 2ms/step - loss: 0.6173 - auc: 0.7282</span><br><span class="line">[0.6173332929611206, 0.7281948924064636]</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这里稍微比较一下和之前YouTubeNet的结果:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>验证集</th>
<th>测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td>YouTubeNet</td>
<td>0.7475</td>
<td>0.7276</td>
</tr>
<tr>
<td>DSSM</td>
<td>0.7567</td>
<td>0.7282</td>
</tr>
</tbody>
</table>
</div>
<p>差别并不大, 原因应该是在这份数据集中, 并没有包含太多物品侧的信息.</p>
<p>以上就是DSSM在推荐系统中召回的全部内容了, DSSM的原理并不复杂, 但是却特别实用, 既能够将用户侧以及物品侧的信息加入训练, 同时在训练完成后通过Embedding的最近邻检索, 也能够快速实现召回.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>DSSM</tag>
      </tags>
  </entry>
  <entry>
    <title>YouTubeNet召回</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/YouTubeNet%E5%8F%AC%E5%9B%9E/</url>
    <content><![CDATA[<p>这一篇中, 将会介绍著名的YouTubeNet, 着重于其召回部分, 并进行简单地代码复现.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对原文Deep Neural Networks for YouTube Recommendations感兴趣的同学, 可以点<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" target="_blank" rel="noopener">这里</a>.</p>
<p>这篇论文大概讲了个啥事呢, 就是说对于YouTube这家很大的视频分享网站来说, 能够快速地, 精准地推荐给用户喜欢的视频, 以提高用户的观看时长, 是有效提高其商业利润的关键. 据此, YouTube整体采用了召回-排序的系统架构, 如下图:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>第一级用候选集生成模型(Candidate Generation Model)完成对海量候选视频的快速筛选; 第二级采用排序模型(Ranking Model), 完成对几百个候选视频的精排.</p>
<p>本文重点介绍YouTubeNet的召回部分, 因为我认为其具有一定的通用性与代表性.</p>
<p>下文中的YouTubeNet都是指的召回部分的模型.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>回想前面介绍的一些召回方法, 如协同过滤一类的模型, 对于这类模型来说, 一个很明显的特征就是只使用到了用户与物品的ID信息.</p>
<p>但是实际上, 还有一些其它特征是可以拿来使用的, 理论上更多的信息一般可以带来更好的, 至少是不一样的效果, 所以是有必要进行尝试的.</p>
<p>而YouTubeNet正是很好地在模型中, 加入了更多的信息, 同时也维持了工程上的实用性, 下面来对YouTubeNet进行具体的讲解.</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>模型结构如上图, 我们从下往上来说.</p>
<p>最下面的是特征输入层, 图中包含了如下一些特征:</p>
<ul>
<li>用户历史观看视频Embedding.</li>
<li>搜索词Embedding.</li>
<li>地理属性特征Embedding.</li>
<li>样本年龄.</li>
<li>性别.</li>
</ul>
<p>其中前面的一些Embedding, 可以是一些通过其他方法(如item2vec)预训练好的向量. 样本年龄, 指的应该是目标推荐视频的上传时间, 距离推荐预测时的时间间隔, 其目的是说, 一个视频通常在刚上传的一段时间有较高热度, 而后下降, 通过这个特征可以来对这种情况进行刻画. 性别当然就是用户的一些基础特征了.</p>
<p>上面的特征有什么共同特点吗, 是的, 它们都是用户侧的特征. 为什么不加物品侧的特征呢, 这与整体的模型设计架构有关, 接着往上看.</p>
<p>在有了特征以后, 往上就是常见的全连接神经网络MLP了, 没啥好说的.</p>
<p>然后是最上面, 咋一看有些复杂, 先看右边的训练部分.</p>
<p>YouTubeNet在训练时, 学习任务是”predicting the next watched video”, 即一个多分类问题, 通过用户的信息, 预测用户下一次会观看哪个视频.</p>
<p>MLP最后一层ReLU输出的向量, 可以当做是用户的Embedding, 因为前面说了, 底层特征全是关于用户的信息.</p>
<p>而softmax层, 可以看成一个$k\times n$的矩阵(最后一层ReLU输出的向量维度为$k$, 物品数量为$n$), 此时YouTubeNet比较精髓的地方来了, 物品的Embedding, 可以用$k\times n$矩阵中的权重系数来进行表示! 物品向量与用户向量都是$k$维, 当两者内积较大时, 表明用户会观看该视频.</p>
<p>接下来考虑一个问题, 这个模型训练好了以后, 怎么使用呢? 作为一个召回模型, 应该能够快速地在海量视频中, 捞出候选集, 如果是这样一个多分类模型的话, 那么是跑一遍模型, 然后将输出概率较大的一些视频作为候选集吗?</p>
<p>这里就要说到一个效率问题了, 对于预测目标很多(海量视频)的softmax来说, 运算一次是费劲的. 所以在训练时, 可以使用负采样的方法.</p>
<p>负采样, 永远滴神!</p>
<p>而在训练完, 进行召回服务的时候, 是不需要再过一遍模型的. 前面说到了, MLP的最后一层可以当做用户的Embedding, softmax对应的矩阵, 可以当做物品的Embedding. 可以把这些Embedding事先进行存储, 然后要对某个用户进行召回时, 直接拿该用户的Embedding, 去物品的Embedding集中使用某种方法进行最近邻检索, 筛选出TOP-N个物品, 可以非常地快.</p>
<p>如果不去深究某些特别细节的东西, YouTubeNet是朴实无华的, 总结一下其整体流程:</p>
<ul>
<li>训练.<ul>
<li>输出用户侧特征.</li>
<li>经过MLP得到用户Embedding.</li>
<li>经过softmax预测用户是否观看视频.</li>
<li>softmax矩阵表示物品Embedding, 可用负采样训练.</li>
</ul>
</li>
<li>服务.<ul>
<li>储存用户与物品的Embedding.</li>
<li>给定用户Embedding, 使用最近邻算法检索出TOP-N物品.</li>
</ul>
</li>
</ul>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>这里有具体的每个用户给每个电影的打分, 这里为了模拟负采样下二分类的学习任务, 将对根据每个用户整体的打分水平, 将其中打分偏高的作为正样本, 打分偏低的作为负样本.</p>
<p>而对于用户侧的信息, 这里仅选择直接给到的特征:</p>
<ul>
<li>用户ID.</li>
<li>性别.</li>
<li>年龄.</li>
<li>职业.</li>
</ul>
<p>在数据预处理的过程中, 为了保证模型能够有效学习, 把记录过少的用户与电影进行过滤, 同时对于过于活跃的用户以及被观看次数过多的电影, 进行随机下采样.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">data_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        data_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;1&apos;, &apos;1193&apos;, &apos;5&apos;, &apos;978300760&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;661&apos;, &apos;3&apos;, &apos;978302109&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;914&apos;, &apos;3&apos;, &apos;978301968&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;3408&apos;, &apos;4&apos;, &apos;978300275&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;2355&apos;, &apos;5&apos;, &apos;978824291&apos;]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6040, 3706)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的用户观看影片数量分布</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对观看影片多于500的用户进行随机抽样, 避免样本占比过多影响模型学习</span></span><br><span class="line">df = pd.DataFrame(data_list, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">sample_user = [x <span class="keyword">for</span> x <span class="keyword">in</span> c_user <span class="keyword">if</span> c_user[x] &gt; <span class="number">500</span>]</span><br><span class="line"></span><br><span class="line">res_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sample_user:</span><br><span class="line">    tmp_df = df.loc[df[<span class="string">'user_id'</span>] == user].sample(n=<span class="number">500</span>, random_state=<span class="number">7</span>)</span><br><span class="line">    res_df = res_df.append(tmp_df)</span><br><span class="line">    </span><br><span class="line">res_df = res_df.append(df.loc[~df[<span class="string">'user_id'</span>].isin(sample_user)])</span><br><span class="line"></span><br><span class="line">data_list = res_df.values.tolist()</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的影片被观看次数分布</span></span><br><span class="line"></span><br><span class="line">c_item = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_item.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_4.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对被观看多于500的电影进行随机抽样, 避免样本占比过多影响模型学习</span></span><br><span class="line">df = pd.DataFrame(data_list, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line"></span><br><span class="line">sample_item = [x <span class="keyword">for</span> x <span class="keyword">in</span> c_item <span class="keyword">if</span> c_item[x] &gt; <span class="number">500</span>]</span><br><span class="line"></span><br><span class="line">res_df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sample_item:</span><br><span class="line">    tmp_df = df.loc[df[<span class="string">'item_id'</span>] == item].sample(n=<span class="number">500</span>, random_state=<span class="number">7</span>)</span><br><span class="line">    res_df = res_df.append(tmp_df)</span><br><span class="line"></span><br><span class="line">res_df = res_df.append(df.loc[~df[<span class="string">'item_id'</span>].isin(sample_item)])</span><br><span class="line"></span><br><span class="line">data_list = res_df.values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计分析数据集中的影片被观看次数分布</span></span><br><span class="line"></span><br><span class="line">c_item = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_item.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_5.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(data_list), len(data_list) * <span class="number">0.7</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(662366, 463656.19999999995)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加用户其它信息</span></span><br><span class="line"></span><br><span class="line">user_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/users.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        user_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">user_df = pd.DataFrame(user_list, columns=[<span class="string">'user_id'</span>, <span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'occupation'</span>, <span class="string">'zip'</span>])</span><br><span class="line">user_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">gender</th>
<th style="text-align:right">age</th>
<th style="text-align:right">occupation</th>
<th style="text-align:right">zip</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">F</td>
<td style="text-align:right">1</td>
<td style="text-align:right">10</td>
<td style="text-align:right">48067</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">M</td>
<td style="text-align:right">56</td>
<td style="text-align:right">16</td>
<td style="text-align:right">70072</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">15</td>
<td style="text-align:right">55117</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">M</td>
<td style="text-align:right">45</td>
<td style="text-align:right">7</td>
<td style="text-align:right">02460</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">M</td>
<td style="text-align:right">25</td>
<td style="text-align:right">20</td>
<td style="text-align:right">55455</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">user_df = user_df.loc[user_df[<span class="string">'user_id'</span>].isin(list(user_set))]</span><br><span class="line"><span class="keyword">del</span> user_df[<span class="string">'zip'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">for</span> ft <span class="keyword">in</span> [<span class="string">'gender'</span>, <span class="string">'age'</span>, <span class="string">'occupation'</span>]:</span><br><span class="line">    led = LabelEncoder()</span><br><span class="line">    user_df[ft] = led.fit_transform(user_df[ft])</span><br><span class="line">    </span><br><span class="line">user_list = user_df.values</span><br><span class="line">user_dict = dict(zip(user_list[:, <span class="number">0</span>], user_list[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">data_list = [x + list(user_dict[x[<span class="number">0</span>]]) <span class="keyword">for</span> x <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;4959&apos;, &apos;2394&apos;, &apos;4&apos;, &apos;962634004&apos;, 1, 2, 9],</span><br><span class="line"> [&apos;1606&apos;, &apos;2394&apos;, &apos;3&apos;, &apos;974736208&apos;, 0, 3, 14],</span><br><span class="line"> [&apos;2308&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974487994&apos;, 0, 4, 8],</span><br><span class="line"> [&apos;3285&apos;, &apos;2394&apos;, &apos;2&apos;, &apos;968118037&apos;, 1, 2, 15],</span><br><span class="line"> [&apos;2325&apos;, &apos;2394&apos;, &apos;5&apos;, &apos;974435017&apos;, 0, 1, 15]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据每个用户的打分, 把样本分为正/负样本</span></span><br><span class="line">user_score_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> data_list:</span><br><span class="line">    user = list_[<span class="number">0</span>]</span><br><span class="line">    score = int(list_[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_score_dict:</span><br><span class="line">        user_score_dict[user] = [score, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        user_score_dict[user][<span class="number">0</span>] += score</span><br><span class="line">        user_score_dict[user][<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">user_mean_score_dict = dict([(x, user_score_dict[x][<span class="number">0</span>] / user_score_dict[x][<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> user_score_dict])</span><br><span class="line">data_list = [x + [int(int(x[<span class="number">2</span>]) &gt; user_mean_score_dict[x[<span class="number">0</span>]])] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间戳排序, 并按排序划分训练集/测试集</span></span><br><span class="line"></span><br><span class="line">data_list = list(sorted(data_list, key=<span class="keyword">lambda</span> x: x[<span class="number">3</span>]))</span><br><span class="line">train_list = data_list[:<span class="number">463656</span>]</span><br><span class="line">test_list = data_list[<span class="number">463656</span>:]</span><br><span class="line">len(train_list), len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(463656, 198710)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 影片被观看次数少于20的剔除</span></span><br><span class="line">c_item_train = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line"></span><br><span class="line">drop_item_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_item_train <span class="keyword">if</span> c_item_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">456504</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 观看影片次数少于20的用户删除</span></span><br><span class="line">c_user_train = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">drop_user_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_user_train <span class="keyword">if</span> c_user_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的物品剔除</span></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">in</span> train_item_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">194354</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的用户剔除</span></span><br><span class="line">train_user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">in</span> train_user_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">66537</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整理用户, 电影的编号</span></span><br><span class="line">train_user_dict = dict(zip(train_user_set, range(len(train_user_set))))</span><br><span class="line"></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">train_item_dict = dict(zip(train_item_set, range(len(train_item_set))))</span><br><span class="line"></span><br><span class="line">train_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], x[<span class="number">4</span>], x[<span class="number">5</span>], x[<span class="number">6</span>], x[<span class="number">-1</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">test_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], x[<span class="number">4</span>], x[<span class="number">5</span>], x[<span class="number">6</span>], x[<span class="number">-1</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> test_list])</span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">np.random.shuffle(train_dataset)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># user_id, gender, age, occupation, item_id</span></span><br><span class="line">train_dataset, train_y = train_dataset[:, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>]], train_dataset[:, <span class="number">-1</span>]</span><br><span class="line">test_dataset, test_y = test_dataset[:, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>]], test_dataset[:, <span class="number">-1</span>]</span><br><span class="line">train_dataset.max(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([4177,    1,    6,   20, 2716])</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_feature</span><span class="params">(name, num, dim=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回离散特征信息字典.</span></span><br><span class="line"><span class="string">    :param name: 特征名称.</span></span><br><span class="line"><span class="string">    :param num: 特征离散属性数量.</span></span><br><span class="line"><span class="string">    :param dim: Embedding维度.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'name'</span>: name, <span class="string">'num'</span>: num, <span class="string">'dim'</span>: dim, <span class="string">'type'</span>: <span class="string">'sparse'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_feature</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回稠密特征信息字典.</span></span><br><span class="line"><span class="string">    :param name: 特征名称.</span></span><br><span class="line"><span class="string">    :return: 信息字典.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'name'</span>: name, <span class="string">'type'</span>: <span class="string">'dense'</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YouTubeNet</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_info_list)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param feature_info_list: 包含各个特征信息(字典)的列表.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.feature_info_list = feature_info_list</span><br><span class="line">        self.item_dim = feature_info_list[<span class="number">-1</span>][<span class="string">'dim'</span>]</span><br><span class="line">        self.item_num = feature_info_list[<span class="number">-1</span>][<span class="string">'num'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_list = []</span><br><span class="line">        <span class="keyword">for</span> ft_info <span class="keyword">in</span> self.feature_info_list[:<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                dim = ft_info[<span class="string">'dim'</span>]</span><br><span class="line">                num = ft_info[<span class="string">'num'</span>]</span><br><span class="line">                self.embedding_list.append(keras.layers.Embedding(num, dim, input_length=<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                self.embedding_list.append(<span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">'特征类型错误!'</span>)</span><br><span class="line"></span><br><span class="line">        self.relu_layer_0 = keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.relu_layer_1 = keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.relu_layer_2 = keras.layers.Dense(self.item_dim, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.item_embedding = keras.layers.Embedding(self.item_num, self.item_dim)</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, feature)</span></span><br><span class="line">        item_vec = self.item_embedding(inputs[:, <span class="number">-1</span>])</span><br><span class="line">        item_vec = tf.reshape(item_vec, shape=(<span class="number">-1</span>, self.item_dim))</span><br><span class="line">        ft_list = []</span><br><span class="line">        <span class="keyword">for</span> i, ft_info <span class="keyword">in</span> enumerate(self.feature_info_list[:<span class="number">-1</span>]):</span><br><span class="line">            ft = inputs[:, i]</span><br><span class="line">            <span class="keyword">if</span> ft_info[<span class="string">'type'</span>] == <span class="string">'sparse'</span>:</span><br><span class="line">                ft = self.embedding_list[i](ft)</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, self.feature_info_list[i][<span class="string">'dim'</span>]))</span><br><span class="line">                ft_list.append(ft)</span><br><span class="line">            <span class="keyword">elif</span> ft_info[<span class="string">'type'</span>] == <span class="string">'dense'</span>:</span><br><span class="line">                ft = tf.reshape(ft, shape=(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">                ft_list.append(ft)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">'特征类型错误!'</span>)</span><br><span class="line"></span><br><span class="line">        ft_concat = tf.concat(ft_list, axis=<span class="number">-1</span>)</span><br><span class="line">        x = self.relu_layer_0(ft_concat)</span><br><span class="line">        x = self.relu_layer_1(x)</span><br><span class="line">        x = self.relu_layer_2(x)</span><br><span class="line">        output = keras.layers.dot([x, item_vec], axes=<span class="number">1</span>)</span><br><span class="line">        output = tf.sigmoid(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [4177,    1,    6,   20, 2716]</span></span><br><span class="line"><span class="comment"># 这里虽然定义了dense_feature, 但全部是当做sparse_feature来做的</span></span><br><span class="line">feature_info_list = [sparse_feature(<span class="string">'user_id'</span>, <span class="number">4178</span>, <span class="number">32</span>),</span><br><span class="line">                    sparse_feature(<span class="string">'gender'</span>, <span class="number">2</span>, <span class="number">8</span>),</span><br><span class="line">                    sparse_feature(<span class="string">'age'</span>, <span class="number">7</span>, <span class="number">16</span>),</span><br><span class="line">                    sparse_feature(<span class="string">'occupation'</span>, <span class="number">21</span>, <span class="number">16</span>),</span><br><span class="line">                    sparse_feature(<span class="string">'item_id'</span>, <span class="number">2717</span>, <span class="number">32</span>)]</span><br><span class="line"></span><br><span class="line">model = YouTubeNet(feature_info_list)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          train_y,</span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          batch_size=<span class="number">64</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>),</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.6270 - auc: 0.6942 - val_loss: 0.6157 - val_auc: 0.7102</span><br><span class="line">Epoch 2/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.6040 - auc: 0.7251 - val_loss: 0.6111 - val_auc: 0.7183</span><br><span class="line">Epoch 3/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.5892 - auc: 0.7437 - val_loss: 0.6000 - val_auc: 0.7336</span><br><span class="line">Epoch 4/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.5665 - auc: 0.7696 - val_loss: 0.5930 - val_auc: 0.7453</span><br><span class="line">Epoch 5/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.5475 - auc: 0.7885 - val_loss: 0.5928 - val_auc: 0.7486</span><br><span class="line">Epoch 6/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.5327 - auc: 0.8019 - val_loss: 0.5999 - val_auc: 0.7475</span><br><span class="line">Epoch 7/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.5052 - auc: 0.8247 - val_loss: 0.6144 - val_auc: 0.7462</span><br><span class="line">Epoch 8/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.4968 - auc: 0.8312 - val_loss: 0.6192 - val_auc: 0.7456</span><br><span class="line">Epoch 9/100</span><br><span class="line">5025/5025 [==============================] - 19s 4ms/step - loss: 0.4950 - auc: 0.8325 - val_loss: 0.6197 - val_auc: 0.7455</span><br><span class="line">Epoch 10/100</span><br><span class="line">5025/5025 [==============================] - 20s 4ms/step - loss: 0.4947 - auc: 0.8328 - val_loss: 0.6198 - val_auc: 0.7455</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate(test_dataset, test_y)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2080/2080 [==============================] - 4s 2ms/step - loss: 0.6245 - auc: 0.7276</span><br><span class="line">[0.6244723796844482, 0.7276008129119873]</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>上面介绍了YouTubeNet召回部分的模型, 由于本身是工业界提出的算法, 并不是特别复杂, 比较容易实现, 同时对于实际的业务场景也比较友好.</p>
<p>相比协同过滤一类的召回算法, YouTubeNet在保证了仍然可以用训练好的Embedding向量进行最近邻检索, 以维持快速响应外, 在用户侧加入了除ID以外的更多用户信息, 理论上可以获得更好的效果.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>用户画像(二)</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F-%E4%BA%8C/</url>
    <content><![CDATA[<p>这里就使用实际的数据, 利用用户画像来进行召回, 做一个简单的展示.</p>
<a id="more"></a>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><p>这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>为了模拟曝光后是否点击这一行为, 统计每一个用户的平均打分, 将评分在其平均分以下的电影, 设置为0, 即曝光而未点击; 评分在其平均分以上的电影, 设置为1, 即曝光后点击.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">df = pd.DataFrame(df, columns=[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'score'</span>, <span class="string">'time'</span>])</span><br><span class="line">df[<span class="string">'score'</span>] = df[<span class="string">'score'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个用户平均分</span></span><br><span class="line">avg_score = df[[<span class="string">'user_id'</span>, <span class="string">'score'</span>]].groupby(<span class="string">'user_id'</span>).agg(avg_score=(<span class="string">'score'</span>, <span class="string">'mean'</span>))</span><br><span class="line">avg_score.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">avg_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">user_id</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4.188679</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">4.114713</td>
</tr>
<tr>
<td style="text-align:right">100</td>
<td style="text-align:right">3.026316</td>
</tr>
<tr>
<td style="text-align:right">1000</td>
<td style="text-align:right">4.130952</td>
</tr>
<tr>
<td style="text-align:right">1001</td>
<td style="text-align:right">3.652520</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个用户平均分以上的电影, 设置为曝光后点击</span></span><br><span class="line">df = pd.merge(left=df, right=avg_score, on=<span class="string">'user_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'score'</span>] &gt;= df[<span class="string">'avg_score'</span>]</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">score</th>
<th style="text-align:right">time</th>
<th style="text-align:right">avg_score</th>
<th style="text-align:right">label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1193</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978300760</td>
<td style="text-align:right">4.188679</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">661</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978302109</td>
<td style="text-align:right">4.188679</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">914</td>
<td style="text-align:right">3</td>
<td style="text-align:right">978301968</td>
<td style="text-align:right">4.188679</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3408</td>
<td style="text-align:right">4</td>
<td style="text-align:right">978300275</td>
<td style="text-align:right">4.188679</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2355</td>
<td style="text-align:right">5</td>
<td style="text-align:right">978824291</td>
<td style="text-align:right">4.188679</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间分为训练集和测试集</span></span><br><span class="line">df = df.sort_values(<span class="string">'time'</span>)</span><br><span class="line">df_train = df.iloc[:<span class="number">700000</span>]</span><br><span class="line">df_test = df.iloc[<span class="number">700000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中新出现的用户与电影过滤掉</span></span><br><span class="line">user_list = df_train[<span class="string">'user_id'</span>].unique().tolist()</span><br><span class="line">item_list = df_train[<span class="string">'item_id'</span>].unique().tolist()</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'user_id'</span>].isin(user_list)]</span><br><span class="line">df_test = df_test.loc[df_test[<span class="string">'item_id'</span>].isin(item_list)]</span><br><span class="line">df_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(135148, 6)</span><br></pre></td></tr></table></figure>
<h1 id="物品画像构建"><a href="#物品画像构建" class="headerlink" title="物品画像构建"></a>物品画像构建</h1><p>这里由于电影本身包含有分类标签, 且没有更多信息, 简单起见, 就直接使用数据给出的标签作为物品画像.</p>
<p>然后结合电影的曝光点击记录, 以及贝叶斯平滑计算电影的分值, 并按标签制作倒排表.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算每个电影的点击率, 曝光/点击次数</span></span><br><span class="line"></span><br><span class="line">item_ctr = df_train[[<span class="string">'item_id'</span>, <span class="string">'label'</span>]].groupby(<span class="string">'item_id'</span>).agg(item_ctr=(<span class="string">'label'</span>, <span class="string">'mean'</span>), </span><br><span class="line">                                                                 exposure=(<span class="string">'label'</span>, <span class="string">'count'</span>), </span><br><span class="line">                                                                 click=(<span class="string">'label'</span>, <span class="string">'sum'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看曝光次数分布</span></span><br><span class="line">item_ctr.loc[item_ctr[<span class="string">'exposure'</span>] &lt; <span class="number">500</span>, <span class="string">'exposure'</span>].hist(bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看点击率分布</span></span><br><span class="line">item_ctr[<span class="string">'item_ctr'</span>].hist(bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_1.png" alt="fig"></p>
<p>从图上可以看到, 有一些曝光次数比较少的电影, 其点击率会比较极端地倾向于0或者1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 贝叶斯平滑, 挑选出记录大于50条的电影的点击率, 用来估计贝塔分布参数</span></span><br><span class="line">avg_ctr = item_ctr.loc[item_ctr[<span class="string">'exposure'</span>] &gt; <span class="number">50</span>, <span class="string">'item_ctr'</span>].mean()</span><br><span class="line">std_ctr = item_ctr.loc[item_ctr[<span class="string">'exposure'</span>] &gt; <span class="number">50</span>, <span class="string">'item_ctr'</span>].std()</span><br><span class="line">print(avg_ctr, std_ctr)</span><br><span class="line">var_ctr = std_ctr**<span class="number">2</span></span><br><span class="line">alpha = avg_ctr * (avg_ctr * (<span class="number">1</span> - avg_ctr) / var_ctr - <span class="number">1</span>)</span><br><span class="line">beta = (<span class="number">1</span> - avg_ctr) * (avg_ctr * (<span class="number">1</span> - avg_ctr) / var_ctr - <span class="number">1</span>)</span><br><span class="line">print(alpha, beta)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.474422332230172 0.1960938288262356</span><br><span class="line">2.6019528481413885 2.88251251399721</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算每个电影经过贝叶斯平滑后的点击率</span></span><br><span class="line">item_ctr[<span class="string">'bayes_ctr'</span>] = item_ctr.apply(<span class="keyword">lambda</span> x: (x[<span class="string">'click'</span>] + alpha) / (x[<span class="string">'exposure'</span>] + alpha + beta), axis=<span class="number">1</span>)</span><br><span class="line">item_ctr[<span class="string">'bayes_ctr'</span>].hist(bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_train = pd.merge(left=df_train, right=item_ctr, on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line">df_train[[<span class="string">'user_id'</span>, <span class="string">'item_id'</span>, <span class="string">'bayes_ctr'</span>]].head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">user_id</th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">bayes_ctr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1980</td>
<td style="text-align:right">2883</td>
<td style="text-align:right">0.523378</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">4593</td>
<td style="text-align:right">924</td>
<td style="text-align:right">0.715299</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">2841</td>
<td style="text-align:right">3863</td>
<td style="text-align:right">0.414818</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">2841</td>
<td style="text-align:right">3456</td>
<td style="text-align:right">0.665445</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">2841</td>
<td style="text-align:right">3298</td>
<td style="text-align:right">0.451444</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加电影信息</span></span><br><span class="line"></span><br><span class="line">item_df = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/movies.dat'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        item_df.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">item_df = pd.DataFrame(item_df, columns=[<span class="string">'item_id'</span>, <span class="string">'title'</span>, <span class="string">'category'</span>])</span><br><span class="line">item_df[<span class="string">'category_list'</span>] = item_df[<span class="string">'category'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">'|'</span>))</span><br><span class="line">item_df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">item_id</th>
<th style="text-align:right">title</th>
<th style="text-align:right">category</th>
<th style="text-align:right">category_list</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">Toy Story (1995)</td>
<td style="text-align:right">Animation\</td>
<td style="text-align:right">Children’s\</td>
<td>Comedy</td>
<td>[Animation, Children’s, Comedy]</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">Jumanji (1995)</td>
<td style="text-align:right">Adventure\</td>
<td style="text-align:right">Children’s\</td>
<td>Fantasy</td>
<td>[Adventure, Children’s, Fantasy]</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Grumpier Old Men (1995)</td>
<td style="text-align:right">Comedy\</td>
<td style="text-align:right">Romance</td>
<td>[Comedy, Romance]</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">Waiting to Exhale (1995)</td>
<td style="text-align:right">Comedy\</td>
<td style="text-align:right">Drama</td>
<td>[Comedy, Drama]</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">Father of the Bride Part II (1995)</td>
<td style="text-align:right">Comedy</td>
<td style="text-align:right">[Comedy]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 制作倒排表</span></span><br><span class="line"></span><br><span class="line">item_df = item_df.loc[item_df[<span class="string">'item_id'</span>].isin(item_list)]</span><br><span class="line">tag_item_dict = &#123;&#125;</span><br><span class="line">tag_set = set([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_df[<span class="string">'category_list'</span>]:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">        tag_set.add(j)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tag_set:</span><br><span class="line">    tag_item_dict[i] = []</span><br><span class="line">item_bayes_dict = dict(zip(item_ctr.index.tolist(), item_ctr[<span class="string">'bayes_ctr'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_df.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    category_list = tmp[<span class="string">'category_list'</span>]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        tag_item_dict[j].append((tmp[<span class="string">'item_id'</span>], item_bayes_dict[tmp[<span class="string">'item_id'</span>]]))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tag_item_dict:</span><br><span class="line">    tag_item_dict[i] = list(sorted(tag_item_dict[i], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tag_item_dict</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;Comedy&apos;: [(&apos;745&apos;, 0.8938807903284889),</span><br><span class="line">  (&apos;1148&apos;, 0.8879161929505215),</span><br><span class="line">  (&apos;1223&apos;, 0.8476214987993209),</span><br><span class="line">  (&apos;3022&apos;, 0.82820970470273),</span><br><span class="line">  (&apos;3307&apos;, 0.8196317140831266),</span><br><span class="line">  </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h1 id="用户画像构建"><a href="#用户画像构建" class="headerlink" title="用户画像构建"></a>用户画像构建</h1><p>接下来构建用户画像, 这里做法比较简单, 主要通过用户与电影的交互, 以及电影本身的物品画像, 来构建用户画像.</p>
<p>在计算用户画像的偏好分值时, 考虑两部分, 一部分是基准值, 即某个标签整体的点击率, 一部分是用户的累计值, 使用如下形式进行计算:</p>
<script type="math/tex; mode=display">
score=\frac{\alpha\times 总体点击数+(1-\alpha)\times 用户点击数}{\alpha\times 总体曝光数+(1-\alpha)\times 用户曝光数}</script><p>这里的$\alpha$是拍脑袋决定的, 考虑到整体用户量约6000, 因此取$\alpha=1/5000$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 遍历数据进行统计, 包括各标签整体的曝光/点击, 以及每个用户的曝光点击</span></span><br><span class="line">df_train = pd.merge(left=df_train, right=item_df[[<span class="string">'item_id'</span>, <span class="string">'category_list'</span>]], on=<span class="string">'item_id'</span>, how=<span class="string">'left'</span>)</span><br><span class="line"></span><br><span class="line">tag_list = list(tag_set)</span><br><span class="line"></span><br><span class="line">all_info_dict = &#123;&#125;</span><br><span class="line">user_info_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_train.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    category_list = tmp[<span class="string">'category_list'</span>]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> all_info_dict:</span><br><span class="line">            all_info_dict[j] = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        all_info_dict[j][<span class="number">0</span>] += tmp[<span class="string">'label'</span>]</span><br><span class="line">        all_info_dict[j][<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    user_id = tmp[<span class="string">'user_id'</span>]</span><br><span class="line">    <span class="keyword">if</span> user_id <span class="keyword">not</span> <span class="keyword">in</span> user_info_dict:</span><br><span class="line">        user_info_dict[user_id] =  &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> category_list:</span><br><span class="line">        <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> user_info_dict[user_id]:</span><br><span class="line">            user_info_dict[user_id][j] = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        user_info_dict[user_id][j][<span class="number">0</span>] += tmp[<span class="string">'label'</span>]</span><br><span class="line">        user_info_dict[user_id][j][<span class="number">1</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算每个用户各个标签的分值, 并排序</span></span><br><span class="line">user_portrait = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_info_dict:</span><br><span class="line">    tmp_list = []</span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> user_info_dict[user]:</span><br><span class="line">        score = (<span class="number">0.0002</span> * all_info_dict[tag][<span class="number">0</span>] + <span class="number">0.9998</span> * user_info_dict[user][tag][<span class="number">0</span>]) / (<span class="number">0.0002</span> * all_info_dict[tag][<span class="number">1</span>] + <span class="number">0.9998</span> * user_info_dict[user][tag][<span class="number">1</span>])</span><br><span class="line">        tmp_list.append((tag, score))</span><br><span class="line">    tmp_list = list(sorted(tmp_list, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">    user_portrait[user] = tmp_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_portrait</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;1980&apos;: [(&apos;Documentary&apos;, 0.91404251131667),</span><br><span class="line">  (&apos;Film-Noir&apos;, 0.7423283533126479),</span><br><span class="line">  (&apos;Drama&apos;, 0.7381769567469151),</span><br><span class="line">  (&apos;Romance&apos;, 0.7117057320809761),</span><br><span class="line">  (&apos;War&apos;, 0.6951509552273142),</span><br><span class="line">  </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h1 id="用户画像召回"><a href="#用户画像召回" class="headerlink" title="用户画像召回"></a>用户画像召回</h1><p>现在有了物品画像, 也有了用户画像, 现在利用它们来进行召回.</p>
<p>这里使用比较简单的方法, 即对用户各标签分值进行归一化处理, 计算得到每个标签下选取物品的数量, 最后选取头部的物品作为返回.</p>
<p>比如每个用户召回100个物品, 用户某个标签归一化后, 分值为$0.2$, 那么就去对应标签下选中头部20个物品.</p>
<p>这里可能更好的做法, 是将倒排表中物品分值等价于选中概率, 即分值越大, 有更大的可能被选中.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 标签分值归一化</span></span><br><span class="line">user_portrait_norm = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_portrait:</span><br><span class="line">    user_portrait_norm[user] = &#123;&#125;</span><br><span class="line">    sum_ = sum([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_portrait[user]])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> user_portrait[user]:</span><br><span class="line">        user_portrait_norm[user][i[<span class="number">0</span>]] = i[<span class="number">1</span>] / sum_</span><br><span class="line">user_portrait_norm[<span class="string">'1980'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;Documentary&apos;: 0.08392623657346782,</span><br><span class="line"> &apos;Film-Noir&apos;: 0.06815965802899726,</span><br><span class="line"> &apos;Drama&apos;: 0.06777848200493675,</span><br><span class="line"> &apos;Romance&apos;: 0.06534792736858534,</span><br><span class="line"> &apos;War&apos;: 0.0638278885285537,</span><br><span class="line"> &apos;Musical&apos;: 0.061805677488681765,</span><br><span class="line"> &apos;Western&apos;: 0.06155638732308014,</span><br><span class="line"> </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户统计已经曝光过的电影</span></span><br><span class="line">user_items_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_train.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    user = tmp[<span class="string">'user_id'</span>]</span><br><span class="line">    item = tmp[<span class="string">'item_id'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict:</span><br><span class="line">        user_items_dict[user] = set([])</span><br><span class="line">    user_items_dict[user].add(item)</span><br><span class="line">user_items_dict[<span class="string">'1980'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;1006&apos;,</span><br><span class="line"> &apos;101&apos;,</span><br><span class="line"> &apos;1041&apos;,</span><br><span class="line"> </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户召回200部电影, 排除已曝光的</span></span><br><span class="line">match_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_portrait_norm:</span><br><span class="line">    match_dict[user] = []</span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> user_portrait_norm[user]:</span><br><span class="line">        num_ = int(user_portrait_norm[user][tag] * <span class="number">200</span>)</span><br><span class="line">        tmp_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> tag_item_dict[tag] <span class="keyword">if</span> x[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> user_items_dict[user]]</span><br><span class="line">        match_dict[user] += tmp_list[: num_]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在测试集上检验评估指标, 查准率, 查全率, F1</span></span><br><span class="line">test_user_click = df_test.loc[df_test[<span class="string">'label'</span>] == <span class="number">1</span>]</span><br><span class="line">test_users_item_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> test_user_click.iterrows():</span><br><span class="line">    tmp = i[<span class="number">1</span>]</span><br><span class="line">    user = tmp[<span class="string">'user_id'</span>]</span><br><span class="line">    item = tmp[<span class="string">'item_id'</span>]</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> test_users_item_dict:</span><br><span class="line">        test_users_item_dict[user] = set([])</span><br><span class="line">    test_users_item_dict[user].add(item)</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> match_dict:</span><br><span class="line">    match_dict[user] = set(match_dict[user])</span><br><span class="line">    </span><br><span class="line">true_pos = <span class="number">0</span></span><br><span class="line">total_num = <span class="number">0</span></span><br><span class="line">match_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> test_users_item_dict:</span><br><span class="line">    tmp_set_0 = test_users_item_dict[user]</span><br><span class="line">    tmp_set_1 = match_dict[user]</span><br><span class="line">    total_num += len(tmp_set_0)</span><br><span class="line">    match_num += len(tmp_set_1)</span><br><span class="line">    true_pos += len(tmp_set_0.intersection(tmp_set_1))</span><br><span class="line">precision = true_pos / match_num</span><br><span class="line">recall = true_pos / total_num</span><br><span class="line">f1 = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">print(<span class="string">'查准率: %.4f'</span> % precision)</span><br><span class="line">print(<span class="string">'查全率: %.4f'</span> % recall)</span><br><span class="line">print(<span class="string">'F1: %.4f'</span> % f1)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查准率: 0.0845</span><br><span class="line">查全率: 0.1776</span><br><span class="line">F1: 0.1145</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 通过实际的数据与代码, 实现了用户画像与物品画像的构建, 然后使用它们来进行召回.</p>
<p>上面的做法是比较简单的, 如果考虑到推荐的精准性, 可以对用户标签分值设计更加复杂的计算方法, 如果考虑推荐的多样性, 可以在对每个标签下物品进行选取时, 按概率进行选取, 而不是简单地只选取靠前的物品.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title>用户画像(一)</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F-%E4%B8%80/</url>
    <content><![CDATA[<p>上一篇介绍了推荐系统中的物品画像, 这一篇来说一说用户画像.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>相比物品画像, 用户画像具有更广的普适性. 因为可能物品画像多数时候只能用在推荐系统等目的比较明确的场景, 而且还得现有物品对吧. 而用户画像, 理论上在任何涉及到与用户有交互的场景下, 都可以构建, 并且可以同时具备多种作用.</p>
<p>比如现在有了一些用户的画像, 那么可以用来做哪些事情呢? 可以结合前面所讲的物品画像, 来进行推荐相关的工作; 可以通过数据分析, 来对用户进行分群, 进行精准营销; 可以结合用户画像, 来指导产品的迭代更新, 使其具有更好的体验…</p>
<p>所以, 用户画像是非常重要的, 值得用一个专门的团队来构建与维护.</p>
<p>用户画像与一些数学, 统计之类的理论不同, 里面很多东西并没有一个非常明确的限制, 比如一定要怎么怎么设计, 一定要如何使用等, 有时候就是”拍脑袋”做出来的, 然后再根据后续的一些业务表现来进行调整.</p>
<p>同时用户画像本身涉及到的面特别广, 写一本书也是可以的(市面上确实不少关于用户画像的书籍). 所以这里只针对推荐系统中的用户画像, 根据自己的学习和理解, 进行一定的总结.</p>
<h1 id="用户画像的构建"><a href="#用户画像的构建" class="headerlink" title="用户画像的构建"></a>用户画像的构建</h1><p>在推荐系统中, 由于用户画像是需要与物品画像”打配合”的, 所以用户画像的构建, 可以基于物品画像来进行. 比如现在已经设计并构建好了物品画像, 即每个物品有了自己的一些分类或者标签, 那么就可以通过过往的日志数据, 利用用户与物品的交互信息, 来构建用户画像.</p>
<p>具体怎么做呢? 其实方法并不唯一, 根据特定的场景, 特定的数据, 来选择特定的做法. 下面就介绍一些可能会用在构建用户画像中的方法.</p>
<h2 id="计算标签分值"><a href="#计算标签分值" class="headerlink" title="计算标签分值"></a>计算标签分值</h2><p>基于物品画像的标签来构建用户画像, 那么对于每个用户来说, 就可以通过一些方式来计算每个标签的分值, 以分值大小来表示用户的喜爱偏好.</p>
<p>最简单的方法, 把某个用户的日志记录进行统计, 比如对某个标签下的物品, 计算该用户对该标签的点击率(点击数/曝光数), 在得到一系列标签的点击率后, 通过点击率的高低, 就可以反映出该用户的喜爱和偏好. 如果认为直接统计点击率太粗暴, 因为某些标签的物品点击率高, 可能是由喜好之外的一些因素导致的, 那么还可以尝试使用TF-IDF. 即把一个用户看成一段文本, 其点击的物品的标签看成文本中的单词, 那么通过TF-IDF得到的用户向量, 可以凸显出用户相比于平均水平, 更喜爱哪些标签的物品.</p>
<p>相对复杂的方法, 是考虑多种因素, 然后把各种因素的影响累加起来, 形成一个最终的对画像的刻画. 首先, 可以统计整个场景下各个标签对应的点击率, 根据业务理解, 给予每个标签一个基准值, 类似于先验信息, 将这个基准值赋予每一个用户. 然后, 可以如上面提到的, 根据用户历史的行为, 比如点击, 收藏, 评价等, 得到一个累计的值, 加到基准值上面.</p>
<p>上面做了以后, 由于是随着用户行为进行一点点累计的, 比如某个用户在某个标签下的点击率为$0.1=10/100$, 而近期如果对曝光的多个物品(10次)进行点击(5次), 那么累计点击率变为$0.14$, 绝对值的变化为$0.04$, 好像没那么大, 如果该用户其它标签的分值较高, 那么这个改变其实难以改变该用户整体的偏好变化. 因此, 可以进一步加入能够反映变化率的量, 比如之前的点击率为$0.1$, 近期点击率$0.5$, 点击率翻了5倍, 将$0.5/0.1$赋予一个权重系数, 加到前面的分值上, 可以更灵敏地反映出用户近期喜好的变化.</p>
<p>此外, 一般在推荐场景下, 都会设置一些负反馈项, 比如”不喜欢”, “踩”等, 当用户给出了负反馈以后, 是比点击, 收藏这样的正反馈更加直接的信号. 所以同样也设置一个权重系数, 将负反馈信息也考虑进对某个标签喜好的分值里面.</p>
<p>总结一下上面计算用户画像的方法, 大致如下:</p>
<script type="math/tex; mode=display">
用户某个标签分值=先验基准值+行为累计值+变化率值+负反馈</script><p>当然这仅仅是做了一个举例, 实际考虑哪些方面, 设置多大的权重系数, 可以会比较考验工程师的经验了.</p>
<h2 id="短-中-长期用户画像"><a href="#短-中-长期用户画像" class="headerlink" title="短/中/长期用户画像"></a>短/中/长期用户画像</h2><p>要知道, 对于用户而言, 不同时期的需求, 爱好是可能会发生变化的, 可能是随季节周期性的, 也可能是随年龄增长而变化的, 那么如果使用单一的一个画像来对用户进行刻画, 是难以同时把握用户整体的持久的偏好, 以及近期临时的短暂的偏好的.</p>
<p>因此, 可以将用户画像按时间跨度来分别进行刻画, 比如可以分成用户的短期画像, 中期画像, 以及长期画像. 比如, 可以拿用户进一天或者半天的日志数据, 来计算短期用户画像; 可以拿用户近一周或者几周的日志数据, 来计算中期用户画像; 可以拿近几个月的日志数据, 来计算长期用户画像.</p>
<p>其中的长/中期用户画像, 可以反映用户一个比较稳定的偏好, 而短期用户画像, 可以反映用户一些短暂的临时偏好, 把它们结合起来使用, 可以达到更好的推荐效果和用户体验.</p>
<p>对于短期用户画像, 在计算时, 由于涉及到的数据量相对较少, 可以直接使用近期日志计算, 然后存入Redis这样的数据库. 对于长/中期画像, 可以采用线下定期更新的方式, 同时在更新时, 也不用每次都全量更新, 可以采用增加更新的方式. 比如之前三个月的累计点击率为$0.5=50/100$, 再算上这个月的, 可以将这个月的点击/曝光次数, 累加到之前的点击/曝光次数上$0.5=(50+10)/(100+20)$, 而不用重新重复统计. 当然, 这也与具体的标签分值计算方式的设计相关, 设计得好, 既可以使得画像刻画更加准确, 还可以使得计算更加快捷.</p>
<h1 id="用户画像的使用"><a href="#用户画像的使用" class="headerlink" title="用户画像的使用"></a>用户画像的使用</h1><p>在上一篇介绍物品画像的时候, 就有说到过如何结合物品画像与用户画像来进行推荐.</p>
<p>假设这时候已经有了物品画像, 每个物品有若干标签, 以及其倒排表, 每个标签下若干物品, 和其对应的分值; 同时还有了基于物品画像的用户画像, 没有用户有若干标签, 和每个标签对应的分值.</p>
<p>在召回中, 对某个用户来说, 就可以按标签的分值排序, 取靠前的一些标签, 到对应标签的物品集合下, 再取一些靠前的物品来进行推荐. 或者可以更细致一些, 用户标签的分值表示用户的偏好程度, 那么可以用其分值来调整召回占比. 比如每个用户要选取100个物品, 某用户的标签与分值(归一化)为: 娱乐0.5, 游戏0.3, 音乐0.2, 可以对娱乐, 游戏, 音乐按照5:3:2的方式, 即50个娱乐相关的物品, 30个游戏相关的物品, 20个音乐相关的物品.</p>
<p>在排序中, 也可以用到用户画像来作为模型的特征, 同样也可以去用户排名靠前的一些标签来作为特征, 比如是否喜欢游戏, 是否喜欢音乐这样的布尔型特征. 或者可以直接拿用户所有标签的分值来作为特征, 可能效果会更好一些.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 就是对推荐系统中用户画像的一个简要介绍了, 主要讲了如何计算用户画像, 以及用户画像在推荐系统中的使用方式.</p>
<p>当然了, 本宝水平有限, 文中其中有不少没有涉及到的方面, 可能还有一些不对的地方. 希望观看的同学能从中受益, 如觉得有疑惑或者自己的想法, 可以自行思考实践, 查阅其它一些资料♪(^∇^*)</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title>物品画像</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%89%A9%E5%93%81%E7%94%BB%E5%83%8F/</url>
    <content><![CDATA[<p>一般来说, 对于面向用户的产品(如某APP), 那么用户画像的构建与使用是非常重要的. 而在推荐系统中, 除了用户画像以外, 还需要有物品画像.</p>
<p>下面就根据我的学习与理解, 来对推荐系统中的物品画像进行一定的介绍.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个推荐系统, 无论是使用规则策略, 模型来实现, 最核心的功能, 就是建立起用户与物品之间的联系, 即找到合适的物品来推荐给用户, 以增加点击率, 停留时长, 转化率等业务指标.</p>
<p>其中一个一般可以用在召回部分的方法, 就是使用物品画像与用户画像, 来进行召回或者推荐.</p>
<p>举个栗子, 比如在文章推荐的场景中, 现在已经有了物品画像, 对每个物品来说, 是一些定制好的标签(娱乐, 体育, 人文等); 同时也有了用户画像, 对每个用户来讲也伴随着一些类似的标签. 那么一种简单直观的做法, 就是那用户画像下的标签与物品画像进行匹配, 比如某用户有”游戏”的标签, 那么在文章中游戏类别下的一些文章推荐给用户即可.</p>
<p>关于用户画像的详细内容, 将会在下一篇中讲述, 本篇着重介绍推荐系统的物品画像的部分知识.</p>
<h1 id="内容画像的构建"><a href="#内容画像的构建" class="headerlink" title="内容画像的构建"></a>内容画像的构建</h1><p>其实, 所谓的画像, 广义来说, 就是关于某个物品的描述, 这个描述可以是方方面面的, 可以是各种形式的.</p>
<p>而这里主要针对的, 是推荐系统中的物品画像, 那么这个画像最终是要给计算机”看”的, 是需要能够和用户画像”打配合”的, 所以一般并不需要全面地刻画物品, 而是根据业务场景, 来选择性地进行刻画.</p>
<p>不过虽然是给计算机”看”的, 但仍然要具备解释性. 具体来说, 比如通过一些算法(如MLP), 对物品进行Embedding, 得到的向量当然包含物品的一些信息, 但是不算是内容画像. 在召回中, 可以分为显式召回和隐式召回, 上面提到的Embedding就算是隐式召回, 而基于物品画像和用户画像的召回, 就是显示召回.</p>
<p>那么显示召回, 或者说具备可解释性, 有什么好处呢? 我认为有两点, 一是可控性, 比如可以人为调整标签权重, 来达到调整各类型物品召回占比的目的; 二是方便查错, 比如当发现有时候出现一些不太好的推荐时, 可以用个画像进行溯源, 看是否是一些画像的刻画出现问题, 或者没有匹配好.</p>
<p>接着说内容画像的构建, 是要根据具体的业务场景的, 比如在一个文章推荐的场景, 一个重要的内容画像就是文章的分类, 可以通过一些对业务理解较深的人员, 来定制分类的类别, 同时还可以设置多级分类, 比如一级分类, 二级分类等.</p>
<p>一般来说, 物品的分类, 是相对比较粗的, 如果想更细化一些, 可以设置标签, 或者在文章场景下叫做关键词. 比如某某大的事件, 或者某某名人, 都可以作为标签.</p>
<p>那么如何获取一个物品的分类或者标签呢? 可以用内部工作人员来打标签, 但这样的做法会耗费大量人力, 可能不是太好. 仍然以文章推荐来说, 完全可以借助NLP的技术, 来构建文章的分类, 或者抽取关键词标签, 这部分的相关内容在我的NLP板块有讲解. 不过完全交由算法, 可能不能覆盖所有情况, 比如一些分类有可能不是很准, 比如有些不合法(涉黄涉政)的文章掺杂其中, 这时候采取算法结合人工, 也许是一种两全的做法.</p>
<p>然鹅, 在一些场景下, 不太容易直接根据算法来获取准确而丰富的内容画像怎么办呢? 根据我个人的理解, 一般推荐系统中的物品, 不少时候是由个人上传的, 比如电商中的商品, 短视频中的视频等. 那么, 就可以事先设置一些类别, 让他们在上传物品时, 选择物品对应的类别, 并给出一些类似标签, 关键词的信息. 并对上传的人员说明, 物品信息越准确, 将可能会有更多的曝光机会, 那么这样一来, 就算是有了天然的物品画像了.</p>
<h1 id="内容画像的使用"><a href="#内容画像的使用" class="headerlink" title="内容画像的使用"></a>内容画像的使用</h1><p>上面一节中, 大致阐述了物品画像的产生, 这一节来进一步说明如何在推荐系统中使用物品画像.</p>
<p>假设在文章推荐的场景中, 现在给定一个用户, 并且这个用户已经有了比较简单的用户画像, 比如是一些兴趣标签(娱乐, 游戏…), 那么就可以拿着这些标签, 去已经构建好物品画像的文章库中, 进行相应的匹配.</p>
<p>具体如何进行匹配呢, 一般来说首先要讲文章, 按分类或者标签来进行倒排. 这里倒排的意思, 就是原本是{文章: 标签一, 标签二, …}的对应关系, 转变为{标签: 文章一, 文章二, …}这样的形式. 有了倒排表以后, 对于某个用户的标签, 到对应的标签列表下抽取一定数量的文章即可.</p>
<p>更进一步, 一个标签下的文章可能是非常多的, 只需要抽取一部分, 怎么抽取呢? 一种方式当然是随机抽取, 在没有更多信息的情况下, 这确实也是一种办法.</p>
<p>而在积累了一些信息以后, 为了使推荐系统获得更好的效果, 肯定是希望将一些高质量的文章拿来进行推荐, 让用户有更好的体验. 那么如何衡量一篇文章的质量呢, 其实是很难的, 但相比随机抽取, 给每个标签下的文章一些分值, 用以排序, 使得排名靠前的有更大概率被选中, 是一件必要的事情.</p>
<p>可以使用点击率, 即点击次数除以曝光次数, 作为文章排名的标准. 但是这个做法也有一些问题, 即当一些文章本身曝光次数很少的时候, 其反应的点击率其实并不真实. 就好比抛硬币, 抛得正面的期望是$0.5$, 但是如果只统计抛10次时的频率, 那么可能会严重偏离$0.5$. 此时需要加入先验信息以加以调整, 比如贝叶斯平滑, 关于具体的原理, 将在下一节中进行讲解.</p>
<h1 id="贝叶斯平滑"><a href="#贝叶斯平滑" class="headerlink" title="贝叶斯平滑"></a>贝叶斯平滑</h1><p>假如一个物品的点击率为$0.5=5/10$, 另一篇文章的点击率为$0.5=50/100$, 都是$0.5$, 哪一篇的可信度高一些呢, 当然是后一篇呀, 因为它对应的样本更多, 更能够反映出用户对其的期望点击率.</p>
<p>所以, 针对曝光次数比较少的物品, 需要对其进行平滑处理, 或者说加入先验信息. 那么, 具体如何进行操作呢?</p>
<p>首先, 对于是否点击这一事件, 可以比较自然的使用伯努利分布来进行描述, 在伯努利分布中, 唯一的参数, 就是其期望$\phi$:</p>
<script type="math/tex; mode=display">
y\sim Bernoulli(\phi)</script><p>对于每个物品, 根据其历史样本记录(曝光后是否点击), 可以写出伯努利分布对应的联合分布, 也可以说是似然分布:</p>
<script type="math/tex; mode=display">
P(Y|\phi)=\phi^m\cdot (1-\phi)^n</script><p>其中$m$表示该物品曝光后被点击的次数, $n$表示该物品被曝光后没有被点击的次数.</p>
<p>如果仅仅通过这个似然分布, 使用MLE来进行求解, 那么:</p>
<script type="math/tex; mode=display">
\phi=\frac{m}{m+n}</script><p>就是我们直接计算的点击率, 前面也说过了, 当$m+n$, 即曝光次数较少时, 是不准确的, 需要加入先验信息.</p>
<p>如果有了解过共轭先验的同学, 应该知道, 对于伯努利分布来说, 其共轭先验分布是贝塔分布:</p>
<script type="math/tex; mode=display">
\phi\sim Beta(\alpha,\beta)</script><p>其对应的期望与方差为:</p>
<script type="math/tex; mode=display">
E(\phi)=\frac{\alpha}{\alpha+\beta} \\[7mm]
D(\phi)=\frac{\alpha\beta}{(\alpha+\beta)^2+(\alpha+\beta+1)}</script><p>是的, 这里贝塔分布是关于伯努利分布参数$\phi$的分布. 借助贝叶斯公式, 在在加入了先验分布后, 现在的后验分布为:</p>
<script type="math/tex; mode=display">
P(\phi|Y)\propto P(Y|\phi)P(\phi) \\[7mm]
Beta(\alpha+m,\beta+n)\propto \phi^m\cdot (1-\phi)^n\cdot Beta(\alpha,\beta)</script><p>这里也体现共轭先验的优势了, 即后验分布与先验分布形式一致, 只是参数发生变化. 那么对于后验分布来说, 其对应的期望为:</p>
<script type="math/tex; mode=display">
E(\phi)=\frac{\alpha+m}{\alpha+m+\beta+n}</script><p>再联系前面说到的, 伯努利分布的期望, 就是参数$\phi$, 所以这里就用后验分布的期望来作为参数$\phi$的估计, 也对应了加入先验信息后, 物品的点击率的估计:</p>
<script type="math/tex; mode=display">
CTR=\frac{\alpha+m}{\alpha+m+\beta+n}</script><p>上式就是贝叶斯平滑的公式了, 现在还剩下一个问题, 如何去估计先验分布的参数$\alpha$和$\beta$呢?</p>
<p>由于在这里贝塔分布是关于参数$\phi$的分布, 所以要估计贝塔分布参数, 就肯定需要拿到一些$\phi$的样本. 而$\phi$不是伯努利分布的参数吗, 又不是是否点击的$y$, 哪来的数据呢? 对于每个物品来说, 对应的$\phi$可以用MLE估计, 也就是每个物品的点击率, 这样多个物品的的点击率, 就可以用来估计贝塔分布的参数了♪(^∇^*)</p>
<p>根据我的理解, 我们使用贝叶斯平滑的目的, 就是为了防止曝光次数少的物品点击率出现过大的偏差. 而在估计贝塔分布的参数时, 是用全量数据呢, 还是有选择地使用数据呢? 我认为是有选择的使用数据进行估计更好, 因为当某个物品曝光次数少时, 其估计的$\phi$就是不准的, 这样的数据多了以后, 贝塔分布就会”自身难保”了, 这样再用它来进行平滑, 可能就会更加不准.</p>
<p>假设现在有了多个物品对应的一系列的点击率, 并计算得到它们的均值为$Avg(CTR)$, 方差为$Var(CTR)$. 那么根据贝塔分布的期望和方差公式, 就可以对其参数进行求解:</p>
<script type="math/tex; mode=display">
\alpha=Avg(CTR)\bigg(\frac{Avg(CTR)(1-Avg(CTR))}{Var(CTR)}-1\bigg) \\[7mm]
\beta=\big(1-Avg(CTR)\big)\bigg(\frac{Avg(CTR)(1-Avg(CTR))}{Var(CTR)}-1\bigg)</script><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 介绍了一些物品画像的构建方式, 以及如何结合用户画像来进行召回. 并对物品画像中, 排序分值(点击率)估计不准的问题, 讲解了一种应对的方式, 即贝叶斯回归.</p>
<p>虽然现在有了很多种方式, 可以用来构建物品的Embedding, 用以表示物品信息, 并且效果可能还会比使用物品画像这种方式更加精准. 但是考虑到画像的可解释性和可控性, 画像仍然是推荐系统中, 必不可少的一部分, 在下一篇中, 将会介绍与物品画像对应的用户画像.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title>NeuralCF</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/NeuralCF/</url>
    <content><![CDATA[<p>NeuralCF, 从名字就可以看得出来, 是将神经网络, 或者说深度学习引入推荐系统的协同过滤.</p>
<p>这是<a href="https://arxiv.org/pdf/1708.05031.pdf" target="_blank" rel="noopener">文章地址</a>, 算法提出地相对比较早(大概2017年), 有一定的创新. 效果并没有多么明显的提升, 在实际的生产环境中使用也存在一些问题, 但是作为一种新方法, 还是有必要学习的.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面的文章中, 介绍了经典的协同过滤方法, 使用用户与物品的交互记录, 通过相似度评估, 来进行推荐. 然后矩阵分解方法, 可以说是经典协同过滤的升级版方法, 在数据稀疏的时候, 可以有更好的表现.</p>
<p>后来有一天, 由于算力, 算法, 数据的发展, 深度学习火了起来, 神经网络理论上强大的”万能”拟合, 人们将其运用到了方方面面. 在推荐系统的协同过滤中, 就出现了NeuralCF.</p>
<p>回想之前的矩阵分解方法, 用户与物品的Embedding交互方式, 为内积, 其实这本身就是一种非常好的交互方式:</p>
<ul>
<li><p>解释性强</p>
<p>内积越大的两个Embedding, 对应地在特征空间中”距离”越近.</p>
</li>
<li><p>操作简单</p>
<p>当训练好了模型以后, 就可以将用户与物品, 和对应的Embedding绑定, 后续需要时直接取用即可, 内积的计算也是非常快的.</p>
</li>
</ul>
<p>神经网络的一大优势, 是复杂的非线性的表示或者拟合能力, 可不可以用神经网络来替换内积的交互方式, 以获得更好的效果呢?</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>NeuralCF本身是提出了一种框架, 框架的核心, 是用神经网络来代替原本的内积交互方式, 或者进行扩展:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 下面是Embedding Layer; 中间是NeuralCF Layer, 这里还没有给出明确的结构, 只是说这里是神经网络; 上面是Output Layer, 与具体学习任务相关.</p>
<p>下面就先介绍NeuralCF的作者在训练时采用的学习任务, 然后再说明具体的神经网络结构.</p>
<h2 id="隐式反馈建模"><a href="#隐式反馈建模" class="headerlink" title="隐式反馈建模"></a>隐式反馈建模</h2><p>想想看, 我们每天会在各种APP, 网站上, 看各种各样的文章, 视频等, 很多时候看完就关了; 或者没看完收藏一下; 或者觉得某一条评论有趣, 跟评一句”确实”; 觉得有意义, 可能会进行点赞, 转发…</p>
<p>但是有多少时候, 会去进行打分呢? 其实现在一些短篇的文章或者视频, 一般是没有打分的, 打分意味着需要思考(我是给4分, 还是给5分呢), 但用户来就是图一乐, 为什么要思考呢. 与之对应的, “喜欢”, “投币”这种bool型的选项, 更加人性化, 但即便如此, 也是经常看了就关了, 经常不会去点赞神马的, 至少我是这样, 有时候是忘了, 有时候是懒得点, 有时候是看点赞都这么多了, 我就不点了吧○( ＾皿＾)っHiahiahia…</p>
<p>所以, 各种隐式行为, 即间接反映对物品喜好(如搜索, 点击等), 是广泛存在的数据; 而显式行为, 即直接反映对物品喜好(如评分, 购买等), 是相对稀少的数据. 基于此, NeuralCF的作者采用的是隐式反馈建模, 即通过一些隐式交互方式的定义, 来制定标签: 标签1表示某用户与某物品直接存在交互, 反之则为0.</p>
<p>进一步, 正样本(标签为1)是通过是否存在隐式交互反馈得到的, 那么负样本(标签为0)呢, 要知道对于一个用户来说, 通常有交互的物品是少数, 没有交互的物品是大多数. 这里采用了负采样的方法, 即从没有交互的物品中, 随机采取一些来作为负样本, 这也是非常经典且有效的做法了.</p>
<h2 id="GMF"><a href="#GMF" class="headerlink" title="GMF"></a>GMF</h2><p>一开始提到了, NeuralCF中间的神经网络层, 可以是各种形式, 而作者给到的其中一种, 就是GMF(Generalized Matrix Factorization), 结构如下图的左边部分:</p>
<p><img src="fig_1.jpg" alt="fig"></p>
<p>需要注意的是, 上图中GMF是作为整体模型的一部分, 但是GMF是可以单独作为一个模型的.</p>
<p>将用户和物品对应的Embedding, 进行元素积, 再通过一层全连接输出层, 本质上就是学习的内积, 就是一个MF, 所以也叫做GMF.</p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p><img src="fig_2.jpg" alt="fig"></p>
<p>上图中的右侧, 为MLP(Multilayer Perceptron), 将用户与物品的Embedding拼接后, 输入多层全连接神经网络, 学习它们之间的非线性关系.</p>
<p>MLP同样可以单独作为一个模型来进行学习.</p>
<p>这里也没啥好说的, 过.</p>
<h2 id="NeuralCF"><a href="#NeuralCF" class="headerlink" title="NeuralCF"></a>NeuralCF</h2><p>好的, 现在把球传给NeuralCF.</p>
<p><img src="fig_3.jpg" alt="fig"></p>
<p>前面的GMF学习到了用户与物品Embedding之间内积的模式, 而MLP学习到了复杂的非线性模式, 在NeuralCF这里, 把两者结合了起来.</p>
<p>怎么结合的呢?</p>
<p>首先对于输入的原始Embedding, GMF和MLP是分开的两套, 这里也容易理解, 因为是两种模式(内积与非线性), 所以分开以后学习起来会更好一些.</p>
<p>然后在经过GMF和MLP后, 将它们的输出向量进行拼接, 再经过一个全连接网络进行输出预测.</p>
<p>从结构上来说, 是比较简单的, 就不多说了, 下面着重对NeuralCF进行一些其它方面的探讨.</p>
<p><strong>探讨一</strong>: 模型具体是如何训练的?</p>
<p>由于这里有两套子模型, 两套Embedding, 所以在进行训练时, 可以先分别训练GMF和MLP, 优化器可以使用Adam, 学得更快.</p>
<p>然后再将学习好的GMF和MLP放在一起进行学习, 这时优化器使用SGD(学习率小一些), 更容易收敛.</p>
<p><strong>探讨二</strong>: 为什么用了MLP来学习用户与物品之间的非线性关系, 还要用到GMF, GMF难道不会拖强大的MLP的后腿吗?</p>
<p>然鹅真实的情况是, GMF的模型效果优于MLP. 其实MLP的强大, 只是理论上的, 在这个问题上的表现, 并不如GMF的内积来得好.</p>
<p>那这能我们带来哪些启发呢? 我认为有两点:</p>
<ul>
<li><p>神经网络的结构.</p>
<p>单说结构化数据, MLP当然是经典的网络结构, 但其实还有很多的结构可以尝试.</p>
<p>除了非线性, 还有一个重点是特征之间的交互方式. 比如MLP是特征之间相加, 那么是否可以添加特征之间相乘, 内积, 外积, 二阶, 三阶等这样的交互呢.</p>
<p>从数学上来理解, 要去拟合一个目标, 最好的情况当然是选择的公式, 和目标正好契合. 但一般对于复杂的模式, 我们事先并不知道怎样的公式是好的, 这时候一个方法就是用各种形式的公式(模型)去尝试, 从中找到一个近似最优解.</p>
</li>
<li><p>浅层与深层的结合.</p>
<p>在这里, 也就是NeuralCF, 通过GMF与MLP的结合, 在模型效果上是有提升的. 如果把GMF看成是浅层的网络, 那么也就是说, 浅层网络得到的信息, 与深层网络得到的信息, 对最终结果都有帮助.</p>
<p>浅层的信息并不能表示全部, 深层的信息不见得比浅层好. 关于这一点, 其实在其它一些模型(如Wide&amp;Deep)中, 体现得更好.</p>
</li>
</ul>
<p><strong>探讨三</strong>: 实用性如何.</p>
<p>在推荐系统中, 协同过滤一般是作为召回模型, 而召回模型的特点, 或者说要求是能够迅速地完成对海量数据的处理, 从中筛选出少量的用户可能感兴趣的物品.</p>
<p>那么一般的协同过滤方法, 或者矩阵分解方法能够完成吗. 对于经典协同过滤方法来说, 计算保存好用户之间或者物品之间的相似度, 然后就可以选择Top-k进行快速推进. 矩阵分解在得到各个用户和物品的Embedding后, 可以在很快的时间内(基于相邻向量快速搜索), 找到与用户Embedding相似的物品.</p>
<p>但是NeuralCF就有问题, 对于一个用户来说, 对于每个物品都需要一起过一次NeuralCF模型, 这个不是说不能做, 但是在召回这里相比其它一些方法, 慢了不少. 在效果没有太明显提升的情况下, 不是那么实用.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>总体来说, NeuralCF是一个有意义的模型, 将神经网络引入协同过滤, 采用将GMF与MLP结合的方式, 使得模型效果得到了提升.</p>
<p>从NeuralCF也可以得到一些启示, 包括MLP并非万能, 我们可以尝试各种不同的模型结构, 采取各种不同的特征交互方式; 浅层网络与深层网络对要学习的目标可能都有帮助, 可以结合起来.</p>
<p>不过NeuralCF作为一个召回模型来说, 也存在一些问题, 使得其实用性并不强.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分解(二)</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 介绍了矩阵分解的原理.</p>
<p>这一篇, 主要用代码来实现矩阵分解算法.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h2><p>这里使用的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>这里只用到了”rating.dat”.</p>
<h2 id="算法概述"><a href="#算法概述" class="headerlink" title="算法概述"></a>算法概述</h2><p>由于这里的数据没有隐式数据, 所以就采用BiasSVD算法.</p>
<p>在这里使用TensorFlow来对算法进行实现, 而实际生产环境中, 面对海量数据时, 可以用Spark来进行实现.</p>
<p>对于BiasSVD来说, 在训练的时候, 一些用户或者一些物品的记录可能过少, 这种情况下可以进行过滤. 当然对于过于活跃的用户或者热门物品, 也可以尝试进行过滤, 来对比前后模型的效果.</p>
<p>同时, 没有出现在训练集中的用户或者物品, 是无法利用BiasSVD进行预测的.</p>
<p>在训练模型的时候, 使用MSE作为评估指标. 然后在进行评估时, 由于这里的评分都是整数, 使用准确率Accuracy, 可以更加直观地评估模型效果.</p>
<p>在经过训练和评估, 得到BiasSVD以后, 就可以在离线的情况下, 将用户与物品的向量计算保存好, 对于每个用户, 使用其向量与各物品向量进行匹配, 返回评分较高的作为召回候选集.</p>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">data_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        data_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;1&apos;, &apos;1193&apos;, &apos;5&apos;, &apos;978300760&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;661&apos;, &apos;3&apos;, &apos;978302109&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;914&apos;, &apos;3&apos;, &apos;978301968&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;3408&apos;, &apos;4&apos;, &apos;978300275&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;2355&apos;, &apos;5&apos;, &apos;978824291&apos;]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 总体的用户, 电影数</span></span><br><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6040, 3706)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间戳排序, 并按排序划分训练集/测试集</span></span><br><span class="line"></span><br><span class="line">data_list = list(sorted(data_list, key=<span class="keyword">lambda</span> x: x[<span class="number">3</span>]))</span><br><span class="line">train_list = data_list[:<span class="number">700000</span>]</span><br><span class="line">test_list = data_list[<span class="number">700000</span>:]</span><br><span class="line">len(train_list), len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(700000, 300209)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 影片被观看次数少于20的剔除</span></span><br><span class="line">c_item_train = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line"></span><br><span class="line">drop_item_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_item_train <span class="keyword">if</span> c_item_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">693867</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 观看影片次数少于20的用户删除</span></span><br><span class="line">c_user_train = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">drop_user_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_user_train <span class="keyword">if</span> c_user_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">618139</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的物品剔除</span></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">in</span> train_item_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的用户剔除</span></span><br><span class="line">train_user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">in</span> train_user_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">111651</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 整理用户, 电影的编号</span></span><br><span class="line">train_user_dict = dict(zip(train_user_set, range(len(train_user_set))))</span><br><span class="line"></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">train_item_dict = dict(zip(train_item_set, range(len(train_item_set))))</span><br><span class="line"></span><br><span class="line">train_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], int(x[<span class="number">2</span>])] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">test_dataset = np.array([[train_user_dict[x[<span class="number">0</span>]], train_item_dict[x[<span class="number">1</span>]], int(x[<span class="number">2</span>])] <span class="keyword">for</span> x <span class="keyword">in</span> test_list])</span><br><span class="line">len(train_user_dict), len(train_item_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(4260, 2827)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataset[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1408,  203,    4],</span><br><span class="line">       [ 858, 1521,    3],</span><br><span class="line">       [2392, 2631,    4],</span><br><span class="line">       [2392, 1146,    4],</span><br><span class="line">       [2392, 2538,    5]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子, 因为后面会进行shuffle操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shuffle数据集</span></span><br><span class="line">np.random.shuffle(train_dataset)</span><br><span class="line"></span><br><span class="line">train_dataset, train_y = train_dataset[:, :<span class="number">2</span>], train_dataset[:, <span class="number">2</span>]</span><br><span class="line">test_dataset, test_y = test_dataset[:, :<span class="number">2</span>], test_dataset[:, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiasSVD</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_user, num_item, embedding_dim, mean)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_user = num_user</span><br><span class="line">        self.num_item = num_item</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.mean = tf.constant(mean, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.user_embedding = keras.layers.Embedding(self.num_user, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                     embeddings_regularizer=keras.regularizers.l2(<span class="number">0.0001</span>))</span><br><span class="line">        self.item_embedding = keras.layers.Embedding(self.num_item, self.embedding_dim, input_length=<span class="number">1</span>,</span><br><span class="line">                                                     embeddings_regularizer=keras.regularizers.l2(<span class="number">0.0001</span>))</span><br><span class="line">        self.user_bias = tf.Variable(tf.zeros(self.num_user))</span><br><span class="line">        self.item_bias = tf.Variable(tf.zeros(self.num_item))</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, 2)</span></span><br><span class="line">        user_ids = inputs[:, <span class="number">0</span>]</span><br><span class="line">        item_ids = inputs[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        user_embedding = tf.reshape(self.user_embedding(user_ids), (<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line">        item_embedding = tf.reshape(self.item_embedding(item_ids), (<span class="number">-1</span>, self.embedding_dim))</span><br><span class="line"></span><br><span class="line">        user_bias = tf.reshape(tf.gather(self.user_bias, user_ids), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">        item_bias = tf.reshape(tf.gather(self.item_bias, item_ids), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        pred = self.mean + keras.layers.dot([user_embedding, item_embedding], [<span class="number">1</span>, <span class="number">1</span>]) + user_bias + item_bias</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练集评分均值</span></span><br><span class="line">mean = np.sum(train_y) / len(train_y)</span><br><span class="line">mean</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3.602078173355831</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = BiasSVD(len(train_user_dict), len(train_item_dict), <span class="number">64</span>, mean)</span><br><span class="line"><span class="comment"># optimizer = keras.optimizers.SGD(learning_rate=1e-4, momentum=0.1)</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          train_y,</span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>),</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/100</span><br><span class="line">15454/15454 [==============================] - 36s 2ms/step - loss: 0.9748 - val_loss: 0.8770</span><br><span class="line">Epoch 2/100</span><br><span class="line">15454/15454 [==============================] - 39s 3ms/step - loss: 0.8512 - val_loss: 0.8437</span><br><span class="line">Epoch 3/100</span><br><span class="line">15454/15454 [==============================] - 36s 2ms/step - loss: 0.8317 - val_loss: 0.8354</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">15454/15454 [==============================] - 34s 2ms/step - loss: 0.8021 - val_loss: 0.8221</span><br><span class="line">Epoch 19/100</span><br><span class="line">15454/15454 [==============================] - 38s 2ms/step - loss: 0.8021 - val_loss: 0.8221</span><br><span class="line">Epoch 20/100</span><br><span class="line">15454/15454 [==============================] - 39s 3ms/step - loss: 0.8021 - val_loss: 0.8221</span><br><span class="line">Epoch 21/100</span><br><span class="line">15454/15454 [==============================] - 34s 2ms/step - loss: 0.8021 - val_loss: 0.8221</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试集MSE</span></span><br><span class="line">model.evaluate(test_dataset, test_y)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3490/3490 [==============================] - 3s 936us/step - loss: 0.9040</span><br><span class="line">0.9040380716323853</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">print(<span class="string">'训练集准确率: %.2f%%'</span> % (<span class="number">100</span> * accuracy_score(train_y, np.round(model.predict(train_dataset), <span class="number">0</span>))))</span><br><span class="line">print(<span class="string">'测试集准确率: %.2f%%'</span> % (<span class="number">100</span> * accuracy_score(test_y, np.round(model.predict(test_dataset), <span class="number">0</span>))))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">训练集准确率: 43.39%</span><br><span class="line">测试集准确率: 41.21%</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分解(一)</title>
    <url>/2020/10/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-%E4%B8%80/</url>
    <content><![CDATA[<p>矩阵分解MF(Matrix Factorization), 广义上来说, 应该就是将一个矩阵, 通过某些方式分解为几个子矩阵.</p>
<p>通过矩阵分解, 可以用来做很多事情, 比如可以是其它算法的一个子过程, 可以用来降维/压缩, 可以用来做推荐.</p>
<p>下面就来介绍矩阵分解相关的理论.</p>
<a id="more"></a>
<h1 id="特征值分解-EVD"><a href="#特征值分解-EVD" class="headerlink" title="特征值分解(EVD)"></a>特征值分解(EVD)</h1><p>首先来回顾一下特征值分解EVD(Eigenvalue Decomposition).</p>
<script type="math/tex; mode=display">
Ax=\lambda x</script><p>如上等式, 其中$A$是一个方阵, 设维度为$n\times n$, $\lambda$称为特征值, $x$称为特征向量. 一般来说, 对矩阵$A$进行特征值分解, 可以得到$n$个特征值$\lambda_1\le\lambda_2\le\dots\le\lambda_n$, 对应$n$个特征向量$\{x_1,x_2,\dots,x_n\}$, 并且这$n$个特征向量是线性无关的, 用矩阵表示为:</p>
<script type="math/tex; mode=display">
AW=W\Sigma \\
A=W\Sigma W^{-1}</script><p>上面的$\Sigma$为对角矩阵, $W$每一列为特征向量.</p>
<p>当$A$是对称矩阵时, 特征向量之间两两正交, 此时$W$为酉矩阵, 满足$W^{-1}=W^T$, 可以得到:</p>
<script type="math/tex; mode=display">
A=W\Sigma W^T</script><p>然鹅, 进行矩阵分解时, 必须要求矩阵为一个方阵, 但是在实际问题中, 多数情况下并非方阵, 此时无法直接使用特征值分解, 就轮到奇异值分解SVD上场了.</p>
<h1 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>奇异值分解SVD(Singular Value Decomposition), 讲的是如果现在有一个矩阵$A$, 其维度为$m\times n$的非方阵, 定义其SVD为:</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T</script><p>其中的$U$为$m\times m$酉矩阵; $\Sigma$为$m\times n$的矩阵, 除了主对角线上的元素以外全为0, 主对角线上的每个元素称为奇异值, $V$是一个$n\times n$的酉矩阵.</p>
<p>那么如何进行SVD, 得到这三个矩阵呢? 先说结论:</p>
<ul>
<li>$U$是$AA^T$进行特征值分解得到的特征向量矩阵.</li>
<li>$V$是$A^TA$进行特征值分解得到的特征向量矩阵.</li>
<li>$\Sigma$中的每个奇异值满足$Av_i=\sigma_iu_i$.</li>
</ul>
<p>下面进行证明:</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T \\
A^T=V\Sigma U^T \\
A^TA=V\Sigma U^TU\Sigma V^T</script><p>由于$U^TU=I$, $\Sigma^T_{m\times n}\Sigma_{m\times n}=\Sigma_{n\times n}^2$, 可得:</p>
<script type="math/tex; mode=display">
A^TA=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T</script><p>这证明了$V$是$A^TA$进行特征值分解得到的特征向量矩阵, 同理可证$U$是$AA^T$进行特征值分解得到的特征向量矩阵.</p>
<p>对于$\Sigma$来说:</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T \\
AV=U\Sigma V^TV=U\Sigma \\
Av_i=\sigma_iu_i</script><p>在得到$U$和$V$后, 即可由此可计算每个奇异值的大小.</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>首先SVD中, 同EVD的特征值类似, 奇异值矩阵也是按从大到小排列的, 并且一般减小较快. 也就是说, 可以用前$k$个奇异值以及对应的左右奇异向量, 来近似等价于原矩阵:</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V^T_{n\times n}\approx U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}</script><p>若$k\ll m$, $k\ll n$, 则可以用三个小矩阵$U_{m\times k},\Sigma_{k\times k},V^T_{k\times n}$来近似地表示原矩阵.</p>
<p>这样做相当于在对原始数据进行压缩与去噪.</p>
<p>然后SVD由于用途较多, 所以在实际计算时, 有高效的库可以进行调用. 其它一些算法在涉及到与SVD相关的问题时, 一般也是直接采用SVD的方法来进行计算.</p>
<p>比如在主成分分析PCA中, 需要对协方差矩阵$X^TX$进行计算, 得到最大的一些特征向量, 来对原始数据进行降维. 而$X^TX$对应的特征向量, 就是SVD中$X$对应的右奇异向量. 所以在PCA计算时, 是可以直接使用SVD的.</p>
<p>再进一步, 如果使用了SVD的右奇异向量, 可以对原始数据的特征进行降维, 那么使用左奇异向量, 会有什么效果呢? 可以对数据的样本进行降维:</p>
<script type="math/tex; mode=display">
X'_{k\times n}=U^T_{k\times m}X_{m\times n}</script><p>很神奇!</p>
<p>此外, SVD也可以使用在推荐系统中, 比如对于一个用户评分二维矩阵, 就可以通过SVD进行矩阵分解, 来预测用户对物品的评分. 但是在这里有个很严重的问题, 即SVD一般要求矩阵是稠密的, 而用户评分矩阵多数时候是稀疏的. 即使对矩阵进行类似均值填充的做法, 其实也不能得到好的效果.</p>
<p>因此, 常规的SVD是难以运用在推荐系统中的, 但是却可以经过改造, 带来质的飞跃, 下面介绍推荐系统中改进的SVD方法.</p>
<h1 id="FunkSVD"><a href="#FunkSVD" class="headerlink" title="FunkSVD"></a>FunkSVD</h1><p>为啥SVD算法的改造版, 叫做Funk SVD呢, 因为这是在2006年, Netflix公司举办的经典的Netflix Prize(电影评分预测比赛)时, 一个叫Simon Funk在博客中公开了这个算法, 所以以此命名.</p>
<p>此外, Funk SVD也被称作LMF(Latent Factor Model), 即隐语义模型.</p>
<h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>原始的SVD, 是结合特征值分解, 将原本的矩阵分解为三个矩阵, 而FunkSVD, 是将原始的矩阵分解为两个矩阵:</p>
<script type="math/tex; mode=display">
A_{m\times n}=P^T_{m\times k}Q_{k\times n}</script><p>那么, 怎么得到这两个矩阵呢?</p>
<p>FunkSVD采用了线性回归拟合的思想, 比如在用户评分矩阵中, 假设每个用户对应一个$k$维向量, 每个物品也对应一个$k$维向量, 矩阵中的评分, 由对应的用户和物品向量的内积得到, 即:</p>
<script type="math/tex; mode=display">
r_{u,i}=p_u^Tq_i</script><p>对应的损失函数, 可以使用MSE:</p>
<script type="math/tex; mode=display">
loss=\frac{1}{m}\sum_{u,i\in\{已有数据\}}(r_{u,i}-p_u^Tq_i)^2</script><p>注意其中的一个重点, 即$u,i\in\{已有数据\}$, 表示只使用已经有记录(评分)的数据进行计算, 而不需要向原始SVD那样进行填充. 这对于推荐系统中稀疏矩阵的分解与估计是非常重要的.</p>
<p>为了防止过拟合, 在此基础上还可以加入正则项:</p>
<script type="math/tex; mode=display">
loss=\frac{1}{m}\sum_{u,i\in\{已有数据\}}(r_{u,i}-p_u^Tq_i)^2+\lambda(||p_u||^2_2+||q_i||^2_2)</script><h2 id="BiasSVD"><a href="#BiasSVD" class="headerlink" title="BiasSVD"></a>BiasSVD</h2><p>BiasSVD正如其名称一样, 是在FunkSVD上加入了偏置项, 如何进行理解呢?</p>
<p>比如在推荐系统中, 评分是1-5分, 有的用户整体打分偏高, 可能5分表示好, 3分表示差; 而有的用户整体打分低, 3分表示好, 1分表示差. 这种情况下, 可以给予每个用户一个偏置项$b_u$, 来表示用户的整体倾向, 类似线性回归中的偏执项, 让其它参数专注于学习”变化”(同一用户对不同物品的评分), 整体获得更好的效果.</p>
<p>有了用户的偏置项, 同理也可以设定物品的偏置项, 比如有一些物品质量就是好, 大部分都给了较高的评分, 有些物品质量就是差, 大部分用户都给了较差的评分, 这些可以用物品偏置$b_i$来表示.</p>
<p>所以原本的线性拟合项变为了如下形式:</p>
<script type="math/tex; mode=display">
r_{u,i}=\mu+b_u+b_i+p_u^Tq_i</script><p>其中的$\mu$表示总体的平均分.</p>
<h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD++"></a>SVD++</h2><p>在一般推荐系统中, 一般来说用户的显式数据(如给某个物品打分), 相比其它一些隐式数据(如点击)是要少很多的, 那么这些隐式数据是否也可以加入到模型中呢?</p>
<p>SVD++就在原本的模型中, 考虑到了隐式的数据, 此时可以认为评分的构成如下:</p>
<script type="math/tex; mode=display">
评分=显式评分+隐式兴趣+偏置</script><p>具体的数学公式为:</p>
<script type="math/tex; mode=display">
r_{u,i}=\mu+b_u+b_i+(p_u^T+|N(u)|^{-1/2}\sum_{j\in N(u)}y_j)q_i</script><p>其中$N(u)$表示用户$u$的隐式行为物品集合, $y_i$表示对物品$j$有隐式行为所反映出的用户偏好.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文主要从特征值分解开始, 介绍到奇异值分解SVD, 然后是真正在推荐系统中可以使用的FunkSVD, 及其改进版本.</p>
<p>矩阵分解方法在推荐系统中, 其优点是实现比较简单, 效果也还不错. 缺点的话, 大概就是在同样的协同过滤类的方法中, 当数据量比较多了以后, 比起一些深度学习的方法还是差了一些.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>KD Tree</title>
    <url>/2020/10/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/KD-Tree/</url>
    <content><![CDATA[<p>在经典的分类算法KNN中, 为了加速邻近点的搜索, 可以使用一种数据结构, 即KD Tree, 本篇就主要讲解KD Tree的原理.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近邻检索在不少场景中, 是一项比较关键的技术. 比如在推荐系统的召回板块中, 使用隐式召回的话, 一般会通过一些算法(如矩阵分解), 给到每个用户和物品一个稠密的Embedding向量. 通过这个向量, 可以用一些方式(如余弦相似度), 来计算用户与物品的相似度, 将相似度高的物品, 作为候选集, 用于推送或者排序.</p>
<p>而现在假如有了一堆同维度的向量, 并使用欧式距离作为指标, 给定一个向量, 想要挑选出离这个向量最近的$N$个向量, 可以怎么做呢?</p>
<p>最简单粗暴的方法, 就是拿着这个向量, 去和所有的向量进行一次距离的计算, 这样遍历一次的时间复杂度为$O(N)$, 当$N$很大时, 会非常耗费时间.</p>
<p>另外的一种思路, 就是使用某种数据结构, 用空间换时间, 先用一种方式把原本的数据组织起来, 然后在后续搜索的时候, 尽可能地排除掉那些明显不合适的向量, 只在少数的向量中计算距离和进行比较.</p>
<p>KD Tree就是这样一种数据结构, 下面对其原理进行讲解.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h2><p>KD Tree的构建原理非常简单, 假设现在有$N$个点, 每个点对应一个向量, 向量维度假设为$M$.</p>
<p>首先, 按照某种顺序来从$M$个维度中, 选择一个维度$m$, 选择的方法一般有如下两种:</p>
<ul>
<li>从$0$到$M-1$维循环选取.</li>
<li>选取当前方差最大的维度.</li>
</ul>
<p>然后找到这个维度上, 所有点该维度上数值处在中间的那个点, 用这个”中间点”来作为划分节点, 用该维度上的值大于或者小于该阈值, 将点划分到两侧.</p>
<p>用来划分的点可以视作树的父节点, 其余的点被划分到子节点上.</p>
<p>不停地重复上面的操作, 知道最后每个树节点(包括内部节点和叶子节点)上, 仅包含一个点, 那么此时KD Tree构建完成.</p>
<p>在维度为2的向量上, 构建完成的KD Tree大概长这样:</p>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>并在平面上的大致分布如下, 其中内部的每条线表示一次划分:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>通过这种方式, 将所有的点(向量)组织到了一棵平衡二叉树中, 或者说可以看成将点划分到了向量空间中的一个个”小方块”中, 借助这样的数据结构, 可以有效地排除掉距离明显很远的点, 减少无效计算.</p>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>下面来讲如何在KD Tree中进行搜索, 在说明具体流程之前, 要定性地明确一些东西, 才能更好的理解.</p>
<p>首先, 在构建好一棵KD Tree后, 给定一个目标点, 从根节点开始, 是可以根据节点的划分, 进入到某个叶子节点的. 一般来说, 这个叶子节点中的点, 和目标点的距离是比较小的.</p>
<p>假如最后想要挑选出$n$个与目标点距离最小的点, 那么下一步应该去哪找呢?</p>
<p>应该去父节点, 因为父节点离当前叶子节点是比较近的.</p>
<p>现在处在父节点上, 下一步可以搜索父节点的另外一侧, 在向量空间中, 相当于超平面隔开的另一侧空间.</p>
<p>那么是否应该搜索父节点的另一侧呢? 这要看目标点到父节点对应分割超平面的距离, 因为处在超平面另一侧的点, 与目标点的距离, 一定是大于该距离的, 如果这个距离比当前已收集点中的最大距离还大, 并且已收集点数量已达到$n$, 那么就没必要去父节点另一侧搜索了.</p>
<p>什么时候算法停止呢? 首先肯定要满足找到了$n$个点, 其次还要当前位于根节点上.</p>
<p>为什么最后位于根节点才能停止算法呢? 因为假设处于某个内部节点上, 那么其父节点, 以及父节点的另一侧, 都有可能存在距离较小的点. 当从叶子节点返回根节点时, 有两种情况, 一种是通过计算目标点与根节点对应超平面的距离, 距离大, 不用去根节点另一侧搜索, 算法停止; 还有一种是再到根节点另一侧进行搜索, 最终再次返回根节点时, 算法停止.</p>
<p>下面对KD Tree搜索过程进行比较准确的描述.</p>
<ul>
<li><p>(一) 初始化.</p>
<p>假设现在已构建好KD Tree, 给定目标点, 要找到$n$个距离最近的点, 目标点一开始位于根节点.</p>
<p>执行(二)步.</p>
</li>
<li><p>(二) 下沉.</p>
<p>按树节点的划分, 下沉到对应叶子节点</p>
<p>执行(三)步.</p>
</li>
<li><p>(三) 计算距离.</p>
<p>计算当前节点的距离, 更新候选点.</p>
<p>若当前不满$n$个点, 直接加入; 若满$n$个点, 且距离比现有候选点中的最大距离小, 那么替换该点.</p>
<p>并将当前节点标记为”到此一游”, 这里的”到此一游”, 不是指在上面(二)步中的经过, 而是要计算距离后.</p>
<p>若当前节点另一侧不存在, 执行(四)步.</p>
<p>若当前不满$n$个点, 那么去另一侧搜索, 执行(二)步.</p>
<p>若当前满$n$个点, 计算目标点与当前节点对应的超平面的距离, 用该距离与当前候选点中的最大距离比较, 来决定是否搜索另一侧. 若搜索, 则执行(二)步, 否则执行(四)步.</p>
</li>
<li><p>(四) 返回父节点.</p>
<p>进入到当前节点的父节点.</p>
<p>若父节点已经被标记”到此一游”, 且不是根节点, 继续执行(四)步.</p>
<p>若父节点已经被标记”到此一游”, 且是根节点, 恭喜, 算法停止.</p>
<p>若父节点未被标记”到此一游”, 执行(三)步.</p>
</li>
</ul>
<h2 id="指标转换"><a href="#指标转换" class="headerlink" title="指标转换"></a>指标转换</h2><p>另外一个需要注意的是, 对于邻近检索中的指标, 原本给出的是欧氏距离, 而对于其它一些指标, 如内积等, 直接照搬原算法不太行, 那么这时候应该怎么做呢?</p>
<p>其实一般来说, 在做邻近搜索的时候, 常用的指标, 就是欧氏距离, 内积, 余弦相似度这些, 具体用什么指标, 应该根据实际的数据和任务来决定.</p>
<p>而这些指标之间其实是有一些联系的, 可以通过一些方式来进行转换.</p>
<p>比如现在给出两个标准化的向量$x,y$:</p>
<script type="math/tex; mode=display">
||x||_2=||y||_2=1</script><p>那么这时候两个向量的内积, 就等于其余弦相似度:</p>
<script type="math/tex; mode=display">
x^\top y=\cos(x,y)=\frac{x^\top y}{||x||_2||y||_2}</script><p>并且两个向量欧氏距离的平方, 与其内积之间的关系如下:</p>
<script type="math/tex; mode=display">
\begin{align*}
||x-y||^2_2&=(x-y)^\top (x-y) \\
&=x^\top x-2x^\top y+y^\top y \\
&=2-2x^\top y
\end{align*}</script><p>所以, 在向量的标准化的情况下, 内积, 余弦相似度, 欧氏距离是等价的.</p>
<p>在特定的场景下, 如果原本是用内积作为指标, 那么可以拿一些样本来进行分析, 比较在对向量做了标准化后, 用余弦相似度是否可以代替内积, 如果可以, 那么像KD Tree这种原本基于欧式距离的算法, 也能派上用场了OvO</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上就是KD Tree这种数据结构的原理了, 讲解了其构建过程与搜索过程, 整体并不太复杂.</p>
<p>但是KD Tree有着其一些问题, 比如在构建KD Tree时, 一般来说要用到所有数据, 将树保存在内存中, 在数据很大的时候, 可能还没等时间复杂度降低, 空间复杂度就先顶不住了QAQ 而真正在大数据场景下, 做到又快, 又节省内存的一个开源库, 叫做Faiss, 在我的大数据板块中有介绍♪(^∇^*)</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title>文本表示</title>
    <url>/2020/10/07/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<p>今天来聊一聊文本表示(Document Embedding)的一些方法. 这里列举得并不完全, 并且对于其中的一些比较复杂的方法, 这里也不会做深入地讲解(可能会单独写成一篇).</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前面的一些文章中, 提及到了不少单词表示(Word Embedding)的方法, 比如词袋法(Bag of Words), 预训练方法word2vec, 以及非常腻害的BERT.</p>
<p>在有了Word Embedding之后, 可以用在各种NLP的任务中, 提升模型效果, 也可以延伸到其它的领域, 如item2vec, node2vec等.</p>
<p>但是只有Word Embedding还是不够的, 在一些任务中, 其实相比单词的表示, 更希望得到一个更好的文本(句子, 段落, 文章)的表示.</p>
<p>假如有了比较好的Document Embedding, 那么一些任务就会变得简单, 如文本分类, 文本相似度比较. 同时在一些其它任务中也能够用到, 如问答系统, 摘要提取等.</p>
<p>所以, 这里就主要来列举一些现有的Document Embedding方法, 不过限于篇幅问题, 以及本宝宝的水平问题QAQ, 列举得并不完全, 并且也不会做详细地探究.</p>
<p>下面会先阐述一些经典的(非深度学习)的方法, 然后会按无监督/有监督划分来列举更多的方法.</p>
<h1 id="经典方法"><a href="#经典方法" class="headerlink" title="经典方法"></a>经典方法</h1><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag-of-Words"></a>Bag-of-Words</h2><p>这个方法相信我不说, 大多数同学应该都知道, 因为它是那么地朴实无华.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>方法非常简单, 就是首先准备一个词表, 包含所有可能会用到的单词, 假如词表大小为N, 那么对应的就有一个长度为N的向量, 向量中每个位置代表一个单词.</p>
<p>给定一份文本, 对于在文本中出现过的单词, 设置向量对应位置为1, 其余位置为0.</p>
<p>这个方法的一些比较明显的缺点是:</p>
<ul>
<li>当词表较大时, 向量会非常稀疏.</li>
<li>没有考虑到语义/语法信息.</li>
</ul>
<p>同时它还有一个升级版, 就是Bag-of-N-Gram, 比如Bi-Gram, 即将单词进行两两组合. 使用组合后的N-Gram来表示文本.</p>
<p>举个栗子, “I love machine learning”这句话, 用Bi-Gram表示为”I love, love machine, machine learning”, 然后将向量上的这三个Bi-Gram对应位置标为1.</p>
<p>原本的Bag-of-Words其实就已经非常稀疏了, Bag-of-N-Gram直接超级加倍…</p>
<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>关于TF-IDF, 在我的<a href="whitemoonlight.top/2020/08/16/自然语言处理/TF-IDF/">这篇文章</a>中有过比较详细的讲解, 原理其实也比较简单, 一句话概括地话, 就是在Bag-of-Words的基础上, 考虑词频(TF)的同时, 还考虑了逆文档频率(IDF).</p>
<p>TF-IDF得到的关于一份文本的向量, 可以直接通过其中每个单词的值的大小, 来了解该文本中, 哪些单词是具有代表性的.</p>
<p>相比Bag-of-Words, 在文本表示的效果上要好一些, 并且还可以用来提取关键词. 关于向量维度(词表大小), 可以通过过滤一些高频词和低频词, 在尽可能保留信息的情况下, 有效地减少维度.</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>Latent Dirichlet Allocation, 又称为文本主题模型, 个人感觉与一些其它NLP中的经典方法, 如CRF, 算是相对比较难的, 涉及到了不少数学/统计的基础知识.</p>
<p>对LDA原理感兴趣, 同时想了解其细节原理的, 非常推荐一份叫做&lt;数学八卦&gt;的资料. 一看这名字, 好像不是讲LDA的啊QAQ, 其实就是讲的LDA, 会从其涉及到的一些数学/统计知识开始, 如伯努利分布, 多项式分布, 贝塔分布, 狄利克雷分布…用同时生动的栗子与图例, 来讲解LDA的原理.</p>
<p>当然了, 如果感觉看完一遍还是有些没看懂怎么办, 那就找个时间再看一遍, 总有一天对这个问题的理解会收敛, 到时候就自然懂了o(*≧▽≦)ツ</p>
<p>咳…如果用通俗的语言来讲LDA到底干了一件神马事, 我觉得是这样的. 对于LDA算法来说, 它认为在生成一篇文档的时候, 会首先确定这篇文档的主题分布(如娱乐30%, 游戏50%, 体育20%), 然后每次按分布, 选取一个主题(如选到了游戏), 然后在这个主题之下, 对应一个单词出现的分布, 再在这个单词分布中, 得到一个单词.</p>
<p>也就是说, 在有了一篇文档主题的分布, 以及各主题下单词的分布后, 通过不停地”摇色子”, 就能够得到一篇文章(词袋).</p>
<p>那么, LDA又是怎么得到一篇文档主题的分布, 和各主题下单词的分布的呢?</p>
<p>根据LDA的生成文档过程, 得到的文档中, 除了单词以外, 还有每个单词对应的主题. 关于一篇文档主题的分布, 可以通过文档词袋中每个单词对应的主题来进行统计. 而各主题下单词的分布, 也可以通过统计所有文档中的单词/主题关系来确定.</p>
<p>而一开始的时候, 对于LDA来说, 模型参数是文档主题的分布, 各主题下单词的分布, 这是需要学习的; 隐变量是每篇文档中每个单词对应的主题, 这个在一开始可以随机初始化.</p>
<p>对于LDA的学习过程来说, 由于有隐变量的存在, 所以其中一种方法就是使用EM(期望最大)算法, 这个也在我的<a href="whitemoonlight.top/2020/08/20/传统机器学习/EM算法/">这篇文章</a>中有详细讲解. 在这里, 用EM算法简单地来说, 就是反复迭代, 通过隐变量来优化模型参数, 然后再通过模型参数来更改隐变量, 直到最后收敛.</p>
<p>最后, 得到文档主题的分布, 这正是我们所需要的, 其维度是预设的, 是一个稠密向量, 每个位置表示表示某种主题(也许可以通过查看各文档分布来确定其含义). 同时在有了各主题下单词的分布后, 对于新的(未加入模型训练)的文档, 也能够很快地确定.</p>
<p>关于LDA的数学原理, 以及效果(比如和TF-IDF)比较, 以后可能会单独用一篇文章来介绍.</p>
<p>本质上来说, LDA是通过文档与单词的共现关系, 来得到了主题的分布, 具有这样思想的模型还有一些, 比如LSA(隐语义模型), 通过文档与单词的共现矩阵, 通过矩阵分解或者其他方法, 得到每篇文档与每个单词的向量表示(Embedding), 这里就不做更多叙述.</p>
<h1 id="无监督学习方法"><a href="#无监督学习方法" class="headerlink" title="无监督学习方法"></a>无监督学习方法</h1><p>其实上面的一些经典方法, 本质上也属于无监督方法, 而这里的无监督学习方法, 特指与深度学习或者神经网络挂钩的一些方法.</p>
<p>其实由于在NLP领域中, 由于本身有着海量的数据, 但有标注的数据却是少数, 所以无监督学习的方法在这里可以大放异彩, 下面介绍几种方法(我知道的).</p>
<h2 id="Averaging-Word-Embeddings"><a href="#Averaging-Word-Embeddings" class="headerlink" title="Averaging Word Embeddings"></a>Averaging Word Embeddings</h2><p>一种在这里比较质朴的一种方法, 就是Averaging Word Embeddings, 即将一段文本中, 出现的Word Embeddings(如word2vec)进行平均, 来得到这段文本所对应的Document Embedding.</p>
<p>这样做确实是有一定效果的, 但其中一个问题是, 当文本的长度比较长的时候, 将众多的Word Embeddings进行平均, 可能会将原本的一些信息掩盖掉.</p>
<p>对于简单的Averaging Word Embeddings可以有一些改进:</p>
<ul>
<li><p>结合TF-IDF</p>
<p>即不是简单的平均, 而是加权平均, 权重可以是TF-IDF; 同时对于长文本, 可以借助TF-IDF得到的关键词, 仅对关键词进行平均, 尽可能保留重要信息.</p>
</li>
<li><p>文本分段</p>
<p>将长文本, 按一定规则分成众多的短文本, 对每个短文本使用Averaging Word Embeddings进行表示. 而在得到了一系列短文本的表示后, 根据具体任务进行具体的使用.</p>
</li>
</ul>
<h2 id="Doc2vec"><a href="#Doc2vec" class="headerlink" title="Doc2vec"></a>Doc2vec</h2><p>既然在单词的表示上, 有word2vec, 那么在文本的表示上, 也有对应的doc2vec, 而且它们不仅名字相似, 算法也是几乎一样, 下面就来介绍一下doc2vec算法.</p>
<p>与word2vec分为CBOW和Skip-Gram一样, doc2vec也有类似的两种形式.</p>
<p>与CBOW对应的形式如下图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>除了word2vec中出现的词向量以外, 还多出了一个代表文本的向量. 在使用滑动窗口, 在一段文本中训练模型时, 根据不同的窗口, 对应不同的输入词向量和目标词向量, 但是一段文本对应一个相同的文本向量. 这样训练了一遍之后, 会得到该文本对应的文本向量, 也会对出现在文本中单词的向量进行学习.</p>
<p>与Skip-Gram对应的形式如下图:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>这里非常直观, 在一段文本的一个窗口中, 用文本向量去预测出现的单词.</p>
<p>通过doc2vec的学习过程, 最终与word2vec一样, 也可以得到各个单词对应的向量表示(在Skip-Gram可以理解为softmax对应想权重系数). 当接收到一段新的文本, 要进行推断时, 可以首先随机初始化该文本的向量, 然后随着滑动窗口进行学习, 这时候仅仅调整文本向量的表示, 而不再修改其它(如单词向量)参数, 最后返回该文本的向量.</p>
<p>可以看到, doc2vec的原理是非常简单的, 不过这里还没有验证过其效果到底如何, 根据其原理, 应该是当文本较长时, 会表示得更加准确.</p>
<h2 id="SBERT"><a href="#SBERT" class="headerlink" title="SBERT"></a>SBERT</h2><p>这里的SBERT的全称是Sentence-BERT, 关于BERT的原理, 可以参看我的<a href="whitemoonlight.top/2020/08/16/自然语言处理/BERT-一/">这篇文章</a>.</p>
<p>了解BERT的同学都知道, 在BERT中有一个特殊字符[CLS], 在学习的时候用于判断是否为下一句话的学习任务, 可以用来表示输入文本的整体含义.</p>
<p>但是在实际使用中发现, BERT的[CLS]字符对应的向量对于分类任务(如文本分类), 有还不错的效果, 但是对于其它的一些任务可能就不太行了. 举个栗子, 给定一个句子, 要在语料库中找出与之相似度最高的一些句子. 如果直接使用BERT的[CLS]对应的向量来表示一个句子, 然后计算句子间两两的相似度(如余弦相似度), 效果不是很好. 如果将两个句子同时输入BERT, 然后再外接输出层, 用以计算相似度, 效果还不错, 但问题是假如语料库中有一千条句子, 那么目标句子需要与这一千条句子都同时过一遍BERT, 使用过BERT的同学应该感受过BERT的速度…</p>
<p>因此这才在BERT的基础上, 有了Sentence-BERT. SBERT的总体思路就是, 对于每个句子, 都用训练好的BERT事先离线过一遍, 保存好对应的向量. 当接受到一条新的句子时, 只需要过一遍BERT, 然后再针对向量计算相似度即可, 矩阵运算非常快.</p>
<p>那么问题来了, 怎么调整BERT, 使得其输出的句子向量可以更好地表示句子呢? 原作者采用了如下的模型框架:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>先看左边的模型, 两个BERT模型的参数是共享(一样)的, 两个句子分别经过BERT模型后, 再经过pooling层, 得到对应的表示向量$u$和$v$. 这里的pooling层, 作者尝试了三种方式, 分别是:</p>
<ul>
<li><p>CLS向量策略</p>
<p>只使用[CLS]表示整句话的向量.</p>
</li>
<li><p>平均池化策略</p>
<p>即用BERT输出的词向量的平均, 作为整句话的表示.</p>
</li>
<li><p>最大池化策略</p>
<p>即对BERT输出的词向量做最大操作, 将得到的向量作为整句话的表示.</p>
</li>
</ul>
<p>从实验结果来看, 平均池化策略效果是最好的.</p>
<p>在得到了两段句子的表示向量$u$和$v$以后, 再通过一些监督学习任务进行微调. 比如在上面左图中, 将两个向量以及它们的向量差$(u-v)$进行拼接, 再经过输出层, 进行分类任务(如两个句子是否相似)的训练. 或者如右图, 对两个向量计算余弦相似度, 用回归任务进行训练.</p>
<p>Sentence-BERT其实在算法上并没有多少创新, 不过通过这样的调整后, 使得在工程上快速地获取文本表示, 匹配相似文本等任务上效率有很大提升.</p>
<h1 id="有监督学习方法"><a href="#有监督学习方法" class="headerlink" title="有监督学习方法"></a>有监督学习方法</h1><p>上面介绍了一些无监督学习的方法, 下面再介绍一些有监督学习的方法.</p>
<h2 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h2><p>一个比较常见的有监督学习任务, 就是文本分类了.</p>
<p>比如现在有了一堆的文本, 每一份文本对应一个类别, 表示其主题, 那么就可以使用一些深度学习的方法, 通过训练以后得到的隐向量, 来作为文本的表示向量.</p>
<p>比较经典的有使用MLP, 即多层全连接神经网络, 输入特征可以是一些预处理好的结构化特征(如TF-IDF). 在通过文本分类任务训练好以后, 可以取输出层前一层的隐向量, 来作为文本的表示向量.</p>
<p>除了使用MLP以外, Bi-LSTM也是不错的选择, 通过训练以后, 将正向与反向的LSTM最后的隐向量拼接得到的向量, 作为文本表示.</p>
<p>不过想单纯地依靠分类任务, 来获取到不错的文本表示向量的话, 应该需要多积累一些训练样本, 在小样本上面的表示应该不会太好.</p>
<h2 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h2><p>DSSM的英文全称为Deep Structured Semantic Model, 可以翻译为深度语义匹配模型.</p>
<p>考虑一个搜索的场景, 在输入一个Query之后, 要在众多的Doc中寻找最为匹配的项, 那么这时候如果将Query经过某种模型, 得到其向量表示, 再与Doc对应的向量进行相似度计算, 将相似度高的进行返回, 即可完成匹配.</p>
<p>其实这个思路好像和前面的SBERT比较相似, 不过一个较大的区别是, SBERT是在有了BERT模型以后, 进行微调; 而DSSM算是从头开始进行有监督地训练.</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>上图就是DSSM的结构, 其中的几个关键部分:</p>
<ul>
<li><p>输入层:</p>
<p>对于输入层, 常规的DSSM采用的是词袋模型.</p>
<p>具体说来, 比如是英文, 那么采用Letter-Trigram, 即用三个英文字母的组合来表示一个token. 这样做以后, 词表(或者说向量)的大小约为两万. 而中文则采用字作为token, 常用的子大概也是两万左右.</p>
<p>将一段原始文本, 切分为token后, 表示为词袋向量, 就是输入了.</p>
</li>
<li><p>中间层:</p>
<p>中间层其实就是一些全连接网络, 用到的激活函数为$\tanh$. 可以取中间层的最后一层的输出, 作为文本的表示向量.</p>
</li>
<li><p>匹配层:</p>
<p>最后的匹配层, 会首先计算Query与各个Doc的余弦相似度, 然后再经过一个softmax层, 来进行学习. 在训练的时候, 一般需要进行采样, 即针对一个Query, 需要有一个真正匹配的Doc, 还要有一些不匹配的Doc作为负例, 类似负采样.</p>
</li>
</ul>
<p>DSSM的结构与原理也比较简单, 而在后来针对原始的模型, 又做了更多的调整. 比如使用CNN来作为中间层, 更好地提取上下文信息; 或者将原始文本先经过一个LSTM, 将LSTM输出的向量代替原本的词袋向量等.</p>
<p>DSSM模型除了在NLP相关的场景中可以使用, 基于其向量匹配的核心思想, 在其它一些场景, 如推荐系统的召回中, 也可以进行使用.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>通过上面列举的一些具体的方法, 包括经典的方法(如词袋模型, TF-IDF, LDA), 无监督学习方法(如Averaging Word Embeddings, Doc2vec, SBERT), 有监督学习方法(如DSSM等).</p>
<p>那么哪种方法最好用呢?</p>
<p>这个问题其实不能简单地获得答案, 应该要根据具体的任务场景, 数据维度, 工程实现, 模型效果来进行综合考虑.</p>
<p>我个人认为, 在面对一个具体的任务时, 可以采用由简到繁的模式, 即先采用一些简单的好实现的方法(如TF-IDF, Averaging Word Embeddings)来作为一个base line, 然后再尝试一些相对复杂一些的方法, 来观察模型在任务上是否有提升.</p>
<p>当然了, 如果是一个有经验的专家, 应该一来就知道哪些方法效果更好, 减少尝试的时间.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>算法之外</title>
    <url>/2020/10/07/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95%E4%B9%8B%E5%A4%96/</url>
    <content><![CDATA[<p>这篇文章中, 将会对一些机器学习中的问题进行探讨, 包括老生常谈的偏差与方差, 以及loss landscape, 数据分布, 样本划分, 迁移学习. 看起来好像内容比较杂, 但是在我看来, 它们都有一个相同点, 即可以指引我们更好地使用机器学习算法.</p>
<p>下面的内容, 有一些是早有定论, 而一些是我个人的理解与认识, 其中不免有不当甚至错误之处, 同学们可以抱着审视的态度来阅读.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>还记得我刚开始学习机器学习相关内容的时候, 我把达叔(吴恩达)网易云以及Coursera上面的&lt;机器学习&gt;课程看了. 当时更多的, 是关注于一些算法的具体原理, 比如线性回归, 而对于一些其它方面的东西, 没有太在意.</p>
<p>最近在重新阅读了达叔的<machine learning yearning>后, 感觉受益良多, 结合自己的经验和理解, 将一些”算法之外”的东西总结在这里, 希望对大家也能有所帮助.</machine></p>
<blockquote>
<p>达叔, 永远滴神♪(^∇^*)</p>
</blockquote>
<h1 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h1><p>在机器学习中, 可以用偏差和方差(和统计学中的有所不同), 来定性地衡量一个模型的好坏, 并可以指导我们往哪些方向进行改进.</p>
<p>要理解偏差与方差, 可以举一些具体的栗子来进行说明.</p>
<p>比如现在使用机器学习算法进行建模, 将原本的数据集划分成了训练集, 验证集, 测试集, 并假设它们都来自相同的分布.</p>
<p>任务是一个二分类任务, 一个”理想的”分类器(比如人类), 在这个任务中能够取得近乎完美的表现, 即错误率接近0, 那么现在有下面一些情况:</p>
<ul>
<li><p>训练集错误率$1\%$, 验证集错误率$11\%$.</p>
<p>这时, 可以说模型的偏差为$1\%$, 方差为$10\%$ ($=11\%-1\%$), 属于低偏差, 高方差.</p>
<p>虽然在训练集上误差率较低, 但是没有能够很好地泛化到验证集上, 这也叫做过拟合.</p>
</li>
<li><p>训练集错误率$15\%$, 验证集错误率$16\%$.</p>
<p>模型的偏差为$15\%$, 方差为$1\%$, 属于高偏差, 低方差.</p>
<p>没有能够较好地拟合训练集, 同时在验证集上表现类似, 这也叫做欠拟合.</p>
</li>
<li><p>训练集错误率$15\%$, 验证集错误率$30\%$.</p>
<p>模型的偏差为$15\%$, 方差为$15\%$, 属于高偏差, 高方差.</p>
<p>没有能够较好地拟合训练集, 同时在验证集上表现更差, 某种程度上可以说是同时过拟合与欠拟合, 过拟合/欠拟合的术语难以准确应用在这里.</p>
</li>
<li><p>训练集错误率$0.5\%$, 验证集错误率$1\%$.</p>
<p>模型的偏差为$0.5\%$, 方差为$0.5\%$, 属于低偏差, 低方差.</p>
<p>在训练集上拟合很好, 同时能够泛化到验证集上, 即没有过拟合, 也没有欠拟合, 算是很棒的模型了.</p>
</li>
</ul>
<p>总体来说, 偏差反映了模型(在训练集上)的拟合能力, 方差反映了模型(在验证集上)的泛化能力. 它们是模型误差的两个主要来源, 我们的目标就是尽可能获得低偏差, 低方差的模型.</p>
<p>上面的栗子中, 假设了一个”理想的”分类器, 其错误率接近0, 然鹅很多实际的任务中, 这样的情况是不多见的, 也就是说存在一定的误差率下限, 这一部分最优错误率(贝叶斯错误率)导致的偏差, 也叫做”不可避免偏差”.</p>
<p>所以, 偏差可以看做是不可避免偏差加上可避免偏差. 而方差则没有不可避免这一说, 因为有一些方法可以有效将方差降得很低, 下面分别列举一些能够降低(可避免)偏差和方差的方法.</p>
<p>降低可避免偏差的方法:</p>
<ul>
<li><p>增加模型复杂度.</p>
<p>比如对GBDT来说, 就是设置更深的树的层数, 更多的树的棵树; 对神经网络来说, 就是增加更多的神经元数量, 更多的隐藏层等.</p>
<p>虽然增加模型复杂度一般能够降低偏差, 但也有增加方差的风险.</p>
</li>
<li><p>更改模型结构.</p>
<p>根据具体的任务与数据类型, 选择合适的模型, 更好地学习数据中通用的模式, 降低一些噪音的影响, 也许能够同时降低偏差与方差.</p>
</li>
<li><p>减少或去除正则化.</p>
<p>比如L1和L2正则, dropout等, 这会减小偏差, 但会增大方差.</p>
</li>
<li><p>更多的特征.</p>
<p>更多的特征意味着更多的信息来源, 能够降低偏差, 但如果特征本身有问题(后文详细讨论), 也会增大方差.</p>
</li>
</ul>
<p>降低方差的方法:</p>
<ul>
<li><p>降低模型复杂度.</p>
<p>同上.</p>
</li>
<li><p>更改模型结构.</p>
<p>同上.</p>
</li>
<li><p>添加或增加正则化.</p>
<p>同上.</p>
</li>
<li><p>更多的样本.</p>
<p>更多的样本意味能够让模型”看得”更加全面, 这是能够降低偏差的方法, 同时一般不会增加方差.</p>
</li>
<li><p>更少的特征.</p>
<p>同上.</p>
</li>
<li><p>加入提前停止.</p>
<p>可以降低方差, 有可能会增大偏差. 某种意义上类似于正则化方法.</p>
</li>
</ul>
<p>从上面可以看出, 偏差与方差有时候是难以同时简单地降低的, 某个方法在优化其中一个的时候, 可能使得另外一个变糟了. 所以更多的时候是一种权衡, 找到一个平衡点.</p>
<h1 id="Loss-Landscape"><a href="#Loss-Landscape" class="headerlink" title="Loss Landscape"></a>Loss Landscape</h1><h2 id="从优化算法说起"><a href="#从优化算法说起" class="headerlink" title="从优化算法说起"></a>从优化算法说起</h2><p>首先说一下loss landscape是个啥, 其实就是损失函数对应的误差随模型参数的分布, 比如对某个一元回归模型, 平方误差损失函数来说, 就是一个凸的曲线. 但是我一时找不到比较合适的中文名称, 所以这里就以loss landscape来指代了.</p>
<p>那么, loss landscape是个啥我们都知道了, 为什么要在这里提它呢?</p>
<p>先从一个栗子说起吧, 大伙都知道, 对于深度学习模型, 不像线性模型那样, 其loss landscape是非常复杂的, 一般来说难以找到所谓的最优解, 但是可以用一些次优解代替, 通常也能有不错的效果. 同时可能也听到过说对于优化算法而言, SGD虽然看起来平凡而陈旧, 但是其得到的模型, 相比其它一些优化算法, 比如Adam, 时常能够具有更好的表现和泛化性, 那么这是为什么呢?</p>
<p>对于一个比较复杂模型的loss landscape来说, 可能会同时有一些比较平坦的极值点, 以及一些比较陡峭的极值点. 一些研究表明, 对于SGD来说, 更容易找到平坦的极值点, 而像Adam这样的优化算法, 相比之下有更大概率陷于陡峭的极值点.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 黑色的曲线表示训练集的loss landscape, 红色的曲线表示测试集的loss landscape. 一般来说, 从同一个分布中获取的训练集和测试集, 如果在样本量不是特别大的情况下, 真实分布仍然会有一些差异. 在这种情况下, 如果是模型学习到平坦的极值点那里, 那么由数据分布的差异, 带来的loss landscape的一些变化, 对应得到的误差会比较接近; 而如果模型学习到平坦的极值点那里, 对应得到的误差会发生比较大的改变, 且通常是更糟.</p>
<p>上面说明了在loss landscape中, 差不多”深度”(极值点的大小)下, 更加平坦的极值点是更好的. 而为什么Adam会更容易陷入陡峭的极值点呢, 我自己的理解是, Adam这样的自适应(自动调节学习率)优化算法, 在遇到比较陡峭的区域时, 会放慢脚步(降低学习率), 这样就容易陷入. 而SGD并不会这样, 当它遇到比较陡峭的区域时, 由于梯度较大, 步子也会迈得更大, 就更可能越过这样的深坑.</p>
<p>以上, 结合loss landscape, 解释了不同优化算法表现的差异.</p>
<p>我认为对于loss landscape, 应该有一些更加深入的理解与研究. 尽管想知道一个复杂模型的loss landscape的全貌是一件困难的事情, 因为高维空间难以展示和想像, 同时可能也没有这个必要, 但是通过对loss landscape的认识, 可以帮助我们更好地去分析与理解模型. 下面提出一些我个人的观点, 如一开始所说, 这里更多的是一些讨论, 并不是定论.</p>
<h2 id="Loss-Landscape-分布变化"><a href="#Loss-Landscape-分布变化" class="headerlink" title="Loss Landscape 分布变化"></a>Loss Landscape 分布变化</h2><p>上文中也说到了, 除了像线性模型这样简单的模型, 复杂一些模型的loss landscape我们是难以想像的, 但是也许可以做一些定性的分析.</p>
<p>首先, 给定了损失函数的形式后, loss landscape是与模型本身绑定的, 或者说只与模型相关吗? 当然不是, 我认为还与一个东西紧密相关, 那就是数据. 而进一步, 给定总体数据分布,  loss landscape确定了吗, 只能说理论上确定了, 这等价于从这个数据分布中抽取足够多的样本, 得到对应的loss landscape.</p>
<p>不过在真实的场景中, 我们拥有的, 是一部分和总体同分布的样本, 但如果样本量不够大, 那么是不足以完全代表总体的, 或者说数据分布会与总体之间存在差异. 假设现在有一个由”无限多”样本得到的loss landscape, 同时还有一个有限多样本得到的loss landscape, 那它们有啥不一样呢? 在模型结构确定的情况下, 前者的loss landscape会相对平滑, 后者的loss landscape相比前者, 可能极值点的位置会发生偏移, 可能会存在更多的陡峭的极值点, 可能会有更深的极值点… 从这里也可以解释, 为什么样本越多总是好的, 为什么同样的模型在小样本上容易过拟合.</p>
<p>上面说完了给定模型, 样本变化对loss landscape分布的影响, 现在再来说一下另外一种情况. 假设现在给定一份足够多的数据样本, 模型复杂度的变化, 会对loss landscape产生怎样的改变呢? 比如线性模型, 其loss landscape凸的, 当变成神经网络, 并添加更多的神经元和隐藏层后, loss landscape会出现更多的坑, 这其中包含一些陡峭的坑, 也包含一些更深的坑.</p>
<p>除了模型, 样本外, 特征, 正则项等, 都会对loss landscape带来改变. 一般来说, 这些改变会朝着两个方向变化, 一是更加平坦, 更加浅; 另一种是更加陡峭, 更加深. 而与之对应的, 更加平坦, 更加浅, 可以减小模型方差, 更加陡峭, 更加深可以减小模型偏差.</p>
<p>利用loss landscape, 可以解释很多东西, 比如可以定性解释为什么添加正则项可以增强模型泛化性. 在loss landscape中搜索最近模型参数时, 正则项可以使参数更加向0处靠拢, 而不是完全落到原本的极值点处. 当应用模型到其它样本(测试集)上时, 由于新的数据分布可能会发生一些变化, 但其loss landscape的极小值一来应该靠近训练集, 二来一般在0附近就有比较好的模型参数, 所以在添加正则项后, 对于分布发生一点变化的数据样本可能更加友好.</p>
<p>如果有一天, 能够有研究者发现对loss landscape的”探测”方法, 从而针对不同的loss landscape, 根据其特点, 调整使用不同的模型和学习策略, 那么也许可以更进一步.</p>
<h1 id="数据分布与样本划分"><a href="#数据分布与样本划分" class="headerlink" title="数据分布与样本划分"></a>数据分布与样本划分</h1><h2 id="数据分布一致"><a href="#数据分布一致" class="headerlink" title="数据分布一致"></a>数据分布一致</h2><p>众所周知, 想要建立一个表现较好的模型, 并且能够准确评估其性能, 是需要不止一份样本集的. 通常用训练集, 来学习模型参数; 用验证集来评估模型表现, 并根据表现调整超参数或者说选择模型; 最后用测试集, 来评估模型的泛化性能.</p>
<p>一般来说, 给定一份数据集, 按照6:2:2的方式, 就是一种可行的划分方法, 但是如果数据量很大, 那么也不一定要这样. 设立验证集的目的是用来选择模型, 那么只要数据集的量能够有效地表现出模型性能的差别就够了, 比如两个模型的准确率相差$1\%$, 那么100个样本是不够的, 怎么也要1000个样本才能更加靠谱地反映出这种差距. 而设立测试集的目的是用于最终评估模型性能, 那么其实也只要根据具体情况, 样本达到一定的量就行了, 将更多的样本放到训练集, 是有益的 ♪(^∇^*)</p>
<p>在上面的内容中, 基本上都是假设各样本来自一个相同的数据分布, 即训练集/验证集/测试集的分布是一致的, 顶多是由于样本量少一些, 导致的一点点差别.</p>
<p>在这种情况下, 有啥好说的吗? 没啥好说的!</p>
<p>其实可以拿一份大一点的数据试一下, 随机打乱, 并随机抽取出训练集/验证集/测试集(验证集和测试集稍微大点), 拿一个GBDT来跑一跑, 超参数别设置得太离谱, 然后观察一下在个样本集上的指标, 会发现验证集和测试集的指标几乎一毛一样(前提别严重过拟合).</p>
<p>当各样本集的分布一致时, 会有哪些好处呢? 可以用更复杂一些的模型, 尽情地在训练集上找”深坑”; 可以尝试不同的建模方法, 然后查看在验证集表现, 然后选验证集上表现最好的那个, 因为同分布, 测试集上的指标一般是跟着验证集走的.</p>
<p>有时候, 如果对过拟合拿捏得比较好的话, 可能都不需要测试集, 只要分出训练集和验证集就够了.</p>
<h2 id="数据分布不一致"><a href="#数据分布不一致" class="headerlink" title="数据分布不一致"></a>数据分布不一致</h2><blockquote>
<p>但是这个世界总是充满了变化, 不是吗?</p>
</blockquote>
<p>在实际的一些场景下, 样本的产生是伴随着时间的, 同时数据分布也可能随时间发生一些变化. 当我们有了一个模型以后, 是需要用这个模型, 去预测未来发生的事情. 也就是说, 我们建模的目的, 是想让模型在未来表现得较好.</p>
<p>但是我们并没有未来的数据, 如果评估模型在未来的表现呢? 可以截取现有样本中, 最新的一部分样本, 作为测试集, 其余样本用作训练集和验证集, 这样可以通过模型在测试集上的表现, 来进行估计.</p>
<p>不用太担心训练模型用不上最新的数据, 当完成前面的工作后, 最后可以将测试集也加入训练, 最为最终的模型.</p>
<p>现在讨论两种情况, 其中一种是, 尽管存在分布的差异, 但随时间变化并不大, 另一种是分布差异比较大的情况.</p>
<p>那么, 怎么去衡量样本随时间的分布差异大小呢?</p>
<blockquote>
<p>实践出真知!</p>
</blockquote>
<p>可以在训练集上, 再按时间划分出一个伪测试集, 然后用不同的超参数下, 训练的模型, 记录在训练集, 验证集, 伪测试集上的评估指标. 用这些指标来绘制一些折线图, 比如按训练集指标排序, 查看指标的走势, 或者按训练集指标排序, 单独看训练集和伪测试集对应指标的走势.</p>
<p>一般来说, 随着训练集上指标升高, 验证集上也会升高, 但是到一定阶段就会开始波动(出现过拟合). 而如果同分布, 或者分布差异不大, 那么验证集的指标和伪测试集的指标也是比较一致的(在没过拟合时). 如果发现不一致, 没有明显相关性, 甚至负相关, 那说明分布差异就比较大了.</p>
<p>对于第一种情况, 即分布差异不大的时候, 可以按照分布一致的情况来做, 相对比较轻松.</p>
<p>对于第二种情况, 即分布差异较大的时候, 首先别忙着建模了, 先分析一下, 是确实数据就是差异这么大, 还是说是其它一些因素, 比如某些特征变化太大, 比如信息泄露, 比如存在一些特殊时期的数据把整体拉偏… 结合分析, 去尽可能地减小数据分布的差异, 当看到验证集和伪测试集存在一定相关性时, 说明有效地减小了分布差异. 进一步进行建模的时候, 在划分验证集时, 除了可以在训练集中随机划分, 还可以有一种思路, 即挑选与测试集尽量相似的样本作为验证集(比如训练集中最新的样本), 这样做某种意义上是把数据分布存在差异这个因素考虑进模型训练, 会更加倾向于选择低方差的模型. 但是这两种划分验证集的方式到底哪一种更好, 也是可以根据实践来知道的.</p>
<p>如果, 一顿操作后, 仍然发现分布差异很大, 怎么办? 随缘吧♪(^∇^*) 这时候可能就不能完全依赖于模型在训练集和验证集上的评估指标来选择模型了, 要有目的地选取低方差的模型, 以保证模型在未来的泛化性. 这样好像就是在说, 效果差点问题不大, 稳定能用就行.</p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>在计算机视觉中, 迁移学习指的是在一个很大很复杂的深度学习模型上, 以有监督学习的方式, 来训练一个模型, 然后在这个模型中, 去掉最后的输出层后, 其输出的向量相当于是在原始数据中抽取的高阶特征, 可以将其用在其它一些场景的任务上.</p>
<p>在NLP中, 词向量的预训练相当于是迁移学习, 利用海量的语料, 来进行无监督学习, 得到的词向量能够表征词本身以及词之间的关系, 可以用在各种下游任务上, 加速模型收敛, 提升模型效果.</p>
<p>那么, 在结构化的数据中, 是否也能够有迁移学习呢?</p>
<p>本质上来讲, 迁移学习不是某种具体的方法, 而是一种思想, 即在一些场景下, 用大量的数据, 复杂的模型进行训练, 然后在另外一些场景下, 用一些方式借鉴其成果, 可以较快地用少量数据就可以达到不错的效果.</p>
<p>在上一节中, 讲到了一些场景中, 数据分布会随时间变化, 导致分布不一致, 然鹅还有一些情形下, 是由于目标分布的样本较少导致的.</p>
<p>举个栗子, 想要做一个分类器, 通过一些类似气温, 湿度这样的特征, 来预测某地的天气, 真实的目标数据样本(假设为A样本集), 是该地的过往记录, 但是如果一开始这样的样本很少, 怎么办呢? 可以在网上找寻大量的其它一些地方(类似)天气的样本(假设为B样本集), 用来帮助模型学习, 但是其它地方的样本与目标样本是会有分布差异的, 这时候可以将部分A样本集作为测试集, 并考虑如下一些做法:</p>
<ul>
<li><p>单样本集</p>
<ul>
<li><p>直接使用A样本集建模.</p>
<p>优点: 分布一致.</p>
<p>缺点: 样本较少.</p>
</li>
<li><p>使用B样本建模, 并使用A样本集作为验证集.</p>
<p>优点: 数据量上来了.</p>
<p>缺点: 分布不一致.</p>
</li>
</ul>
</li>
<li><p>拼接样本集</p>
<ul>
<li><p>直接将A样本集与B样本集拼接, 并建模.</p>
<p>优点: 数据全部用上了.</p>
<p>缺点: 由于A样本集量少, B样本集占主导, 可能会导致验证集的涨跌与测试集不一致.</p>
</li>
<li><p>将A样本集与B样本集拼接, 并添加一列特征来区别AB样本集.</p>
<p>优点: 数据全部用上了, 并且模型可以通过添加的特征区别两种分布, 可以在A样本集上学得更好.</p>
<p>缺点: 同上.</p>
</li>
</ul>
<p>这里有个技巧, 就是拼接后, 仍然使用一部分的A样本集来作为验证集, 这样可以一定程度上避免上面说到的缺点.</p>
</li>
<li><p>迁移学习</p>
<ul>
<li><p>用B样本集训练模型, 并将该模型输出作为A样本集的特征.</p>
<p>用B样本集训练一个模型, 将其输出作为样本集A的一个特征, 再结合A样本集中挑选出的重要特征, 再在样本集A上训练一个模型, 作为最终模型.</p>
<p>优点: 利用了B样本集的信息, 也确保了最终模型是在A样本集上训练的.</p>
<p>缺点: 没啥大缺点.</p>
</li>
<li><p>用B样本集加部分样本集A训练模型, 再作为A样本集的特征.</p>
<p>相比上面的方法, 就是在训练第一个模型的时候, 也将A样本集中的部分样本加入B样本集, 还可以尝试加大A样本集的权重. 然后再将模型输出作为A样本集的特征, 再在A样本集上训练一个模型, 作为最终模型.</p>
<p>优点: 加入部分A样本和B样本一起训练, 可以让前一个模型的输出质量更高, 并确保了最终模型是在A样本集上训练的.</p>
<p>缺点: 原本就少的A样本集再分了一部分后, 更加雪上加霜了QAQ</p>
</li>
</ul>
</li>
</ul>
<p>上面的方法, 在样本很多, 且分布差异不大的情况下, 作用并不明显. 但是如果目标样本(对应测试集分布的样本)较少, 同时拥有大量差异较大的样本时, 用上面的方法可以获得明显的效果提升.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 阐述了关于偏差与方差, loss landscape的理解, 以及讨论数据分布与样本划分的情况, 并且对结构化数据给出了包含迁移学习思想的方法.</p>
<p>数据是多变的, 当面对一个新的任务, 一份新的数据时, 需要先用一些方法, 来分析数据的模式, 并尝试建模, 根据模型的表现, 来尝试可能带来更好效果的方法. 理解通用的机器学习理论, 多知道一些技巧, 并灵活运用.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Faiss</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/Faiss/</url>
    <content><![CDATA[<p>Faiss是Facebook开源的一个能够在海量数据中, 高效地进行邻近搜索的一个库, 下面就来介绍一下这个库的一些基本算法原理, 以及简单的使用.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在我的之前一篇讲<a href="whitemoonlight.top/2020/10/09/数据结构与算法/KD-Tree/">KD Tree</a>的文章中, 就介绍了一种在邻近搜索中, 用空间换时间的数据结构, KD Tree, 在数据较小的情况下, 还是可以使用的, 能够把时间复杂度, 从$O(N)$降低到$O(\log N)$.</p>
<p>然鹅在大数据场景下, 数据(向量)的个数可能达到亿的量级, 此外, 向量维度的增加, 也会导致数据量进一步增大, 此时若想在内存中构建一棵KD Tree, 单机是肯定完成不了的.</p>
<p>如果能有一个库, 能够支持多种邻近搜索模式, 能够根据实际情况的需求, 能够在搜索精度, 速度, 内存占用之间进行平衡, 能够在磁盘上也能进行搜索, 那就好了♪(^∇^*)</p>
<p>Faiss站了出来: 没错, 正是在下OvO</p>
<p>Faiss是Facebook开源的一个库, 用C++写成, 并非常友好地提供了Python接口. Faiss可以用在实际的生产环境中, 下面对Faiss的部分基本原理进行介绍, 并展示简单应用.</p>
<h1 id="精确模式"><a href="#精确模式" class="headerlink" title="精确模式"></a>精确模式</h1><p>Faiss是支持多种模式的, 其中必然就少不了暴力搜索模式, 也可以称作精确模式, 因为要对所有的向量进行无损的遍历, 所以得到的结果也必然是准确的.</p>
<p>精确模型没有太多好说的, 下面看一下代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">d = <span class="number">64</span>  <span class="comment"># 向量维度</span></span><br><span class="line">nb = <span class="number">100000</span>  <span class="comment"># 保存的向量数量</span></span><br><span class="line">nq = <span class="number">10000</span>  <span class="comment"># 需要查询的向量数量</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">xb = np.random.random((nb, d)).astype(<span class="string">'float32'</span>)  <span class="comment"># 注意需要限制32位浮点数</span></span><br><span class="line">xq = np.random.random((nq, d)).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<p>要使用Faiss进行搜索, 需要先创建索引, 这里的<code>IndexFlatL2</code>表示暴力搜索对应的索引, 并用欧氏距离作为指标.</p>
<p>后面会看到, 会用一些算法来加速或者压缩内存, 这时候除了创建索引, 还需要一个训练的过程, 不过<code>IndexFlatL2</code>并不需要训练.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">index = faiss.IndexFlatL2(d)  <span class="comment"># 创建索引</span></span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line">index.add(xb)  <span class="comment"># 添加向量</span></span><br><span class="line">print(<span class="string">'向量个数: '</span>, index.ntotal)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是否已训练: True</span><br><span class="line">向量个数:  100000</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索</span></span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span>  <span class="comment"># 最近邻个数</span></span><br><span class="line">D, I = index.search(xb[:<span class="number">5</span>], k)  <span class="comment"># 直接对保存的向量进行搜索, 查看结果是否正确</span></span><br><span class="line">print(<span class="string">'Index:'</span>)</span><br><span class="line">print(I)</span><br><span class="line">print(<span class="string">'Distance:'</span>)</span><br><span class="line">print(D)</span><br><span class="line">print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># 用真实查询向量进行搜索</span></span><br><span class="line">print(<span class="string">'Index:'</span>)</span><br><span class="line">print(I[:<span class="number">5</span>])  <span class="comment"># 前5个查询向量的结果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Index:</span><br><span class="line">[[    0 97820 94546 51393]</span><br><span class="line"> [    1 58334 87280 12181]</span><br><span class="line"> [    2 24444 71046 91180]</span><br><span class="line"> [    3 37506 12216 26707]</span><br><span class="line"> [    4 77045 47784 13208]]</span><br><span class="line">Distance:</span><br><span class="line">[[0.        4.6321406 4.7470975 4.9057417]</span><br><span class="line"> [0.        4.5316086 4.9688716 5.0430927]</span><br><span class="line"> [0.        5.3778887 5.4905343 5.74937  ]</span><br><span class="line"> [0.        4.600462  4.725213  5.1498823]</span><br><span class="line"> [0.        5.570385  5.8674355 5.8756237]]</span><br><span class="line">##########################################</span><br><span class="line">Index:</span><br><span class="line">[[75581 78376 18034 48733]</span><br><span class="line"> [ 2006 39322 19294 50124]</span><br><span class="line"> [48158 68786 84956 10466]</span><br><span class="line"> [40157 76112 20177 74830]</span><br><span class="line"> [30148 10533 99308 45521]]</span><br></pre></td></tr></table></figure>
<p>为了同后面的加速模式做一下比较, 这里测试一下暴力搜索的速度:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit index.search(xq, k)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">416 ms ± 1.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></pre></td></tr></table></figure>
<h1 id="加速搜索"><a href="#加速搜索" class="headerlink" title="加速搜索"></a>加速搜索</h1><p>下面来讲解Faiss中的加速搜索的方法, Faiss应该在实际工程细节中, 加入了大量的调优, 而这里仅从部分加速算法的角度来进行介绍.</p>
<p>搜索的时间, 一般来说取决于向量的数量以及维度, 不过可能多数时候向量的维度并不会太大, 更多的是取决于向量的数量.</p>
<p>但是这又不是训练模型, 总不能对向量进行抽样吧, 而像KD Tree那样又太费内存, 怎么办呢?</p>
<p>还是那个思路, 如果能够用一些方法, 减小不必要的搜索, 即把一些明显距离较远的向量排除, 再在小范围中搜索, 就会快很多.</p>
<p>Faiss这里的方法简单而有效, 用到了聚类(K-means)的方法. 即先用K-means对向量进行聚类, 每个向量都被划分到一个组中, 当对一个向量进行邻近搜索时, 先根据每个组的中心向量, 来判断哪些组中可能有该向量的邻近向量, 然后再到对应的组中进行精细地搜索.</p>
<p>这里的原理比较简单, 但有一些需要注意的地方. 首先是这种加速是有一定的代价的, 即得到的结果, 相比上面的暴力搜索模式, 可能并不完全一样.</p>
<p>同时, 有两个参数<code>nlist</code>与<code>nprobe</code>, 需要进行尝试与调节, 才能在实际的任务中, 取得较好的结果:</p>
<ul>
<li><p><code>nlist</code></p>
<p>聚类中心的数量.</p>
<p>在<code>nlist</code>给定的情况下, 这个参数越大, 搜索的时间越小.</p>
</li>
<li><p><code>nprobe</code></p>
<p>每次在多少个组中进行搜索.</p>
<p>在<code>nlist</code>给定的情况下, 这个参数越大, 搜索的时间越多, 结果越精准.</p>
</li>
</ul>
<p>Faiss中一个比较重要的索引就是<code>IndexIVFFlat</code>, 利用聚类以及倒排方法进行加速, 在初始化的时候, 还需要一个编码器<code>quantizer</code>, 这里使用<code>IndexFlatL2</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加速搜索</span></span><br><span class="line"></span><br><span class="line">nlist = <span class="number">100</span>  <span class="comment"># 聚类中心数量</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># 需要一个编码器</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line">index.train(xb)</span><br><span class="line">print(<span class="string">'是否已训练:'</span>, index.is_trained)</span><br><span class="line"></span><br><span class="line">index.add(xb)  <span class="comment"># 添加向量</span></span><br><span class="line">index.nprobe = <span class="number">10</span>  <span class="comment"># 设置搜索聚类组群的数量, 默认为1</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[:<span class="number">5</span>])  <span class="comment"># 前5个查询向量的结果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是否已训练: False</span><br><span class="line">是否已训练: True</span><br><span class="line">[[78376 18034 48733  8985]</span><br><span class="line"> [ 2006 19294   430 30718]</span><br><span class="line"> [48158 68786 84956 51185]</span><br><span class="line"> [40157 20177 74830 17291]</span><br><span class="line"> [45521 33644 17126 22048]]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">97.7 ms ± 3.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</span><br></pre></td></tr></table></figure>
<p>对比上面精确模式的约400ms, 这里花费的时间大约是其四分之一, 确实能够加速搜索.</p>
<h1 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h1><p>如果仅仅是能够加速, 是不够的, 还需要在尽量保证精度的情况下, 节省内存占用.</p>
<p>如何建设内存占用呢? 主要就是从向量维度上, 来进行考虑了, 即降维.</p>
<p>一提到降维, 可能就会想到PCA, 它是一个有效的无监督, 线性降维算法. 通俗地来说, 就是把原来高维的特征空间中的点, 映射到低维空间中. 关于PCA的具体算法原理, 还行做更多了解的同学, 可以去网上进行查阅. 在Faiss中, 也有对PCA的实现, 但这不是这里的重点, 就不做展示了~</p>
<p>除了使用PCA算法进行降维, 在Faiss中还有一种关键的算法, 即<a href="https://hal.inria.fr/file/index/docid/514462/filename/paper_hal.pdf" target="_blank" rel="noopener">Product Quantizer</a>, 简称PQ, 下面对PQ的原理进行阐述.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>这里假设有一个$50000\times 1024$的向量矩阵, 每一行为一个$1024$维的向量.</p>
<p>现在把每个向量平均分成$m$个子向量, 假设$m=8$, 每个子向量$128$维, 如图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>对于这8组子向量, 分别使用K-means算法, 并设聚类中心数量为$256$, 结果如下:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>这时候, 可以对每组子向量, 都用聚类的组群的编号(0-255), 来进行编码, 那么原本的一个$1024$维浮点数向量, 就可以用8维无符号整型向量代替:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>这样, 内存占用缩减了多少呢? 原本的字节数为:</p>
<script type="math/tex; mode=display">
1024\times 4=4096</script><p>现在的字节数为:</p>
<script type="math/tex; mode=display">
8\times 1=8</script><p>缩减了512 ($=4096/8$) 倍!</p>
<p>上面的$m$可以用来控制具体的缩减程度, $m$越小, 则越更高缩减内存占用, 但是在搜索精度上会差一些.</p>
<p>那么… 这样转换了以后, 如何进行计算呢, 总不能直接拿着编号来计算吧OvO</p>
<p>要知道, 每组子向量的每个整数编号, 都是映射到一个聚类中心的, 那么在计算距离或者相似度的时候, 就可以将编号转变为真实向量(有损), 再进行计算.</p>
<p>具体有两种方式:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>上图中, $x$表示待查询向量, $y$表示数据集中的向量, $q(\cdot)$表示将原本向量根据聚类, 划分到某个组群的中心向量. 左边的方式, 表示将$x,y$向量都先进行转换, 然后再计算; 右边的方式, 表示使用原始的$x$向量, 和转换后的$y$向量进行计算.</p>
<p>运用PQ的方式, 是能够有效地减少内存占用空间的, 并且也能够和上一节中的加速算法一起结合起来使用, 它们并不冲突♪(^∇^*)</p>
<p>在Faiss中, <code>IndexIVFPQ</code>就是同时使用了PQ算法来减少内存占用, 并也使用聚类和倒排来进行加速. 在原有的基础上, 增加了一个参数$m$, 用以控制子向量的划分个数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 减小内存占用</span></span><br><span class="line"></span><br><span class="line">nlist = <span class="number">100</span></span><br><span class="line">m = <span class="number">8</span>  <span class="comment"># 子向量个数</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="number">8</span>)  <span class="comment"># 8表示每个子向量用8个比特来进行编码</span></span><br><span class="line">index.train(xb)</span><br><span class="line">index.add(xb)</span><br><span class="line">index.nprobe = <span class="number">10</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[38423 69668  2747 90800]</span><br><span class="line"> [30739 11101 36633 18976]</span><br><span class="line"> [84956 55762 43311 48158]</span><br><span class="line"> [ 2845 58680 13501 32725]</span><br><span class="line"> [33644 23032 74010 76153]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%timeit index.search(xq, k)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">46.2 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</span><br></pre></td></tr></table></figure>
<p>从测试结果来看, “降维”后在搜索时间上也有所降低.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在这一篇中, 主要介绍了Faiss这个开源库的相关内容, 包括其用途, 部分特性及原理.</p>
<p>但关于Faiss的内容, 其实还远不止这些, 在真实的场景中, 是否将全部向量放入内存, 可能会用到GPU来加速计算, 使用哪一种索引, 参数如何选择, 在精度, 速度, 内存之间如何平衡, 都是需要试验与分析的.</p>
<p>那么需要优化的点不止一处, 如何进行抉择呢, 我认为一种不错的方法, 是对多数方面, 设定一个阈值, 达到阈值即可, 在此基础上, 对某一项, 或者少数某几项来进行优化, 就像一个带约束的优化问题那样. 比如搜索速度, 内存占用, 都可以设定一个阈值, 而搜索结果的精度, 作为在此基础上的优化目标.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(三)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/</url>
    <content><![CDATA[<p>这一篇中, 将在泰坦尼克数据集(可以在kaggle上下载)上, 利用PySpark进行处理与建模.</p>
<a id="more"></a>
<h1 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .master(<span class="string">'local'</span>) \</span><br><span class="line">                    .appName(<span class="string">'titanic'</span>) \</span><br><span class="line">                    .getOrCreate()</span><br></pre></td></tr></table></figure>
<h1 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.read.csv(<span class="string">'./data/train.csv'</span>, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(df.count())</span><br><span class="line">print(df.printSchema())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">891</span><br><span class="line">root</span><br><span class="line"> |-- PassengerId: integer (nullable = true)</span><br><span class="line"> |-- Survived: integer (nullable = true)</span><br><span class="line"> |-- Pclass: integer (nullable = true)</span><br><span class="line"> |-- Name: string (nullable = true)</span><br><span class="line"> |-- Sex: string (nullable = true)</span><br><span class="line"> |-- Age: double (nullable = true)</span><br><span class="line"> |-- SibSp: integer (nullable = true)</span><br><span class="line"> |-- Parch: integer (nullable = true)</span><br><span class="line"> |-- Ticket: string (nullable = true)</span><br><span class="line"> |-- Fare: double (nullable = true)</span><br><span class="line"> |-- Cabin: string (nullable = true)</span><br><span class="line"> |-- Embarked: string (nullable = true)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    print(df[[col]].describe().show())</span><br></pre></td></tr></table></figure>
<h1 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Survived'</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+-----+</span><br><span class="line">|Survived|count|</span><br><span class="line">+--------+-----+</span><br><span class="line">|       1|  342|</span><br><span class="line">|       0|  549|</span><br><span class="line">+--------+-----+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Sex'</span>).avg(<span class="string">'Survived'</span>).sort(<span class="string">'avg(Survived)'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-------------------+</span><br><span class="line">|   Sex|      avg(Survived)|</span><br><span class="line">+------+-------------------+</span><br><span class="line">|  male|0.18890814558058924|</span><br><span class="line">|female| 0.7420382165605095|</span><br><span class="line">+------+-------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Pclass'</span>).avg(<span class="string">'Survived'</span>).sort(<span class="string">'avg(Survived)'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-------------------+</span><br><span class="line">|Pclass|      avg(Survived)|</span><br><span class="line">+------+-------------------+</span><br><span class="line">|     3|0.24236252545824846|</span><br><span class="line">|     2|0.47282608695652173|</span><br><span class="line">|     1| 0.6296296296296297|</span><br><span class="line">+------+-------------------+</span><br></pre></td></tr></table></figure>
<h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><p>查看哪些列有缺失值, 缺失多少数量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">null_value_count</span><span class="params">(df)</span>:</span></span><br><span class="line">    null_columns_counts = []</span><br><span class="line">    numRows = df.count()</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        nullRows = df.where(df[col].isNull()).count()</span><br><span class="line">        <span class="keyword">if</span>(nullRows &gt; <span class="number">0</span>):</span><br><span class="line">            temp = col, nullRows</span><br><span class="line">            null_columns_counts.append(temp)</span><br><span class="line">    <span class="keyword">return</span> null_columns_counts</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_columns_count_list = null_value_count(df)</span><br><span class="line">spark.createDataFrame(null_columns_count_list, [<span class="string">'Column_With_Null_Value'</span>, <span class="string">'Null_Values_Count'</span>]).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------------+-----------------+</span><br><span class="line">|Column_With_Null_Value|Null_Values_Count|</span><br><span class="line">+----------------------+-----------------+</span><br><span class="line">|                   Age|              177|</span><br><span class="line">|                 Cabin|              687|</span><br><span class="line">|              Embarked|                2|</span><br><span class="line">+----------------------+-----------------+</span><br></pre></td></tr></table></figure>
<p>发现有3列缺失, 且缺失程度不同.</p>
<p>根据缺失程度的不同, 这里采取不同的处理方式. 对于<code>Cabin</code>, 缺失过多, 直接删除. 对于<code>Embarked</code>, 缺失很少, 可使用众数填充. 对于<code>Age</code>, 小部分缺失, 如果直接使用均值填充, 在这里不太好, 注意到有一列<code>Name</code>, 其中有一些称谓如<code>Mr</code>, 可以反映一个人的年龄, 所以可以结合其它信息进行填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">'Name'</span>]].show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">|                Name|</span><br><span class="line">+--------------------+</span><br><span class="line">|Braund, Mr. Owen ...|</span><br><span class="line">|Cumings, Mrs. Joh...|</span><br><span class="line">|Heikkinen, Miss. ...|</span><br><span class="line">|Futrelle, Mrs. Ja...|</span><br><span class="line">|Allen, Mr. Willia...|</span><br><span class="line">|    Moran, Mr. James|</span><br><span class="line">|McCarthy, Mr. Tim...|</span><br><span class="line">|Palsson, Master. ...|</span><br><span class="line">|Johnson, Mrs. Osc...|</span><br><span class="line">|Nasser, Mrs. Nich...|</span><br><span class="line">|Sandstrom, Miss. ...|</span><br><span class="line">|Bonnell, Miss. El...|</span><br><span class="line">|Saundercock, Mr. ...|</span><br><span class="line">|Andersson, Mr. An...|</span><br><span class="line">|Vestrom, Miss. Hu...|</span><br><span class="line">|Hewlett, Mrs. (Ma...|</span><br><span class="line">|Rice, Master. Eugene|</span><br><span class="line">|Williams, Mr. Cha...|</span><br><span class="line">|Vander Planke, Mr...|</span><br><span class="line">|Masselmani, Mrs. ...|</span><br><span class="line">+--------------------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure>
<p>使用正则表达式, 将<code>Name</code>中的称谓提取出来.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_extract</span><br><span class="line">df = df.withColumn(<span class="string">"Initial"</span>,regexp_extract(df[<span class="string">"Name"</span>], <span class="string">"([A-Za-z]+)\."</span>, <span class="number">1</span>))</span><br><span class="line">df[[<span class="string">'Initial'</span>]].distinct().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+</span><br><span class="line">| Initial|</span><br><span class="line">+--------+</span><br><span class="line">|     Don|</span><br><span class="line">|    Miss|</span><br><span class="line">|Countess|</span><br><span class="line">|     Col|</span><br><span class="line">|     Rev|</span><br><span class="line">|    Lady|</span><br><span class="line">|  Master|</span><br><span class="line">|     Mme|</span><br><span class="line">|    Capt|</span><br><span class="line">|      Mr|</span><br><span class="line">|      Dr|</span><br><span class="line">|     Mrs|</span><br><span class="line">|     Sir|</span><br><span class="line">|Jonkheer|</span><br><span class="line">|    Mlle|</span><br><span class="line">|   Major|</span><br><span class="line">|      Ms|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure>
<p>有一些不同的称谓, 其实表达的是同一个意思, 因此这里进行整合.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.replace([<span class="string">'Mlle'</span>,<span class="string">'Mme'</span>, <span class="string">'Ms'</span>, <span class="string">'Dr'</span>,<span class="string">'Major'</span>,<span class="string">'Lady'</span>,<span class="string">'Countess'</span>,<span class="string">'Jonkheer'</span>,<span class="string">'Col'</span>,<span class="string">'Rev'</span>,<span class="string">'Capt'</span>,<span class="string">'Sir'</span>,<span class="string">'Don'</span>],</span><br><span class="line">            [<span class="string">'Miss'</span>,<span class="string">'Miss'</span>,<span class="string">'Miss'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>,  <span class="string">'Mrs'</span>,  <span class="string">'Mrs'</span>,  <span class="string">'Other'</span>,  <span class="string">'Other'</span>,<span class="string">'Other'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>,<span class="string">'Mr'</span>])</span><br><span class="line">df[[<span class="string">'Initial'</span>]].distinct().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+</span><br><span class="line">|Initial|</span><br><span class="line">+-------+</span><br><span class="line">|   Miss|</span><br><span class="line">|  Other|</span><br><span class="line">| Master|</span><br><span class="line">|     Mr|</span><br><span class="line">|    Mrs|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
<p>看一下各个称谓的平均年龄.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">'Initial'</span>).avg(<span class="string">'Age'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+------------------+</span><br><span class="line">|Initial|          avg(Age)|</span><br><span class="line">+-------+------------------+</span><br><span class="line">|   Miss|             21.86|</span><br><span class="line">|  Other|45.888888888888886|</span><br><span class="line">| Master| 4.574166666666667|</span><br><span class="line">|     Mr| 32.73960880195599|</span><br><span class="line">|    Mrs|35.981818181818184|</span><br><span class="line">+-------+------------------+</span><br></pre></td></tr></table></figure>
<p>利用以上结果, 对缺失部分的年龄进行填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> when</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Miss"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">22</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Other"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">46</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Master"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">5</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Mr"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">33</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">"Age"</span>,</span><br><span class="line">    when((df[<span class="string">"Initial"</span>] == <span class="string">"Mrs"</span>) &amp; (df[<span class="string">"Age"</span>].isNull()),</span><br><span class="line">         <span class="number">36</span>).otherwise(df[<span class="string">"Age"</span>]))</span><br></pre></td></tr></table></figure>
<p>填充<code>Embarked</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">"Embarked"</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+-----+</span><br><span class="line">|Embarked|count|</span><br><span class="line">+--------+-----+</span><br><span class="line">|       Q|   77|</span><br><span class="line">|    null|    2|</span><br><span class="line">|       C|  168|</span><br><span class="line">|       S|  644|</span><br><span class="line">+--------+-----+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.fillna(&#123;<span class="string">"Embarked"</span> : <span class="string">'S'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>删除<code>Cabin</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.drop(<span class="string">'Cabin'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="构造特征"><a href="#构造特征" class="headerlink" title="构造特征"></a>构造特征</h1><p>对于<code>SibSp</code>与<code>Parch</code>这两列, 分别表示不与自己同辈以及与自己同辈的亲人数量.</p>
<p>那么可以考虑它们的和,来构造一个特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.withColumn(<span class="string">"Family_Size"</span>, df[<span class="string">'SibSp'</span>] + df[<span class="string">'Parch'</span>])</span><br><span class="line">df.groupBy(<span class="string">"Family_Size"</span>).count().show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-----------+-----+</span><br><span class="line">|Family_Size|count|</span><br><span class="line">+-----------+-----+</span><br><span class="line">|          1|  161|</span><br><span class="line">|          6|   12|</span><br><span class="line">|          3|   29|</span><br><span class="line">|          5|   22|</span><br><span class="line">|          4|   15|</span><br><span class="line">|          7|    6|</span><br><span class="line">|         10|    7|</span><br><span class="line">|          2|  102|</span><br><span class="line">|          0|  537|</span><br><span class="line">+-----------+-----+</span><br></pre></td></tr></table></figure>
<p>进一步可以构造是否一个人的特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df = df.withColumn(<span class="string">'Alone'</span>, lit(<span class="number">0</span>))</span><br><span class="line">df = df.withColumn(<span class="string">"Alone"</span>,</span><br><span class="line">                   when(df[<span class="string">"Family_Size"</span>] == <span class="number">0</span>, <span class="number">1</span>).otherwise(df[<span class="string">"Alone"</span>]))</span><br><span class="line">df.groupBy(<span class="string">'Alone'</span>).avg(<span class="string">'survived'</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-----+-------------------+</span><br><span class="line">|Alone|      avg(survived)|</span><br><span class="line">+-----+-------------------+</span><br><span class="line">|    1|0.30353817504655495|</span><br><span class="line">|    0| 0.5056497175141242|</span><br><span class="line">+-----+-------------------+</span><br></pre></td></tr></table></figure>
<p>从这里可以看到, 独自一人的生存率要…低…??? 懂了, 谈恋爱可以增加生存率!</p>
<h1 id="类别特征编码"><a href="#类别特征编码" class="headerlink" title="类别特征编码"></a>类别特征编码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line">indexers = [StringIndexer(inputCol=column, outputCol=column+<span class="string">"_index"</span>) <span class="keyword">for</span> column <span class="keyword">in</span> [<span class="string">"Sex"</span>,<span class="string">"Embarked"</span>,<span class="string">"Initial"</span>]]</span><br><span class="line">pipeline = Pipeline(stages=indexers)</span><br><span class="line">df = pipeline.fit(df).transform(df)</span><br><span class="line"></span><br><span class="line">df = df.drop(<span class="string">"PassengerId"</span>,<span class="string">"Name"</span>,<span class="string">"Ticket"</span>,<span class="string">"Cabin"</span>,<span class="string">"Embarked"</span>,<span class="string">"Sex"</span>,<span class="string">"Initial"</span>)</span><br><span class="line"></span><br><span class="line">df.columns</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&apos;Survived&apos;,</span><br><span class="line"> &apos;Pclass&apos;,</span><br><span class="line"> &apos;Age&apos;,</span><br><span class="line"> &apos;SibSp&apos;,</span><br><span class="line"> &apos;Parch&apos;,</span><br><span class="line"> &apos;Fare&apos;,</span><br><span class="line"> &apos;Family_Size&apos;,</span><br><span class="line"> &apos;Alone&apos;,</span><br><span class="line"> &apos;Sex_index&apos;,</span><br><span class="line"> &apos;Embarked_index&apos;,</span><br><span class="line"> &apos;Initial_index&apos;]</span><br></pre></td></tr></table></figure>
<h1 id="整合特征-amp-标签"><a href="#整合特征-amp-标签" class="headerlink" title="整合特征 &amp; 标签"></a>整合特征 &amp; 标签</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line">vector_assembler = VectorAssembler(inputCols=df.columns[<span class="number">1</span>:],</span><br><span class="line">                                   outputCol=<span class="string">"features"</span>)</span><br><span class="line">df = feature.transform(df)</span><br><span class="line"></span><br><span class="line">df = df.withColumnRenamed(<span class="string">'Survived'</span>, <span class="string">'label'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集/测试集.</span></span><br><span class="line">train, test = df[[<span class="string">'features'</span>, <span class="string">'label'</span>]].randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>], seed=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h1 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h1><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载逻辑回归模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归实例.</span></span><br><span class="line">lr = LogisticRegression(labelCol=<span class="string">"label"</span>,</span><br><span class="line">                        featuresCol=<span class="string">"features"</span>,</span><br><span class="line">                        maxIter=<span class="number">10</span>,</span><br><span class="line">                        regParam=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型.</span></span><br><span class="line">lr_model = lr.fit(train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型信息.</span></span><br><span class="line"><span class="comment"># print("Coefficients: " + str(lr_model.coefficients))</span></span><br><span class="line">print(<span class="string">"Intercept: "</span> + str(lr_model.intercept))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用模型进行预测.</span></span><br><span class="line">preds = lr_model.transform(test)</span><br><span class="line">preds.printSchema()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- features: vector (nullable = true)</span><br><span class="line"> |-- label: integer (nullable = true)</span><br><span class="line"> |-- rawPrediction: vector (nullable = true)</span><br><span class="line"> |-- probability: vector (nullable = true)</span><br><span class="line"> |-- prediction: double (nullable = false)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># AUC</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>, metricName=<span class="string">'areaUnderROC'</span>)</span><br><span class="line">print(evaluator.getMetricName())</span><br><span class="line">print(evaluator.evaluate(preds))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">areaUnderROC</span><br><span class="line">0.8511334610472546</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(lr.regParam, [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line">             .addGrid(lr.elasticNetParam, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=lr,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8584770114942533</span><br></pre></td></tr></table></figure>
<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> GBTClassifier</span><br><span class="line"></span><br><span class="line">gbdt = GBTClassifier(seed=<span class="number">7</span>)</span><br><span class="line">gbdt_model = gbdt.fit(train)</span><br><span class="line">print(evaluator.evaluate(gbdt_model.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8702107279693482</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(gbdt.maxDepth, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">             .addGrid(gbdt.minInstancesPerNode, [<span class="number">1</span>, <span class="number">20</span>])</span><br><span class="line">             .addGrid(gbdt.maxIter, [<span class="number">20</span>, <span class="number">100</span>])</span><br><span class="line">             .addGrid(gbdt.stepSize, [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">1</span>])</span><br><span class="line">             .addGrid(gbdt.subsamplingRate, [<span class="number">0.7</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=gbdt,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.8785919540229884</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(二)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/</url>
    <content><![CDATA[<p>本章节主要总结下一些简单基础的, 与机器学习相关的内容.</p>
<a id="more"></a>
<h1 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 文本匹配.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> when</span><br><span class="line"></span><br><span class="line">con_0 = df[<span class="string">'col'</span>].like(<span class="string">'%a%'</span>)</span><br><span class="line">con_1 = df[<span class="string">'col'</span>].like(<span class="string">'%b%'</span>)</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, (when(con_0, <span class="number">1</span>).when(con_1, <span class="number">0</span>).otherwise(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split, explode</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'list_'</span>, split(df[<span class="string">'string_col'</span>], <span class="string">', '</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原本1条样本, 拆分成多条.</span></span><br><span class="line">ex_df = df.withColumn(<span class="string">'ex_list'</span>, explode(df[<span class="string">'list_'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载函数.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_replace</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用正则表达式去除标点和数值.</span></span><br><span class="line">df = df.withColumn(<span class="string">'text'</span>, regexp_replace(df[<span class="string">'text'</span>], <span class="string">'[_():;,.!?\\-]'</span>, <span class="string">' '</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'text'</span>, regexp_replace(df[<span class="string">'text'</span>], <span class="string">'[0-9]'</span>, <span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词(使用空格).</span></span><br><span class="line">df = Tokenizer(inputCol=<span class="string">'text'</span>, outputCol=<span class="string">'words'</span>).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StopWordsRemover</span><br><span class="line"></span><br><span class="line">df = StopWordsRemover(inputCol=<span class="string">'words'</span>, outputCol=<span class="string">'words'</span>).transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> HashingTF, IDF</span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF.</span></span><br><span class="line">df = HashingTF(inputCol=<span class="string">'words'</span>, outputCol=<span class="string">'hash'</span>, numFeatures=<span class="number">1024</span>).transform(df)</span><br><span class="line"></span><br><span class="line">df = IDF(inputCol=<span class="string">'hash'</span>, outputCol=<span class="string">'features'</span>).fit(df).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0-1编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Binarizer</span><br><span class="line"></span><br><span class="line">binarizer = Binarizer(threshold=<span class="number">5.0</span>, inputCol=<span class="string">'col'</span>, outputCol=<span class="string">'target'</span>)</span><br><span class="line"></span><br><span class="line">df = binarizer.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分箱编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Bucketizer</span><br><span class="line"></span><br><span class="line">splits = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, float(<span class="string">'Inf'</span>)]</span><br><span class="line">buck = Bucketizer(splits=splits, inputCol=<span class="string">'col'</span>, outputCol=<span class="string">'bucket_col'</span>)</span><br><span class="line"></span><br><span class="line">df_bucket = buck.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热编码.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer, OneHotEncoderEstimator</span><br><span class="line"></span><br><span class="line">string_indexer = StringIndexer(inputCol=<span class="string">'a'</span>, outputCol=<span class="string">'b'</span>)</span><br><span class="line">df = string_indexer.fit(df).transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 直接转换.</span></span><br><span class="line">encoder = OneHotEncoder(dropLast=<span class="literal">False</span>, inputCol=<span class="string">"b"</span>, outputCol=<span class="string">"c"</span>)</span><br><span class="line">df = encoder.transform(df)</span><br><span class="line"><span class="comment">## fit &amp; transform</span></span><br><span class="line">encoder = OneHotEncoderEstimator(inputCols=[<span class="string">"b"</span>], outputCols=[<span class="string">"c"</span>])</span><br><span class="line">model = encoder.fit(df)</span><br><span class="line">df = model.transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一系列数据处理模块.</span></span><br><span class="line">stages = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将入模特征作为向量.</span></span><br><span class="line">assembler = VectorAssembler(inputCols=ft_list, outputCol=<span class="string">"features"</span>)</span><br><span class="line">stages += [assembler]</span><br><span class="line"><span class="comment"># 利用Pipline处理数据.</span></span><br><span class="line">pipeline = Pipeline(stages=stages)</span><br><span class="line">pipeline_fit = pipeline.fit(df)</span><br><span class="line">df = pipeline_fit.transform(df)</span><br></pre></td></tr></table></figure>
<h1 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train, test = df.randomSplit([<span class="number">.8</span>, <span class="number">.2</span>], seed=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载逻辑回归模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归实例.</span></span><br><span class="line">lr = LogisticRegression(labelCol=<span class="string">"label"</span>,</span><br><span class="line">                        featuresCol=<span class="string">"features"</span>,</span><br><span class="line">                        maxIter=<span class="number">10</span>,</span><br><span class="line">                        regParam=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型.</span></span><br><span class="line">linear_fit = lr.fit(train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型信息.</span></span><br><span class="line">print(<span class="string">"Coefficients: "</span> + str(linear_fit.coefficients))</span><br><span class="line">print(<span class="string">"Intercept: "</span> + str(linear_fit.intercept))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用模型进行预测.</span></span><br><span class="line">preds = linear_fit.transform(test)</span><br><span class="line">preds.printSchema()</span><br></pre></td></tr></table></figure>
<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准确率.</span></span><br><span class="line">preds.filter(preds.label == preds.prediction).count() / preds.count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># AUC</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>, metricName=<span class="string">'areaUnderROC'</span>)</span><br><span class="line">print(evaluator.getMetricName())</span><br><span class="line">print(evaluator.evaluate(preds))</span><br></pre></td></tr></table></figure>
<h1 id="交叉验证-amp-网格调参"><a href="#交叉验证-amp-网格调参" class="headerlink" title="交叉验证 &amp; 网格调参"></a>交叉验证 &amp; 网格调参</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建调参网格.</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(lr.regParam, [<span class="number">0.01</span>, <span class="number">0.5</span>])</span><br><span class="line">             .addGrid(lr.elasticNetParam, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建交叉验证.</span></span><br><span class="line">cv = CrossValidator(estimator=lr,</span><br><span class="line">                    estimatorParamMaps=paramGrid,</span><br><span class="line">                    evaluator=evaluator, numFolds=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行交叉验证 &amp; 网格调参.</span></span><br><span class="line">cvModel = cv.fit(train)</span><br><span class="line">print(evaluator.evaluate(cvModel.transform(test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取最佳模型.</span></span><br><span class="line">best_model = cv.bestModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看最佳模型stages.</span></span><br><span class="line">print(best_model.stages)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得最佳模型的LinearRegression参数.</span></span><br><span class="line">best_model.stages[<span class="number">3</span>].extractParamMap()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用最佳模型预测并进行评估.</span></span><br><span class="line">predictions = best_model.transform(test)</span><br><span class="line">evaluator.evaluate(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有超参数组合下, CV上的模型表现.</span></span><br><span class="line">avg_auc = cv.avgMetrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最佳模型的CV表现.</span></span><br><span class="line">best_model_auc = max(cv.avgMetrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最佳模型的某个超参数.</span></span><br><span class="line">opt_regParam = cv.bestModel.explainParam(<span class="string">'maxDepth'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="保存-amp-加载模型"><a href="#保存-amp-加载模型" class="headerlink" title="保存 &amp; 加载模型"></a>保存 &amp; 加载模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型.</span></span><br><span class="line">model.save(<span class="string">'lr.model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression.load(<span class="string">'lr.model'</span>)</span><br></pre></td></tr></table></figure>
<p>下一篇将以一个实际的数据集为例, 利用PySpark完整地做一遍数据处理的操作.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark学习笔记(一)</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/PySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<p>我暂时的工作还没有怎么用到Spark, 但作为一个处理大数据几乎必会的东西, 我认为还是很有必要学习的. 我记得我大概在一年多以前, 一个劲在那学习Hadoop, Spark, Hive什么的, 处理数据的方法没学多少, 搭建分布式环境倒是捣鼓了好久. 后来工作以后我才知道, 术业有专攻, 有一种职位叫做大数据工程师, 像搭建和维护大数据环境这些工作, 交给他们就好了. 如果不是专门做这个的, 了解一下当然好, 不过如果时间有限的话, 应该优先学习自己用的到的部分.</p>
<a id="more"></a>
<h1 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h1><p>Spark是一种集群计算平台, Spark可以让我们分布式地在集群的多个节点上处理数据. 当数据集很大时, 分布式处理会更加地快捷.</p>
<p>那么在考虑是否Spark可以作为一个较好的解决我们问题的方案时, 可以考虑一下两个问题:</p>
<ul>
<li>是否数据量很大, 以至于单机难以处理. 以我的经验, 当数据量达到百万量级时, pandas就会有些吃力了.</li>
<li>处理的方法是否容易被并行化.</li>
</ul>
<p>Spark的整体架构采用的是多个节点相互连接, 其中有一个主节点Master, 对应多个子节点Worker. 主节点主要负责任务的调度管理, 子节点负责实际的计算处理.</p>
<p>Spark可以在多种模式下运行:</p>
<ul>
<li>单机模式.</li>
<li>Standalone, 即Spark自带的集群模式.</li>
<li>YARN等其它模式下运行.</li>
</ul>
<p>考虑到学习成本问题, Spark本身的源码是由Scala写的, 但是拥有Java, R, Python的接口, PySpark就是Python对应的接口.</p>
<h1 id="单机环境安装"><a href="#单机环境安装" class="headerlink" title="单机环境安装"></a>单机环境安装</h1><p>如果想在单机环境下测试运行PySpark, 非常简单, 直接利用<code>pip</code>安装就行. 对于集群环境的安装和配置, 这里暂时不涉及. 当我们要使用一个已经搭建好的Spark集群的时候, 只需要在本地连接上集群使用即可, 与单机环境在表面上没有明显差别.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install pyspqrk</span><br></pre></td></tr></table></figure>
<h1 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h1><p>在老版本的Spark中, 通过<code>SparkContext</code>, <code>SQLContext</code>等来进行连接和交互. 而在新版本中, 老的方式仍然可用, 不过大部分常用的功能整合到了<code>SparkSession</code>这个类里面, 建议使用<code>SparkSession</code>进行连接与交互.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载模块.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建环境(单机).</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .master(<span class="string">'local[*]'</span>) \</span><br><span class="line">                    .appName(<span class="string">'test'</span>) \</span><br><span class="line">                    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印版本号</span></span><br><span class="line">print(spark.version)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结束会话.</span></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># SparkContext可用通过spark获取.</span></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取本次应用名称.</span></span><br><span class="line">app_name = spark.conf.get(<span class="string">'spark.app.name'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取driver端口.</span></span><br><span class="line">driver_tcp_port = spark.conf.get(<span class="string">'spark.driver.port'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取partitions数量.</span></span><br><span class="line">num_partitions = spark.conf.get(<span class="string">'spark.sql.shuffle.partitions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改partitions数量.</span></span><br><span class="line">spark.conf.set(<span class="string">'spark.sql.shuffle.partitions'</span>, <span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>在Spark中, 核心数据结构被称为RDD(Resilient Distributed Dataset), 即弹性分布式数据集. 这是一种偏底层的数据结构, 相对DataFrame不方便进行使用, 但仍然需要学习一些基础的操作.</p>
<ul>
<li><p>构建RDD.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用列表构建.</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用字符串构建.</span></span><br><span class="line">rdd = sc.parallelize(<span class="string">'Hello world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本构建.</span></span><br><span class="line">rdd = sc.textFile(file_path, minPartitions=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看元素信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定数量查看.</span></span><br><span class="line">rdd.take(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看全部元素.</span></span><br><span class="line">rdd.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计元素个数.</span></span><br><span class="line">rdd.count()</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># map操作.</span></span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># filter操作.</span></span><br><span class="line">rdd = rdd.filter(<span class="keyword">lambda</span> x: x == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># flatMap操作.</span></span><br><span class="line">rdd = sc.parallelize([<span class="string">'aaa'</span>, <span class="string">'bbb'</span>, <span class="string">'ccc'</span>])</span><br><span class="line">rdd = rdd.flatMap(<span class="keyword">lambda</span> x: list(x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Pair RDD.</p>
<p>一些时候, 如果数据是成对出现的, 如key-value的形式, 那么Pair RDD可以给到很多方便的处理方式.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用列表构建Pair RDD.</span></span><br><span class="line">rdd = sc.parallelize([(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">6</span>), (<span class="number">4</span>, <span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对相同的key进行reduce.</span></span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用key进行排序.</span></span><br><span class="line">rdd = rdd.sortByKey(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同key的元素进行计数.</span></span><br><span class="line">rdd_count = rdd.countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历输出.</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> rdd.collect(): </span><br><span class="line">    print(<span class="string">"Key &#123;&#125; Value &#123;&#125;"</span>.format(num[<span class="number">0</span>], num[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同元素计数.</span></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> rdd_count.items(): </span><br><span class="line">		print(<span class="string">"key"</span>, k, <span class="string">"has"</span>, v, <span class="string">"counts"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><p>Spark的DataFrame类似于pandas的DataFrame, 同时还支持SQL. Spark对DataFrame进行了大量地优化, 所以在使用时应该尽量使用内部已有的方法.</p>
<h2 id="使用SQL"><a href="#使用SQL" class="headerlink" title="使用SQL"></a>使用SQL</h2><ul>
<li><p>查看在目录中的表.</p>
<p>类似数据库的表, 单机模式没有预设的情况想输出为<code>[]</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>将已有DF加入catalog.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">'table_name'</span>)</span><br><span class="line"></span><br><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用SQL进行查询.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SQL</span></span><br><span class="line">query = <span class="string">"FROM table_name SELECT * LIMIT 10"</span></span><br><span class="line"></span><br><span class="line">df = spark.sql(query)</span><br><span class="line"></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><ul>
<li><p>csv.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create an DataFrame from file_path</span></span><br><span class="line">df = spark.read.csv(file_path, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>parquet.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save the df DataFrame in Parquet format</span></span><br><span class="line">df.write.parquet(<span class="string">'file.parquet'</span>, mode=<span class="string">'overwrite'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the Parquet file into a new DataFrame</span></span><br><span class="line">spark.read.parquet(<span class="string">'file.parquet'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>schema.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import the pyspark.sql.types library</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a new schema using the StructType method</span></span><br><span class="line">people_schema = StructType([</span><br><span class="line">  <span class="comment"># Define a StructField for each field</span></span><br><span class="line">  StructField(<span class="string">'name'</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">  StructField(<span class="string">'age'</span>, IntegerType(), <span class="literal">False</span>),</span><br><span class="line">  StructField(<span class="string">'city'</span>, StringType(), <span class="literal">False</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">df = spark.read.csv(<span class="string">"df.csv"</span>, sep=<span class="string">';'</span>, header=<span class="literal">False</span>, schema=schema)</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="不同数据格式转换"><a href="#不同数据格式转换" class="headerlink" title="不同数据格式转换"></a>不同数据格式转换</h2><ul>
<li><p>RDD转DF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a list of tuples</span></span><br><span class="line">sample_list = [(<span class="string">'Mona'</span>,<span class="number">20</span>), (<span class="string">'Jennifer'</span>,<span class="number">34</span>),(<span class="string">'John'</span>,<span class="number">20</span>), (<span class="string">'Jim'</span>,<span class="number">26</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a RDD from the list</span></span><br><span class="line">rdd = sc.parallelize(sample_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a PySpark DataFrame</span></span><br><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'Name'</span>, <span class="string">'Age'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>Spark DF转换pandas DF.</p>
<p>当数据较小时(百万量级以下), 可以用pandas处理. 或者先抽样, 再使用pandas处理.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽样后再转换.</span></span><br><span class="line">sample_df = df.select([<span class="string">'a'</span>, <span class="string">'b'</span>]).sample(<span class="literal">False</span>, <span class="number">0.5</span>, <span class="number">42</span>)</span><br><span class="line">pandas_df = sample_df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图.</span></span><br><span class="line">sns.lmplot(x=<span class="string">'a'</span>, y=<span class="string">'b'</span>, data=pandas_df)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>pandas DF转换Spark DF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame(df)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><ul>
<li><p>查看有哪些列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看一些统计信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># describe</span></span><br><span class="line">df.describe().show()</span><br><span class="line">df.describe(<span class="string">'a'</span>, <span class="string">'b'</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集大小.</span></span><br><span class="line">df.count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性.</span></span><br><span class="line">df.corr(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独特值.</span></span><br><span class="line">df.select(<span class="string">'col'</span>).distinct().show(<span class="number">40</span>, truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改列名称.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.withColumnRenamed(<span class="string">'old'</span>, <span class="string">'new'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择某些列的数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法一.</span></span><br><span class="line">df = df[[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二.</span></span><br><span class="line">df = df.select(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(df[<span class="string">'a'</span>], df[<span class="string">'b'</span>], df[<span class="string">'c'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三.</span></span><br><span class="line">col = (df[<span class="string">'a'</span>] / (df[<span class="string">'b'</span>] / <span class="number">60</span>)).alias(<span class="string">"col"</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, col)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法四.</span></span><br><span class="line">df = df.selectExpr(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"a/(b/60) as col"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.drop(<span class="string">'a'</span>)</span><br><span class="line"></span><br><span class="line">df = df.drop(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>生成新的一列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过已有列转换.</span></span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, df[<span class="string">'col'</span>] / <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用已有外部数据赋予(不常用), 这里暂时的办法比较繁琐.</span></span><br><span class="line">id_df = df[[<span class="string">'id'</span>]].toPandas()</span><br><span class="line">id_df[<span class="string">'new_col'</span>] = data</span><br><span class="line"></span><br><span class="line">id_df = spark.createDataFrame(id_df)</span><br><span class="line"></span><br><span class="line">df = df.join(id_df, on=<span class="string">'id'</span>, how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>修改数据类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> FloatType, IntegerType, StringType</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'col'</span>, df[<span class="string">'col'</span>].cast(FloatType()))</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>条件筛选.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># .where 与 .filter 一样.</span></span><br><span class="line">df = df.where(df[<span class="string">'col'</span>] &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">df = df.filter(df[<span class="string">'col'</span>] &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可使用SQL.</span></span><br><span class="line">df = df.where(<span class="string">'col &gt; 0'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>聚合.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计一些信息时, 需要先进行groupBy.</span></span><br><span class="line">df.groupBy().min(<span class="string">"col"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对某一列进行groupBy.</span></span><br><span class="line">df_g = df.groupBy(<span class="string">"col"</span>)</span><br><span class="line"></span><br><span class="line">df_g.count().show()</span><br><span class="line">df_g.avg(<span class="string">"a"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载内置函数.</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">df_g = df.groupBy(<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">df_g.avg(<span class="string">'c'</span>).show()</span><br><span class="line"></span><br><span class="line">df_g.agg(F.stddev(<span class="string">'d'</span>)).show()</span><br><span class="line"></span><br><span class="line">df_g.count().sort(<span class="string">"count"</span>,ascending=<span class="literal">True</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>拼接.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 横向拼接.</span></span><br><span class="line">df = df.join(df_1, on=<span class="string">'b'</span>, how=<span class="string">'left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 纵向拼接.</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>交叉表.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.crosstab(<span class="string">'a'</span>, <span class="string">'b'</span>).sort(<span class="string">"a_b"</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>缺失值填充.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.fillna(&#123;<span class="string">"col"</span>: <span class="string">'val'</span>&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>常用函数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># when.</span></span><br><span class="line">df = df.withColumn(<span class="string">'col_1'</span>, (when(con_0, <span class="number">1</span>)</span><br><span class="line">                             .when(con_1, <span class="number">0</span>)</span><br><span class="line">                             .otherwise(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># contains.</span></span><br><span class="line">df = df.filter(df[<span class="string">'col'</span>].contains(<span class="string">'a'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split.</span></span><br><span class="line">df = df.withColumn(<span class="string">'splits'</span>, F.split(df[<span class="string">'col'</span>], <span class="string">'\s+'</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'first_name'</span>, df.splits.getItem(<span class="number">0</span>))</span><br><span class="line">df = df.withColumn(<span class="string">'last_name'</span>, df.splits.getItem(F.size(<span class="string">'splits'</span>) - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># rand.</span></span><br><span class="line"><span class="comment">## 生成随机数.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># monotonically_increasing_id.</span></span><br><span class="line"><span class="comment">## 获得一列递增不重复的列.</span></span><br><span class="line">df = df.withColumn(<span class="string">'ROW_ID'</span>, F.monotonically_increasing_id())</span><br><span class="line"></span><br><span class="line">previous_max_ID = voter_df_march.select(<span class="string">'ROW_ID'</span>).rdd.max()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">df_new = df_new.withColumn(<span class="string">'ROW_ID'</span>, F.monotonically_increasing_id() + previous_max_ID)</span><br></pre></td></tr></table></figure>
</li>
<li><p>UDF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">spark_my_func = F.udf(my_func, IntegerType())</span><br><span class="line">  </span><br><span class="line">df = df.withColumn(<span class="string">'new_col'</span>, spark_my_func(df[<span class="string">'col'</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>持久化.</p>
<p>由于Spark独特的机制, 使得一些时候, 对于主要的并且重复使用的内容进行持久化的操作, 会很大地加快运行速度.</p>
<p>持久化的方法有两个, cache与persist:</p>
<ul>
<li>cache: 使用非序列化的方式将数据全部尝试持久化到内存中.</li>
<li>persist: 手动选择持久化级别. 默认级别同cache.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.cache()</span><br><span class="line"></span><br><span class="line">df.is_cached</span><br><span class="line"></span><br><span class="line">df.unpersist()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><ul>
<li><p>时间转换.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> to_date</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'date'</span>, to_date(<span class="string">'date'</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取日期中的元素.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> dayofweek, year</span><br><span class="line"></span><br><span class="line">df = df.withColumn(<span class="string">'dayofweek'</span>, dayofweek(<span class="string">'date'</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算前后时间间隔.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lag, datediff</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create window</span></span><br><span class="line">w = Window().orderBy(df[<span class="string">'date'</span>])</span><br><span class="line"><span class="comment"># Create lag column</span></span><br><span class="line">df = df.withColumn(<span class="string">'lag-1'</span>, lag(<span class="string">'date'</span>, count=<span class="number">1</span>).over(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate difference between date columns</span></span><br><span class="line">df = df.withColumn(<span class="string">'date_diff'</span>, datediff(<span class="string">'date'</span>, <span class="string">'lag-1'</span>))</span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line">df.select(<span class="string">'date_diff'</span>).distinct().show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>本篇主要总结了基础的RDD, DataFrame的操作方法, 下一篇中将总结一些常见的机器学习相关的方法.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark的安装与使用</title>
    <url>/2020/09/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Spark是使用Scala实现的基于内存计算的大数据开源集群计算环境. 提供了 Java, Scala, Python, R等语言的调用接口. </p>
<p>本篇主要介绍Spark的基本原理, 以及安装流程.</p>
<a id="more"></a>
<h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><p>Spark在英文中的意思, 为火花, 其中有”快”的含义, 而事实上在数据处理上, Spark也确实比基于MapReduce的Hadoop要快很多.</p>
<p>那么, 为什么会快呢? 是因为Hadoop在一次MapReduce运算之后, 会将数据的运算结果从内存写入到磁盘中, 第二次MapReduce运算时在从磁盘中读取数据, 所以其瓶颈在运算间的多余 IO 消耗. 而Spark则是将数据一直缓存在内存中, 直到计算得到最后的结果, 再将结果写入到磁盘, 所以多次运算的情况下, Spark是比较快的.</p>
<p>Spark由伯克利大学研发, 在核心框架 Spark 的基础上, 主要提供四个范畴的计算框架:</p>
<ul>
<li><p>Spark SQL:</p>
<p>提供了类 SQL 的查询, 返回 Spark-DataFrame 的数据结构(类似 Hive).</p>
</li>
<li><p>Spark Streaming:</p>
<p>流式计算,主要用于处理线上实时时序数据(类似 storm).</p>
</li>
<li><p>MLlib:</p>
<p>提供机器学习的各种模型和调优.</p>
</li>
<li><p>GraphX:</p>
<p>提供基于图的算法, 如PageRank.</p>
</li>
</ul>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>在Spark中, RDD(Resilent Distributed Datasets), 俗称弹性分布式数据集, 是 Spark 底层的分布式存储的数据结构, 可以说是 Spark 的核心, Spark API 的所有操作都是基于 RDD 的. 数据不只存储在一台机器上, 而是分布在多台机器上, 实现数据计算的并行化. 弹性的意思是数据丢失时, 可以进行重建. 在Spark 1.5版以后, 新增了数据结构 Spark-DataFrame, 仿造的 R 和Python 的类 SQL 结构DataFrame, 底层为 RDD,  能够让数据从业人员更好的操作 RDD.</p>
<p>RDD中记录了通过哪些操作得到的, 这些操作形成一个有向无环图 DAG(Directed Acyclic Graph),反映RDD之间的依赖关系. 整个计算过程中, 将不需要将中间结果落地到磁盘进行容错, 假如某个节点出错, 则只需要通过DAG关系重新计算即可.</p>
<p>Spark的架构如下, 其实感觉和Hadoop那里的YARN比较相似:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>架构中的一些关键部分:</p>
<ul>
<li><p>Application</p>
<p>用户编写的Spark应用程序, 一个Application包含多个Job.</p>
</li>
<li><p>Driver Program</p>
<p>控制程序, 负责为Application构建DAG图, 并且创建SparkContext.</p>
</li>
<li><p>Cluster Manager</p>
<p>集群资源管理中心, 负责分配计算资源. 可以是由Spark本身控制(Standalone模式), 或者YARN模式, Mesos模式.</p>
</li>
<li><p>Worker Node</p>
<p>工作节点, 负责完成具体计算.</p>
</li>
<li><p>Executor</p>
<p>是运行在工作节点 Worker Node 上的一个进程, 负责运行Task, 并为应用程序存储数据.</p>
</li>
<li><p>Job</p>
<p>作业, 一个Job包含多个RDD及作用于相应RDD上的各种操作.</p>
</li>
<li><p>Stage</p>
<p>阶段, 是Job的基本调度单位, 一个Job会分为多个子Job, 也就是Stage.</p>
</li>
<li><p>Task</p>
<p>任务, 每个Stage中包含一个或多个Task, 运行在Executor上的工作单元.</p>
</li>
</ul>
<p>就是说, Spark的分布式计算架构, 在资源调配上, 其实与YARN比较相似, 所以用YARN作为其集群资源管理, 是自然的想法.</p>
<p>同时, 一个Application包含多个Job, 一个Job包含多个Stage, 一个Stage包含多个Task. 为什么会这样套娃呢? 首先一个Application包含多个Job好理解, 一个应用一般来说都是由一系列过程(函数)来完成的; 然后一个Job包含多个Stage, 这里的Stage的划分, 主要是根据Shuffle操作来定的, 关于Shuffle操作, 后文会再进行介绍; 接着一个Stage包含多个Task, 这里就是并行运算了, 可以简单理解为每次对一部分数据进行某个操作.</p>
<p>关于Spark的常见部署模式, 有单机模式和上面提到的三种模式, 这里主要介绍YARN模式. 此外, 还有<code>client</code>和<code>cluster</code>之分, 它们有什么区别呢?</p>
<p>我们知道, 当在YARN上运行Spark作业时, 每个Spark Executor对应一个YARN容器Container运行. YARN-cluster和YARN-client模式的区别其实就是Application Master进程的区别. 在YARN-cluster模式下, Driver运行在AM(Application Master)中, 它负责向YARN申请资源, 并监督作业的运行状况. 当用户提交了作业之后, 就可以关掉Client, 作业会继续在YARN上运行. 然而YARN-cluster模式不适合运行交互类型的作业.  在YARN-client模式下, Driver运行在Client端, Application Master仅仅向YARN请求Container, Client会和请求的Container通信来调度他们工作, 也就是说Client不能离开. 下面的图形象表示了两者的区别:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p><img src="fig_3.png" alt="fig"></p>
<p>前面说到, Spark是根据Shuffle类算子来进行Stage的划分. 如果我们的代码中执行了某个Shuffle类算子(比如reduceByKey), 那么就会在该算子处, 划分出一个Stage界限来. 可以大致理解为, Shuffle算子执行之前的代码会被划分为一个Stage, Shuffle算子执行以及之后的代码会被划分为下一个Stage. 如下图:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>因此一个Stage刚开始执行的时候, 它的每个Task可能都会从上一个Stage的Task所在的节点, 去通过网络传输拉取需要自己处理的所有key, 然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作, 这个过程就是Shuffle. 正是因为在执行Shuffle操作时, 会涉及到数据传输, 中断原本单个Executor的任务, 所以以此划分Stage. 同时, Shuffle操作是比较耗费资源的, 所以在处理数据时, 要避免使用大量的Shuffle操作.</p>
<p>总结一下Spark的特点:</p>
<ul>
<li><p>高效性.</p>
<p>不同于MapReduce将中间计算结果放入磁盘中, Spark采用内存存储中间计算结果, 减少了迭代运算的磁盘IO. 并通过并行计算DAG图的优化, 减少了不同任务之间的依赖, 降低了延迟等待时间.</p>
</li>
<li><p>易用性.</p>
<p>相比MapReduce仅支持Map和Reduce两种编程算子, Spark提供了超过80种不同的Transformation和Action算子, 并且采用函数式编程风格, 实现相同的功能需要的代码量极大缩小. 同时支持多种编程语言接口.</p>
</li>
<li><p>通用性.</p>
<p>Spark提供了统一的解决方案, Spark可以用于批处理, 交互式查询(Spark SQL), 实时流处理(Spark Streaming), 机器学习(Spark MLlib)和图计算(GraphX). 可以独立使用, 也可以与其它大数据平台(如Hadoop)搭配使用.</p>
</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">官网</a>下载, 如果搭配Hadoop使用, 需要注意一下Spark的版本, 这里选择2.4.7版. 下载后解压放置到<code>/usr/local/spark</code>.</p>
<p>同时在<a href="https://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">官网</a>下载2.11版本的Scala进行安装, 解压后放置到<code>/usr/local/scala</code>. 较新版的Spark内置了Scala, 所以其实不专门安装Scala也可以使用Spark.</p>
<p>配置环境变量<code>.bashrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># scala</span><br><span class="line">export SCALA_HOME=/usr/local/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br><span class="line"></span><br><span class="line"># spark</span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br></pre></td></tr></table></figure>
<p>设置日志目录, 上传jar包.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -mkdir /spark</span><br><span class="line">$ hadoop fs -mkdir /spark/logs</span><br><span class="line">$ hadoop fs -mkdir /spark/jars</span><br><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ hadoop fs -put jars/* /spark/jars/</span><br></pre></td></tr></table></figure>
<p>修改位于<code>/usr/local/spark/conf</code>的配置文件<code>spark-env.sh</code>. 这里主要针对使用YARN作为集群资源管理来进行配置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以YARN作为集群资源管理</span><br><span class="line">export SPARK_CONF_DIR=/usr/local/spark/conf</span><br><span class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://localhost:9000/spark/logs&quot;</span><br><span class="line"></span><br><span class="line"># 为后续使用PySpark进行配置</span><br><span class="line">export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip</span><br><span class="line">export PYSPARK_PYTHON=/home/shy/app/anaconda3/bin/python</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=/home/shy/app/anaconda3/bin/python</span><br></pre></td></tr></table></figure>
<p>修改<code>spark-defaults.conf</code>, 其中的配置为默认配置, 可以在提交任务时另行定义, 会覆盖这里的默认设置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.master                     yarn</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/spark/logs</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.yarn.am.memory             512m</span><br><span class="line">spark.executor.cores             1</span><br><span class="line">spark.executor.memory            512m</span><br><span class="line">spark.executor.instances         4</span><br><span class="line">spark.submit.deployMode          client</span><br><span class="line"># spark.submit.deployMode          cluster</span><br><span class="line">spark.yarn.jars                  hdfs://localhost:9000/spark/jars/*</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br></pre></td></tr></table></figure>
<p>配置<code>slaves</code>, 因为是单机上模拟分布式运算, 所以只添加<code>localhost</code>.</p>
<p>安装Python包, 在使用Jupyter时需要.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ pyspark==2.4.7</span><br></pre></td></tr></table></figure>
<p>在配置得差不多以后, 可以尝试运行自带的小程序, 若出现错误, 则根据错误进行检查. 并且由于Hadoop和Spark都有<code>start-all.sh</code>这个命令, 所以在使用这个命令时最好到对应目录下运行, 或者分开启动各项组件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 因为是基于YARN, 所以先开启Hadoop</span><br><span class="line">$ cd $HADOOP_HOME</span><br><span class="line">$ ./sbin/start-all.sh</span><br><span class="line"># 运行计算圆周率的Spark任务</span><br><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/run-example SparkPi 10</span><br></pre></td></tr></table></figure>
<p>若运行成功, 则会在倒数几行中, 看到如下信息:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pi is roughly 3.144335144335144</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2><p>总体说来, 有两种提交任务的方式, 一种是使用<code>spark-submit</code>命令, 如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/spark-submit \</span><br><span class="line">	--master yarn \</span><br><span class="line"> 	--name &quot;app_name&quot; \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --driver-memory 2g \</span><br><span class="line">  --driver-cores 2 \</span><br><span class="line">  --executor-memory 1g \</span><br><span class="line">  --executor-cores 15 \</span><br><span class="line">  --num-executors 10 \</span><br><span class="line">  job_file.py</span><br></pre></td></tr></table></figure>
<p>另一种方式, 是用过shell来进行交互, 使用PySpark的话, 可以使用命令行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $SPARK_HOME</span><br><span class="line">$ ./bin/pyspark</span><br></pre></td></tr></table></figure>
<p>或者使用Jupyter, 然后加载<code>pyspark</code>包来进行交互.</p>
<h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>这里列举一些常用参数及其意义.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">—master</td>
<td style="text-align:center">指定运行模式. 可选spark://host:port, mesos://host:port, yarn, local等</td>
</tr>
<tr>
<td style="text-align:center">—deploy-mode</td>
<td style="text-align:center">可选client, cluster</td>
</tr>
<tr>
<td style="text-align:center">—name</td>
<td style="text-align:center">应用程序的名称</td>
</tr>
<tr>
<td style="text-align:center">—conf</td>
<td style="text-align:center">指定的spark配置属性, 以—conf spark.xxx=xxx来进行配置</td>
</tr>
<tr>
<td style="text-align:center">—driver-cores</td>
<td style="text-align:center">Driver的核数, 默认是1</td>
</tr>
<tr>
<td style="text-align:center">—driver-memory</td>
<td style="text-align:center">Driver内存, 默认1G</td>
</tr>
<tr>
<td style="text-align:center">—executor-core</td>
<td style="text-align:center">每个executor的核数</td>
</tr>
<tr>
<td style="text-align:center">—executor-memory</td>
<td style="text-align:center">每个executor的内存，默认是1G</td>
</tr>
<tr>
<td style="text-align:center">—num-executors</td>
<td style="text-align:center">启动的executor数量, 默认为2</td>
</tr>
</tbody>
</table>
</div>
<h2 id="运行调优"><a href="#运行调优" class="headerlink" title="运行调优"></a>运行调优</h2><p>在上文中提到编写Spark程序时, 要尽量避免Shuffle操作, 这其实就算一种调优, 此外想要让Spark程序更加顺畅地运行, 还有一些方法. 下面列举一些可以优化的地方.</p>
<ul>
<li><p>Shuffle的优化.</p>
<p>举一个栗子, 下面两个操作从结果上来说是等价的:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 操作一</span></span><br><span class="line">rdd.groupByKey().mapValues(_.sum)</span><br><span class="line"><span class="comment">// 操作二</span></span><br><span class="line">rdd.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure>
<p>但是操作一需要把全部的数据通过网络传递一遍, 而操作二根据每个 key 局部的 partition 累积结果, 在 shuffle 的之后把局部的累积值相加后得到结果, 更加高效.</p>
<p>更多的优化方法, 需要实践以及对Spark的深刻理解, 这里不做深入讲解.</p>
</li>
<li><p>缓存优化.</p>
<p>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的: 每次你对一个RDD执行一个算子操作时, 都会重新从源头处计算一遍, 计算出那个RDD来, 然后再对这个RDD执行你的算子操作. 这种方式的性能是很差的. 因此对于这种情况, 建议是对多次使用的RDD进行持久化.</p>
<p>举一个栗子:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file_path"</span>).<span class="type">Map</span>(...).filter(...)</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.count()</span><br></pre></td></tr></table></figure>
<p>对应的DAG图为:</p>
<p><img src="fig_5.png" alt="fig"></p>
<p>而如果对RDD使用<code>cache</code>函数进行缓存, 就不用再从头计算了.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file_path"</span>).<span class="type">Map</span>(...).filter(...).cache()</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd.<span class="type">Map</span>(...)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.count()</span><br></pre></td></tr></table></figure>
<p>对应的DAG图为:</p>
<p><img src="fig_6.png" alt="fig"></p>
<p>此外要认识到: <code>cache</code>的RDD会一直占用内存, 当后期不需要再依赖时, 可以使用<code>unpersist</code>释放掉.</p>
</li>
<li><p>参数优化.</p>
<p>根据可用的计算资源和处理问题的规模, 设定合适的参数, 可以在充分利用资源的同时, 加速计算的流程.</p>
<ul>
<li><p>num-executors</p>
<p>默认的数量是很少的, 一般较大的任务可以设置几十个或者上百个的Executor. 设置太少无法利用集群资源, 设置太多可能出现闲置Executor的情况.</p>
</li>
<li><p>executor-memory</p>
<p>单个Executor如果太小, 则容易出现OOM, 设置大一些如4G~8G比较合适. 一般num-executors乘以executor-memory为总共申请的内存资源, 这个资源最好不要超过最大内存的1/3~1/2, 否则可能导致其他同学的程序无法正常运行.</p>
</li>
<li><p>executor-core</p>
<p>CPU核心的数量决定了Executor并行执行Task的能力, 一般设置为2~4个较为合适. 同样的, num-executors乘以executor-core为总共申请的CPU资源, 最好也不要超过CPU核数的1/3~1/2.</p>
</li>
<li><p>driver-memory</p>
<p>一般来说Driver不需要太大内存, 1G就够了. 但是如果需要使用<code>collect</code>算子将数据拉到Driver上处理, 那么需要将内存设置大一些, 以免出现OOM的问题.</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB的安装与使用</title>
    <url>/2020/09/07/%E5%A4%A7%E6%95%B0%E6%8D%AE/MongoDB%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>MongoDB 是一个基于分布式文件存储的数据库, 由 C++ 语言编写. 旨在为 WEB 应用提供可扩展的高性能数据存储解决方案.</p>
<p>MongoDB 是一个介于关系数据库和非关系数据库之间的产品, 是非关系数据库当中功能最丰富, 最像关系数据库的.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>MongoDB首先是属于NoSQL的, 那什么是NoSQL呢.</p>
<p>在现代的计算系统上每天网络上都会产生庞大的数据量, 这些数据可以由关系数据库管理系统(RDBMS)来处理. 但是随着大数据时代的来临, 数据量越来越大, 数据维度也越来越多, 数据类型各式各样, 这时完全依赖比较”古板”的RDBMS就有问题.</p>
<p>NoSQL, 指的是非关系型的数据库, 有时也称作Not Only SQL的缩写, 是对不同于传统的关系型数据库的数据库管理系统的统称.</p>
<p>NoSQL用于超大规模数据的存储, 这些类型的数据存储不需要固定的模式, 无需多余操作就可以横向扩展.</p>
<p>MongoDB是由C++语言编写的, 是一个基于分布式文件存储的开源数据库系统, 是当前NoSQL数据库产品中最热门的一种. 在高负载的情况下, 添加更多的节点, 可以保证服务器性能. MongoDB旨在为WEB应用提供可扩展的高性能数据存储解决方案.</p>
<p>MongoDB将数据存储为一个文档, 数据结构由键值(key =&gt; value)对组成. MongoDB 文档类似于JSON对象.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>MongoDB的主要目标是在键/值存储方式(提供了高性能和高度伸缩性)以及传统的RDBMS系统(丰富的功能)架起一座桥梁, 集两者的优势于一身. 比较适用于如下场景:</p>
<ul>
<li>实时的插入，更新与查询.</li>
<li>由于性能很高, 也适合作为信息基础设施的缓存层.</li>
<li>高伸缩性, 非常适合由数十或数百台服务器组成的数据库.</li>
<li>BSON数据格式非常适合文档化格式的存储及查询.</li>
</ul>
<p>但是也有一些限制:</p>
<ul>
<li>高度事务性的系统, 如银行系统.</li>
<li>不能直接使用SQL.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p><a href="https://www.mongodb.com/try/download/community?jmp=nav" target="_blank" rel="noopener">官网</a>下载对应压缩包, 解压后可改文件名为<code>mongodb</code>放入<code>/usr/local/</code>中.</p>
<p>在过程中可能会遇到几次Mac的检查, 在”安全与隐私”里面点允许即可.</p>
<p>修改shell的配置文件, 这里是<code>.zshrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mongodb</span><br><span class="line">alias mongo=/usr/local/mongodb/bin/mongo</span><br><span class="line">alias mongod=&quot;sudo /usr/local/mongodb/bin/mongod --dbpath=/Users/shy/opt/data/mongodb&quot;</span><br></pre></td></tr></table></figure>
<p>MongoDB有一个默认的数据库储存目录<code>/data/db/</code>, 但是在根目录创建文件需要额外权限, 而每次都指定另外的目录又比较麻烦, 所以用<code>alias</code>进行指代.</p>
<p>在修改了配置后, 使用<code>source .zshrc</code>激活, 然后启动MongoDB:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongod</span><br></pre></td></tr></table></figure>
<p>会发现MongoDB处于监听状态, 然后再在另外一个端口进入MongoDB的shell界面:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br></pre></td></tr></table></figure>
<p>若要关闭MongoDB, 比较好的方式是在MongoDB的shell界面输入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use admin;</span><br><span class="line">&gt; db.shutdownServer();</span><br><span class="line">&gt; exit;</span><br></pre></td></tr></table></figure>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><p>直接使用命令行安装, 版本可能不是最新的, 但是简单.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install mongodb</span><br></pre></td></tr></table></figure>
<p>通过如下命令可以查看是否启动, 一般安装好以后默认启动.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ps aux | grep mongo</span><br></pre></td></tr></table></figure>
<p>进入MongoDB的shell:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br></pre></td></tr></table></figure>
<p>启动, 关闭MongoDB:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">$ service mongodb start</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">$ service mongodb stop</span><br></pre></td></tr></table></figure>
<p>配置远程连接, 修改配置文件<code>/etc/mongodb.conf</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在bind_ip后面用逗号隔开, 添加绑定的服务器(Linux本机)的地址</span><br><span class="line">bind_ip = 127.0.0.1,服务器IP</span><br><span class="line">port = 27017</span><br></pre></td></tr></table></figure>
<p>这样在其它机器上, 可使用如下代码进行连接:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo 服务器IP:27017</span><br></pre></td></tr></table></figure>
<p>添加用户及密码.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use admin</span><br><span class="line"># 添加一名超级用户, 拥有所有权限</span><br><span class="line">&gt; db.createUser(</span><br><span class="line">... &#123;</span><br><span class="line">... user: &quot;name&quot;,</span><br><span class="line">... pwd: &quot;passwd&quot;,</span><br><span class="line">... roles: [&quot;root&quot;]</span><br><span class="line">... &#125;</span><br><span class="line">... )</span><br><span class="line"># 添加一名拥有所有数据库读写权限的用户</span><br><span class="line">&gt; db.createUser(</span><br><span class="line">... &#123;</span><br><span class="line">... user: &quot;name&quot;,</span><br><span class="line">... pwd: &quot;passwd&quot;,</span><br><span class="line">... roles: [&quot;&quot;readWriteAnyDatabase&quot;]</span><br><span class="line">... &#125;</span><br><span class="line">... )</span><br></pre></td></tr></table></figure>
<p>关于更多添加用户的方法, 可以参考<a href="https://segmentfault.com/a/1190000015603831" target="_blank" rel="noopener">这里</a>.</p>
<p>将配置文件中设置进行修改, 需要进行验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Turn on/off security.  Off is currently the default</span><br><span class="line">#noauth = true</span><br><span class="line">auth = true</span><br></pre></td></tr></table></figure>
<p>然后使用账户密码进行登录:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo 服务器IP:27017 -u &quot;name&quot; -p &quot;passwd&quot; -authenticationDatabase &quot;admin&quot;</span><br></pre></td></tr></table></figure>
<p>或者先登录, 然后再验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br><span class="line">&gt; use admin</span><br><span class="line">&gt; db.auth(&quot;name&quot;, &quot;passwd&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul>
<li><p>查看现有数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; show dbs</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; use db_name</span><br></pre></td></tr></table></figure>
<p>若存在则使用, 若不存在则创建. 不过创建后, 在没有数据的情况下, 使用<code>show dbs</code>不会显示.</p>
</li>
<li><p>删除数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.dropDatabase()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><p>MongoDB中的集合, 可以类比MySQL中的数据表.</p>
<ul>
<li><p>查看集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; show collections</span><br><span class="line"># 或者</span><br><span class="line">&gt; show tables</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.createCollection(name)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除集合.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.drop()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p>MongoDB里面的文档, 可以类比于MySQL中的一条数据.</p>
<ul>
<li><p>插入文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 插入一条数据</span><br><span class="line">&gt; db.table_name.insert(dict)</span><br><span class="line"># 插入多条数据</span><br><span class="line">&gt; db.table_name.insert([dict_0, dict_1, ...])</span><br></pre></td></tr></table></figure>
</li>
<li><p>查找文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(条件, 显示的字段)</span><br><span class="line">&gt; db.table_name.find(&#123;key: val, ...&#125;, &#123;key: 1, ...&#125;)</span><br></pre></td></tr></table></figure>
<p>比较, 大于<code>$gt</code>, 大于等于<code>$gte</code>, 小于<code>$lt</code>, 小于等于<code>$lte</code>, 等于<code>$eq</code>, 不等于<code>$ne</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$gt: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>在指定的元素里面, <code>$in</code>, 不在为<code>$nin</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$in: [val_0, val_1, ...]&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>列表完全匹配, <code>$all</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$all: [val_0, val_1, ...]&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>正则匹配查找, <code>$regex</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find(&#123;key: &#123;$regex: 正则表达式&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>条件否, 条件或, 条件且.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 非</span><br><span class="line">&gt; db.table_name.find(&#123;key: &#123;$not: &#123;$gt: val&#125;&#125;&#125;)</span><br><span class="line"></span><br><span class="line"># 且</span><br><span class="line">&gt; db.table_name.find(&#123;key_0: 条件_0, key_1: 条件_1&#125;)</span><br><span class="line"></span><br><span class="line"># 或</span><br><span class="line">&gt; db.table_name.find(&#123;$or: [&#123;key_0: 条件_0&#125;, &#123;key_1: 条件_1&#125;]&#125;)</span><br></pre></td></tr></table></figure>
<p>设置跳过, 显示条数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find().skip(10).limit(5)</span><br></pre></td></tr></table></figure>
<p>排序.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.find().sort(&#123;key: 1 or -1&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(查找条件, 修改方式)</span><br></pre></td></tr></table></figure>
<p>直接设置.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$set: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>增加值, 加法.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$inc: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>删除某个key.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$unset: &#123;key: 1&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>列表中的值.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$push: &#123;key: val&#125;&#125;)</span><br><span class="line"># 删除</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$pop: &#123;key: 1 or -1&#125;&#125;)</span><br><span class="line"># 指定删除</span><br><span class="line">&gt; db.table_name.update(&#123;key: val&#125;, &#123;$pull: &#123;key: val&#125;&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除文档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.table_name.remove(查找条件)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>使用PyMongo这个包, 可以用于连接与使用MongoDB.</p>
<ul>
<li><p>安装.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install pymongo</span><br></pre></td></tr></table></figure>
</li>
<li><p>连接.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接</span></span><br><span class="line">mongo_client = pymongo.MongoClient(<span class="string">'mongodb://user_id:passwd@服务器IP:端口号/'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前数据库</span></span><br><span class="line">print(mongo_client.list_database_names())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择数据库</span></span><br><span class="line">mongo_db = mongo_client[<span class="string">'db_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择集合</span></span><br><span class="line">mongo_table = mongo_db[<span class="string">'test_table'</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>文档查询.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查询一条</span></span><br><span class="line">print(mongo_table.find_one())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询所有</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> mongo_table.find():</span><br><span class="line">    print(i)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 按条件查询</span></span><br><span class="line">mongo_table.find(&#123;查询条件&#125;, &#123;显示字段&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>以表格DataFrame形式返回.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(list(mongo_table.find(condition, fields)))</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>结构化查询语言SQL</title>
    <url>/2020/09/06/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80SQL/</url>
    <content><![CDATA[<p>什么是SQL呢, 即Structured Query Language, 是一种语言, 可以让专业的或者非专业的人员, 对关系型数据库(如MySQL)中的数据进行查询, 处理, 转换等操作.</p>
<p>SQL本身比较简单, 同时用途也很广, 除了关系型数据库外, 一些其它系统(如Hive)在查询和处理数据时, 也是使用的SQL的语法规则. 所以掌握基础的SQL使用方法, 是很有必要的.</p>
<a id="more"></a>
<p>一般来说, 我认为有两种使用SQL的方式, 一种是针对某个任务, 全程使用SQL, 最终得到想要的结果, 可以是一个很复杂的SQL, 或者多个SQL按顺序执行; 一种是首先使用SQL将需要的数据提取出来, 然后使用其它一些工具(如Python)进行处理. 根据场景选择不同的方式, 我个人倾向于第二种(才不是我SQL比较菜呢QAQ</p>
<h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><h2 id="简单的查询"><a href="#简单的查询" class="headerlink" title="简单的查询"></a>简单的查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query for a specific columns</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable;</span><br><span class="line"></span><br><span class="line"># Select query for all columns</span><br><span class="line">SELECT * </span><br><span class="line">FROM mytable;</span><br></pre></td></tr></table></figure>
<h2 id="带条件的查询"><a href="#带条件的查询" class="headerlink" title="带条件的查询"></a>带条件的查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with constraints</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition</span><br><span class="line">    AND/OR another_condition</span><br><span class="line">    AND/OR …;</span><br></pre></td></tr></table></figure>
<p>常用的一些与数值相关的条件操作有:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">operator</th>
<th style="text-align:center">example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=, !=, &lt; &lt;=, &gt;, &gt;=</td>
<td style="text-align:center">col_name <strong>!=</strong> 4</td>
</tr>
<tr>
<td style="text-align:center">BETWEEN … AND …</td>
<td style="text-align:center">col_name <strong>BETWEEN</strong> 1.5 <strong>AND</strong> 10.5</td>
</tr>
<tr>
<td style="text-align:center">NOT BETWEEN … AND …</td>
<td style="text-align:center">col_name <strong>NOT BETWEEN</strong> 1 <strong>AND</strong> 10</td>
</tr>
<tr>
<td style="text-align:center">IN (…)</td>
<td style="text-align:center">col_name <strong>IN</strong> (2, 4, 6)</td>
</tr>
<tr>
<td style="text-align:center">NOT IN (…)</td>
<td style="text-align:center">col_name <strong>NOT IN</strong> (1, 3, 5)</td>
</tr>
</tbody>
</table>
</div>
<p>常用的一些与文本相关的条件操作有:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">operator</th>
<th style="text-align:center">example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td style="text-align:center">col_name <strong>=</strong> “abc”</td>
</tr>
<tr>
<td style="text-align:center">!= or &lt;&gt;</td>
<td style="text-align:center">col_name <strong>!=</strong> “abcd”</td>
</tr>
<tr>
<td style="text-align:center">LIKE</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “ABC”</td>
</tr>
<tr>
<td style="text-align:center">NOT LIKE</td>
<td style="text-align:center">col_name <strong>NOT LIKE</strong> “ABCD”</td>
</tr>
<tr>
<td style="text-align:center">%</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “%AT%”<br>(matches “AT”, “ATTIC”, “CAT” or even “BATS”)</td>
</tr>
<tr>
<td style="text-align:center">_</td>
<td style="text-align:center">col_name <strong>LIKE</strong> “AN_”<br>(matches “AND”, but not “AN”)</td>
</tr>
<tr>
<td style="text-align:center">IN (…)</td>
<td style="text-align:center">col_name <strong>IN</strong> (“A”, “B”, “C”)</td>
</tr>
<tr>
<td style="text-align:center">NOT IN (…)</td>
<td style="text-align:center">col_name <strong>NOT IN</strong> (“D”, “E”, “F”)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with ordered results</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column ASC/DESC;</span><br></pre></td></tr></table></figure>
<h2 id="设置查询范围"><a href="#设置查询范围" class="headerlink" title="设置查询范围"></a>设置查询范围</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with limited rows</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column ASC/DESC</span><br><span class="line">LIMIT num_limit OFFSET num_offset;</span><br></pre></td></tr></table></figure>
<h2 id="多表联合查询"><a href="#多表联合查询" class="headerlink" title="多表联合查询"></a>多表联合查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with INNER/LEFT/RIGHT/FULL JOINs on multiple tables</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">INNER/LEFT/RIGHT/FULL JOIN another_table </span><br><span class="line">    ON mytable.id = another_table.matching_id</span><br><span class="line">WHERE condition(s)</span><br><span class="line">ORDER BY column, … ASC/DESC</span><br><span class="line">LIMIT num_limit OFFSET num_offset;</span><br></pre></td></tr></table></figure>
<h2 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with constraints on NULL values</span><br><span class="line">SELECT column, another_column, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE column IS/IS NOT NULL</span><br><span class="line">AND/OR another_condition</span><br><span class="line">AND/OR …;</span><br></pre></td></tr></table></figure>
<h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with expression aliases</span><br><span class="line">SELECT col_expression AS expr_description, …</span><br><span class="line">FROM mytable;</span><br></pre></td></tr></table></figure>
<h2 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h2><ul>
<li><p>获取字符个数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LENGTH (&apos;abc&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>拼接字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT (&apos;abc&apos;, &apos;def&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串大写.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT UPPER(&apos;ABC&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串小写.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LOWER(&apos;ABC&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>截取子字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUBSTR(&apos;abcd&apos;, 1, 3);</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断子字符串.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 若存在, 则返回起始位置, 否则返回0</span><br><span class="line">SELECT INSTR(&apos;abcd&apos;, &apos;cd&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>去除首尾空格.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 则表示去除空格</span><br><span class="line">SELECT TRIM(&apos;   abc##d# &apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符填充.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 用指定字符左填充到指定长度</span><br><span class="line">SELECT LPAD(&apos;abc&apos;, 4, &apos;#&apos;);</span><br><span class="line"># 用指定字符右填充到指定长度</span><br><span class="line">SELECT RPAD(&apos;abc&apos;, 4, &apos;#&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符替换.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT REPLACE(&apos;abc&apos;, &apos;b&apos;, &apos;#&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>小数近似.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 四舍五入</span><br><span class="line">SELECT ROUND(3.1415);</span><br><span class="line"># 保留指定位数</span><br><span class="line">SELECT ROUND(3.1415, 2);</span><br><span class="line"># 向上取整</span><br><span class="line">SELECT CEIL(-1.01);</span><br><span class="line"># 向下取整</span><br><span class="line">SELECT FLOOR(9.99);</span><br><span class="line"># 截断</span><br><span class="line">SELECT TRUNCATE(1.6699, 2);</span><br></pre></td></tr></table></figure>
</li>
<li><p>取模.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MOD(10, 3);</span><br></pre></td></tr></table></figure>
</li>
<li><p>日期相关.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前日期+时间</span><br><span class="line">SELECT NOW();</span><br><span class="line"># 当前日期</span><br><span class="line">SELECT CURDATE();</span><br><span class="line"># 当前时间</span><br><span class="line">SELECT CURTIME();</span><br><span class="line"># 返回日期对应星期的英文名字</span><br><span class="line">SELECT DAYNAME(NOW());</span><br><span class="line"># 字符串转时间</span><br><span class="line">SELECT STR_TO_DATE(&apos;2020-01-02&apos;, &apos;%Y-%m-%d&apos;);</span><br><span class="line"># 时间转字符串</span><br><span class="line">SELECT DATE_FORMAT(NOW(), &apos;%Y-%m-%d&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>流程控制.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># IF</span><br><span class="line">SELECT IF(1&gt;0, &apos;yes&apos;, &apos;no&apos;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>其它.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取版本号</span><br><span class="line">SELECT VERSION();</span><br><span class="line"># 获取当前数据库</span><br><span class="line">SELECT DATABASE();</span><br><span class="line"># 获取登录用户名</span><br><span class="line">SELECT USER();</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Select query with aggregate functions over all rows</span><br><span class="line">SELECT AGG_FUNC(column_or_expression) AS aggregate_description, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE constraint_expression;</span><br><span class="line"></span><br><span class="line"># Select query with HAVING constraint</span><br><span class="line">SELECT group_by_column, AGG_FUNC(column_expression) AS aggregate_result_alias, …</span><br><span class="line">FROM mytable</span><br><span class="line">WHERE condition</span><br><span class="line">GROUP BY column</span><br><span class="line">HAVING group_condition;</span><br></pre></td></tr></table></figure>
<p>常用的聚合函数:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">function</th>
<th style="text-align:center">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">COUNT(*) or COUNT(col)</td>
<td style="text-align:center">统计非NULL数据条数.</td>
</tr>
<tr>
<td style="text-align:center">MIN(col)</td>
<td style="text-align:center">取最大值.</td>
</tr>
<tr>
<td style="text-align:center">MAX(col)</td>
<td style="text-align:center">取最小值.</td>
</tr>
<tr>
<td style="text-align:center">AVG(col)</td>
<td style="text-align:center">取非NULL的平均值.</td>
</tr>
<tr>
<td style="text-align:center">SUM(col)</td>
<td style="text-align:center">取总和.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用另外一个查询结果, 来作为本次查询的条件</span><br><span class="line">SELECT *</span><br><span class="line">FROM sales_associates</span><br><span class="line">WHERE salary &gt; </span><br><span class="line">   (SELECT AVG(revenue_generated)</span><br><span class="line">    FROM sales_associates);</span><br></pre></td></tr></table></figure>
<h2 id="查询执行顺序"><a href="#查询执行顺序" class="headerlink" title="查询执行顺序"></a>查询执行顺序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Complete SELECT query</span><br><span class="line">SELECT DISTINCT column, AGG_FUNC(column_or_expression), …</span><br><span class="line">FROM mytable</span><br><span class="line">    JOIN another_table</span><br><span class="line">      ON mytable.column = another_table.column</span><br><span class="line">    WHERE constraint_expression</span><br><span class="line">    GROUP BY column</span><br><span class="line">    HAVING constraint_expression</span><br><span class="line">    ORDER BY column ASC/DESC</span><br><span class="line">    LIMIT count OFFSET COUNT;</span><br></pre></td></tr></table></figure>
<p>执行顺序如下:</p>
<ul>
<li>FROM AND JOIN.</li>
<li>WHERE.</li>
<li>GROUP BY.</li>
<li>HAVING.</li>
<li>SELECT.</li>
<li>DISTINCT.</li>
<li>ORDER BY.</li>
<li>LIMIT / OFFSET.</li>
</ul>
<h1 id="添加"><a href="#添加" class="headerlink" title="添加"></a>添加</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Insert statement with values for all columns</span><br><span class="line">INSERT INTO mytable</span><br><span class="line">VALUES (value_or_expr, another_value_or_expr, …),</span><br><span class="line">       (value_or_expr_2, another_value_or_expr_2, …),</span><br><span class="line">       …;</span><br><span class="line">       </span><br><span class="line"># Insert statement with specific columns</span><br><span class="line">INSERT INTO mytable</span><br><span class="line">(column, another_column, …)</span><br><span class="line">VALUES (value_or_expr, another_value_or_expr, …),</span><br><span class="line">      (value_or_expr_2, another_value_or_expr_2, …),</span><br><span class="line">      …;</span><br></pre></td></tr></table></figure>
<h1 id="更改"><a href="#更改" class="headerlink" title="更改"></a>更改</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Update statement with values</span><br><span class="line">UPDATE mytable</span><br><span class="line">SET column = value_or_expr, </span><br><span class="line">    other_column = another_value_or_expr, </span><br><span class="line">    …</span><br><span class="line">WHERE condition;</span><br></pre></td></tr></table></figure>
<h1 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Delete statement with condition</span><br><span class="line">DELETE FROM mytable</span><br><span class="line">WHERE condition;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>编程基础</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>REmote DIctionary Server (Redis) 是一个开源的key - value存储系统, 使用C语言编写, 可基于内存亦可持久化的数据库, 并提供多种语言的API.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>Redis 与其他 key-value 缓存产品有以下三个特点：</p>
<ul>
<li><p>支持数据的持久化.</p>
<p>可以将内存中的数据保存在磁盘中, 重启的时候可以再次加载进行使用.</p>
</li>
<li><p>支持多种数据结构.</p>
<p>不仅仅支持简单的key-value类型的数据, 同时还提供list, set, zset, hash等数据结构的存储.</p>
</li>
<li><p>Redis支持数据的备份.</p>
<p>即master-slave模式的数据备份.</p>
</li>
</ul>
<p>此外, Redis还有如下一些优势:</p>
<ul>
<li><p>性能极高.</p>
<p>Redis能读的速度约100000次/s, 写的速度约80000次/s .</p>
</li>
<li><p>原子性.</p>
<p>Redis的所有操作都是原子性的.</p>
</li>
</ul>
<p>因此, Redis可以作为一个单独的数据库使用, 也可以配合其它数据库(如MySQL)使用. 可以作为缓存层, 将部分数据存入Redis中, 优先对Redis进行查询, 当查询不到时, 再到全量数据库(如MySQL)中进行查询.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p>在<a href="https://redis.io/download" target="_blank" rel="noopener">官网</a>下载, 移到指定目录, 然后解压后编译.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-6.0.7.tar.gz</span><br><span class="line">$ mv redis-6.0.7.tar.gz /usr/local/</span><br><span class="line">$ cd /usr/local</span><br><span class="line">$ sudo tar -zxvf redis-6.0.7.tar.gz</span><br><span class="line">$ sudo mv redis-6.0.7 redis</span><br><span class="line">$ cd redis</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo test make</span><br></pre></td></tr></table></figure>
<p>修改配置文件<code>/etc/local/redis/redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Log file</span><br><span class="line">logfile /usr/local/redis/redis.log</span><br><span class="line"></span><br><span class="line"># The working directory</span><br><span class="line">dir /usr/local/redis/redis_dbfile/</span><br></pre></td></tr></table></figure>
<p>并检查以上目录/文件的权限, 保证有读写的权利.</p>
<p>修改<code>.zshrc</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis</span><br><span class="line">alias redis-server=/usr/local/redis/src/redis-server</span><br><span class="line">alias redis-cli=/usr/local/redis/src/redis-cli</span><br></pre></td></tr></table></figure>
<p>启动Redis.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-server</span><br></pre></td></tr></table></figure>
<p>关闭Redis, 可使用<code>kill 进程号</code>的方式, 也可以:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IP:PORT&gt; SHUTDOWN</span><br></pre></td></tr></table></figure>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><p>同样在<a href="https://redis.io/download" target="_blank" rel="noopener">官网</a>下载, 移到指定目录, 然后解压后编译.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-6.0.7.tar.gz</span><br><span class="line">$ mv redis-6.0.7.tar.gz /usr/local/</span><br><span class="line">$ cd /usr/local</span><br><span class="line">$ sudo tar -zxvf redis-6.0.7.tar.gz</span><br><span class="line">$ sudo mv redis-6.0.7 redis</span><br><span class="line">$ cd redis</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo make test</span><br></pre></td></tr></table></figure>
<p>若出现<code>make</code>未安装的错误:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install make</span><br><span class="line">$ sudo apt-get install pkg-config</span><br><span class="line">$ sudo apt-get install tcl tk</span><br><span class="line">$ sudo make distclean</span><br><span class="line">$ sudo make clean</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo make test</span><br></pre></td></tr></table></figure>
<p>修改配置文件<code>/etc/local/redis/redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Log file</span><br><span class="line">logfile /usr/local/redis/redis.log</span><br><span class="line"></span><br><span class="line"># The working directory</span><br><span class="line">dir /usr/local/redis/redis_dbfile/</span><br></pre></td></tr></table></figure>
<p>并检查以上目录/文件的权限, 保证有读写的权利.</p>
<p>修改<code>.bashrc</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis</span><br><span class="line">alias redis-server=&quot;/usr/local/redis/src/redis-server /usr/local/redis/redis.conf&quot;</span><br><span class="line">alias redis-cli=/usr/local/redis/src/redis-cli</span><br></pre></td></tr></table></figure>
<p>启动Redis.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-server /usr/local/redis/redis.conf</span><br></pre></td></tr></table></figure>
<p>关闭Redis, 可使用<code>kill 进程号</code>的方式, 也可以:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IP:PORT&gt; SHUTDOWN</span><br></pre></td></tr></table></figure>
<p>远程连接配置<code>redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 注释掉bind</span><br><span class="line"># bind 127.0.0.1</span><br><span class="line"></span><br><span class="line"># 修改protected-mode</span><br><span class="line">protected-mode no</span><br></pre></td></tr></table></figure>
<p>这样可以在其它客户端直接登录.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379</span><br></pre></td></tr></table></figure>
<p>使用密码认证登录, 修改<code>redis.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改protected-mode</span><br><span class="line">protected-mode yes</span><br><span class="line"></span><br><span class="line"># 解除注释, 添加密码</span><br><span class="line">requirepass passwd</span><br></pre></td></tr></table></figure>
<p>在连接时认证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379 -a passwd --raw</span><br></pre></td></tr></table></figure>
<p>或者先登录再验证:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ redis-cli -h IP -p 6379</span><br><span class="line">IP:PORT&gt; auth passwd</span><br><span class="line">IP:PORT&gt; CONFIG GET requirepass</span><br><span class="line">1) &quot;requirepass&quot;</span><br><span class="line">2) &quot;passwd&quot;</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>Redis中没有类似MySQL中的数据库, 数据表的结构, 支持5种数据类型:</p>
<ul>
<li>字符串(String).</li>
<li>哈希(Hash).</li>
<li>列表(List).</li>
<li>集合(Set).</li>
<li>有序集合(Zset)</li>
</ul>
<p>下面分别介绍相关的操作.</p>
<ul>
<li><p>键(Key).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查询某个key是否存在</span><br><span class="line">IP:PORT&gt; EXISTS key</span><br><span class="line"># 删除指定key</span><br><span class="line">IP:PORT&gt; DEL key</span><br><span class="line"># 查看所有key</span><br><span class="line">IP:PORT&gt; keys *</span><br></pre></td></tr></table></figure>
</li>
<li><p>字符串(String)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置指定key的值</span><br><span class="line">IP:PORT&gt; SET key val</span><br><span class="line"># 获取指定key的值</span><br><span class="line">IP:PORT&gt; GET key</span><br><span class="line"># 获取多个指定key的值</span><br><span class="line">IP:PORT&gt; MGET key_0 key_1 ...</span><br><span class="line"># 将key中储存的数字值加一</span><br><span class="line">IP:PORT&gt; INCR key</span><br><span class="line"># 将key中储存的数字值减一</span><br><span class="line">IP:PORT&gt; DECR key</span><br></pre></td></tr></table></figure>
</li>
<li><p>哈希(Hash).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将key中field设置为val</span><br><span class="line">IP:PORT&gt; HSET key field val</span><br><span class="line"># 同时设置多个field-val</span><br><span class="line">IP:PORT&gt; HMSET key field_0 val_0  field_1 val_1 ...</span><br><span class="line"># 获取哈希表中所有字段field</span><br><span class="line">IP:PORT&gt; HKEYS key</span><br><span class="line"># 获取哈希表中所有值val</span><br><span class="line">IP:PORT&gt; HVALS key</span><br><span class="line"># 获取指定字段的值</span><br><span class="line">IP:PORT&gt; HGET key field</span><br><span class="line"># 同时获取多个字段的值</span><br><span class="line">IP:PORT&gt; HMGET key field_0 field_1</span><br><span class="line"># 查看所有的键值对</span><br><span class="line">IP:PORT&gt; HGETALL key</span><br><span class="line"># 删除一个或多个哈希表字段</span><br><span class="line">IP:PORT&gt; HDEL key field_0 field_1</span><br></pre></td></tr></table></figure>
</li>
<li><p>列表(List).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将一个或多个值插到列表头部</span><br><span class="line">IP:PORT&gt; LPUSH key val_0 val_1 ...</span><br><span class="line"># 将一个或多个值插到列表尾部</span><br><span class="line">IP:RORT&gt; LPUSH key val_0 val_1 ...</span><br><span class="line"># 移出并获取列表的第一个元素</span><br><span class="line">IP:RORT&gt; LPOP key</span><br><span class="line"># 移出并获取列表的最后一个元素</span><br><span class="line">IP:RORT&gt; RPOP key</span><br><span class="line"># 通过索引获取指定元素值</span><br><span class="line">IP:RORT&gt; LINDEX key index</span><br><span class="line"># 通过索引范围获取指定范围元素值</span><br><span class="line">IP:RORT&gt; LRANGE key start stop</span><br><span class="line"># 通过索引设置列表元素的值</span><br><span class="line">IP:RORT&gt; LSET key index val</span><br></pre></td></tr></table></figure>
</li>
<li><p>集合(Set).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向集合添加元素</span><br><span class="line">IP:PORT&gt; SADD key val_0 val_1 ...</span><br><span class="line"># 返回成员中所有成员</span><br><span class="line">IP:PORT&gt; SMEMBERS key</span><br><span class="line"># 移除集合中的元素</span><br><span class="line">IP:PORT&gt; SREM key val_0 val_1 ...</span><br></pre></td></tr></table></figure>
</li>
<li><p>有序集合(Zset).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向集合中添加一个或者多个成员和分数, 或更新分数</span><br><span class="line">IP:PORT&gt; ZADD key score_0 member_0 score_1 member_1 ...</span><br><span class="line"># 移除一个或多个成员</span><br><span class="line">IP:PORT&gt; ZREM key member_0 member_1 ...</span><br><span class="line"># 通过索引区间(有序)返回成员及分数</span><br><span class="line">IP:PORT&gt; ZRANGE key start stop [WITHSCORES]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>更多的操作可以在谷歌上进行查找.</p>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>安装Python包.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip isntall redis</span><br></pre></td></tr></table></figure>
<p>进行简单的操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">redis_con = redis.Redis(host=<span class="string">'IP'</span>,port=<span class="number">6379</span>,password=<span class="string">'passwd'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前键</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> redis_con.keys():</span><br><span class="line">    print(k.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 删除指定key</span></span><br><span class="line">redis_con.delete(<span class="string">'key'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其余操作与直接使用命令行操作几乎完全一致</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hbase%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>使用过Hive的同学一定都有过这样的感受, 有时候想要进行一个非常简单的查询, 也要耗费不少时间, 在一些需要实时查询的场景是用不上的.</p>
<p>而基于谷歌提出的Bigtable数据模型, 在HDFS的基础上, 出现了Hbase. 这是一个真正的分布式数据库.</p>
<a id="more"></a>
<h1 id="Hbase简介"><a href="#Hbase简介" class="headerlink" title="Hbase简介"></a>Hbase简介</h1><p>首先, 为什么会有Hbase这样的分布式数据库呢? 很简单, 因为单机的数据库在大数据时代不能胜任所有任务. 一般来说, 像MySQL这样的数据库, 大概可以处理TB级别的数据, 但是有些互联网公司一天的数据量可能就是TB级别.</p>
<p>然后又问, 不是已经有了HDFS以及Hive了吗, 数据量已经不是问题了呀. 因为HDFS和Hive属于文件系统, 或者数据仓库, 而数据库是以某种有组织的方式储存的数据集合. 作为数据库的Hbase的一个很明显的特点就是, 能够高并发地随机写和支持实时查询, 这是HDFS不具备的.</p>
<p>下面来介绍Hbase的架构, Hbase是一种基于Key-Value的NoSQL数据库, 这样设计的好处是, 可以避免数据稀疏带来的储存空间浪费, 同时可以非常方便地对字段进行扩展.</p>
<p>在Hbase里面也有表的概念, 表的结构如下图:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>其中的几个关键元素:</p>
<ul>
<li><p>列族.</p>
<p>可以理解为列的归类. 在一个列族下可以创建多个列.</p>
</li>
<li><p>列.</p>
<p>就是一般意义上的列, 字段名.</p>
</li>
<li><p>行键.</p>
<p>用于标识一行的标识. 用有相同行键的数据, 属于一行. 需要保证唯一, 且设计合理, 方便加速查询.</p>
</li>
</ul>
<p>上面虽然以类似二维表的形式来展示, 但是Hbase本质是NoSQL, 所以一般来说Key就是”行键-列族-列”, Value就是对应的值.</p>
<p>此外, 还有一个元素是时间戳, 当数据写入Hbase, 更改, 甚至删除的时候, 都会有一个时间戳, 可以当成一个数据的版本. 所以在查询时, 可以根据时间戳查询过去的值.</p>
<p>Hbase整体的结构如下图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<ul>
<li><p>Client.</p>
<p>客户端, 提供Hbase的访问接口.</p>
</li>
<li><p>Zookeeper.</p>
<p>储存Hbase的元数据(meta表), 在读写数据时, 去Zookeeper中拿到meta数据, 告诉客户端去哪台机器读写数据.</p>
</li>
<li><p>HRegion.</p>
<p>在上面介绍表结构时候, 说到了列族, 行键的概念, 而在具体储存时, 会按照行键进行切分, 划分为多个HRegion.</p>
</li>
<li><p>HMaster.</p>
<p>监控HRegionServer的状态, 处理元数据的变更, 处理HRegion的分配或转移. 如当某个HRegion过大时, 自动切分为多个HRegion.</p>
</li>
<li><p>HRegionServer.</p>
<p>真正干活的节点, 负责与HDFS底层交互, 处理客户端请求. 一个HRegionServer中有多个HRegion.</p>
</li>
</ul>
<p>在每个HRegion内部, 有多个按列族划分的Store, 每个Store内, 又有Mem Store和Store File. 其中Mem Store负责在内存接收数据, 当超过一定阈值时, 再刷写到硬盘的Store File上.</p>
<p><img src="fig_2.jpg" alt="fig"></p>
<p>还漏了一个Hlog, 这是干嘛用的呢. 其实就是在写数据的时候, 会先写到内存, 但为了防止机器宕机, 内存中的数据没刷到硬盘就挂了, 所以在写Mem Store时还会写一份Hlog, 按顺序快速写到硬盘.</p>
<p>总结一下Hbase的特点:</p>
<ul>
<li>采用Key-Value列式存储, 节约空间.</li>
<li>可自动切分数据, 能够水平拓展.</li>
<li>可提供高并发, 低延迟的读写操作.</li>
<li>不支持丰富的条件筛选查询, 比较依赖行键.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在<a href="https://hbase.apache.org/downloads.html" target="_blank" rel="noopener">这里</a>下载, 我选择的是2.3.1版本.</p>
<p>下载完成后, 解压放置到目录<code>/usr/local/hbase</code>.</p>
<p>在Ubuntu操作系统上, <code>.bashrc</code>添加如下内容:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hbase</span><br><span class="line">export HBASE_HOME=/usr/local/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>
<p>在<code>/usr/local/hbase/conf</code>下的<code>hbase-env.sh</code>文件进行修改, 具体路径根据实际情况改动:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The java implementation to use.  Java 1.8+ required.</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="comment"># Tell HBase whether it should manage it's own instance of ZooKeeper or not.</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>其中<code>HBASE_MANAGES_ZK</code>表示设置由Hbase自己管理Zookeeper, 不需要单独配置.</p>
<p>再配置<code>hbase-site.xml</code>文件:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中<code>hbase.tmp.dir</code>需要与HDFS的地址端口相匹配.</p>
<p>启动Hbase:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动Hadoop</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-all.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Hbase</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-hbase.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看进程</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> jps</span></span><br><span class="line">7012 HQuorumPeer</span><br><span class="line">4661 NameNode</span><br><span class="line">5542 ResourceManager</span><br><span class="line">7625 HRegionServer</span><br><span class="line">5242 SecondaryNameNode</span><br><span class="line">7260 HMaster</span><br><span class="line">7965 Jps</span><br><span class="line">4911 DataNode</span><br><span class="line">5759 NodeManager</span><br></pre></td></tr></table></figure>
<p>在浏览器上<code>IP:60010</code>可以看到交互界面.</p>
<p>进入Hbase Shell:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hbase shell</span></span><br></pre></td></tr></table></figure>
<p>使用<code>help</code>查看帮助, <code>help &#39;cmd&#39;</code>查看具体命令帮助, <code>exit</code>退出shell.</p>
<p>在关闭时, 也是<code>stop-hbase.sh</code>先关闭Hbase, 再<code>stop-all.sh</code>关闭Hadoop.</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><p>下面列举一些Hbase的常用命令行操作.</p>
<ul>
<li><p>表操作.</p>
<p>创建.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建表, 以及对应的列族</span><br><span class="line">&gt; create &apos;tb_name&apos;, &apos;col_family_0&apos;, &apos;col_family_0&apos;, ...</span><br><span class="line"># 添加列族</span><br><span class="line">&gt; alter &apos;tb_name&apos;, &apos;col_family&apos;</span><br></pre></td></tr></table></figure>
<p>删除.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除列族</span><br><span class="line">&gt; alter &apos;tb_name&apos;, &apos;delete&apos;=&gt;&apos;col_family&apos;</span><br><span class="line"># 删除表</span><br><span class="line">&gt; disable &apos;tb_name&apos;</span><br><span class="line">&gt; drop &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
<p>查看.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示Hbase中的表</span><br><span class="line">&gt; list</span><br><span class="line"># 查看表结构</span><br><span class="line">&gt; describe &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据操作.</p>
<p>插入/覆盖数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; put &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;, val</span><br></pre></td></tr></table></figure>
<p>获取数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取指定行数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;</span><br><span class="line"># 获取指定行, 指定列族数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family&apos;</span><br><span class="line"># 获取指定单元格数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;</span><br><span class="line"></span><br><span class="line">&gt; 获取指定行, 指定多个列族或列的数据</span><br><span class="line">&gt; get &apos;tb_name&apos;, &apos;row_key&apos;, &#123;COLUMN=&gt;[&apos;col_family_0:col_0&apos;, &apos;col_family_1&apos;, ...]&#125;</span><br><span class="line"></span><br><span class="line"># 查看表中所有数据, &#123;FORMATTER =&gt; &apos;toString&apos;&#125;可识别中文</span><br><span class="line">&gt; scan &apos;tb_name&apos;, &#123;FORMATTER =&gt; &apos;toString&apos;&#125;</span><br><span class="line"># 获取指定多个列族或列的数据</span><br><span class="line">&gt; scan &apos;tb_name&apos;, &#123;COLUMN=&gt;[&apos;col_family_0:col_0&apos;, &apos;col_family_1&apos;, ...]&#125;</span><br></pre></td></tr></table></figure>
<p>删除数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 指定单元格内容</span><br><span class="line">&gt; delete &apos;tb_name&apos;, &apos;row_key&apos;, &apos;col_family:col&apos;</span><br><span class="line"># 整行删除</span><br><span class="line">&gt; deleteall &apos;tb_name&apos;, &apos;row_key&apos;</span><br><span class="line"># 清空数据表</span><br><span class="line">&gt; truncate &apos;tb_name&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>如果要对Hbase进行一些复杂的流程处理或者操作, 命令行的方式是比较麻烦的, 其原生的编程接口语言是Java, 不过借助thrift接口服务器, 因此也可以采用其他语言来使用Hbase, 这里使用Python.</p>
<p>在使用Python包之前, 需要启动Hbase的thrift-server:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hbase-daemon.sh start thrift</span><br><span class="line">$ jps</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">34533 ThriftServer</span><br></pre></td></tr></table></figure>
<p>不止一个Python包可以操作Hbase, 不过个人觉得<a href="https://happybase.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">HappyBase</a>比较好用, 不需要再进行更多的额外配置, 比如安装系统的thrift和对应依赖, 直接连接即可使用.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line"><span class="comment"># thrift默认端口9090</span></span><br><span class="line">connection = happybase.Connection(<span class="string">'hostname'</span>)</span><br><span class="line">table = connection.table(<span class="string">'table-name'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加/修改</span></span><br><span class="line">table.put(<span class="string">b'row-key'</span>, &#123;<span class="string">b'family:qual1'</span>: <span class="string">b'value1'</span>,</span><br><span class="line">                       <span class="string">b'family:qual2'</span>: <span class="string">b'value2'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">row = table.row(<span class="string">b'row-key'</span>)</span><br><span class="line">print(row[<span class="string">b'family:qual1'</span>])  <span class="comment"># prints 'value1'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, data <span class="keyword">in</span> table.rows([<span class="string">b'row-key-1'</span>, <span class="string">b'row-key-2'</span>]):</span><br><span class="line">    print(key, data)  <span class="comment"># prints row key and data for each row</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, data <span class="keyword">in</span> table.scan(row_prefix=<span class="string">b'row'</span>):</span><br><span class="line">    print(key, data)  <span class="comment"># prints 'value1' and 'value2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定row-key</span></span><br><span class="line">row = table.delete(<span class="string">b'row-key'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 更多API可以查询文档</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具, 可以将结构化的数据文件映射为一张数据库表, 并提供简单的SQL查询功能, 可以将SQL语句转换为MapReduce任务进行运行.  其优点是学习成本低, 可以通过类SQL语句快速实现简单的MapReduce统计, 不必开发专门的MapReduce应用, 十分适合数据仓库的统计分析.</p>
<a id="more"></a>
<h1 id="Hive介绍"><a href="#Hive介绍" class="headerlink" title="Hive介绍"></a>Hive介绍</h1><p><img src="fig_0.png" alt="fig"></p>
<p>上图是Hive的一个整体架构, Hive的数据是存储在HDFS上的, Hive中的库和表可以看作是对HDFS上数据做的一个映射. 所以Hive必须是运行在一个Hadoop集群上的.</p>
<p>Hive的元数据是一般是存储在MySQL这种关系型数据库上, Hive和MySQL之间通过MetaStore服务交互.</p>
<p>Hive中的执行器, 是将SQL转化为MapReduce程序, 然后将要执行的MapReduce程序放到YARN上以一系列Job的方式去执行.</p>
<p>总体来说Hive有如下一些特点:</p>
<ul>
<li><p>容易上手.</p>
<p>通过类SQL来处理大数据, 而避免了写MapReduce程序, 这样使得分析数据更容易.</p>
</li>
<li><p>逻辑表.</p>
<p>数据是存储在HDFS上的, Hive本身并不提供数据的存储功能. 将数据映射成数据库和一张张的表, 库和表的元数据信息一般存在关系型数据库上.</p>
</li>
<li><p>数据存储方面.</p>
<p>能够存储很大的数据集, 对数据完整性, 格式要求并不严格.</p>
</li>
<li><p>数据处理方面.</p>
<p>因为Hive语句最终会生成MapReduce任务去计算, 所以不适用于实时计算查询的场景, 它适用于离线批量处理.</p>
</li>
</ul>
<p>相比一些关系型数据库(如MySQL), Hive还有一些概念:</p>
<ul>
<li>内部表/外部表.</li>
<li>分区.</li>
<li>Array, Map, Struct数据结构.</li>
</ul>
<p>这些是处于大数据的安全, 灵活, 更快地处理而提出的, 这里暂不做更多的介绍.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>系统为Ubuntu, 在安装Hive之前, 确保已经安装好了Hadoop和MySQL.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysql -V</span></span><br><span class="line">mysql  Ver 14.14 Distrib 5.7.31, for Linux (x86_64) using  EditLine wrapper</span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop version</span></span><br><span class="line">Hadoop 2.10.0</span><br><span class="line">Subversion ssh://git.corp.linkedin.com:29418/hadoop/hadoop.git -r e2f1f118e465e787d8567dfa6e2f3b72a0eb9194</span><br><span class="line">Compiled by jhung on 2019-10-22T19:10Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 7b2d8877c5ce8c9a2cca5c7e81aa4026</span><br><span class="line">This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.10.0.jar</span><br></pre></td></tr></table></figure>
<p>下载合适的Hive版本, <a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/" target="_blank" rel="noopener">下载地址</a>. 这里下载的是2.3.7版.</p>
<p>下载后解压, 移到<code>/usr/local/hive/</code>.</p>
<p>修改环境配置<code>.bashrc</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hive</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>
<p>修改Hive配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_CONF_DIR</span><br><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br><span class="line">$ nano hive-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/tmp/hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>因为在<code>hive-site.xml</code>默认有以上配置, 所以在Hadoop集群中进行创建.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">$ hadoop fs -chmod -R 777 /user/hive/warehouse</span><br><span class="line">$ hadoop fs -mkdir -p /tmp/hive/</span><br><span class="line">$ hadoop fs -chmod -R 777 /tmp/hive</span><br><span class="line">$ hadoop fs -ls /user/hive</span><br><span class="line">$ hadoop fs -ls /tmp/hive</span><br></pre></td></tr></table></figure>
<p>将配置中的<code>${system:java.io.tmpdir}</code>替换为<code>/usr/local/hive/tmp/</code>, 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/hive/tmp/$&#123;hive.session.id&#125;_resources&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>并创建对应文件夹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME</span><br><span class="line">$ mkdir tmp</span><br><span class="line">$ chmod -R 777 tmp/</span><br></pre></td></tr></table></figure>
<p>将配置文件中<code>${system:user.name}</code>都替换为<code>root</code>, 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/hive/tmp/root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>修改数据库相关配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql://MySQL对应IP:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;登录MySQL的ID&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;登录MySQL的密码&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>添加MySQL驱动包, <a href="https://dev.mysql.com/downloads/connector/j/5.1.html" target="_blank" rel="noopener">下载地址</a>, 解压后移动到Hive里面.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cp mysql-connector-java-5.1.49.jar $HIVE_HOME/lib</span><br></pre></td></tr></table></figure>
<p>新建<code>hive-env.sh</code>文件并修改:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_CONF_DIR</span><br><span class="line">$ cp hive-env.sh.template hive-env.sh</span><br><span class="line">$ nano hive-env.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line">HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hive/conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Folder containing extra libraries required for hive compilation/execution c$</span></span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/usr/<span class="built_in">local</span>/hive/lib</span><br></pre></td></tr></table></figure>
<p>初始化MySQL数据库:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME/bin</span><br><span class="line">$ schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure>
<p>若成功, 则会在MySQL中生成Hive元数据库; 若失败, 则检查配置是否正确.</p>
<p>启动Hive:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd $HIVE_HOME/bin</span><br><span class="line">$ ./hive</span><br></pre></td></tr></table></figure>
<p>进行简单测试:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show functions;</span><br><span class="line">OK</span><br><span class="line">!</span><br><span class="line">!=</span><br><span class="line">$sum0</span><br><span class="line">%</span><br><span class="line">&amp;</span><br><span class="line">*</span><br><span class="line">+</span><br><span class="line">-</span><br><span class="line">/</span><br><span class="line">&lt;</span><br><span class="line">&lt;=</span><br><span class="line">&lt;=&gt;</span><br><span class="line">&lt;&gt;</span><br><span class="line">=</span><br><span class="line">==</span><br><span class="line">&gt;</span><br><span class="line">&gt;=</span><br><span class="line">^</span><br><span class="line">abs</span><br><span class="line">acos</span><br><span class="line">add_months</span><br><span class="line">aes_decrypt</span><br><span class="line">aes_encrypt</span><br><span class="line">and</span><br><span class="line">array</span><br><span class="line">...</span><br><span class="line">hive&gt; desc function sum;</span><br><span class="line">OK</span><br><span class="line">sum(x) - Returns the sum of a set of numbers</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><ul>
<li><p>数据库操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database db_name;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 标准方式</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database <span class="keyword">if</span> not exists db_name;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定路径(hdfs)</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create database <span class="keyword">if</span> not exists db_name location <span class="string">'hdfs_path/db_name.db'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 模糊查找, 使用*和.</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases like <span class="string">'abc*'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use db_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看数据库字段格式</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc database db_name;</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc database extended db_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop database db_name;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>数据表操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 普通创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table IF NOT EXISTS</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> db_name.table_name(</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> id string,</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> user string)</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> COMMENT <span class="string">'abc'</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> STORED AS TEXTFILE;</span></span><br><span class="line">		</span><br><span class="line"><span class="meta">#</span><span class="bash"> 子查询方式创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table IF NOT EXISTS db_name.table_name_0</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> AS select * from table_name_1;</span></span><br><span class="line">		</span><br><span class="line"><span class="meta">#</span><span class="bash"> like方式创建表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table db_name.table_name_0 like table_name_1;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空表的内容，保留表的结构</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> truncate table table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop table <span class="keyword">if</span> exists table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 插入数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert into table_name</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> values</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> (..., ..., ...),</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ...</span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> (..., ..., ...);</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加载本地文件到hive表</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">'path/file'</span> into table db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载hdfs文件到hive中, overwrite表示覆盖原数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">'path/file'</span> overwrite into table db_name.table_name;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数据.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取数据到本地</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert overwrite <span class="built_in">local</span> directory <span class="string">'path'</span> </span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></span></span><br><span class="line"><span class="meta">		&gt;</span><span class="bash"> select * from db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在终端使用重定向到本地</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive -e <span class="string">"select * from db_name.table_name;"</span> &gt; <span class="string">'path/file'</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取数据到hdfs</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert overwrite directory <span class="string">'hdfs_path'</span> select * from db_name.table_name;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若发现获取的数据中分隔符为hive默认分隔符\x01, 可替换</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed <span class="string">'s/\x01/,/g'</span> file &gt; new_file</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>一提到大数据, 可能大家首先就会想到Hadoop, 这里将会简单介绍Hadoop的基本原理, 在Linux上的安装过程, 以及一些基础的文件操作方式.</p>
<a id="more"></a>
<h1 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h1><p>为什么Hadoop会成为大数据框架的首选呢? 可以从其两个主要的功能说起:</p>
<ul>
<li>HDFS</li>
<li>MapReduce</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS(Hadoop Distributed File System)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统. 它和现有的分布式文件系统有很多共同点, 但同时, 它和其他的分布式文件系统的区别也是很明显的. HDFS 是一个<strong>高度容错性</strong>的系统, 适合部署在廉价的机器上. HDFS能提供高吞吐量的数据访问, 非常适合大规模数据集上的应用.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 其中各个组件分别为:</p>
<ul>
<li>Client: 客户端.</li>
<li>NameNode: 对应master, 它相当于是一个管理者, 存储元数据.</li>
<li>DataNode: 对应slave, 执行操作并存储实际数据.</li>
<li>SecondaryNameNode: 用于辅助NameNode.</li>
</ul>
<p>HDFS读写文件的方式如下两图, 本质上都是先与NameNode进行互动, 然后再在DataNode上进行操作, 最后再向NameNode报告.</p>
<p>读文件:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>写文件:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>对于HDFS的文件操作命令, 与Linux的文件操作命令非常类似, 在后面会进行介绍.</p>
<p>总结一下HDFS的优缺点:</p>
<ul>
<li>优点:<ul>
<li>支持海量数据存储.</li>
<li>高容错性.</li>
<li>流式数据访问.</li>
<li>…</li>
</ul>
</li>
<li>缺点:<ul>
<li>不能做到低延迟数据访问.</li>
<li>不适合大量的小文件存储.</li>
<li>不支持直接修改文件.</li>
<li>…</li>
</ul>
</li>
</ul>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>MapReduce是一种针对处理大数据的算法范式, 某种意义上来说, 就是一种分治算法.</p>
<p>而所谓分治算法, 分而治之, 先将大问题转化为一个个小问题, 再将小问题逐一解决后, 汇合成大问题的结果.</p>
<p>MapReduce算法, 具体来说可以有如下一些步骤:</p>
<ul>
<li>Inout</li>
<li>Split</li>
<li>Map</li>
<li>Shuffle</li>
<li>Reduce</li>
<li>Finalize</li>
</ul>
<p>比如下面一个餐馆做饭的栗子:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>首先将原材料(Input)分发(Split)给各个厨师, 然后各个厨师对其进行加工(Map), 然后按菜的品类进行整理(Shuffle), 接着用整理好的单品组合制作成一份份快餐(Reduce), 最后用户点菜时就将对应快餐送出(Finalize).</p>
<p>再举一个统计单词出现次数的栗子吧:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>以上的栗子都是比较简单直观的, 但是对于一些复杂的任务, 可能要执行多次交替的Map操作和Reduce操作, 并且一般用Java来编写具体的代码. 对于非专业人员来说, 是有一定难度的, 正是基于此, 才有了后面的Hive.</p>
<h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>上面说到了HDFS是分布式文件系统, MapReduce是运行在上面的算法, YARN的英文全称为Yet Another Resource Negotiator, 意思是”另一种资源调度器”. 那么为什么会有YARN呢?</p>
<p>直接的原因, 就是在Hadoop1.x中, 在MapReduce时, JobTracker负责的事情太多了, 接受任务, 调度资源, 监控TaskTracker运行情况. 这样实现简单, 但是容易出错, 也不容易扩展.</p>
<p>因此在后来的Hadoop2.x中, 将JobTracker进行了拆分, 开发出了YARN. 其实Hadoop能有今天的地位, YARN功不可没, 正因为有了YARN, 更多的算法框架(如Spark)可以接入到HDFS中. HDFS可能不是最优秀的分布式文件系统, 但却是应用最广泛的.</p>
<p>接下来详细介绍YARN的架构:</p>
<p><img src="fig_5.jpg" alt="fig"></p>
<ul>
<li><p>Container</p>
<p>容器在YARN这里是对(计算)资源做的一种抽象, 比如将一定的CPU核数和内存封装成一个个的容器.</p>
<p>同时, 容易由NodeManager启动和管理, 并被其监控; 被ResourceManager进行调度.</p>
</li>
<li><p>ResourceManager</p>
<p>从名字上看, 就知道RM是负责资源管理的, 整个系统只有一个RM, 其包含两个主要的组件:</p>
<ul>
<li><p>Scheduler</p>
<p>定时调度器, 本质是一种策略或者算法. 当Client提交一个任务时, 它会根据所需要的资源和当前集群的资源状况进行分配. 它只负责分配资源, 不做监控以及状态跟踪.</p>
</li>
<li><p>ApplicationManager</p>
<p>应用管理器, 每当Client提交一个Application时, 就会新建一个对应的ApplicationManager, 由这个AM去向RM申请容器资源, 获得资源后将要运行的程序发送到容器运行, 并进行监控.</p>
</li>
</ul>
</li>
<li><p>NodeManager</p>
<p>节点管理器是每台机器上的代理, 负责容器的管理和监控, 并向ResourceManager/Scheduler提供资源使用情况.</p>
</li>
</ul>
<p>当提交一个任务(Application)到YARN时, 后续整个流程如下:</p>
<ul>
<li>Client向RM提交Application.</li>
<li>RM向NM通信, 为该Application分配第一个容器, 并在这个容器上运行AM.</li>
<li>在AM启动后, 对Application进行拆分, 拆分出来的task可以运行在一个或者多个容器中. 然后向RM申请运行程序的容器, 并定时向RM发送”心跳”.</li>
<li>申请到容器后, AM会去和对应的NM通信, 将Application的task发送到对应NM的容器运行.</li>
<li>容器中运行的task也会向AM发送”心跳”, 当程序运行完成后, AM向RM注销并释放容器资源.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>这里不配置真正分布式的Hadoop, 一来比较麻烦, 需要多台电脑或者多个虚拟机, 二来更多的是想使用基于Hadoop的Hive等大数据套件.</p>
<p>所以这里安装的是伪分布式, 意思就是其系统是按真正的分布式去运作的, 不过所有流程都在一台电脑上执行. 这里的操作系统为Ubuntu.</p>
<p>由于Hadoop的底层语言为Java, 所以需要事先安装Java.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install openjdk-8-jdk</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java -version</span></span><br></pre></td></tr></table></figure>
<p>同时还要配置<code>JAVA_HOME</code>, 寻找Java安装路径:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">which</span> java</span></span><br><span class="line">/usr/bin/java</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -lrt /usr/bin/java</span></span><br><span class="line">lrwxrwxrwx 1 root root 22 9月  11 14:05 /usr/bin/java -&gt; /etc/alternatives/java</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -lrt /etc/alternatives/java</span></span><br><span class="line">lrwxrwxrwx 1 root root 46 9月  11 14:05 /etc/alternatives/java -&gt; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java</span><br></pre></td></tr></table></figure>
<p>编辑<code>.bashrc</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># java</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$JAVA_HOME</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JRE_HOME</span>/lib:<span class="variable">$CLASSPATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/hadoop/lib/native:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib/native"</span></span><br></pre></td></tr></table></figure>
<p>在<a href="https://mirrors.bfsu.edu.cn/apache/hadoop/common/" target="_blank" rel="noopener">这里</a>选择要下载的Hadoop版本, 我选择的是2.10.0版本. 下载好以后解压到<code>/usr/local/hadoop</code>.</p>
<p>下面是修改配置文件, Hadoop的配置文件一般位于<code>/usr/local/hadoop/etc/hadoop/</code>, 首先是<code>hadoop-env.sh</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The java implementation to use.</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br></pre></td></tr></table></figure>
<p>此时可以测试一下Word Count的小程序.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd /usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">$ mkdir input</span><br><span class="line">$ 在input文件夹中放入一些文本</span><br><span class="line">$ hadoop jar hadoop-mapreduce-examples-2.10.0.jar wordcount input/ output/</span><br></pre></td></tr></table></figure>
<p>若执行成功, 则会生成一个<code>output</code>文件夹, 里面有计数的结果.</p>
<p>或者可以执行计算圆周率的小程序:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop jar hadoop-mapreduce-examples-2.10.0.jar pi 16 100000</span><br></pre></td></tr></table></figure>
<p>接下来搭建伪分布式集群, 包括三个核心配置文件<code>core-site.xml</code>,  <code>hdfs-site.xml</code>,  <code>yarn-site.xml</code>.</p>
<p>修改<code>core-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>hdfs-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>yarn-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改<code>mapred-site.xml</code>:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改从节点配置文件<code>slaves</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>
<p>配置完成, 格式化NameNode:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs namenode -format</span></span><br></pre></td></tr></table></figure>
<p>开启守护进程:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-dfs.sh</span></span><br></pre></td></tr></table></figure>
<p>通过<code>jps</code>查看Java进程的状态:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> jps</span></span><br><span class="line">23620 SecondaryNameNode</span><br><span class="line">23879 Jps</span><br><span class="line">23049 NameNode</span><br><span class="line">23293 DataNode</span><br></pre></td></tr></table></figure>
<p>再启动YARN:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动YARN</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-yarn.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启历史服务器, 才能在Web中查看任务运行情况</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mr-jobhistory-daemon.sh start historyserver</span></span><br></pre></td></tr></table></figure>
<p>通过<code>jps</code>查看Java进程的状态:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ jps</span><br><span class="line">24275 ResourceManager</span><br><span class="line">24483 NodeManager</span><br><span class="line">23620 SecondaryNameNode</span><br><span class="line">23049 NameNode</span><br><span class="line">24955 Jps</span><br><span class="line">23293 DataNode</span><br></pre></td></tr></table></figure>
<p>此外, 可以通过<code>start-all.sh</code>一次性启动, <code>stop-all.sh</code>一次性关闭.</p>
<p>在浏览器上可以在<code>IP:50070</code>查看HDFS的情况, 这里IP是NameNode对应的地址.</p>
<p>同时, 还可以在<code>IP:8088</code>查看YARN的情况.</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>这里暂时就不讲怎么写MapReduce程序了, 虽然用其它一些脚本语言(如Python)也能够写, 但是还是比较麻烦.</p>
<p>主要介绍HDFS的一些命令行操作, 其实感觉和Linux的文件系统命令很类似. 输入<code>hadoop fs</code>可以查看到相关命令说明. 下面是一些常用的命令.</p>
<ul>
<li><p>文件夹.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建文件夹, 一般首先创建/user/user_name作为默认目录</span><br><span class="line">$ hadoop fs -mkdir -p /user/user_name</span><br><span class="line"></span><br><span class="line"># 删除文件夹</span><br><span class="line">$ hadoop fs -rm -r -f path</span><br><span class="line">$ hadoop fs -rmdir path</span><br></pre></td></tr></table></figure>
</li>
<li><p>文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将Client端的文件传到HDFS中, 若文件存在, 可使用-f参数覆盖</span><br><span class="line">$ hadoop fs -put -f local_file hdfs_path</span><br><span class="line"></span><br><span class="line"># 从HDFS中获取文件到Client端</span><br><span class="line">$ hadoop fs -get hdfs_file local_path</span><br><span class="line"></span><br><span class="line"># 复制</span><br><span class="line">$ hadoop fs -cp path_0 path_1</span><br><span class="line"></span><br><span class="line"># 移动</span><br><span class="line">$ hadoop fs -mv path_0 path_1</span><br><span class="line"></span><br><span class="line"># 删除</span><br><span class="line">$ hadoop fs -rm path</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看指定目录下文件</span><br><span class="line">$ hadoop fs -ls path</span><br><span class="line"></span><br><span class="line"># 查看指定目录已使用空间</span><br><span class="line">$ hadoop fs -du -h path</span><br><span class="line"></span><br><span class="line"># 查看指定目录剩余空间</span><br><span class="line">$ hadoop fs -df -h path</span><br><span class="line"></span><br><span class="line"># 查看文件内容</span><br><span class="line">$ hadoop fs -cat file</span><br><span class="line">$ hadoop fs -tail file</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL的安装与使用</title>
    <url>/2020/09/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/MySQL%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>MySQL是最流行的关系型数据库管理系统, 在WEB应用方面MySQL是最好的 RDBMS(Relational Database Management System: 关系数据库管理系统)应用软件之一.</p>
<p>本篇主要介绍MySQL的基本知识, 掌握MySQL数据库的使用.</p>
<a id="more"></a>
<h1 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h1><p>数据库(Database)是按照数据结构来组织, 存储和管理数据的仓库.</p>
<p>每个数据库都有一个或多个不同的 API 用于创建, 访问, 管理, 搜索和复制所保存的数据.</p>
<p>我们也可以将数据存储在文件中, 但是在文件中读写数据速度相对较慢.</p>
<p>所以, 现在我们使用关系型数据库管理系统(RDBMS)来存储和管理大数据量. 所谓的关系型数据库, 是建立在关系模型基础上的数据库, 借助于集合代数等数学概念和方法来处理数据库中的数据.</p>
<p>关系数据库管理系统(Relational Database Management System)的特点：</p>
<ul>
<li>数据以表格的形式出现.</li>
<li>每行为各种记录名称.</li>
<li>每列为记录名称所对应的数据域.</li>
<li>许多的行和列组成一张表单.</li>
<li>若干的表单组成database.</li>
</ul>
<p>以下为RDBMS的一些术语:</p>
<ul>
<li><strong>数据库:</strong> 数据库是一些关联表的集合.</li>
<li><strong>数据表:</strong> 表是数据的矩阵. 在一个数据库中的表看起来像一个简单的电子表格.</li>
<li><strong>列:</strong> 一列(数据元素)包含了相同类型的数据, 例如邮政编码的数据.</li>
<li><strong>行:</strong> 一行(元组, 或记录)是一组相关的数据, 例如一条用户订阅的数据.</li>
<li><strong>主键:</strong> 主键是唯一的, 一个数据表中只能包含一个主键. 你可以使用主键来查询数据.</li>
<li><strong>外键:</strong> 外键用于关联两个表.</li>
<li><strong>索引:</strong> 使用索引可快速访问数据库表中的特定信息. 索引是对数据库表中一列或多列的值进行排序的一种结构, 类似于书籍的目录.</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">官网</a>下载对应的dmg安装包.</p>
<p>一路点确定, 中间过程中注意关于密码的设置或者提示.</p>
<p>在安装以后默认启动, 可以通过<code>ps aux | grep mysql</code>查看是否有相关进程.</p>
<p>修改配置文件, 方便后续控制, 修改<code>.bash_profile</code>或者<code>.zshrc</code>, 这里设置了登录, 启动, 关闭, 重新启动的别名.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mysql</span><br><span class="line">alias mysql=/usr/local/mysql/bin/mysql</span><br><span class="line">alias mysql_start=&quot;sudo /usr/local/mysql/support-files/mysql.server start&quot;</span><br><span class="line">alias mysql_stop=&quot;sudo /usr/local/mysql/support-files/mysql.server stop&quot;</span><br><span class="line">alias mysql_restart=&quot;sudo /usr/local/mysql/support-files/mysql.server restart&quot;</span><br></pre></td></tr></table></figure>
<p>然后再使修改生效:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ source .bash_profile</span><br><span class="line"># 或者</span><br><span class="line">$ source .zshrc</span><br></pre></td></tr></table></figure>
<p>查看端口号, 登录MySQL后, 运行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show global variables like &apos;port&apos;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| port          | 3306  |</span><br><span class="line">+---------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>若不想MySQL开机启动, 可在偏好设置中关闭开机启动.</p>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux(Ubuntu)"></a>Linux(Ubuntu)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install mysql-server</span><br><span class="line"># 配置</span><br><span class="line">$ sudo mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>一般来说需要让Linux上的MySQL作为服务端, 所以需要设置允许远程登录. 需要修改配置文件, 配置文件具体位置一般在<code>\etc\mysql\my.cnf</code>或者<code>/etc/mysql/mysql.conf.d/mysqld.cnf</code>, 将其中的<code>bind-address=127.0.0.1</code>进行注释.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ netstat -nlt | grep 3306</span><br></pre></td></tr></table></figure>
<p>此时可以看到:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tcp6       0      0 :::3306                 :::*                    LISTEN</span><br></pre></td></tr></table></figure>
<p>网络监听从<code>127.0.0.1:3306</code>变成 <code>0 0.0.0.0:3306</code>, 表示MySQL已经允许远程登陆访问.</p>
<p>进一步设置, 登录MySQL, 然后根据需求输入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;user_name&apos;@&apos;IP&apos; IDENTIFIED BY &apos;passwd&apos;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>
<p>其中第一个<code>*</code>是数据库, 可以改成允许访问的数据库名称; 第二个<code>*</code>是数据库的表名称, 代表允许访问任意的表; <code>user_name</code>表示允许远程登录的用户名, 自定义; <code>IP</code>表示远程登录的IP, 可用<code>%</code>表示任意IP; <code>passwd</code>表示登录密码.</p>
<p>使用如下代码可以查看当前有哪些用户, 及相关信息:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; USE mysql;</span><br><span class="line">mysql&gt; SELECT User, Host, plugin FROM mysql.user;</span><br></pre></td></tr></table></figure>
<p>关于MySQL的启动, 关闭和重启:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">$ service mysql stop</span><br><span class="line"></span><br><span class="line"># 重启</span><br><span class="line">$ service mysql restart</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul>
<li><p>创建数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; DROP DATABASE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择数据库.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; USE 数据库名;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="数据表"><a href="#数据表" class="headerlink" title="数据表"></a>数据表</h2><ul>
<li><p>常用数据类型.</p>
<ul>
<li>INT: 整数.</li>
<li>FLOAT: 浮点数.</li>
<li>DATE: 日期, YYYY-MM-DD.</li>
<li>TIME: 时间, HH:MM:SS.</li>
<li>YEAR: 年, YYYY.</li>
<li>DATATIME:  YYYY-MM-DD HH:MM:SS.</li>
<li>TIMESTAMP: 时间戳.</li>
<li>CHAR: 定长字符串.</li>
<li>VARCHAR: 变长字符串.</li>
</ul>
</li>
<li><p>创建数据表.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE IF NOT EXISTS 数据表名(</span><br><span class="line">		-&gt; idx INT UNSIGNED NOT NULL AUTO_INCREMENT,</span><br><span class="line">		-&gt; name VARCHAR(10) DEFAULT NULL,</span><br><span class="line">		-&gt; height FLOAT DEFAULT NULL,</span><br><span class="line">		-&gt; date DATE DEFAULT NULL,</span><br><span class="line">		-&gt; PRIMARY KEY(idx)</span><br><span class="line">		-&gt; )ENGINE=innoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除数据表.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; DROP TABLE 数据表名;</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; INSERT INTO 数据表名</span><br><span class="line">		-&gt; (field1,...,fieldN)</span><br><span class="line">		-&gt; VALUES</span><br><span class="line">		-&gt; (value1,...,valueN);</span><br></pre></td></tr></table></figure>
<p>从文件导入.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; LOAD DATA LOCAL INFILE &apos;文件路径&apos; INTO TABLE test_table CHARACTER SET utf8</span><br><span class="line">		-&gt; (name, height, date)</span><br><span class="line">		-&gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">		-&gt; LINES TERMINATED BY &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
<p>其中<code>LOCAL</code>表示本地, 不加表示服务器路径. 后面两行指定分隔符和换行符.</p>
</li>
<li><p>导出数据.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查询允许导出路径</span><br><span class="line">mysql&gt; SHOW VARIABLES LIKE &apos;%secure%&apos;;</span><br><span class="line"></span><br><span class="line"># 导出</span><br><span class="line">mysql&gt; SELECT * FROM 数据表名</span><br><span class="line">		-&gt; INTO OUTFILE &apos;文件路径&apos;</span><br><span class="line">		-&gt; FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">		-&gt; ENCLOSED BY &apos;&quot;&apos;</span><br><span class="line">		-&gt; LINES TERMINATED BY &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>其它增/删/改/查的操作.</p>
<p>可查看我的另外一篇博客<a href="https://whitemoonlight.top/2020/09/06/编程基础/结构化查询语言SQL/" target="_blank" rel="noopener">结构化查询语言SQL</a>.</p>
</li>
</ul>
<h2 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h2><p>当要借助Python来查询MySQL中的数据时, 可以使用pymysql这个包, 用法如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装</span><br><span class="line">$ pip install pymysql</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接</span></span><br><span class="line">connection = pymysql.connect(host=<span class="string">'ip'</span>,</span><br><span class="line">                             port=<span class="number">3306</span>,</span><br><span class="line">                             user=<span class="string">'user_name'</span>,</span><br><span class="line">                             password=<span class="string">'passwd'</span>,</span><br><span class="line">                             db=<span class="string">'database_name'</span>,</span><br><span class="line">                             charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的查询, 并以DataFrame形式返回</span></span><br><span class="line">sql = <span class="string">'SELECT * FROM table_name;'</span></span><br><span class="line">df = pd.read_sql(sql, connection)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取游标</span></span><br><span class="line">cursor = connection.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行查询</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取单条数据</span></span><br><span class="line">cursor.fetchone()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取前N条数据</span></span><br><span class="line">cursor.fetchmany(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有数据</span></span><br><span class="line">cursor.fetchall()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据表</span></span><br><span class="line">cursor.execute(<span class="string">'''CREATE TABLE ...'''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据(元组或列表)</span></span><br><span class="line">cursor.execute(<span class="string">'INSERT INTO `users` (`name`, `age`) VALUES (%s, %s)'</span>, (<span class="string">'mary'</span>, <span class="number">18</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 插入数据(字典)</span></span><br><span class="line">info = &#123;<span class="string">'name'</span>: <span class="string">'fake'</span>, <span class="string">'age'</span>: <span class="number">15</span>&#125;</span><br><span class="line">cursor.execute(<span class="string">'''INSERT INTO `users`</span></span><br><span class="line"><span class="string">									(`name`, `age`)</span></span><br><span class="line"><span class="string">									VALUES</span></span><br><span class="line"><span class="string">									(%(name)s, %(age)s)'''</span>, info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># INSERT, UPDATE, DELETE等修改数据的语句需手动执行完成对数据修改的提交</span></span><br><span class="line">connection.commit()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤算法(三)</title>
    <url>/2020/09/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95-%E4%B8%89/</url>
    <content><![CDATA[<p>这一篇中使用Python来实现<strong>物品协同过滤</strong>算法.</p>
<a id="more"></a>
<h1 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h1><p>与上一篇的用户协同一样, 这里使用的数据, 仍然是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>由于本篇只讲协同过滤算法, 所以只用到了”rating.dat”.</p>
<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p>同用户协同差不多, 物品协同在实现时, 有那么几个重要的板块:</p>
<ul>
<li><p>数据准备</p>
<p>对原始数据进行简单分析, 原数据中有时间戳这一项, 可根据对其进行排序, 然后按7:3进行划分. 针对测试集中的新物品, 由于无法使用物品协同进行推荐, 所以进行过滤.</p>
</li>
<li><p>数据结构的选择.</p>
<p>考虑到稀疏性的存在, 这里不使用二维数据(Array)存放信息, 而使用字典(Dict)进行储存. </p>
</li>
<li><p>相似度选择.</p>
<p>布尔型的仿余弦相似度.</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{|p\cap q|}{\sqrt{|p|\times|q|}}</script><p>其中的$p$和$q$表示两个物品对应的用户集合.</p>
</li>
<li><p>测试集评估.</p>
<p>对测试集各用户进行推荐, 并结合推荐的结果和实际的结果, 计算查准率和查全率.</p>
</li>
</ul>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">data_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        data_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;1&apos;, &apos;1193&apos;, &apos;5&apos;, &apos;978300760&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;661&apos;, &apos;3&apos;, &apos;978302109&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;914&apos;, &apos;3&apos;, &apos;978301968&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;3408&apos;, &apos;4&apos;, &apos;978300275&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;2355&apos;, &apos;5&apos;, &apos;978824291&apos;]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6040, 3706)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的用户观看影片数量分布</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间戳排序, 并按排序划分训练集/测试集</span></span><br><span class="line"></span><br><span class="line">data_list = list(sorted(data_list, key=<span class="keyword">lambda</span> x: x[<span class="number">3</span>]))</span><br><span class="line">train_list = data_list[:<span class="number">700000</span>]</span><br><span class="line">test_list = data_list[<span class="number">700000</span>:]</span><br><span class="line">len(train_list), len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(700000, 300209)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 影片被观看次数少于20的剔除</span></span><br><span class="line">c_item_train = Counter([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line"></span><br><span class="line">drop_item_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_item_train <span class="keyword">if</span> c_item_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_item_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">693867</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的物品剔除</span></span><br><span class="line">train_item_set = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">1</span>] <span class="keyword">in</span> train_item_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">296059</span><br></pre></td></tr></table></figure>
<p>在上一篇中, 测试集中有不少的新用户, 而在这里可以看到并没有什么新物品.</p>
<h1 id="物品协同"><a href="#物品协同" class="headerlink" title="物品协同"></a>物品协同</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">基于物品的协同过滤算法.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, Counter, OrderedDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemCFWithBoolSimilarity</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    物品协同过滤, 布尔余弦相似度.</span></span><br><span class="line"><span class="string">    输入数据格式为list[list[user_id, item_id, rating], ...].</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.user_item_dict = defaultdict(dict)  <span class="comment"># 建立用户-物品字典, &#123;用户: &#123;物品: 打分, ...&#125;, ...&#125;</span></span><br><span class="line">        self.item_count_dict = defaultdict(int)  <span class="comment"># 记录每个物品的积累数</span></span><br><span class="line">        self.item_item_count_dict = defaultdict(Counter)  <span class="comment"># 建立物品-物品共现字典, &#123;物品: &#123;物品: num, ...&#125;, ...&#125;</span></span><br><span class="line">        self.item_item_sim_dict = defaultdict(OrderedDict)  <span class="comment"># 计算相似度, &#123;物品: &#123;物品: sim, ...&#125;, ...&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_user_item_info_dict</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">            user = list_[<span class="number">0</span>]</span><br><span class="line">            item = list_[<span class="number">1</span>]</span><br><span class="line">            rating = int(list_[<span class="number">2</span>])</span><br><span class="line">            self.user_item_dict[user][item] = rating</span><br><span class="line">            self.item_count_dict[item] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_item_item_info_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _, item_rat_dict <span class="keyword">in</span> self.user_item_dict.items():</span><br><span class="line">            c = Counter(item_rat_dict.keys())</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> item_rat_dict:</span><br><span class="line">                self.item_item_count_dict[item] += c</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self.item_item_count_dict:</span><br><span class="line">            self.item_item_count_dict[item].pop(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_item_item_sim_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item_a <span class="keyword">in</span> self.item_item_count_dict:</span><br><span class="line">            tmp_list = []</span><br><span class="line">            <span class="keyword">for</span> item_b, count <span class="keyword">in</span> self.item_item_count_dict[item_a].items():</span><br><span class="line">                sim = count / np.sqrt(self.item_count_dict[item_a] * self.item_count_dict[item_b])</span><br><span class="line">                tmp_list.append([item_b, sim])</span><br><span class="line">            self.item_item_sim_dict[item_a] = OrderedDict(tmp_list)</span><br><span class="line">        <span class="comment"># 相似度归一化</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self.item_item_sim_dict:</span><br><span class="line">            max_sim = max(self.item_item_sim_dict[item].values())</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> self.item_item_sim_dict[item]:</span><br><span class="line">                self.item_item_sim_dict[item][i] /= max_sim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练用户协同过滤模型.</span></span><br><span class="line"><span class="string">        :param data: list[list[user_id, item_id, rating], ...].</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立用户-物品字典, &#123;用户: &#123;物品: 打分, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'建立用户-物品字典...'</span>)</span><br><span class="line">        self._gen_user_item_info_dict(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立物品-物品共现字典, &#123;物品: &#123;物品: num, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'建立物品-物品字典...'</span>)</span><br><span class="line">        self._gen_item_item_info_dict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算相似度, &#123;物品: &#123;物品: sim, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'计算物品-物品相似度...'</span>)</span><br><span class="line">        self._gen_item_item_sim_dict()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'训练完毕.'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, user, k=<span class="number">5</span>, top_n=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        给定用户, 相似物品数, 推荐物品数量, 返回推荐结果.</span></span><br><span class="line"><span class="string">        :param user: 用户ID.</span></span><br><span class="line"><span class="string">        :param k: 相似物品数.</span></span><br><span class="line"><span class="string">        :param top_n: 推荐物品数量.</span></span><br><span class="line"><span class="string">        :return: order_dict&#123;物品: 感兴趣程度&#125;.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 对每个已接触物品, 根据相似度找到非接触的k个物品, 并累计计算感兴趣程度</span></span><br><span class="line">        item_rating_dict = self.user_item_dict[user].copy()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> item_rating_dict:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.item_item_sim_dict:</span><br><span class="line">                item_rating_dict.pop(item)</span><br><span class="line">        <span class="keyword">if</span> len(item_rating_dict) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        item_set = set(item_rating_dict.keys())</span><br><span class="line">        info_dict = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> item_set:</span><br><span class="line">            tmp_item_list = list(self.item_item_sim_dict[item].keys())</span><br><span class="line">            tmp_item_list = [x <span class="keyword">for</span> x <span class="keyword">in</span> tmp_item_list <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> item_set]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tmp_item_list[: k]:</span><br><span class="line">                info_dict[i] += self.user_item_dict[user][item] * self.item_item_sim_dict[item][i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 排序得到前top-n个并返回</span></span><br><span class="line">        res_list = list(sorted(info_dict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))[: top_n]</span><br><span class="line">        <span class="keyword">return</span> OrderedDict(res_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, data, k=<span class="number">5</span>, top_n=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        对给定数据集, 以及对应参数, 使用查准率和查全率进行评估.</span></span><br><span class="line"><span class="string">        :param data: list[list[user_id, item_id], ...].</span></span><br><span class="line"><span class="string">        :param k:  相似物品数.</span></span><br><span class="line"><span class="string">        :param top_n:  推荐物品数量.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        user_item_dict = defaultdict(set)</span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">            user = list_[<span class="number">0</span>]</span><br><span class="line">            item = list_[<span class="number">1</span>]</span><br><span class="line">            user_item_dict[user].add(item)</span><br><span class="line">        TP = <span class="number">0</span></span><br><span class="line">        TP_plus_FP = <span class="number">0</span>  <span class="comment"># precision = TP / TP_plus_FP</span></span><br><span class="line">        TP_plus_FN = <span class="number">0</span>  <span class="comment"># recall = TP / TP_plus_FN</span></span><br><span class="line">        <span class="keyword">for</span> user, item_set <span class="keyword">in</span> user_item_dict.items():</span><br><span class="line">            pred_items = self.predict(user, k, top_n)</span><br><span class="line">            <span class="keyword">if</span> pred_items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            pred_items = set(pred_items.keys())</span><br><span class="line">            TP += len(pred_items.intersection(item_set))</span><br><span class="line">            TP_plus_FP += top_n</span><br><span class="line">            TP_plus_FN += len(item_set)</span><br><span class="line">        precision = TP / TP_plus_FP</span><br><span class="line">        recall = TP / TP_plus_FN</span><br><span class="line">        <span class="comment"># print('查准率: %.2f%%, 查全率%.2f%%.' % (precision * 100, recall * 100))</span></span><br><span class="line">        <span class="keyword">return</span> precision, recall</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_cf = ItemCFWithBoolSimilarity()</span><br><span class="line">item_cf.train(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">建立用户-物品字典...</span><br><span class="line">建立物品-物品字典...</span><br><span class="line">计算物品-物品相似度...</span><br><span class="line">训练完毕.</span><br></pre></td></tr></table></figure>
<h1 id="测试集评估"><a href="#测试集评估" class="headerlink" title="测试集评估"></a>测试集评估</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">precision_list = []</span><br><span class="line">recall_list = []</span><br><span class="line">f1_list = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>, <span class="number">1600</span>]:</span><br><span class="line">    tmp_precision_list = []</span><br><span class="line">    tmp_recall_list = []</span><br><span class="line">    tmp_f1_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> top_n <span class="keyword">in</span> [<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>]:    </span><br><span class="line">        precision, recall = item_cf.evaluate(test_list, k=k, top_n=top_n)</span><br><span class="line">        tmp_precision_list.append(precision)</span><br><span class="line">        tmp_recall_list.append(recall)</span><br><span class="line">        tmp_f1_list.append(<span class="number">2</span>*precision*recall/(precision+recall))</span><br><span class="line">        </span><br><span class="line">    precision_list.append(tmp_precision_list)</span><br><span class="line">    recall_list.append(tmp_recall_list)</span><br><span class="line">    f1_list.append(tmp_f1_list)</span><br><span class="line">    </span><br><span class="line">df_precision = pd.DataFrame(precision_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>, <span class="number">1600</span>])</span><br><span class="line">df_recall = pd.DataFrame(recall_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>, <span class="number">1600</span>])</span><br><span class="line">df_f1 = pd.DataFrame(f1_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>, <span class="number">1600</span>])</span><br></pre></td></tr></table></figure>
<p>以<code>k</code>为行, <code>top_n</code>为列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_precision</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">50</td>
<td style="text-align:center">0.135133</td>
<td style="text-align:center">0.090018</td>
<td style="text-align:center">0.089743</td>
<td style="text-align:center">0.082248</td>
<td style="text-align:center">0.071190</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">0.202920</td>
<td style="text-align:center">0.146248</td>
<td style="text-align:center">0.117407</td>
<td style="text-align:center">0.103235</td>
<td style="text-align:center">0.086533</td>
</tr>
<tr>
<td style="text-align:center">200</td>
<td style="text-align:center">0.255310</td>
<td style="text-align:center">0.187115</td>
<td style="text-align:center">0.155602</td>
<td style="text-align:center">0.126606</td>
<td style="text-align:center">0.104673</td>
</tr>
<tr>
<td style="text-align:center">400</td>
<td style="text-align:center">0.282124</td>
<td style="text-align:center">0.203310</td>
<td style="text-align:center">0.171673</td>
<td style="text-align:center">0.142142</td>
<td style="text-align:center">0.112084</td>
</tr>
<tr>
<td style="text-align:center">800</td>
<td style="text-align:center">0.300708</td>
<td style="text-align:center">0.225929</td>
<td style="text-align:center">0.191558</td>
<td style="text-align:center">0.157115</td>
<td style="text-align:center">0.123520</td>
</tr>
<tr>
<td style="text-align:center">1600</td>
<td style="text-align:center">0.311416</td>
<td style="text-align:center">0.240088</td>
<td style="text-align:center">0.206673</td>
<td style="text-align:center">0.169951</td>
<td style="text-align:center">0.132369</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_recall</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">50</td>
<td style="text-align:center">0.011479</td>
<td style="text-align:center">0.038233</td>
<td style="text-align:center">0.076233</td>
<td style="text-align:center">0.139731</td>
<td style="text-align:center">0.241891</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">0.017237</td>
<td style="text-align:center">0.062115</td>
<td style="text-align:center">0.099732</td>
<td style="text-align:center">0.175385</td>
<td style="text-align:center">0.294023</td>
</tr>
<tr>
<td style="text-align:center">200</td>
<td style="text-align:center">0.021687</td>
<td style="text-align:center">0.079473</td>
<td style="text-align:center">0.132176</td>
<td style="text-align:center">0.215092</td>
<td style="text-align:center">0.355657</td>
</tr>
<tr>
<td style="text-align:center">400</td>
<td style="text-align:center">0.023965</td>
<td style="text-align:center">0.086351</td>
<td style="text-align:center">0.145828</td>
<td style="text-align:center">0.241485</td>
<td style="text-align:center">0.380840</td>
</tr>
<tr>
<td style="text-align:center">800</td>
<td style="text-align:center">0.025544</td>
<td style="text-align:center">0.095958</td>
<td style="text-align:center">0.162719</td>
<td style="text-align:center">0.266923</td>
<td style="text-align:center">0.419697</td>
</tr>
<tr>
<td style="text-align:center">1600</td>
<td style="text-align:center">0.026453</td>
<td style="text-align:center">0.101972</td>
<td style="text-align:center">0.175558</td>
<td style="text-align:center">0.288731</td>
<td style="text-align:center">0.449766</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_f1</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">50</td>
<td style="text-align:center">0.021160</td>
<td style="text-align:center">0.053670</td>
<td style="text-align:center">0.082438</td>
<td style="text-align:center">0.103547</td>
<td style="text-align:center">0.110005</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">0.031775</td>
<td style="text-align:center">0.087196</td>
<td style="text-align:center">0.107850</td>
<td style="text-align:center">0.129968</td>
<td style="text-align:center">0.133713</td>
</tr>
<tr>
<td style="text-align:center">200</td>
<td style="text-align:center">0.039979</td>
<td style="text-align:center">0.111562</td>
<td style="text-align:center">0.142936</td>
<td style="text-align:center">0.159392</td>
<td style="text-align:center">0.161743</td>
</tr>
<tr>
<td style="text-align:center">400</td>
<td style="text-align:center">0.044177</td>
<td style="text-align:center">0.121218</td>
<td style="text-align:center">0.157698</td>
<td style="text-align:center">0.178950</td>
<td style="text-align:center">0.173195</td>
</tr>
<tr>
<td style="text-align:center">800</td>
<td style="text-align:center">0.047088</td>
<td style="text-align:center">0.134704</td>
<td style="text-align:center">0.175964</td>
<td style="text-align:center">0.197801</td>
<td style="text-align:center">0.190866</td>
</tr>
<tr>
<td style="text-align:center">1600</td>
<td style="text-align:center">0.048764</td>
<td style="text-align:center">0.143146</td>
<td style="text-align:center">0.189849</td>
<td style="text-align:center">0.213962</td>
<td style="text-align:center">0.204541</td>
</tr>
</tbody>
</table>
</div>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>从测试集评估的结果来看, 查准率随着<code>k</code>的增加而增加, 随着<code>top_n</code>的增加而降低. 在<code>k=1600, top_n=10</code>达到最大的31%.</p>
<p>查全率随着<code>k</code>的增加而增加, 随着<code>top_n</code>的增加而增加. 在<code>k=1600, top_n=400</code>达到最大的45%.</p>
<p>F1值随着<code>k</code>的增大而增大, 随着<code>top_n</code>的增加, 则先增大后减小. 在<code>k=1600, top_n=200</code>达到最大, 为0.21, 此时对应的查准率为17%, 查全率为29%.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤算法(二)</title>
    <url>/2020/09/02/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 主要介绍了协同过滤算法的原理与特点.</p>
<p>这一篇中, 则使用Python来实现<strong>用户协同过滤</strong>算法.</p>
<a id="more"></a>
<h1 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h1><p>这里使用的数据, 是经典的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">movielens</a>的MovieLens 1M Dataset.</p>
<p>其中包含约100万的用户对电影的打分记录, 由约6000用户对约4000电影打分而得到.</p>
<p>包含3份数据, 一份是打分文件”rating.dat”, 格式为<code>用户ID::电影ID::打分::时间戳</code>, 并且:</p>
<ul>
<li>用户ID在1到6040之间.</li>
<li>电影ID在1到3952之间.</li>
<li>打分有5个等级.</li>
<li>每个用户至少有20条打分记录.</li>
</ul>
<p>一份是用户文件”user.dat”, 格式为<code>用户ID::性别::年龄::职业::邮编</code>.</p>
<p>还有一份电影文件”movies.dat”, 格式为<code>电影ID::电影名称::风格流派</code>.</p>
<p>由于本篇只讲协同过滤算法, 所以只用到了”rating.dat”.</p>
<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p>对于协同过滤的实现其实不止一种方式, 在这里主要为了体现协同算法的算法流程, 选择了在上面比较小的数据集上, 用Python的原生数据结构实现, 并在众多的相似度中, 选择布尔型的仿余弦相似度.</p>
<p>那么了解了使用协同过滤算法进行推荐的流程后, 在实现时有那么几个重要的板块:</p>
<ul>
<li><p>数据准备</p>
<p>对原始数据进行简单分析, 原数据中有时间戳这一项, 可根据对其进行排序, 然后按7:3进行划分. 针对测试集中的新用户, 由于无法使用用户协同进行推荐, 所以进行过滤.</p>
</li>
<li><p>数据结构的选择.</p>
<p>考虑到稀疏性的存在, 这里不使用二维数据(Array)存放信息, 而使用字典(Dict)进行储存. 同时为了减少算法时间复杂度, 除了<code>{用户:物品}</code>的字典, 同时生成<code>{物品:用户}</code>的倒排表(倒排字典).</p>
</li>
<li><p>相似度选择.</p>
<p>布尔型的仿余弦相似度.</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{|p\cap q|}{\sqrt{|p|\times|q|}}</script><p>其中的$p$和$q$表示两个用户的物品集合.</p>
</li>
<li><p>测试集评估.</p>
<p>对测试集各用户进行推荐, 并结合推荐的结果和实际的结果, 计算查准率和查全率.</p>
</li>
</ul>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">data_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/ml-1m/ratings.dat'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        line = line.strip()</span><br><span class="line">        data_list.append(line.split(<span class="string">'::'</span>))</span><br><span class="line">    f.close()</span><br><span class="line">data_list[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[&apos;1&apos;, &apos;1193&apos;, &apos;5&apos;, &apos;978300760&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;661&apos;, &apos;3&apos;, &apos;978302109&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;914&apos;, &apos;3&apos;, &apos;978301968&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;3408&apos;, &apos;4&apos;, &apos;978300275&apos;],</span><br><span class="line"> [&apos;1&apos;, &apos;2355&apos;, &apos;5&apos;, &apos;978824291&apos;]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6040, 3706)</span><br></pre></td></tr></table></figure>
<p>有6000用户, 近4000物品.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计分析数据集中的用户观看影片数量分布</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">c_user = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list])</span><br><span class="line">sns.distplot(list(c_user.values()), bins=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 考虑到推荐效果, 将特别活跃的用户过滤掉</span></span><br><span class="line">drop_user_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_user <span class="keyword">if</span> c_user[x] &gt; <span class="number">1000</span>])</span><br><span class="line">new_data_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> data_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_user_set:</span><br><span class="line">        new_data_list.append(list_)</span><br><span class="line">data_list = new_data_list</span><br><span class="line"></span><br><span class="line">n_user = len(set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_item = len(set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data_list]))</span><br><span class="line">n_user, n_item</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(6000, 3691)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按时间戳排序, 并按排序划分训练集/测试集</span></span><br><span class="line"></span><br><span class="line">data_list = list(sorted(data_list, key=<span class="keyword">lambda</span> x: x[<span class="number">3</span>]))</span><br><span class="line">train_list = data_list[:<span class="number">700000</span>]</span><br><span class="line">test_list = data_list[<span class="number">700000</span>:]</span><br><span class="line">len(train_list), len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(700000, 250316)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将训练集中, 观看影片数量少于20的用户剔除</span></span><br><span class="line">c_user_train = Counter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line"></span><br><span class="line">drop_user_set = set([x <span class="keyword">for</span> x <span class="keyword">in</span> c_user_train <span class="keyword">if</span> c_user_train[x] &lt; <span class="number">20</span>])</span><br><span class="line">new_train_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> train_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> drop_user_set:</span><br><span class="line">        new_train_list.append(list_)</span><br><span class="line">train_list = new_train_list</span><br><span class="line"></span><br><span class="line">len(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">698662</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将测试集中, 没有在训练集出现过的用户剔除</span></span><br><span class="line">train_user_set = set([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_list])</span><br><span class="line">new_test_list = []</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> test_list:</span><br><span class="line">    <span class="keyword">if</span> list_[<span class="number">0</span>] <span class="keyword">in</span> train_user_set:</span><br><span class="line">        new_test_list.append(list_)</span><br><span class="line">test_list = new_test_list</span><br><span class="line"></span><br><span class="line">len(test_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">90288</span><br></pre></td></tr></table></figure>
<p>可以看到, 在这份样本中, 有许多新增用户, 而这部分用户难以使用用户协同进行推荐.</p>
<h1 id="用户协同"><a href="#用户协同" class="headerlink" title="用户协同"></a>用户协同</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">基于用户的协同过滤算法.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, Counter, OrderedDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserCFWithBoolSimilarity</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    用户协同过滤, 布尔余弦相似度.</span></span><br><span class="line"><span class="string">    输入数据格式为list[list[user_id, item_id, rating], ...].</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.user_item_dict = defaultdict(dict)  <span class="comment"># 建立用户-物品字典, &#123;用户: &#123;物品: 打分, ...&#125;, ...&#125;</span></span><br><span class="line">        self.item_user_dict = defaultdict(set)  <span class="comment"># 建立物品-用户字典, &#123;物品: &#123;用户, ...&#125;, ...&#125;</span></span><br><span class="line">        self.user_count_dict = defaultdict(int)  <span class="comment"># 记录每个用户有过记录的物品数</span></span><br><span class="line">        self.user_user_count_dict = defaultdict(Counter)  <span class="comment"># 建立用户-用户共现字典, &#123;用户: &#123;用户: num, ...&#125;, ...&#125;</span></span><br><span class="line">        self.user_user_sim_dict = defaultdict(OrderedDict)  <span class="comment"># 计算相似度, &#123;用户: &#123;用户: sim, ...&#125;, ...&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_user_item_info_dict</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">            user = list_[<span class="number">0</span>]</span><br><span class="line">            item = list_[<span class="number">1</span>]</span><br><span class="line">            rating = int(list_[<span class="number">2</span>])</span><br><span class="line">            self.user_item_dict[user][item] = rating</span><br><span class="line">            self.user_count_dict[user] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_item_user_info_dict</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">            user = list_[<span class="number">0</span>]</span><br><span class="line">            item = list_[<span class="number">1</span>]</span><br><span class="line">            self.item_user_dict[item].add(user)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_user_user_info_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _, user_set <span class="keyword">in</span> self.item_user_dict.items():</span><br><span class="line">            c = Counter(user_set)</span><br><span class="line">            <span class="keyword">for</span> user <span class="keyword">in</span> user_set:</span><br><span class="line">                self.user_user_count_dict[user] += c</span><br><span class="line">        <span class="keyword">for</span> user <span class="keyword">in</span> self.user_user_count_dict:</span><br><span class="line">            self.user_user_count_dict[user].pop(user)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_user_user_sim_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> user_a <span class="keyword">in</span> self.user_user_count_dict:</span><br><span class="line">            tmp_list = []</span><br><span class="line">            <span class="keyword">for</span> user_b, count <span class="keyword">in</span> self.user_user_count_dict[user_a].items():</span><br><span class="line">                sim = count / np.sqrt(self.user_count_dict[user_a] * self.user_count_dict[user_b])</span><br><span class="line">                tmp_list.append([user_b, sim])</span><br><span class="line">            self.user_user_sim_dict[user_a] = OrderedDict(tmp_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练用户协同过滤模型.</span></span><br><span class="line"><span class="string">        :param data: list[list[user_id, item_id, rating], ...].</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立用户-物品字典, &#123;用户: &#123;物品: 打分, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'建立用户-物品字典...'</span>)</span><br><span class="line">        self._gen_user_item_info_dict(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立物品-用户字典, &#123;物品: &#123;用户, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'建立物品-用户字典...'</span>)</span><br><span class="line">        self._gen_item_user_info_dict(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立用户-用户共现字典, &#123;用户: &#123;用户: num, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'建立用户-用户字典...'</span>)</span><br><span class="line">        self._gen_user_user_info_dict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算相似度, &#123;用户: &#123;用户: sim, ...&#125;, ...&#125;</span></span><br><span class="line">        print(<span class="string">'计算用户-用户相似度...'</span>)</span><br><span class="line">        self._gen_user_user_sim_dict()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'训练完毕.'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, user, k=<span class="number">5</span>, top_n=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        给定用户, 相似用户数, 推荐物品数量, 返回推荐结果.</span></span><br><span class="line"><span class="string">        :param user: 用户ID.</span></span><br><span class="line"><span class="string">        :param k: 相似用户数.</span></span><br><span class="line"><span class="string">        :param top_n: 推荐物品数量.</span></span><br><span class="line"><span class="string">        :return: order_dict&#123;物品: 感兴趣程度&#125;.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> self.user_user_sim_dict:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 前k个相似用户</span></span><br><span class="line">        k_user_list = list(self.user_user_sim_dict[user].keys())[: k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 收集前k个用户的物品</span></span><br><span class="line">        item_set = set()</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> k_user_list:</span><br><span class="line">            tmp_item_set = set(list(self.user_item_dict[u].keys()))</span><br><span class="line">            item_set = item_set.union(tmp_item_set)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 排除用户已接触物品</span></span><br><span class="line">        item_set = item_set.difference(set(list(self.user_item_dict[user].keys())))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每个候选物品计算感兴趣程度</span></span><br><span class="line">        res_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> item_set:</span><br><span class="line">            score = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> u <span class="keyword">in</span> k_user_list:</span><br><span class="line">                score += self.user_user_sim_dict[user][u] * self.user_item_dict[u].get(i, <span class="number">0</span>)</span><br><span class="line">            res_list.append([i, score])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 排序得到top-n个物品</span></span><br><span class="line">        res_list = list(sorted(res_list, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))[: top_n]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以有序字典形式返回</span></span><br><span class="line">        <span class="keyword">return</span> OrderedDict(res_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, data, k=<span class="number">5</span>, top_n=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        对给定数据集, 以及对应参数, 使用查准率和查全率进行评估.</span></span><br><span class="line"><span class="string">        :param data: list[list[user_id, item_id], ...].</span></span><br><span class="line"><span class="string">        :param k:  相似用户数.</span></span><br><span class="line"><span class="string">        :param top_n:  推荐物品数量.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        user_item_dict = defaultdict(set)</span><br><span class="line">        <span class="keyword">for</span> list_ <span class="keyword">in</span> data:</span><br><span class="line">            user = list_[<span class="number">0</span>]</span><br><span class="line">            item = list_[<span class="number">1</span>]</span><br><span class="line">            user_item_dict[user].add(item)</span><br><span class="line">        TP = <span class="number">0</span></span><br><span class="line">        TP_plus_FP = <span class="number">0</span>  <span class="comment"># precision = TP / TP_plus_FP</span></span><br><span class="line">        TP_plus_FN = <span class="number">0</span>  <span class="comment"># recall = TP / TP_plus_FN</span></span><br><span class="line">        <span class="keyword">for</span> user, item_set <span class="keyword">in</span> user_item_dict.items():</span><br><span class="line">            pred_items = self.predict(user, k, top_n)</span><br><span class="line">            <span class="keyword">if</span> pred_items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            pred_items = set(pred_items.keys())</span><br><span class="line">            TP += len(pred_items.intersection(item_set))</span><br><span class="line">            TP_plus_FP += top_n</span><br><span class="line">            TP_plus_FN += len(item_set)</span><br><span class="line">        precision = TP / TP_plus_FP</span><br><span class="line">        recall = TP / TP_plus_FN</span><br><span class="line"><span class="comment">#         print('查准率: %.2f%%, 查全率%.2f%%.' % (precision * 100, recall * 100))</span></span><br><span class="line">        <span class="keyword">return</span> precision, recall</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_cf = UserCFWithBoolSimilarity()</span><br><span class="line">user_cf.train(train_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">建立用户-物品字典...</span><br><span class="line">建立物品-用户字典...</span><br><span class="line">建立用户-用户字典...</span><br><span class="line">计算用户-用户相似度...</span><br><span class="line">训练完毕.</span><br></pre></td></tr></table></figure>
<h1 id="测试集评估"><a href="#测试集评估" class="headerlink" title="测试集评估"></a>测试集评估</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">precision_list = []</span><br><span class="line">recall_list = []</span><br><span class="line">f1_list = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>]:</span><br><span class="line">    tmp_precision_list = []</span><br><span class="line">    tmp_recall_list = []</span><br><span class="line">    tmp_f1_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> top_n <span class="keyword">in</span> [<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>]:    </span><br><span class="line">        precision, recall = user_cf.evaluate(test_list, k=k, top_n=top_n)</span><br><span class="line">        tmp_precision_list.append(precision)</span><br><span class="line">        tmp_recall_list.append(recall)</span><br><span class="line">        tmp_f1_list.append(<span class="number">2</span>*precision*recall/(precision+recall))</span><br><span class="line">        </span><br><span class="line">    precision_list.append(tmp_precision_list)</span><br><span class="line">    recall_list.append(tmp_recall_list)</span><br><span class="line">    f1_list.append(tmp_f1_list)</span><br><span class="line">    </span><br><span class="line">df_precision = pd.DataFrame(precision_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>])</span><br><span class="line">df_recall = pd.DataFrame(recall_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>])</span><br><span class="line">df_f1 = pd.DataFrame(f1_list, columns=[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">400</span>], index=[<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>])</span><br></pre></td></tr></table></figure>
<p>以<code>k</code>为行, <code>top_n</code>为列.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_precision</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.123868</td>
<td style="text-align:center">0.104527</td>
<td style="text-align:center">0.084949</td>
<td style="text-align:center">0.056168</td>
<td style="text-align:center">0.036870</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.210905</td>
<td style="text-align:center">0.153169</td>
<td style="text-align:center">0.128200</td>
<td style="text-align:center">0.105170</td>
<td style="text-align:center">0.081412</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">0.229835</td>
<td style="text-align:center">0.170905</td>
<td style="text-align:center">0.143477</td>
<td style="text-align:center">0.117233</td>
<td style="text-align:center">0.091775</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">0.249794</td>
<td style="text-align:center">0.188045</td>
<td style="text-align:center">0.159115</td>
<td style="text-align:center">0.129362</td>
<td style="text-align:center">0.100972</td>
</tr>
<tr>
<td style="text-align:center">40</td>
<td style="text-align:center">0.260082</td>
<td style="text-align:center">0.192922</td>
<td style="text-align:center">0.164300</td>
<td style="text-align:center">0.134578</td>
<td style="text-align:center">0.104545</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_recall</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.013335</td>
<td style="text-align:center">0.056264</td>
<td style="text-align:center">0.091452</td>
<td style="text-align:center">0.120935</td>
<td style="text-align:center">0.158770</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.022705</td>
<td style="text-align:center">0.082447</td>
<td style="text-align:center">0.138014</td>
<td style="text-align:center">0.226442</td>
<td style="text-align:center">0.350578</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">0.024743</td>
<td style="text-align:center">0.091995</td>
<td style="text-align:center">0.154461</td>
<td style="text-align:center">0.252414</td>
<td style="text-align:center">0.395202</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">0.026892</td>
<td style="text-align:center">0.101221</td>
<td style="text-align:center">0.171296</td>
<td style="text-align:center">0.278531</td>
<td style="text-align:center">0.434809</td>
</tr>
<tr>
<td style="text-align:center">40</td>
<td style="text-align:center">0.027999</td>
<td style="text-align:center">0.103845</td>
<td style="text-align:center">0.176878</td>
<td style="text-align:center">0.289762</td>
<td style="text-align:center">0.450193</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_f1</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">10</th>
<th style="text-align:center">50</th>
<th style="text-align:center">100</th>
<th style="text-align:center">200</th>
<th style="text-align:center">400</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.024078</td>
<td style="text-align:center">0.073152</td>
<td style="text-align:center">0.088080</td>
<td style="text-align:center">0.076709</td>
<td style="text-align:center">0.059843</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.040997</td>
<td style="text-align:center">0.107194</td>
<td style="text-align:center">0.132926</td>
<td style="text-align:center">0.143631</td>
<td style="text-align:center">0.132139</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">0.044676</td>
<td style="text-align:center">0.119607</td>
<td style="text-align:center">0.148767</td>
<td style="text-align:center">0.160105</td>
<td style="text-align:center">0.148958</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">0.048556</td>
<td style="text-align:center">0.131602</td>
<td style="text-align:center">0.164981</td>
<td style="text-align:center">0.176671</td>
<td style="text-align:center">0.163886</td>
</tr>
<tr>
<td style="text-align:center">40</td>
<td style="text-align:center">0.050556</td>
<td style="text-align:center">0.135015</td>
<td style="text-align:center">0.170358</td>
<td style="text-align:center">0.183794</td>
<td style="text-align:center">0.169685</td>
</tr>
</tbody>
</table>
</div>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>从测试集评估的结果来看, 查准率随着<code>k</code>的增加而增加, 随着<code>top_n</code>的增加而降低. 在<code>k=40, top_n=10</code>达到最大的26%.</p>
<p>查全率随着<code>k</code>的增加而增加, 随着<code>top_n</code>的增加而增加. 在<code>k=40, top_n=400</code>达到最大的45%.</p>
<p>F1值随着<code>k</code>的增大而增大, 随着<code>top_n</code>的增加, 则先增大后减小. 在<code>k=40, top_n=200</code>达到最大, 为0.18, 此时对应的查准率为13%, 查全率为29%.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤算法(一)</title>
    <url>/2020/09/02/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95-%E4%B8%80/</url>
    <content><![CDATA[<p>推荐系统由来已久, 在过往的时代背景下, 可能由于数据, 计算资源, 算法等方面的限制, 采用的是一些相对简单的方法, 这些方法一般来说可解释性强, 硬件环境要求低, 易于快速训练与部署.</p>
<p>传统方法实现了从零到一的突破, 为后来的方法提供了基础与思路, 并且其中一些方法即使在今天也仍在使用.</p>
<p>本篇主要介绍协同过滤算法.</p>
<a id="more"></a>
<p>说到推荐系统, 必然离不开协同过滤(Collaborative Filtering), 顾名思义, “协同过滤”就是协同大家的反馈, 评价和意见, 来一起对海量的信息进行过滤, 从中筛选出用户可能感兴趣的信息的推荐过程. </p>
<p>或者更加通俗地来说, 就是根据过往用户与物品的记录, 来<strong>找相似</strong>, 可以找相似的用户, 也可以找相似的物品来进行推荐.</p>
<h1 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h1><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>协同过滤算法的流程比较简单, 主要分为以下几个步骤:</p>
<ul>
<li>收集整理用户的偏好(包括评分, 浏览记录, 购买记录等).</li>
<li>根据以上数据, 找到相似的用户或物品.</li>
<li>根据相似度进行筛选, 推荐.</li>
</ul>
<p>具体来说, 分别以用户和物品为维度, 可以构建一个二维矩阵, 在这个矩阵中的每个元素, 代表着用户对物品的评价(打分), 或者行为(是否购买/观看).</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 每一个格子表示对应用户对一部电影的评分, 而一些格子是空的, 表示用户未观看或者未评分.</p>
<p>通过这个共现矩阵, 可以想办法获取表征用户或者物品的向量, 再通过相似度计算, 就可以找到兴趣相似的用户或者属性相当的物品, 从而进行推荐.</p>
<p>下面分别介绍基于用户, 和基于物品的协同过滤算法.</p>
<h2 id="相似度指标"><a href="#相似度指标" class="headerlink" title="相似度指标"></a>相似度指标</h2><p>首先介绍一些相似度指标.</p>
<p>给定两个向量,</p>
<script type="math/tex; mode=display">
p=(p_1,p_2,\dots,p_n) \\
q=(q_1,q_2,\dots,q_n)</script><p>计算其相似度的方式有很多, 总体说来有以下几类:</p>
<ul>
<li><p>欧式距离.</p>
<script type="math/tex; mode=display">
E(p,q)=\sqrt{\sum_i(p_i-q_i)^2}</script><p>其取值范围为$[0,\infty)$, 而一般相似度的度量结果希望在$[-1,1]$或者$[0,1]$之间, 所以可以进行转换:</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{1}{1+E(p,q)}</script><p>欧式距离衡量的是空间中两个点的绝对差异.</p>
</li>
<li><p>余弦相似度.</p>
<script type="math/tex; mode=display">
similarity(p, q)=\cos(p,q)=\frac{p\cdot q}{\parallel p\parallel \cdot\parallel q\parallel}</script><p>其取值范围为$[-1,1]$, 与向量的长度无关, 而与两个向量的夹角相关, 夹角越小, 相似度越大.</p>
<p>所谓遇事不决, 余弦相似.</p>
<p>余弦相似度在度量文本相似度, 用户相似度, 物品相似度时都比较常用.</p>
</li>
<li><p>皮尔森相关性.</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{\sum_i(p_i-\bar p)(q_i-\bar q)}{\sqrt{\sum_i(p_i-\bar p)^2}\sqrt{\sum_i(q_i-\bar q)}}</script><p>相比余弦相似度, 皮尔森相关性将每个向量使用自身均值进行平移. 比如一些用户对电影整体打分偏高, 一些用户偏低, 此时这种减去均值后再计算相似度的做法可能更有效.</p>
</li>
<li><p>杰卡德(Jaccard)相似度.</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{|p\cap q|}{|p\cup q|}</script><p>当向量为布尔型, 即只包含0, 1时, 可以使用杰卡德相似度.</p>
<p>或者仿余弦相似度:</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{|p\cap q|}{\sqrt{|p|\times|q|}}</script></li>
</ul>
<p>此外, 考虑到活跃/非活跃用户, 热门/非热门物品的影响, 可以进一步对协同过滤的相似度做改进.</p>
<p>对于物品协同的相似度, 做归一化处理:</p>
<script type="math/tex; mode=display">
similarity_{norm}(p,q)=\frac{similarity(p,q)}{\max_i(similarity(p,i))}</script><p>这里可以这么理解, 有的类型的物品整体和别的物品相似度低, 而有的整体高, 那么如果直接使用未归一化的相似度加权, 那些相似度高(比如一些热门物品), 就可能占据了整个推荐列表. 实验表明, 在使用归一化相似度后, 在覆盖度上有增加, 流行度上有降低.</p>
<p>以仿余弦相似度来说, 还可以改变相似度的形式, 如:</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{|p\cap q|}{|p|^{1-\alpha}\times|q|^\alpha}</script><p>当$\alpha=0.5$时, 与原相似度等价. 当$\alpha&gt;0.5$时, 会增加对活跃用户或者热门物品的惩罚, 随着$\alpha$的增加, 覆盖度增加, 流行度降低.</p>
<p>更复杂一些的形式如下:</p>
<script type="math/tex; mode=display">
similarity(p,q)=\frac{\sum_{u\in p\cap q}\frac{1}{\log(1+|u|)}}{|p|^{1-\alpha}\times|q|^\alpha}</script><p>这个相似度中, 同时考虑了热门物品与活跃用户, 比如$p$与$q$表示两个物品对应的用户集合, 在分子中, $u$为$p$与$q$的交集用户, 若其为活跃用户, 则$|u|$就较大, 其对分子的贡献就小.</p>
<h2 id="用户协同"><a href="#用户协同" class="headerlink" title="用户协同"></a>用户协同</h2><p>UserCF核心思想是, “兴趣相似的朋友喜欢的物品, 我也喜欢”. 因此, 首先通过用户向量, 找到 Top-N​相似的用户, 然后根据相似用户的已有评价对目标用户的偏好进行预测.</p>
<p>而具体来说, 可以拿用户对所有物品的评分向量, 作为计算其相似度的因子. 如果采用余弦相似度, 皮尔森相关性等, 可以将未知评分设置为0, 但是这样看起来就像是用户给这个物品打了一个很低的分数一样. 或者更加科学的做法是将其设置为其它值, 如某个物品所有的分数的均值, 作为其缺失的填充值.</p>
<p>若使用布尔型的杰卡德相似度或者仿余弦相似度, 则根据每个用户对应的物品集合来进行计算.</p>
<p>在有了用户与用户之间的相似度以后, 当要为某个用户进行推荐时, 首先挑选出K个与该用户最相似的用户, 对应的有这K个用户的物品集合. 对物品集合中的每个物品, 其感兴趣程度按下式计算:</p>
<script type="math/tex; mode=display">
score(u,i)=\sum_{v\in U_K} similarity(u,v)\times rating_{v,i}</script><p>其中的$rating_{v,i}$表示用户$v$对物品$i$的评分.</p>
<p>然后对物品集合根据感兴趣程度进行排序, 将Top-N作为推荐结果.</p>
<h2 id="物品协同"><a href="#物品协同" class="headerlink" title="物品协同"></a>物品协同</h2><p>ItemCF的计算流程与UserCF类似, 首先获取物品之间的相似度矩阵, 在物品协同这里, 用物品对应的用户评分向量来作为计算相似度的因子.</p>
<p>在具体计算相似度时, 与用户协同类似.</p>
<p>计算好物品之间的相似度后, 当要为某个用户推荐时, 首先对于该用户有过评分的每个物品, 根据物品相似度, 挑选出K个相似的物品, 这样得到一个大的物品集合. 对于集合中的每个物品, 感兴趣程度计算如下:</p>
<script type="math/tex; mode=display">
score(u,i)=\sum_{j\in I_u} similarity(i,j)\times rating_{u,j}</script><p>其中的$rating_{u,j}$表示用户$u$对物品$j$的评分.</p>
<p>同样排序后, 将Top-N作为推荐结果.</p>
<h1 id="用户协同与物品协同的使用场景"><a href="#用户协同与物品协同的使用场景" class="headerlink" title="用户协同与物品协同的使用场景"></a>用户协同与物品协同的使用场景</h1><p>对于用户协同来说, 是推荐给用户与其相似的用户喜欢的物品, 这种情况下当某个用户偏好某个新的物品时, 可以较快地”传递”给其它相似用户. 所以用户协同的推荐方式更加”社会化”, 可以反映用户所在的相似的用户团体中物品的热门程度.</p>
<p>比如在新闻类的推荐中, 用户协同是一种相对不错的方式, 原因如下:</p>
<ul>
<li><p>热门与时效是重点.</p>
<p>在新闻领域, 相比用户某个具体的偏好领域的新闻, 最新的热点新闻才是更加重要的. 热门程度与时效性是新闻推荐的重点, 个性化需求相对偏弱.</p>
</li>
<li><p>技术实现考虑.</p>
<p>新闻一般更新的非常快的, 如果使用物品协同, 维护物品与物品的相似性, 频繁更新是有难度的. 而采用用户协同, 维护用户与用户的相似性, 是不需要频繁更新的. 从技术的角度上来说, 也是用户协同更加合适.</p>
</li>
</ul>
<p>而对于物品协同来说, 可以在电商场景, 电影, 美食推荐等场景使用, 有以下原因:</p>
<ul>
<li><p>个性化需求是重点.</p>
<p>在这些场景中, 用户的兴趣一般相对持久, 比如一个用户是程序员, 这次买了<c++从入门到精通>, 下一次可能就会买&lt;头发护理实战&gt;, 可能并不会太关心最近出了什么新书.</c++从入门到精通></p>
</li>
<li><p>技术实现考虑.</p>
<p>在这里, 物品更新一般不会太快, 所以在维护的物品和物品相似性时, 也不用频繁更新.</p>
</li>
</ul>
<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>在协同过滤这里, 可以使用精确率/查准率(Precision), 召回率/查全率(Recall)来进行评估:</p>
<script type="math/tex; mode=display">
Precision=\frac{\sum_{u\in U}|R(u)|\cap|T(u)|}{\sum_{u\in U}|R(u)|} \\
Recall=\frac{\sum_{u\in U}|R(u)|\cap|T(u)|}{\sum_{u\in U}|T(u)|}</script><p>其中$R(u)$表示用户$u$的推荐集合, $T(u)$表示用户实际的行为集合.</p>
<p>可以在线下评估时, 按时间切分训练集/测试集, 以防止信息泄露, 然后在测试集上使用以上指标进行评估.</p>
<p>但是用户的兴趣往往是多变的, 这一段时间可能喜欢游戏方面的内容, 过一段时间可能喜欢学习方面的内容. 所以最终模型的效果, 以线下指标作为参考, 并根据线上实际的情况进行评判.</p>
<p>同时, 单一的指标一般难以衡量一个模型的好坏, 存在众多的业务指标, 如点击率, 用户体验, 停留时长等. 可以根据当前侧重的业务指标, 去进行分析和调整.</p>
<h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><h2 id="数据问题"><a href="#数据问题" class="headerlink" title="数据问题"></a>数据问题</h2><ul>
<li><p>稀疏性.</p>
<p>现实场景中, 往往用户/物品的数量都很大, 而其中有过交互记录的相比整个用户-物品矩阵来说, 非常稀疏, 这会比较严重地影响协同过滤算法的效果.</p>
</li>
<li><p>新用户</p>
<p>在使用用户协同时, 对于新用户, 或者说记录很少的用户, 难以使用用户协同进行推荐. 但可以使用物品协同进行推荐.</p>
</li>
<li><p>新物品.</p>
<p>使用物品协同时, 对于新物品, 难以使用物品协同进行推荐. 但只要有了一些记录, 可以使用用户协同进行推荐.</p>
</li>
<li><p>活跃用户.</p>
<p>同时, 还存在非常活跃的用户, 这可能会导致在很多用户的Top-N相似用户中都有这些非常活跃的用户. 若有这种情况, 也可以对非常活跃的用户进行适当过滤, 改进相似度指标来改善.</p>
</li>
<li><p>热门物品.</p>
<p>除了用户外, 热门物品也会对推荐结果造成一些不好的影响. 按照常规协同过滤的做法, 更加倾向于推荐热门物品(更高的相似度), 而会忽略众多的尾部物品. 所以也可以对热门物品使用过滤, 改进相似度等方式.</p>
</li>
</ul>
<h2 id="隐性的用户偏好"><a href="#隐性的用户偏好" class="headerlink" title="隐性的用户偏好"></a>隐性的用户偏好</h2><p>在一些网站上, 如豆瓣网, 用户会给出直接的分数来进行评价, 但是在很多其它场景下, 用户的偏好不会直接表现出来.</p>
<p>比如新闻类的场景下, 用户阅读了一篇文章, 并不能明确地表现出用户是否喜欢这篇文章, 有时候要综合用户的点击, 阅读时长, 收藏, 点赞, 评论等, 来得到一个可以表示用户打分的值.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这里总结一下协同过滤算法的特点.</p>
<ul>
<li>算法思想简单, 实现起来也比较容易.</li>
<li>在面对稀疏性数据时, 并不能很好地处理.</li>
<li>热门物品与活跃用户影响算法效果, 会使得推荐结果覆盖率低, 流行度高.</li>
</ul>
<p>对比用户协同与物品协同的差异.</p>
<ul>
<li><p>性能.</p>
<p>用户协同适用于用户较少的场景, 而物品协同则适用于物品相对用户数量少的场景. 否则维护相似度矩阵会变得困难.</p>
</li>
<li><p>领域.</p>
<p>用户协同适用于倾向热门物品. 物品协同更关注用户个性化需求.</p>
</li>
<li><p>实时性.</p>
<p>当用户行为发生变化时, 用户协同的推荐结果并不会马上变化(因为相似用户可能变化不大), 而物品协同可以很快地发生变化.</p>
</li>
<li><p>解释性.</p>
<p>用户协同相对难以解释. 物品协同结合用户的历史行为给出推荐结果, 可以很好地解释.</p>
</li>
<li><p>冷启动.</p>
<p>对于用户协同来说, 如果是新用户, 则需要积累一定数据后才能计算相似性; 如果是新物品, 则只要某用户对其产生行为, 则可以将其推送给相似用户. 对物品协同来说, 如果是新用户, 只要与某物品产生行为, 则可根据物品相似性进行推荐; 如果是新物品, 则需要积累一定数据才能进行推荐.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2020/08/20/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>期望最大化EM(Expectation Maximization)算法, 与其说是一种算法, 不如说是一种解决问题的思想, 解决一类问题的框架.</p>
<p>和逻辑回归, 决策树等一些具体的算法不同, EM算法更加抽象, 是很多具体算法的基础.</p>
<a id="more"></a>
<h1 id="MLE与EM"><a href="#MLE与EM" class="headerlink" title="MLE与EM"></a>MLE与EM</h1><p>MLE(Maximum Likelihood Estimation), 是极大似然估计的缩写.</p>
<p>EM(Expectation Maximization), 是期望最大化的缩写.</p>
<p>我们肯定对MLE都非常熟悉, 而在这里将它们放在一起, 目的其实是想从MLE引出ME.</p>
<p>举一个栗子, 考虑一个投硬币的实验,  现在有两枚硬币$A$和$B$, 这两枚硬币和普通硬币不同, 他们抛出正反面的概率是不一样的, 记分别抛出正面的概率为$\theta_A$和$\theta_B$, 服从伯努利分布.</p>
<p>接下来独立地做5次实验, 每次先随机从两枚硬币中选择1枚, 然后用该硬币连抛10次, 统计正面出现的次数, 得到实验结果表如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">实验序号</th>
<th style="text-align:center">投掷的硬币</th>
<th style="text-align:center">出现正面的次数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">B</td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">A</td>
<td style="text-align:center">9</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">A</td>
<td style="text-align:center">8</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">B</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">A</td>
<td style="text-align:center">7</td>
</tr>
</tbody>
</table>
</div>
<p>在这个实验中, 有两组随机变量:</p>
<script type="math/tex; mode=display">
\begin{aligned}
X&=(X_1,X_2,X_3,X_4,X_5) \\
Z&=(Z_1,Z_2,Z_3,Z_4,Z_5)
\end{aligned}</script><p>其中$X_i\in(1,2,\dots,10)$, $Z_i\in(A,B)$.</p>
<p>我们的目标是通过实验数据, 来估计参数$\theta$. 那这个时候MLE就派上用场了, 由于这个问题的模型参数$\theta$非常简单, 所以可以直接通过分别统计硬币$A$与硬币$B$的正面的频率, 来估计参数.</p>
<p>如果要按照常规的MLE的流程, 需要有如下步骤:</p>
<ul>
<li>写出似然函数(概率).</li>
<li>对似然函数取对数, 并整理.</li>
<li>对参数求导, 令导数等于0, 得到似然方程.</li>
<li>求解似然方程, 得到参数.</li>
</ul>
<p>以上步骤中, 之所以要对似然函数取对数(或者负对数), 一是因为对于多样本, 似然函数是连乘的形式, 取对数后能够化连乘为连加; 而是许多很小的数连乘后, 考虑到计算机对小数的精度问题, 取负对数后可以避免. 此外, 在求导时, 若不能一次得到最优解, 可使用迭代方式求解, 如SGD.</p>
<p>那么在这个栗子中, 似然函数大概长这样:</p>
<script type="math/tex; mode=display">
L(\theta)=\prod_i P(X_i, Z_i|\theta) \\
P(X_i, Z_i|\theta)=Z_i\times \theta_A^{X_i}+(1-Z_i)\times \theta_B^{X_i}</script><p>其中$Z_i$表示第$i$次实验是否为$A$硬币, 是为1, 否则为0. 求解以上似然函数非常简单, 这里就不再说了.</p>
<p>说了这么一阵的MLE, 但是还没说MLE和EM有啥关联呢. 在上面一个栗子中, 两部分数据$X$和$Z$都是知道的, 但是如果现在将$Z$数据当成未知, 也就是说我们知道抛硬币的实验流程, 但是不知道每次实验到底抛的是哪一枚硬币, 这时候应该怎么做才能继续估计参数$\theta$呢?</p>
<h1 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h1><p>仍然接着上面抛硬币的栗子, 如果现在能够知道隐藏变量$Z$, 那么就仍然能够用MLE求解了, 这好像是废话(&gt;▽&lt;) 既然现在没有$Z$, 我们可不可以假设出$Z$来做呢?</p>
<p>是可以的, 在抛硬币的栗子中, 可以先对每次实验, 都随机赋予$Z$一个初始值(是$A$还是$B$), 接下来使用MLE估计出$\theta$.</p>
<p>然后重点来了, 现在有了估计出来的$\theta$, 结合观测变量$X$, 反过来对隐藏变量$Z$进行估计. 比如MLE的结果得到的$\theta_A=0.3,\ \theta_B=0.8$, 此时第一次实验的$X=5$, 由此可以推得第一次实验是硬币$A$的概率为:</p>
<script type="math/tex; mode=display">
P(A|X;\theta)=\frac{0.3^{5}\times 0.7^{5}}{0.3^{5}\times 0.7^{5}+0.8^{5}\times 0.2^{5}}=0.999</script><p>那么根据重新估计得到的$Z$的结果, 再进行MLE迭代, 直到达到停止条件(如$\theta$稳定).</p>
<p>以上是不太准确的, 但比较形象的说明, 下面是具体的数学推导.</p>
<p>对于$m$个相互独立的样本$X=(X^{(1)},X^{(2)},\dots,X^{(m)})$, 对应隐藏变量$Z=(Z^{(1)},Z^{(2)},\dots,Z^{(m)})$. 此时$(X, Z)$即为完全数据, 模型参数为$\theta$, 则可得到观测变量$X^{(i)}$的似然概率为$P(X^{(i)}|\theta)$, 完全数据$(X, Z)$的似然函数为$P(X^{(i)}, Z^{(i)}|\theta)$.</p>
<p>假如不考虑隐藏变量, 仅需找到合适的$\theta$极大化对数似然函数即可:</p>
<script type="math/tex; mode=display">
\theta=arg\max\limits_{\theta}\sum_{i=1}^m\log P(X^{(i)};\theta)</script><p>在增加了隐藏变量$Z$后, 可以使用边缘概率的方法进行变换:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta&=arg\max\limits_{\theta}\sum_{i=1}^m\log P(X^{(i)};\theta) \\
&=arg\max\limits_{\theta}\sum_{i=1}^m\log\sum_{Z^{(i)}}P(X^{(i)},Z^{(i)};\theta)
\end{aligned}</script><p>上面这个式子是很难求出的, 因为其中带有$\log(f_1+f_2,\dots,f_n)$这样的形式. 所以要进行进一步的变换:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^m\log\sum_{Z^{(i)}}P(X^{(i)},Z^{(i)};\theta)&=\sum_{i=1}^m\log\sum_{Z^{(i)}}Q_i(Z^{(i)})\frac{P(X^{(i)},Z^{(i)};\theta)}{Q_i(Z^{(i)})} \\
&\ge \sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log\frac{P(X^{(i)},Z^{(i)};\theta)}{Q_i(Z^{(i)})}
\end{aligned}</script><p>其中用到了凸函数的Jensen不等式, 即:</p>
<script type="math/tex; mode=display">
f(E(x))\le E(f(x))</script><p>不过这里的$\log$为凹函数, 所以不等号会反过来. </p>
<p>$Q_i(Z^{(i)})$为引入的关于$Z$的式子, 满足:</p>
<script type="math/tex; mode=display">
0\le Q_i(Z^{(i)})\le1 \\[3mm]
\sum_{Z^{(i)}}Q_i(Z^{(i)})=1</script><p>也就是说, $Q_i(Z^{(i)})$可以看做是函数求期望中的权重系数, 对其后面紧跟的整个$\log$函数做加权平均, 这就是Expectation的来历了.</p>
<p>在经过不等式变换后, 如果要让等式成立, 由Jensen不等式可知, 需要满足如下条件:</p>
<script type="math/tex; mode=display">
\frac{P(X^{(i)},Z^{(i)};\theta)}{Q_i(Z^{(i)})}=c</script><p>其中$c$为常数, 进一步变换:</p>
<script type="math/tex; mode=display">
P(X^{(i)},Z^{(i)};\theta)=cQ_i(Z^{(i)}) \\[3mm]
\sum_{Z^{(i)}}P(X^{(i)},Z^{(i)};\theta)=c\sum_{Z^{(i)}}Q_i(Z^{(i)})=c</script><p>所以可得:</p>
<script type="math/tex; mode=display">
Q_i(Z^{(i)})=\frac{P(X^{(i)},Z^{(i)};\theta)}{c}=\frac{P(X^{(i)},Z^{(i)};\theta)}{\sum_{Z^{(i)}}P(X^{(i)},Z^{(i)};\theta)}=\frac{P(X^{(i)},Z^{(i)};\theta)}{P(X^{(i)};\theta)}=P(Z^{(i)}|X^{(i)};\theta)</script><p>一顿操作后, 发现$Q_i(Z^{(i)})$就是隐藏变量$Z$在已知参数$\theta$和观测变量$X$下的分布.</p>
<p>现在, 只要极大化如下式子, 就能够完成最初的目标, 小小不等式既然暗藏如此玄机:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta&=arg\max\limits_{\theta}\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log\frac{P(X^{(i)},Z^{(i)};\theta)}{Q_i(Z^{(i)})} \\
&=arg\max\limits_{\theta}\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log P(X^{(i)},Z^{(i)};\theta)
\end{aligned}</script><p>上面的推导中, 将$Q_i(Z^{(i)})$视为常数去除. 到了这里, 最终需要求解的似然函数, 就是在$Q_i(Z^{(i)})$加权下的原对数似然函数, 此时利用MLE即可求解, 这里就是Maximization.</p>
<p>下面总结EM算法流程:</p>
<ul>
<li><p>观测变量$X$, 隐藏变量$Z$, 联合分布$P(X^{(i)},Z^{(i)};\theta)$, 条件分布$P(Z^{(i)}|X^{(i)};\theta)$.</p>
</li>
<li><p>初始化模型参数$\theta$.</p>
</li>
<li><p>进行E步和M步的迭代:</p>
<ul>
<li><p>E步, 计算联合分布的条件分布期望.</p>
<script type="math/tex; mode=display">
Q_i(Z^{(i)})=P(Z^{(i)}|X^{(i)};\theta^{old}) \\[3mm]
L(\theta,\theta^{old})=\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log P(X^{(i)},Z^{(i)};\theta)</script></li>
<li><p>M步, 极大化$L(\theta,\theta^{old})$, 得到$\theta^{new}$.</p>
<script type="math/tex; mode=display">
\theta^{new}=arg\max\limits_\theta L(\theta,\theta^{old})</script></li>
</ul>
</li>
<li><p>若$\theta$收敛, 算法结束.</p>
</li>
</ul>
<h1 id="EM算法的收敛性"><a href="#EM算法的收敛性" class="headerlink" title="EM算法的收敛性"></a>EM算法的收敛性</h1><p>上面详细讲解了EM算法的原理和流程, 现在还有两个问题:</p>
<ul>
<li>EM算法能够保证收敛吗?</li>
<li>EM算法如果收敛, 能够保证收敛到全局最优吗?</li>
</ul>
<p>首先是第一个问题, 要证明EM算法的收敛性, 则需要证明对数似然函数在迭代的过程成一直在增大, 即:</p>
<script type="math/tex; mode=display">
\sum_{i=1}^m\log P(X^{(i)};\theta^{j+1})\ge \sum_{i=1}^m\log P(X^{(i)};\theta^{j})</script><p>因为</p>
<script type="math/tex; mode=display">
L(\theta,\theta^{j})=\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log P(X^{(i)},Z^{(i)};\theta)</script><p>令:</p>
<script type="math/tex; mode=display">
H(\theta,\theta^{j})=\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log P(Z^{(i)}|X^{(i)};\theta)</script><p>于是可以通过上两式相减, 得到:</p>
<script type="math/tex; mode=display">
L(\theta,\theta^{j})-H(\theta,\theta^{j})=\sum_{i=1}^m\log P(X^{(i)};\theta)</script><p>将上式中的$\theta$分别取为$\theta^j$和$\theta^{j+1}$, 相减可以得到:</p>
<script type="math/tex; mode=display">
\sum_{i=1}^m\log P(X^{(i)};\theta^{j+1})-\sum_{i=1}^m\log P(X^{(i)};\theta^{j})=[L(\theta^{j+1},\theta^{j})-L(\theta^j,\theta^{j})]-[H(\theta^{j+1},\theta^{j})-H(\theta^j,\theta^{j})]</script><p>由于$\theta^{j+1}$使得$L(\theta,\theta^{j})$极大, 所以必然有:</p>
<script type="math/tex; mode=display">
L(\theta^{j+1},\theta^{j})-L(\theta^j,\theta^{j})\ge0</script><p>对于第二部分, 再一次利用Jensen不等式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
H(\theta^{j+1},\theta^{j})-H(\theta^j,\theta^{j})&=\sum_{i=1}^m\sum_{Z^{(i)}}Q_i(Z^{(i)})\log \frac{P(Z^{(i)}|X^{(i)};\theta^{j+1})}{P(Z^{(i)}|X^{(i)};\theta^j)} \\
&\le\sum_{i=1}^m\log\sum_{Z^{(i)}}Q_i(Z^{(i)})\frac{P(Z^{(i)}|X^{(i)};\theta^{j+1})}{Q_i(Z^{(i)})} \\
&=\sum_{i=1}^m\log\sum_{Z^{(i)}}P(Z^{(i)}|X^{(i)};\theta^{j+1}) \\
&=0
\end{aligned}</script><p>到这里, 证明了随着迭代的进行, 似然函数会持续增大, 即EM算法的收敛性.</p>
<p>虽然EM算法可以保证收敛到一个稳定点, 但是却不能保证收敛到全局的最大值点, 因此它是局部最优的算法. 如果优化的目标函数$L(\theta,\theta^{j})$是凸函数(或者凹函数, 根据是否加负号), 则EM算法可以收敛到全局最优.</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>EM算法总体说来, 就是针对问题当中存在隐藏变量, 难以正常使用MLE解决时, 可以采用的方法.</p>
<p>算法流程分为两步, E步估计隐藏变量的分布, 并结合估计的隐藏变量分布计算整体的似然函数; M步则是利用MLE算法, 极大化E步得到的似然函数.</p>
<p>在一些算法中, 会直接用到EM算法, 如HMM, 混合高斯模型等; 而一些算法有着与EM算法类似的思想, 如K-means, 坐标轴下降法, SMO算法. 后者本质上都是在优化时, 难以同时优化所有参数, 于是采取分步优化的方式, 来达到最终整体优化的目的.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-使用GPU</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E4%BD%BF%E7%94%A8GPU/</url>
    <content><![CDATA[<p>深度学习的训练过程常常非常耗时, 一个模型训练几个小时是家常便饭, 训练几天也是常有的事情.</p>
<p>训练过程的耗时主要来自于两个部分, 一部分来自数据准备, 另一部分来自参数迭代.</p>
<p>当数据准备过程还是模型训练时间的主要瓶颈时, 我们可以使用更多进程来准备数据. 当参数迭代过程成为训练时间的主要瓶颈时, 我们通常的方法是应用GPU或者Google的TPU来进行加速.</p>
<a id="more"></a>
<p>TensorFlow从CPU切换成单GPU训练模型都是非常方便的, 几乎无需更改任何代码.</p>
<p>当存在可用的GPU时, 如果不特意指定device, TensorFlow会自动优先选择使用GPU来创建张量和执行张量计算.</p>
<p>但如果是在公司或者学校实验室的服务器环境, 存在多个GPU和多个使用者时, 为了不让单个使用者的任务占用全部GPU资源导致其他同学无法使用(TensorFlow默认获取全部GPU的全部内存资源权限, 但实际上只使用一个GPU的部分资源), 我们通常会在<strong>开头</strong>增加以下几行代码以控制每个任务使用的GPU编号和显存大小, 以便其他人也能够同时训练模型.</p>
<h1 id="指定CPU或者GPU"><a href="#指定CPU或者GPU" class="headerlink" title="指定CPU或者GPU"></a>指定CPU或者GPU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看可用物理CPU</span></span><br><span class="line">tf.config.list_physical_devices(<span class="string">"CPU"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:CPU:0&apos;, device_type=&apos;CPU&apos;)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看可用物理GPU</span></span><br><span class="line">tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:GPU:0&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>为了得知我们的操作和张量被配置到哪个设备(GPU还是CPU)上, 我们可以使用<code>tf.debugging.set_log_device_placement</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create some tensors</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果希望在自己选择的设备上运行特定的操作, 而不是TensorFlow自动选择, 我们可以使用<code>tf.device</code>来创建设备上下文, 该上下文中的所有操作都将在相同的指定设备上运行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place tensors on the CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">    a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<h1 id="比较CPU与GPU"><a href="#比较CPU与GPU" class="headerlink" title="比较CPU与GPU"></a>比较CPU与GPU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp() % (<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts // <span class="number">3600</span> + <span class="number">8</span>, tf.int32) % tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts % <span class="number">3600</span>) // <span class="number">60</span>, tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts % <span class="number">60</span>), tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>, m)) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"0&#123;&#125;"</span>, m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"&#123;&#125;"</span>, m))</span><br><span class="line"></span><br><span class="line">    timestring = tf.strings.join(</span><br><span class="line">        [timeformat(hour),</span><br><span class="line">         timeformat(minite),</span><br><span class="line">         timeformat(second)],</span><br><span class="line">        separator=<span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span> * <span class="number">4</span> + timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在GPU下进行矩阵运算</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/GPU:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">-2.</span>, maxval=<span class="number">2.</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">-2.</span>, maxval=<span class="number">2.</span>)</span><br><span class="line">    c = a @ b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=<span class="number">0</span>), axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">========================================21:32:10</span><br><span class="line">-890166.625</span><br><span class="line">========================================21:32:11</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在CPU下进行矩阵运算</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/CPU:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">0.</span>, maxval=<span class="number">3.</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">10000</span>, <span class="number">10000</span>), minval=<span class="number">0.</span>, maxval=<span class="number">3.</span>)</span><br><span class="line">    c = a @ b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=<span class="number">0</span>), axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">========================================21:32:14</span><br><span class="line">2.2501049e+12</span><br><span class="line">========================================21:32:36</span><br></pre></td></tr></table></figure>
<h1 id="限制GPU内存增长"><a href="#限制GPU内存增长" class="headerlink" title="限制GPU内存增长"></a>限制GPU内存增长</h1><p>默认情况下, TensorFlow会将所有GPU(取决于CUDA_VISIBLE_DEVICES)的几乎所有GPU内存映射到进程. 这样做是为了通过减少内存碎片更有效地使用设备上相对宝贵的GPU内存资源.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">gpus</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PhysicalDevice(name=&apos;/physical_device:GPU:0&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>按需分配内存, 开始分配非常少的内存, 随着程序运行根据需求分配更多内存.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行这样的配置, 需要在一开始就声明, 否则可能会报错</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Memory growth must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<p>直接指定具体使用多少内存.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进行这样的配置, 需要在一开始就声明, 否则可能会报错</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>],</span><br><span class="line">        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<h1 id="多GPU的使用"><a href="#多GPU的使用" class="headerlink" title="多GPU的使用"></a>多GPU的使用</h1><h1 id="指定在某个GPU上运行"><a href="#指定在某个GPU上运行" class="headerlink" title="指定在某个GPU上运行"></a>指定在某个GPU上运行</h1><p>指定只在某个物理GPU上运行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">tf.config.set_visible_devices([gpus[<span class="number">0</span>]], <span class="string">'GPU'</span>)</span><br></pre></td></tr></table></figure>
<p>使用虚拟GPU模拟多GPU环境…</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 虚拟出两个内存1G的GPU</span></span><br><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>], [</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>),</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>)</span><br><span class="line">        ])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">    print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 Physical GPU, 2 Logical GPUs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logical_gpus = tf.config.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">logical_gpus</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[LogicalDevice(name=&apos;/device:GPU:0&apos;, device_type=&apos;GPU&apos;),</span><br><span class="line"> LogicalDevice(name=&apos;/device:GPU:1&apos;, device_type=&apos;GPU&apos;)]</span><br></pre></td></tr></table></figure>
<p>如果我们的系统里有不止一个GPU, 则默认情况下, ID最小的GPU将被选用. 如果想在不同的GPU上运行, 我们需要显式地指定优先项.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/device:GPU:1'</span>):</span><br><span class="line">    a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[22. 28.]</span><br><span class="line"> [49. 64.]], shape=(2, 2), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果希望TensorFlow自动选择一个现有且受支持的设备来运行操作, 以避免指定的设备不存在, 那么可以调用<code>tf.config.set_soft_device_placement(True)</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.config.set_soft_device_placement(<span class="literal">True</span>)</span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<h2 id="在多GPU上运行"><a href="#在多GPU上运行" class="headerlink" title="在多GPU上运行"></a>在多GPU上运行</h2><p>可以用<code>tf.distribute.MirroredStrategy</code>来使用多个GPU.</p>
<p>MirroredStrategy过程简介：</p>
<ul>
<li>训练开始前, 该策略在所有 N 个计算设备上均各复制一份完整的模型.</li>
<li>每次训练传入一个批次的数据时, 将数据分成 N 份, 分别传入 N 个计算设备(即数据并行).</li>
<li>N 个计算设备使用本地变量(镜像变量)分别计算自己所获得的部分数据的梯度.</li>
<li>使用分布式计算的 All-reduce 操作, 在计算设备间高效交换梯度数据并进行求和, 使得最终每个设备都有了所有设备的梯度之和.</li>
<li>使用梯度求和的结果更新本地变量(镜像变量).</li>
<li>当所有设备均更新本地变量后, 进行下一轮训练(即该并行策略是同步的).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 虚拟出两个内存1G的GPU</span></span><br><span class="line">gpus = tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>], [</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>),</span><br><span class="line">            tf.config.experimental.VirtualDeviceConfiguration(</span><br><span class="line">                memory_limit=<span class="number">1024</span>)</span><br><span class="line">        ])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">    print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 Physical GPU, 2 Logical GPUs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (train_x, train_y)).shuffle(buffer_size=<span class="number">512</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (test_x, test_y)).shuffle(buffer_size=<span class="number">512</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里增加两行</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()  </span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(</span><br><span class="line">        layers.Dense(<span class="number">100</span>,</span><br><span class="line">                     activation=<span class="string">'relu'</span>,</span><br><span class="line">                     kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">    model.add(</span><br><span class="line">        layers.Dense(<span class="number">50</span>,</span><br><span class="line">                     activation=<span class="string">'relu'</span>,</span><br><span class="line">                     kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/1000</span><br><span class="line">15/15 [==============================] - 2s 124ms/step - loss: 43.3710 - auc: 0.5209 - val_loss: 4.4724 - val_auc: 0.7976</span><br><span class="line">Epoch 2/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 4.6899 - auc: 0.7616 - val_loss: 3.9844 - val_auc: 0.8571</span><br><span class="line">Epoch 3/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 2.1644 - auc: 0.8892 - val_loss: 1.7289 - val_auc: 0.9446</span><br><span class="line">Epoch 4/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.7997 - auc: 0.9065 - val_loss: 1.5541 - val_auc: 0.9600</span><br><span class="line">Epoch 5/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.4721 - auc: 0.9127 - val_loss: 1.5341 - val_auc: 0.9560</span><br><span class="line">Epoch 6/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.3007 - auc: 0.9227 - val_loss: 1.5797 - val_auc: 0.9430</span><br><span class="line">Epoch 7/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.1857 - auc: 0.9286 - val_loss: 1.1779 - val_auc: 0.9611</span><br><span class="line">Epoch 8/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 1.0409 - auc: 0.9353 - val_loss: 0.9681 - val_auc: 0.9602</span><br><span class="line">Epoch 9/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 0.9415 - auc: 0.9387 - val_loss: 0.8586 - val_auc: 0.9651</span><br><span class="line">Epoch 10/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.8615 - auc: 0.9441 - val_loss: 0.7910 - val_auc: 0.9706</span><br><span class="line">Epoch 11/1000</span><br><span class="line">15/15 [==============================] - 0s 20ms/step - loss: 0.7812 - auc: 0.9496 - val_loss: 0.6869 - val_auc: 0.9702</span><br><span class="line">Epoch 12/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.7197 - auc: 0.9573 - val_loss: 0.5969 - val_auc: 0.9704</span><br><span class="line">Epoch 13/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6657 - auc: 0.9610 - val_loss: 0.5601 - val_auc: 0.9754</span><br><span class="line">Epoch 14/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6996 - auc: 0.9505 - val_loss: 0.5677 - val_auc: 0.9780</span><br><span class="line">Epoch 15/1000</span><br><span class="line">15/15 [==============================] - 1s 34ms/step - loss: 0.7026 - auc: 0.9440 - val_loss: 0.5424 - val_auc: 0.9850</span><br><span class="line">Epoch 16/1000</span><br><span class="line">15/15 [==============================] - 0s 19ms/step - loss: 0.6049 - auc: 0.9652 - val_loss: 0.5115 - val_auc: 0.9871</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-训练模型</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>根据不同的任务, 简单或者复杂, 可以灵活地使用TensorFlow提供的多种训练模型的方法, 下面就来介绍几种比较常用的.</p>
<a id="more"></a>
<h1 id="训练模型的三种方法"><a href="#训练模型的三种方法" class="headerlink" title="训练模型的三种方法"></a>训练模型的三种方法</h1><h2 id="内置fit方法"><a href="#内置fit方法" class="headerlink" title="内置fit方法"></a>内置fit方法</h2><p>如果是一些比较普通的建模任务, 那么在TensorFlow2.x中, 可以使用类似于sklearn的<code>fit</code>方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">64</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用内置fit训练模型</span></span><br><span class="line"></span><br><span class="line">model.fit(train_x,</span><br><span class="line">          train_y,</span><br><span class="line">          validation_data=(test_x, test_y),</span><br><span class="line">          batch_size=<span class="number">32</span>,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                            patience=<span class="number">5</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">                                            mode=<span class="string">'max'</span>,</span><br><span class="line">                                            min_delta=<span class="number">0.0003</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_auc'</span>,</span><br><span class="line">                                                factor=<span class="number">0.2</span>,</span><br><span class="line">                                                patience=<span class="number">1</span>,</span><br><span class="line">                                                mode=<span class="string">'max'</span>,</span><br><span class="line">                                                min_delta=<span class="number">0.0003</span>),</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/100</span><br><span class="line">15/15 [==============================] - 0s 24ms/step - loss: 5.6850 - auc: 0.6227 - val_loss: 1.0607 - val_auc: 0.8588</span><br><span class="line">Epoch 2/100</span><br><span class="line">15/15 [==============================] - 0s 4ms/step - loss: 1.1538 - auc: 0.8317 - val_loss: 0.8551 - val_auc: 0.8851</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="内置train-on-batch方法"><a href="#内置train-on-batch方法" class="headerlink" title="内置train_on_batch方法"></a>内置train_on_batch方法</h2><p>上面的<code>fit</code>方法一般来说只能以epoch为最小单位来进行操作, 而如果想进行更加精细的操作, 比如以batch为单位, 那么就可以使用<code>train_on_batch</code>方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"></span><br><span class="line">train_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (train_x, train_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">test_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (test_x, test_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">        tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用内置train_on_batch训练模型</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.reset_metrics()</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_set:</span><br><span class="line">        train_result = model.train_on_batch(batch_x, batch_y)</span><br><span class="line">        <span class="comment"># 根据需求还可以做其它事情, 如调解学习率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估</span></span><br><span class="line">    <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> test_set:</span><br><span class="line">        valid_result = model.test_on_batch(batch_x,</span><br><span class="line">                                          batch_y,</span><br><span class="line">                                          reset_metrics=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line">    print(<span class="string">"train:"</span>, dict(zip(model.metrics_names, train_result)))</span><br><span class="line">    print(<span class="string">"valid:"</span>, dict(zip(model.metrics_names, valid_result)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##########################################</span><br><span class="line">train: &#123;&apos;loss&apos;: 0.1489427238702774, &apos;auc&apos;: 1.0&#125;</span><br><span class="line">valid: &#123;&apos;loss&apos;: 0.2625027000904083, &apos;auc&apos;: 0.9689153432846069&#125;</span><br><span class="line">##########################################</span><br><span class="line">train: &#123;&apos;loss&apos;: 0.14858269691467285, &apos;auc&apos;: 1.0&#125;</span><br><span class="line">valid: &#123;&apos;loss&apos;: 0.26238003373146057, &apos;auc&apos;: 0.9689152836799622&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="自定义训练"><a href="#自定义训练" class="headerlink" title="自定义训练"></a>自定义训练</h2><p>此方法不需要使用<code>compile</code>编译模型, 直接根据损失函数反向传播梯度, 利用优化器进行迭代, 具有最高的灵活性, 可以胜任一些复杂的任务.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">train_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    ((train_x / np.linalg.norm(train_x, axis=<span class="number">0</span>)).astype(<span class="string">'float32'</span>),</span><br><span class="line">     train_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">         tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">test_set = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    ((test_x / np.linalg.norm(test_x, axis=<span class="number">0</span>)).astype(<span class="string">'float32'</span>),</span><br><span class="line">     test_y)).shuffle(buffer_size=<span class="number">1000</span>).batch(<span class="number">32</span>).prefetch(</span><br><span class="line">         tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    keras.layers.Dense(<span class="number">64</span>,</span><br><span class="line">                       activation=<span class="string">'relu'</span>,</span><br><span class="line">                       kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置损失函数/优化器/评估指标等</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_func = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集loss</span></span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line"><span class="comment"># 训练集评估指标</span></span><br><span class="line">train_metric = keras.metrics.AUC(name=<span class="string">'train_auc'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证集loss</span></span><br><span class="line">val_loss = keras.metrics.Mean(name=<span class="string">'val_loss'</span>)</span><br><span class="line"><span class="comment"># 验证集评估指标</span></span><br><span class="line">val_metric = keras.metrics.AUC(name=<span class="string">'val_auc'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义训练/评估步骤</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        preds = model(features)</span><br><span class="line">        loss = loss_func(labels, preds)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, preds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    preds = model(features)</span><br><span class="line">    loss = loss_func(labels, preds)</span><br><span class="line">    val_loss.update_state(loss)</span><br><span class="line">    val_metric.update_state(labels, preds)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> train_set:</span><br><span class="line">        train_step(model, features, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> test_set:</span><br><span class="line">        valid_step(model, features, labels)</span><br><span class="line"></span><br><span class="line">    logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,AUC:&#123;&#125;,Val Loss:&#123;&#125;,Val AUC:&#123;&#125;'</span></span><br><span class="line">    tf.print(</span><br><span class="line">        tf.strings.format(logs,</span><br><span class="line">                          (epoch, train_loss.result(), train_metric.result(),</span><br><span class="line">                           val_loss.result(), val_metric.result())))</span><br><span class="line">    tf.print(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    val_loss.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line">    val_metric.reset_states()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch=0,Loss:0.676754653,AUC:0.698328137,Val Loss:0.633566141,Val AUC:0.978174567</span><br><span class="line">##########################################</span><br><span class="line">Epoch=1,Loss:0.634832323,AUC:0.944220841,Val Loss:0.55745548,Val AUC:0.976521134</span><br><span class="line">##########################################</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-构建模型</title>
    <url>/2020/08/17/TensorFlow/TensorFlow-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>这里重点总结一下TensorFlow2.x的几种构建模型, 以及保存模型的方式.</p>
<a id="more"></a>
<h1 id="构建模型的三种方法"><a href="#构建模型的三种方法" class="headerlink" title="构建模型的三种方法"></a>构建模型的三种方法</h1><p>可以使用以下三种方法构建模型:</p>
<ul>
<li>使用<code>Sequential</code>按层顺序构建模型.</li>
<li>使用函数式API构建任意结构模型.</li>
<li>继承Model基类构建自定义模型.</li>
</ul>
<p>对于顺序结构的模型, 优先使用<code>Sequential</code>方法构建.</p>
<p>如果模型有多输入或者多输出, 或者模型需要共享权重, 或者模型具有残差连接等非顺序结构, 推荐使用函数式API进行创建.</p>
<p>如果无特定必要, 尽可能避免使用Model子类化的方式构建模型, 这种方式提供了极大的灵活性, 但也有更大的概率出错.</p>
<h2 id="Sequential按层顺序创建模型"><a href="#Sequential按层顺序创建模型" class="headerlink" title="Sequential按层顺序创建模型"></a>Sequential按层顺序创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 73us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 77us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 68us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h2 id="函数式API创建模型"><a href="#函数式API创建模型" class="headerlink" title="函数式API创建模型"></a>函数式API创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=(<span class="number">30</span>, ))</span><br><span class="line"></span><br><span class="line">x = layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))(inputs)</span><br><span class="line"></span><br><span class="line">x = layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))(x)</span><br><span class="line"></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs=inputs, outputs=outputs)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 69us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 84us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 71us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h2 id="Model子类创建模型"><a href="#Model子类创建模型" class="headerlink" title="Model子类创建模型"></a>Model子类创建模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">data, target = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data,</span><br><span class="line">                                                    target,</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">7</span>,</span><br><span class="line">                                                    stratify=target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.dense_0 = layers.Dense(<span class="number">100</span>,</span><br><span class="line">                                    activation=<span class="string">'relu'</span>,</span><br><span class="line">                                    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        self.dense_1 = layers.Dense(<span class="number">50</span>,</span><br><span class="line">                                    activation=<span class="string">'relu'</span>,</span><br><span class="line">                                    kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        self.dense_2 = layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line">        super().build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.dense_0(x)</span><br><span class="line">        x = self.dense_1(x)</span><br><span class="line">        outputs = self.dense_2(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_x,</span><br><span class="line">                    y=train_y,</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(test_x, test_y),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Train on 455 samples, validate on 114 samples
Epoch 1/1000
455/455 [==============================] - 1s 1ms/sample - loss: 4.0375 - AUC: 0.7336 - val_loss: 2.0691 - val_AUC: 0.9501
Epoch 2/1000
455/455 [==============================] - 0s 69us/sample - loss: 1.6957 - AUC: 0.9030 - val_loss: 1.2848 - val_AUC: 0.9608
Epoch 3/1000
455/455 [==============================] - 0s 87us/sample - loss: 1.3280 - AUC: 0.9362 - val_loss: 1.0363 - val_AUC: 0.9735

...

Epoch 36/1000
455/455 [==============================] - 0s 63us/sample - loss: 0.5452 - AUC: 0.9659 - val_loss: 0.4713 - val_AUC: 0.9845

...
</code></pre><h1 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h1><p>TensorFlow2.x有两种方式保存模型, 可以使用Keras方式保存模型, 也可以使用TensorFlow原生方式保存.</p>
<p>前者仅仅适合使用Python环境恢复模型, 后者则可以跨平台进行模型部署, 推荐使用后一种方式进行保存.</p>
<h2 id="Keras方式保存"><a href="#Keras方式保存" class="headerlink" title="Keras方式保存"></a>Keras方式保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 原模型测试集评估</span></span><br><span class="line"></span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 81us/sample - loss: 0.4713 - AUC: 0.9845

[0.4712578717030977, 0.9844577]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./model/keras_model.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/keras_model.h5'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 1ms/sample - loss: 0.4713 - AUC: 0.9845

[0.4712578717030977, 0.9844577]
</code></pre><h2 id="TensorFlow原生方式保存"><a href="#TensorFlow原生方式保存" class="headerlink" title="TensorFlow原生方式保存"></a>TensorFlow原生方式保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./model/tf_model'</span>, save_format=<span class="string">'tf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/tf_model'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 2ms/sample - loss: 0.4713 - AUC: 0.9845

[0.4712575684513962, 0.9844577]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构</span></span><br><span class="line">model_json = model.to_json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型权重</span></span><br><span class="line">model.save_weights(<span class="string">'./model/tf_model_weight'</span>, save_format=<span class="string">'tf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model = models.model_from_json(model_json)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型权重</span></span><br><span class="line">model.load_weights(<span class="string">'./model/tf_model_weight'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验是否一致</span></span><br><span class="line">model = models.load_model(<span class="string">'./model/tf_model'</span>)</span><br><span class="line">model.evaluate(test_x, test_y)</span><br></pre></td></tr></table></figure>
<pre><code>114/114 [==============================] - 0s 1ms/sample - loss: 0.4713 - AUC: 0.9845
</code></pre>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT(二)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/BERT-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 讲解了BERT的原理, 在这一篇中, 来实际地使用一下BERT, 看一下效果.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>BERT可以用于完成各种任务, 而在这里选择文本分类任务, 因为之前也做过一些文本分类的任务, 这里正好可以做一些对比.</p>
<p>使用 THUCNews的一个子集进行训练与测试, 使用了其中的 10 个分类, 每个分类 6500 条数据.</p>
<p>类别包含: 体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐.</p>
<p>数据集划分如下:</p>
<ul>
<li>训练集: 5000 * 10</li>
<li>验证集: 500 * 10</li>
<li>测试集: 1000 * 10</li>
</ul>
<p>那么如何使用BERT模型呢? 其实BERT模型可以看成是一种模型框架, 并不指代某一个预训练好的具体的模型, 比如在原始的<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">Google BERT</a>中, 就有像”BERT-Base”, “BERT-Large”这样的模型, 可以参考其文档进行使用.</p>
<p>在这里, 我选择了一个叫做<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a>的Python包, 其优点是包含了各种类Transformer模型框架(如BERT, GPT等), 包含了多种任务API(如分类, 序列标注等), 同时可以根据具体的任务场景选择具体的预训练模型, 比如这里主要是中文的场景, 那么我就选择了一个叫做”bert-base-chinese”的模型.</p>
<p>关于transformers的具体的用法, 可以参看它们的<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">文档</a>, 写得还算比较详细.</p>
<p>所以, 这里就主要运用transformers中的BERT来实现对中文文本的分类, 而了解BERT原理的同学应该知道, BERT对于序列的单次输入长度是有限制的, 并不能像RNN那样”无限套娃”, 一般来说单次输入序列最大长度为510, 因为还有两个特殊字符[CLS]和[SEP].</p>
<p>那么对于超过510长度的文本怎么办呢? 首先, 在分类任务当中, 有时候截取文本的一部分来进行分类, 就可以达到很好的效果. 而如果想通过BERT使用到全部的文本信息, 那么一种思路就是BERT+BiLSTM, 即先将长文本按某种规则进行分块, 使用微调好的BERT对每段文本进行Embedding, 再将Embedding作为BiLSTM的输入.</p>
<p>所以, 下面就分别使用这两种方法来进行试验:</p>
<ul>
<li>截取文本长度, 直接使用BERT进行微调.</li>
<li>使用BERT+BiLSTM</li>
</ul>
<h1 id="微调BERT"><a href="#微调BERT" class="headerlink" title="微调BERT"></a>微调BERT</h1><p>在transformers中, 将原始序列数据转变为入模数据, 需要先经过Tokenizer, Tokenizer会根据具体模型对应的词表, 特殊字符, 最大序列长度等信息来进行编码. 这样就不用再自己预处理数据了, 非常方便.</p>
<p>由于数据量不大, 所以大部分时候都是直接将数据读入内存处理的, 同时这里只使用了[CLS]对应的Embedding, 使用全部序列输出进行pool得到的Embedding效果应该会更好一点.</p>
<p>下面看具体代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/home/data/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/home/data/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/home/data/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子, 因为后面会进行shuffle操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line"><span class="comment"># from tensorflow.keras.mixed_precision import experimental as mixed_precision</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># policy = mixed_precision.Policy('mixed_float16')</span></span><br><span class="line"><span class="comment"># mixed_precision.set_policy(policy)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shuffle原始数据, 只shuffle训练集即可</span></span><br><span class="line">train_data_list = np.array(train_data_list)</span><br><span class="line">train_idx = np.arange(len(train_data_list))</span><br><span class="line">np.random.shuffle(train_idx)</span><br><span class="line">train_data_list = train_data_list[train_idx]</span><br><span class="line">train_label_list = np.array(train_label_list)[train_idx]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分类标签编码0-9</span></span><br><span class="line">encode_label_dict = dict(zip([<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>], range(<span class="number">10</span>)))</span><br><span class="line">decode_label_dict = dict(zip(range(<span class="number">10</span>), [<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>]))</span><br><span class="line">train_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> train_label_list]</span><br><span class="line">val_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> val_label_list]</span><br><span class="line">test_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> test_label_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载Tokenizer和预训练模型, 注意需要对模型进行一些设置,</span></span><br><span class="line"><span class="comment"># 比如原始的模型是二分类, 这是是多分类, 所以设置num_labels=10</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, TFBertForSequenceClassification</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = TFBertForSequenceClassification.from_pretrained(<span class="string">"bert-base-chinese"</span>, num_labels=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Tokenizer转换原始文本数据</span></span><br><span class="line">train_encodings = tokenizer(list(train_data_list), truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>)</span><br><span class="line">val_encodings = tokenizer(list(val_data_list), truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>)</span><br><span class="line">test_encodings = tokenizer(list(test_data_list), truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (dict(train_encodings), list(train_label_encode))).shuffle(<span class="number">10000</span>, seed=<span class="number">7</span>).batch(<span class="number">8</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">val_dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (dict(val_encodings), list(val_label_encode))).shuffle(<span class="number">10000</span>, seed=<span class="number">7</span>).batch(<span class="number">8</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">test_dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (dict(test_encodings), list(test_label_encode))).shuffle(<span class="number">10000</span>, seed=<span class="number">7</span>).batch(<span class="number">8</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练时间可能较长, 所以使用logging进行记录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(filename=<span class="string">'/home/work/log/bert/bert.log'</span>, level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoggingCallback</span><span class="params">(keras.callbacks.Callback)</span>:</span></span><br><span class="line">    <span class="string">"""Callback that logs message at end of epoch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, print_fcn=print)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.print_fcn = print_fcn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        msg = <span class="string">"Epoch: %i, %s"</span> % (epoch, <span class="string">" - "</span>.join(<span class="string">"%s: %f"</span> % (k, v) <span class="keyword">for</span> k, v <span class="keyword">in</span> logs.items()))</span><br><span class="line">        self.print_fcn(msg)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 微调模型</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer = keras.optimizers.Adam(learning_rate=5e-5)</span></span><br><span class="line">optimizer = keras.optimizers.SGD(learning_rate=<span class="number">1e-4</span>, momentum=<span class="number">0.1</span>)</span><br><span class="line">model.compile(optimizer=optimizer,</span><br><span class="line">              loss=model.compute_loss,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          validation_data=val_dataset,</span><br><span class="line">          epochs=<span class="number">100</span>,</span><br><span class="line">          callbacks=[</span><br><span class="line">              keras.callbacks.EarlyStopping(monitor=<span class="string">'val_accuracy'</span>,</span><br><span class="line">                                            patience=<span class="number">2</span>,</span><br><span class="line">                                            restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">              keras.callbacks.ReduceLROnPlateau(factor=<span class="number">0.2</span>, patience=<span class="number">1</span>),</span><br><span class="line">              LoggingCallback(logging.info)</span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在测试集评估模型, 并保持模型</span></span><br><span class="line">logging.info(model.evaluate(test_dataset))</span><br><span class="line">model.save_pretrained(<span class="string">'/home/work/model/bert/bert_small'</span>)</span><br></pre></td></tr></table></figure>
<p>输出日志为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO:root:Epoch: 0, loss: 0.570259 - accuracy: 0.884700 - val_loss: 0.145340 - val_accuracy: 0.971400 - lr: 0.000100</span><br><span class="line">INFO:root:Epoch: 1, loss: 0.147304 - accuracy: 0.962540 - val_loss: 0.109097 - val_accuracy: 0.975600 - lr: 0.000100</span><br><span class="line">INFO:root:Epoch: 2, loss: 0.112050 - accuracy: 0.970140 - val_loss: 0.101350 - val_accuracy: 0.976800 - lr: 0.000100</span><br><span class="line">INFO:root:Epoch: 3, loss: 0.095725 - accuracy: 0.972740 - val_loss: 0.101873 - val_accuracy: 0.975800 - lr: 0.000100</span><br><span class="line">INFO:root:Epoch: 4, loss: 0.082858 - accuracy: 0.976320 - val_loss: 0.095420 - val_accuracy: 0.976400 - lr: 0.000020</span><br><span class="line">INFO:root:[0.10843876749277115, 0.972100019454956]</span><br></pre></td></tr></table></figure>
<p>可以看到, 在第三轮(Epoch: 2)时, 验证集上准确率为0.977, 测试集上准确率为0.972, 效果很不错.</p>
<h1 id="BERT-BiLSTM"><a href="#BERT-BiLSTM" class="headerlink" title="BERT+BiLSTM"></a>BERT+BiLSTM</h1><p>下面再使用BERT+BiLSTM进行试验.</p>
<p>这里就不放全部的代码了, 只放几段不一样的代码, 首先是文本分块的代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将长句分为短句</span></span><br><span class="line">train_data_list_big = []  <span class="comment"># [[sent_0, sent_1, ...], ...]</span></span><br><span class="line">val_data_list_big = []</span><br><span class="line">test_data_list_big = []</span><br><span class="line"></span><br><span class="line">train_label_encode_big = []  <span class="comment"># [[label_0, label_1, ...], ...]</span></span><br><span class="line">val_label_encode_big = []</span><br><span class="line">test_label_encode_big = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    sent, label = data[<span class="number">0</span>], data[<span class="number">1</span>]</span><br><span class="line">    sent_list = []</span><br><span class="line">    label_list = []</span><br><span class="line">    len_ = len(sent) // <span class="number">510</span></span><br><span class="line">    <span class="keyword">if</span> len_ == <span class="number">0</span>:</span><br><span class="line">        len_ = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len_):</span><br><span class="line">        start = i * <span class="number">400</span></span><br><span class="line">        sent_list.append(sent[start: start + <span class="number">510</span>])</span><br><span class="line">        label_list.append(label)</span><br><span class="line">    <span class="keyword">return</span> sent_list, label_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">train_data_list_big = ordered_multiprocess_task(split_data, zip(train_data_list, train_label_encode), <span class="number">12</span>)</span><br><span class="line">val_data_list_big = ordered_multiprocess_task(split_data, zip(val_data_list, val_label_encode), <span class="number">12</span>)</span><br><span class="line">test_data_list_big = ordered_multiprocess_task(split_data, zip(test_data_list, test_label_encode), <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">train_label_encode_big = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list_big]</span><br><span class="line">train_data_list_big = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list_big]</span><br><span class="line">val_label_encode_big = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list_big]</span><br><span class="line">val_data_list_big = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list_big]</span><br><span class="line">test_label_encode_big = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list_big]</span><br><span class="line">test_data_list_big = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list_big]</span><br></pre></td></tr></table></figure>
<p>在上面的<code>split_data</code>函数中, 有两个数字, 其中<code>510</code>表示分块后的每段文本的长度, 同时在切分时, 还是的相邻的文本之间有重叠部分, 所以<code>400</code>表示两端相邻文本的不重叠长度. 除了(510, 400)这组参数外, 还尝试了(250, 200)这组参数.</p>
<p>经过上面的处理, 得到的数据形式为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文本数据:</span><br><span class="line">[</span><br><span class="line">[sent_0, sent_1, ...],</span><br><span class="line">[sent_2, sent_3, ...],</span><br><span class="line">...</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">标签数据:</span><br><span class="line">[</span><br><span class="line">[label_0, label_1, ...],</span><br><span class="line">[label_2, label_3, ...],</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将上面得到的数据展平, 获得训练bert数据</span></span><br><span class="line">train_data_encode_bert = []</span><br><span class="line">train_label_encode_bert = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_data_list_big:</span><br><span class="line">    train_data_encode_bert += i</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_label_encode_big:</span><br><span class="line">    train_label_encode_bert += i</span><br><span class="line">    </span><br><span class="line">val_data_encode_bert = []</span><br><span class="line">val_label_encode_bert = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> val_data_list_big:</span><br><span class="line">    val_data_encode_bert += i</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> val_label_encode_big:</span><br><span class="line">    val_label_encode_bert += i</span><br><span class="line">    </span><br><span class="line">test_data_encode_bert = []</span><br><span class="line">test_label_encode_bert = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> test_data_list_big:</span><br><span class="line">    test_data_encode_bert += i</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> test_label_encode_big:</span><br><span class="line">    test_label_encode_bert += i</span><br></pre></td></tr></table></figure>
<p>现在数据形式为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文本数据:</span><br><span class="line">[sent_0, sent_1, ...]</span><br><span class="line">标签数据:</span><br><span class="line">[label_0, label_1, ...]</span><br></pre></td></tr></table></figure>
<p>然后就如上一节一样, 微调BERT模型. 在完成对BERT的微调后, 使用BERT对每一段文本进行编码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bert_pool_out</span><span class="params">(sent_list)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model(tokenizer(sent_list,</span><br><span class="line">                           return_tensors=<span class="string">'tf'</span>,</span><br><span class="line">                           truncation=<span class="literal">True</span>,</span><br><span class="line">                           padding=<span class="literal">True</span>),</span><br><span class="line">                 output_hidden_states=<span class="literal">True</span>)[<span class="number">1</span>][<span class="number">-1</span>][:, <span class="number">0</span>].numpy()</span><br><span class="line"></span><br><span class="line">train_lstm = []</span><br><span class="line">val_lstm = []</span><br><span class="line">test_lstm = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_data_list_big:</span><br><span class="line">    train_lstm.append(bert_pool_out(i))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> val_data_list_big:</span><br><span class="line">    val_lstm.append(bert_pool_out(i))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> test_data_list_big:</span><br><span class="line">    test_lstm.append(bert_pool_out(i))</span><br><span class="line"></span><br><span class="line"><span class="comment"># padding, 得到的数据维度为(None, 20, 768)</span></span><br><span class="line">train_lstm_dataset = keras.preprocessing.sequence.pad_sequences(train_lstm, maxlen=<span class="number">20</span>, padding=<span class="string">'post'</span>, truncating=<span class="string">'post'</span>, dtype=<span class="string">'float32'</span>)</span><br><span class="line">val_lstm_dataset = keras.preprocessing.sequence.pad_sequences(val_lstm, maxlen=<span class="number">20</span>, padding=<span class="string">'post'</span>, truncating=<span class="string">'post'</span>, dtype=<span class="string">'float32'</span>)</span><br><span class="line">test_lstm_dataset = keras.preprocessing.sequence.pad_sequences(test_lstm, maxlen=<span class="number">20</span>, padding=<span class="string">'post'</span>, truncating=<span class="string">'post'</span>, dtype=<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用BiLSTM进行训练</span></span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">lstm_model = keras.models.Sequential()</span><br><span class="line">lstm_model.add(keras.layers.Bidirectional(keras.layers.LSTM(<span class="number">128</span>)))</span><br><span class="line">lstm_model.add(keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'tanh'</span>))</span><br><span class="line">lstm_model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">lstm_model.compile(optimizer=optimizer, loss=<span class="string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">lstm_model.fit(x=train_lstm_dataset, y=np.array(train_label_encode),</span><br><span class="line">               validation_data=(val_lstm_dataset, np.array(val_label_encode)),</span><br><span class="line">               epochs=<span class="number">100</span>,</span><br><span class="line">               shuffle=<span class="literal">True</span>,</span><br><span class="line">               batch_size=<span class="number">64</span>,</span><br><span class="line">               callbacks=[</span><br><span class="line">                   keras.callbacks.EarlyStopping(monitor=<span class="string">'val_accuracy'</span>,</span><br><span class="line">                                                 patience=<span class="number">10</span>,</span><br><span class="line">                                                 restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                   keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">'val_accuracy'</span>, factor=<span class="number">0.5</span>, patience=<span class="number">2</span>),</span><br><span class="line">                   LoggingCallback(logging.info)</span><br><span class="line">               ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评估测试集, 并保存模型</span></span><br><span class="line">logging.info(lstm_model.evaluate(x=test_lstm_dataset, y=np.array(test_label_encode), batch_size=<span class="number">64</span>))</span><br><span class="line">lstm_model.save(<span class="string">'/home/work/model/bert/lstm'</span>, save_format=<span class="string">'tf'</span>)</span><br></pre></td></tr></table></figure>
<p>对于(250, 200)这组分割文本的参数, BiLSTM在验证集上的准确率为0.962, 测试集准确率0.971.</p>
<p>对于(510, 400)这组分割文本的参数, BiLSTM在验证集上的准确率为0.969, 测试集准确率0.970.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>首先来对比分析一下试验的结果, 如果忽略一两个点的千分位差距, 可以发现直接微调BERT的结果, 与BERT+BiLSTM得到的结果是几乎一样的.</p>
<p>同时在BERT+BiLSTM的微调BERT阶段, 250的文本长度, BERT在验证集上对应的准确率为0.936; 而510的文本长度, BERT在验证集上对应的准确率为0.951, 有比较明显的差距.</p>
<p>综上结果表明, 在文本分类中, 当截取文本较短时, 由于获取信息较少, 准确率会相对较低; 当截取文本达到一定长度后, 就可以达到一个不错的准确率; 对于长文本, 想尽可能使用其全部文本进行分类, 可以用BERT+BiLSTM, 但效果不一定有显著提升.</p>
<p>然后再来比较一下在同一份数据集上, 与之前的一些文本分类方法的效果(Accuracy):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">TF-IDF</td>
<td style="text-align:center">0.946</td>
<td style="text-align:center">0.943</td>
</tr>
<tr>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">0.861</td>
<td style="text-align:center">0.871</td>
</tr>
<tr>
<td style="text-align:center">word2vec+keyword</td>
<td style="text-align:center">0.945</td>
<td style="text-align:center">0.941</td>
</tr>
<tr>
<td style="text-align:center">word2vec+LSTM</td>
<td style="text-align:center">0.917</td>
<td style="text-align:center">0.915</td>
</tr>
<tr>
<td style="text-align:center">Attention+LSTM</td>
<td style="text-align:center">0.946</td>
<td style="text-align:center">0.945</td>
</tr>
<tr>
<td style="text-align:center">BERT</td>
<td style="text-align:center">0.977</td>
<td style="text-align:center">0.972</td>
</tr>
<tr>
<td style="text-align:center">BERT+BiLSTM</td>
<td style="text-align:center">0.969</td>
<td style="text-align:center">0.970</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到, BERT与BERT+BiLSTM的效果是最好的; 而使用简单的TF-IDF和word2vec+keyword也有不错的效果; 单纯的LSTM效果不好, 而在使用word2vec作为初始Embedding, 以及增加Attention后, 效果也有明显提高.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>文本分类</tag>
        <tag>BERT</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT(一)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/BERT-%E4%B8%80/</url>
    <content><![CDATA[<p>好, 今天来说说大名鼎鼎的BERT(Bidirectional Encoder Representations from Transformers), BERT是NLP领域的龙骨级模型, 它的重要意义不止其在各项任务上效果特别好, 下面就来进行介绍.</p>
<a id="more"></a>
<h1 id="BERT之前的一些模型"><a href="#BERT之前的一些模型" class="headerlink" title="BERT之前的一些模型"></a>BERT之前的一些模型</h1><p>在说BERT之前, 先稍微回顾一下早期的一些模型.</p>
<p>首先是<a href="whitemoonlight.top/2020/08/16/自然语言处理/word2vec-一/">word2vec</a>, 在2013年有了word2vec算法以后, 在NLP领域可谓遍地开花, 用word2vec的预训练向量来代替随机初始化的词向量, 可以加快模型收敛, 并且获得更好的表现.</p>
<p>同时在这里还值得一提的是word2vec中, 使用了负采样的方法, 来代替原本的层次softmax. 在word2vec中, 负采样是指词级别的采样, 两个词是否相关, 相关作为正样本, 不相关则作为负样本. 在BERT中也有使用负采样, 只不过从词级别变成了句级别.</p>
<p>然鹅, word2vec的一个明显问题是, 其向量是固定的, 是上下文无关的. 在一些任务中, 需要消歧, 一般可以用RNN类模型来做.</p>
<p>那么后来就想, 既然需要上下文相关, 为什么不直接就在预训练模型中达成这一点呢? 于是乎<a href="whitemoonlight.top/2020/08/16/自然语言处理/ELMo/">ELMo</a>出现了. 利用两层双向LSTM, 可以实现动态的, 上下文相关的词向量.</p>
<p>但是ELMo也是有问题的, 总感觉它并不是”完全”的上下文相关. ELMo基于双向LSTM, 将其结果进行拼接, 但是每个单向的LSTM只看到了一侧的信息, 并不是真正意义上的全局上下文相关.</p>
<p>所以到后面, 也就出现了BERT(Bidirectional Encoder Representations from Transformers), 首先, 如果是了解Transformer的同学, 应该看到这个名称, 就大概知道了BERT的模型结构; 然后, BERT也是芝麻街的一个角色, 作者挺能整活的:</p>
<p><img src="fig_0.jpeg" alt="fig"></p>
<p>BERT的重要意义, 不在于用了什么模型, 怎样训练的, 而是展现了一种新的范式. 在之前, 为每个NLP任务, 去深度定制泛化能力极差的复杂模型, 感觉是不优雅的, 走偏了方向. 想想我们人类的语言能力, 需要针对各种具体的场景进行独立地艰难地训练吗, 一般不需要, 在熟练掌握一门语言后, 可以稍加学习适应各种相关任务. 所以, 在NLP中是否也可以有这样的模式呢? BERT做到了, 作为一个泛化能力极强的龙骨级模型, 可以描述字符级, 词级, 句级的关系特征, 对于不同的NLP任务, 只需要加一个轻量级的输出层, 在少量的数据上训练即可.</p>
<h1 id="BERT的原理"><a href="#BERT的原理" class="headerlink" title="BERT的原理"></a>BERT的原理</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img src="fig_1.png" alt="fig"></p>
<p>如图, BERT的模型结构, 就是<a href="whitemoonlight.top/2020/08/16/自然语言处理/Transformer/">Transformer</a>的Encoder部分.</p>
<p>其结构核心, 就是Self-Attention, 相比RNN的单向结构, 和CNN的局部结构, Self-Attention真正做到了全局的Embedding.</p>
<p>在谷歌的<bidirectional encoder representations from transformers>论文中, 根据参数的不同, 提出了Base和Large两种BERT模型.</bidirectional></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">Layers</th>
<th style="text-align:center">Hidden Size</th>
<th style="text-align:center">Attention Head</th>
<th style="text-align:center">参数总量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Base</td>
<td style="text-align:center">12</td>
<td style="text-align:center">768</td>
<td style="text-align:center">12</td>
<td style="text-align:center">110M</td>
</tr>
<tr>
<td style="text-align:center">Large</td>
<td style="text-align:center">24</td>
<td style="text-align:center">1024</td>
<td style="text-align:center">16</td>
<td style="text-align:center">340M</td>
</tr>
</tbody>
</table>
</div>
<p>其中的Layers表示的是Encoder的层数, Hidden Size表示向量维度, Attention Head表示Multi-head的数量. 可以看到, 模型参数是非常大的, Large的参数量接近是Base的三倍, 性能上也确实有提升.</p>
<p>可以说, 如果明白了Attention, Transformer, 那么BERT的结构就是这些东西, 没有太多新的东西. 不过BERT的训练方式是有独到之处的, 下面进行讲解.</p>
<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p><img src="fig_2.png" alt="fig"></p>
<p>上图是BERT的输入, 咋一看, 怎么一个输入都这么复杂, 其实都挺好理解的, 下面一一进行说明.</p>
<ul>
<li><p>Token Embedding:</p>
<p>就是单词, 或者字符对应的向量, 由训练获得.</p>
</li>
<li><p>Position Embedding:</p>
<p>也很好理解, 因为Self-Attention本身并不支持序列位置信息的表示, 所以在输入的时候需要额外地添加位置信息.</p>
<p>不过这里并没有像Transformer那样, 使用数学公式(三角函数)来直接进行表示, 而是通过学习来确定.</p>
</li>
<li><p>Segment Embedding:</p>
<p>由于训练任务(下文说明), 需要区分两个句子, 除了在Token那里使用[SEP]进行分割, 还额外增加了Segment Embedding. 在同一句子中, Segment Embedding是一致的.</p>
</li>
</ul>
<p>将上面三种向量相加后, 就得到了BERT的输入, 就可以根据Self-Attention的算法进行运算了.</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>现在的核心问题是, 如何定义学习任务, 让模型通过这个任务来调整内部的参数.</p>
<ul>
<li><p>Masked LM</p>
<p>首先这个任务肯定不能是有监督(带标签)任务, 因为这么大的模型, 需要海量的数据进行训练, 这样的带标注的数据一般是不存在的. 而在无监督的学习任务里面, 在NLP中一般会想到语言模型, 但是BERT就是要的全局Embedding, 语言模型也不合适. 想我们以前的英语作业, 除了做阅读, 写作文以外, 还有啥, 完形填空!</p>
<p>没错, BERT正是用到完形填空这个学习任务来进行训练, 用论文中专业的名称叫做Masked LM(Masked Language Model). 所谓Masked Language Model, 就是说不像传统语言模型那样, 给定已经出现的词, 去预测下一个词, 而是直接把一整句话的一部分(随机选择)盖住(标记为[MASK]), 让模型去进行预测.</p>
<p>不过这时候出现了一个小问题, 即训练过程和推断过程不一致, 因为在训练结束后使用模型时, 是没有[MASK]标记的. 在训练的过程中, 模型可能学着学着, 会从[MASK]这个标记本身中学出一些模式, 而我们的本意, 是让模型忽略这个[MASK], 从其它的Token上获取信息去进行预测.</p>
<p>如何让模型尽可能地忽略[MASK]标记呢, 其实就要让它本身变得”没有规律”. 对于要进行预测的Token, 如果在输入端, 有时候出现[MASK]标记, 有时候出现某个意义不明的其它Token, 有时候就是要预测的Token本身, 没有固定的模式, 模型可能就会把这个[MASK]当成噪音对待.</p>
<p>具体的做法, 就是首先对一个句子中, 将其随机选取的15%的Token, 作为要预测的, 然后对于对应位置的输入, 进行如下设置:</p>
<ul>
<li>80%概率, 使用[MASK]标记.</li>
<li>10%概率, 随机替换为某个Token.</li>
<li>10%概率, 直接告诉答案, 即用原Token.</li>
</ul>
<p>这样做了以后, 既可以尽可能让模型忽略[MASK]标记, 又避免了大量地让其它的Token来客串, 导致其它Token学偏了. 至于这个具体比例为什么是这样的, 我猜测应该是拍脑袋吧哈哈.</p>
<p>通过Masked LM, BERT可以学习到字符级, 或者词级的模式, 而谷歌更进一步, 还想要学习句级的模式.</p>
</li>
<li><p>Next Sentence Prediction</p>
<p>还记得一开始回顾word2vec时提到的负采样吗, BERT在这里就用到了, 不过这次不是采样词, 而是采样句子. 具体的学习任务, 是给定一个句子对(两个句子), 判断两个句子是否相关, 或者说给定一个句子, 判断另一个句子是否是它的下一句.</p>
<p>再联系到上面讲解BERT的输入, 其中的Segment Embedding用以区分两个句子. 此外还有两个特殊的Token, 一个是[SEP], 用以表示句子的分割; 还有一个是[CLS], 在对应的输出位置上添加一个分类器(二分类), 通过Next Sentence Prediction的学习任务, 其对应的向量可以用于表示句子, 即获得了句子级的表示.</p>
</li>
</ul>
<p>谷歌使用了BooksCorpus(800M词汇量)和英文Wikipedia(2500M词汇量)进行预训练. Base和Large模型分别使用4台Cloud TPU(16张TPU)和16台Cloud TPU(64张TPU)训练了4天. 啊这…有钱就是可以为所欲为.jpg</p>
<p>在训练好了BERT以后, 下游任务大都变得非常简单:</p>
<p><img src="fig_3.jpg" alt="fig"></p>
<p>句子对的分类任务, 单句的分类任务, 问答系统, 序列标注等, 很多时候只需要添加简单的输出层, 使用不大的特定任务的样本进行训练, 即可得到很好的效果.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>其实可以发现, BERT中的很多东西都是已有的, Self-Attention, Transformer-Encoder, 负采样, 但是BERT能够获得这样优越的效果, 说明建模的出发点(构建一个龙骨级模型), 数据量, 算力, 模型结构, 都是不可或缺的东西.</p>
<p>BERT的优点自不必说, 谁用谁知道, 能够做到全局的上下文表示, 词级, 句级均可. 至于缺点, 比如…嗯…对了, 跑起来太费资源了[狗头].</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>BERT</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/</url>
    <content><![CDATA[<p>在上一篇介绍Attention时, 就提到了Transformer, 那么什么是Transformer呢, 一句话概括的话, 大概就是用Self-Attention实现的Encoder-Decoder, 下面进行详细的讲解.</p>
<a id="more"></a>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><attention is all you need>, 感兴趣的同学可以阅读原文.</attention></p>
<p>在讲具体的结构之前, 来整体看一下Transformer的模型结构, 如下图:</p>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>作为Encoder-Decoder框架, 比较明显的, 其左边部分是Encoder, 而右边部分是Decoder.</p>
<p>Transformer一个经典的自然语言处理任务就是翻译, 比如从中文翻译到英文. 左边Encoder可以并行输入, 计算和输出, 右边Decoder需要按顺序进行解码输出.</p>
<p>在图中的${\rm N}\times$的标识, 标识该模块重复N次, 如同全连接网络中的多层隐藏层一样.</p>
<p>用橙色填充的部分, 就是Self-Attention, Encoder中有一种, Decoder中有两种, 这三种有一些差别. Self-Attention是Transformer的核心. 下面就首先讲解Self-Attention.</p>
<h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><p>在<a href="whitemoonlight.top/2020/08/16/自然语言处理/Attention/">Attention</a>中, 就提到过Self-Attention, 作为Attention家族中的一员, 它的独特之处在于, 其计算Attention Value用到的Query, Key, Value, 均来自同一数据源.</p>
<p>一般的Attention结构如下:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>而在Transformer中, Query, Key, Value都是通过输入得到的:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>在有了Query, Key, Value以后, 就可以按照通用的Attention计算方式继续计算了. 在Self-Attention这里用矩阵来表示, 流程如下:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>其中的I表示输入矩阵, 维度为(向量维度$\times$序列长度), 在分别与三个矩阵相乘以后, 得到了Q, K, V三个矩阵, 维度为(向量维度$\times$序列长度). 用Q矩阵与K矩阵的转置相乘后, 得到维度为(序列长度$\times$序列长度)的矩阵, 然后再经过softmax得到概率分布矩阵. 最后V矩阵与概率分布矩阵相乘, 得到输出矩阵.</p>
<h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>在Encoder中, 首先要说明的是Encoder的输入, 其实和一般的RNN类似, 通过一个lookup table, 将原本序列的编号转换为对应的Embedding向量.</p>
<p>而一般来说此时又有两种方法, 一种是使用预训练好的向量, 一种是随机初始化然后随着模型进行训练. Transformer在这里使用的第二种方法.</p>
<p>然鹅这时候出现了一个严重的问题, 如果以这种形式输入, 再经过Self-Attention Layer的话, 哪来的位置信息呢, 如果没有位置信息, 就不算处理序列问题呀.</p>
<p>在RNN中, 天然地存在随时间的序列关系, 相同的一些单词, 以不同的顺序输入, 会得到不同的结果. 但是在Transformer这里却不行, 不同的顺序得到的结果最后是一样的.</p>
<p>所以作者想了一个办法, 就是在原本的Embedding上, 添加”位置信息”, 或者说位置的Embedding. 这里又有两种方法来添加位置Embedding, 一种是每个位置随机初始化, 然后跟着模型一起学; 还有一种是给一个连续变化的数学公式, 根据不同的位置来给定不同的Embedding, 并不跟随模型训练, 即固定不变. 之所以这样选择, 是因为作者发现两种做法得到的模型表现差不多, 且后者需要学习的参数较少, 有更强的泛用性.</p>
<p>具体的添加位置信息的方式, 是与原输入的Embedding直接按位相加. 我想此时细心的同学可能就要问了, 为什么是相加而不是拼接呢, 可以用下图来进行解释:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>原本的输入和位置信息, 都可以看做one-hot编码, 将它们拼接在一起, 再去乘以对应的Embedding矩阵, 最终的效果其实就等价于, 分别将两者的lookup table上得到的向量, 按位相加.</p>
<p>同时位置编码的数学公式如下:</p>
<script type="math/tex; mode=display">
PE(pos,2i)=\sin(pos\times10000^{2i/d_{model}}) \\
PE(pos,2i+1)=\cos(pos\times10000^{2i/d_{model}})</script><p>其中$pos$表示在序列中的位置, $2i, 2i+1$表示某一个序列位置对应的位置向量上的位置, $d_{model}$表示向量维度大小.</p>
<h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p><img src="fig_5.jpg" alt="fig"></p>
<p>好的, 现在说明了Encoder的输入以后, 接下来就是Multi-head Attention.</p>
<p>其实没啥好说的, 就是说由原本的一组Query, Key, Value, 变成了多组的Query, Key, Value, 每一组重点去关注数据中不同的模式, 最后输出时将结果进行拼接.</p>
<p>用数学表示如下:</p>
<script type="math/tex; mode=display">
head_i=Attention(q_i,K,V) \\
MultiHead(Q,K,V)=Concat(head_1,\dots,head_n)W^O</script><p>原文中, 在使用Query和Key计算Attention时, 具体方式如下:</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)={\rm softmax}(\frac{QK^T}{\sqrt{d}})V</script><p>其中的$d$表示向量维度.</p>
<h2 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h2><p><img src="fig_6.jpg" alt="fig"></p>
<p>在Multi-head Attention之后便是Add &amp; Norm.</p>
<p>其中的Add, 其实就是类似残差网络的结构, 将原本的输入直接加到Multi-head Attention的输出上. 可以解决多层神经网络训练困难的问题, 通过将前一层的信息无差的传递到下一层, 可以有效的仅关注差异部分.</p>
<p><img src="fig_7.png" alt="fig"></p>
<p>这里的Norm, 表示Layer Norm, 与Batch Norm作用在同一维度的batch上不同, Layer Norm作用在同一样本的不同维度上. 一般来说在序列任务中, 更多的是使用Layer Norm, 可以加速模型训练和收敛.</p>
<h2 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h2><p><img src="fig_8.jpg" alt="fig"></p>
<p>这里就比较寻常的全连接网络, 不过需要注意的是针对每一个Add &amp; Norm输出的向量, 而不是它们拼接而成的向量.</p>
<p><img src="fig_9.png" alt="fig"></p>
<p>具体的传播方式, 用数学公式表示如下:</p>
<script type="math/tex; mode=display">
FFN(x)=\max(0,xW_1+b_1)W_2+b_2</script><p>本质上就是一层Relu层再加一层线性层.</p>
<h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><p>假设现在已经有了Encoder的输出结果, 是一系列的向量, 那么现在来讲解Decoder部分.</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="fig_10.jpg" alt="fig"></p>
<p>这里先说Decoder的输出部分, 图上比较直观, 就是一个全连接层, 再加上一个softmax层, 用以预测当前的输出.</p>
<h2 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h2><p><img src="fig_11.jpg" alt="fig"></p>
<p>然后是输入, 一般来说, 就像一个语言模型一样, 是随着序列逐步输入的. 比如第一次只输入一个起始符<start>, 然后得到新的输出以后, 再将输出添加到输入中.</start></p>
<p>这里需要像Encoder的输入一样, 也需要添加位置信息, 添加方式与Encoder一致.</p>
<p>不过还存在两个问题, 一是如果已经把模型训练好了, 在使用的时候每次自然是把上一次的输出添加到新的输入, 但是在训练的时候, 这么做是否合适呢? 其实不太合适, 因为在一开始模型还很不准的时候, 一开始输出就是错的, 然后又将错误的输出作为输入, 只会累计更大的误差. 所以为了帮助模型学习得更好, 一开始是将正确答案作为输入, 随着训练的进行, 才将上次的输出添加到新的输入.</p>
<p>第二个问题是, 对于Encoder的输入的向量, 是随机初始化的一个lookup table, 那在Decoder这边呢? 难道也是随机初始化的一个look table吗, 查阅了一些资料没有发现对此有说明, 我在这里估计应该是使用softmax那里的矩阵, 来作为Decoder的lookup table. 原因是这本身就是一个天然的一对一关系, 同时当给定某个正确输出时, 会对应地强化其系数向量, 使其计算后得到的概率更大, 所以使用softmax那里的矩阵作为Decoder的lookup table是非常自然的.</p>
<h2 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h2><p><img src="fig_12.jpg" alt="fig"></p>
<p>在前面Encoder的时候, 说明了Multi-Head Attention, 这里多了一个Masked, 这又是什么意思呢?</p>
<p>因为Decoder部分整体来说类似一个语言模型, 而语言模型是”从左到右”按顺序进行的, 当前的输出依赖于前面的信息.</p>
<p>所以这里的Masked, 就是指的在进行Attention计算时, 对于位置在$i$的向量, 只能与位置$i$之前的向量计算Attention. 比如位置1的就与自己计算, 位置2上的与位置1, 2计算, 位置3上的与位置1, 2, 3计算…</p>
<p>然后每次在Masked Multi-Head Attention输出的结果, 再与Encoder输出的结果一起再经过一个Multi-Head Attention, 这样当前的Decoder输出结果就用到了Encoder的所有信息, 以及Decoder之前的信息, 逐步完成序列输出.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这里放一张Transformer整体的流程动态度, 也许可以帮助更好地理解其运作方式:</p>
<p><img src="fig_13.webp" alt="fig"></p>
<p>第一步是在Encoder上完成编码, 输出序列的编码信息; 第二步是在Decoder上, 逐步地进行解码, 每次用到之前输出的信息以及Encoder的输出信息.</p>
<p>Transformer的优点, 首先是效果确实好, Attention is all you need, 能够摒弃CNN, RNN, 具有足够的创新; 其次是结构(Encoder)是可以并行化的, 加速了模型的训练与推断; 此外还具有一定的可解释性:</p>
<p><img src="fig_14.jpg" alt="fig"></p>
<p>上图为Self-Attention中权重概率分布的一个栗子, 从中其实可以看出在训练好的Self-Attention上, 是具有对应的可解释性的, 可以知道每个位置上的词在语法/语义上与其它哪些词相关.</p>
<p>而Transformer的缺点, 非要说的话, Transformer的位置信息的处理可能还有改进之处, 使用数学公式在输入向量中加入位置向量有点权宜之计的意思.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>自然语言处理</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Attention/</url>
    <content><![CDATA[<p>注意力机制(Attention), 可以说是现在深度学习中不可或缺的一个组成部分, 在多个场景下, 将模型的性能明显提高.</p>
<p>本篇主要介绍Attention的原理, 并展示其在文本分类上的应用.</p>
<a id="more"></a>
<h1 id="Attention介绍"><a href="#Attention介绍" class="headerlink" title="Attention介绍"></a>Attention介绍</h1><h2 id="Attention的由来"><a href="#Attention的由来" class="headerlink" title="Attention的由来"></a>Attention的由来</h2><p>在以前上学的时候, 有时候老师会强调听课要注意力集中, 此外在学习的时候注意力是否能够集中, 也是衡量一个学生学习能力的指标之一. 为什么要集中注意力呢, 其中一个原因是我们通过五官, 每时每刻能够接收到外界的大量信息, 但是首先并不是所有信息都是重要的, 其次我们的CPU(大脑)并不能有效处理所有信息, 所以需要分清主次, 针对性处理重要的信息.</p>
<p>那么, 这样的思想可以引入机器学习/深度学习吗, 是可以的.</p>
<p>深度学习中的注意力机制借鉴了人类的注意力思维方式. 比如视觉注意力机制是人类视觉所特有的大脑信号处理机制. 人类视觉通过快速扫描全局图像, 获得需要重点关注的目标区域, 也就是所说的注意力焦点, 然后对这一区域投入更多的注意力资源, 以获取更多所需要关注目标的细节信息, 从而抑制其它无用信息. 这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段, 是人类在长期进化中形成的一种生存机制. 人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性.</p>
<p><img src="fig_0.jpg" alt="fig"></p>
<p>比如在上图中, 如果一眼看过去, 一般来说会优先注意到&lt;锦江大饭店&gt;的招牌, 而其它的一些信息(如招牌上的电话), 可能就不会太注意. 所以当我们实际看到的图片可能是这样的:</p>
<p><img src="fig_1.jpg" alt="fig"></p>
<h2 id="深度学习中的Attention"><a href="#深度学习中的Attention" class="headerlink" title="深度学习中的Attention"></a>深度学习中的Attention</h2><p>在深度学习中的, 其实Attention首先出现在计算机视觉当中, 而随后在NLP中开始应用, 并且由于BERT和GPT这些出色模型的出现, 其核心Attention开始被大众所关注, 发扬光大. BERT, Transformer, Attention的关系大致如下图:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>这里不讲在计算机视觉中的Attention, 主要以NLP中的Attention使用, 来说明Attention的原理.</p>
<p>在深度学习中, 有一种Encoder-Decoder框架, 框架分为两部分, 其中的Encoder部分, 负责将源数据进行编码, 而Decoder则负责将编码的信息进行解码. 比如在”看图说话”中, 可以使用CNN-RNN的具体结构; 在机器翻译中, 可以使用RNN-RNN的具体结构, 而这样的序列到序列的模型, 也称为Seq2Seq.</p>
<p>为什么要提Encoder-Decoder呢, 不是在讲Attention吗? 因为Attention将这一类框架提升到了一个新的高度. 需要注意的是, Attention是一种方法, 本身并不依赖于任何一种框架或者模型. 下面来看一看经典的Seq2Seq框架:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>上图是一个使用LSTM-LSTM的翻译模型, 从英语翻译到法语. 从Encoder传输向Decoder的, 就是LSTM中的$c$和$h$两个向量, 下面统一称之为上下文向量(context vector). 然后Decoder在接收到上下文向量后, 开始逐步解码, 即翻译.</p>
<p>其实中以上的模型结构中, 很容易发现其问题, 一个小小的上下文向量, 如何表示前面语句的语法和语义信息, 让Decoder接受到以后能够正确地翻译出来? 这需要一个Encoder和Decoder的精妙配合, 谁出了点问题都不行.</p>
<p>其实在Encoder的LSTM中, 其输出的信息可不止最后的上下文向量, 而是同样输出了序列信息, 如果能够把整个序列的输出向量都利用起来, 是不是就会比只用最后一个向量, 得到更好的结果呢?</p>
<p>这时候就可以让Attention上场了:</p>
<p><img src="fig_4.png" alt="fig"></p>
<p>上图中, 在加入了Attention了以后, Decoder在$t$时刻解码时, 会将$h_{t}^{Decoder}$, 作为Query向量, 与Encoder中的各个时刻$t\in T$输出的向量$h_{t}^{Encoder}$进行运算(如内积), 得到一系列的Score(标量), 然后利用Softmax得到一个分布. 这个分布的含义, 就是表示Encoder中各个时刻的输出向量, 对Decoder当前的贡献或者相关性. 然后利用这个分布, 对Encoder的输出向量$h_{t}^{Encoder}$进行加权平均, 得到的向量再与原本的$h_{t}^{Decoder}$进行拼接, 来进行当前的预测(翻译).</p>
<p>在加入了Attention以后, Decoder中每个时刻都用到了Encoder的所有输出向量, 同时每次通过加权平均, 关注相关性高的输出向量, 使得无论是Eecoder还是Decoder都可以学得更加轻松.</p>
<p>认识了Attention在Encoder-Decoder中的形式后, 下面来介绍Attention的通用的抽象结构.</p>
<p><img src="fig_5.png" alt="fig"></p>
<p>Attention的计算需要三类元素:</p>
<ul>
<li>Query</li>
<li>Key</li>
<li>Value</li>
</ul>
<p>由Query与Key计算得到权值, 然后用权值与Value一起得到最终的Attention Value(加权向量).</p>
<p>更加具体的过程如下图:</p>
<p><img src="fig_6.png" alt="fig"></p>
<ul>
<li><p>阶段一</p>
<p>由Query和各个Key经过某种计算方式, 得到一系列的Score.</p>
</li>
<li><p>阶段二</p>
<p>对得到的Score进行Softmax, 转化为概率分布.</p>
</li>
<li><p>阶段三</p>
<p>利用得到的概率分布, 对Value进行加权平均, 输出最终的Attention Value.</p>
</li>
</ul>
<h2 id="不同类型的Attention"><a href="#不同类型的Attention" class="headerlink" title="不同类型的Attention"></a>不同类型的Attention</h2><p>上面通过Encoder-Decoder框架, 介绍了Attention的一种使用方式, 但是构建Attention的方式绝不唯一, 使用场景也可以多种多样, 因此下面对Attention的一些类型来进行不完全总结.</p>
<ul>
<li><p>Global Attention &amp; Local Attention</p>
<p>按计算区域来进行划分, 可以分为Global Attention与Local Attention. 其中全局很好理解, 就是比如在上面的Encoder-Decoder中, 计算Attention时利用到了Encoder所有的输出向量.</p>
<p>Global Attention也属于Soft Attention(加权平均), 其缺点是当序列较长时, 每次都要计算整个序列的Attention, 复杂度较高, 且由于其中可能包含大量不相关信息, 导致效果反而下降.</p>
<p>与之对应的是Hard Attention, 即每次通过一些方法选择一个最相关的向量, 具体实现不了解, 但是据说不可微, 需要用到强化学习.</p>
<p>Local Attention算是Global Attention和Hard Attention的折中, 每次选择一部分向量进行计算, 这样可以减少计算量, 效果也不错, 同时可微. 主要思路为</p>
<p>Local Attention具体可以有两种做法:</p>
<ul>
<li><p>Monotonic alignment</p>
<p>这种做法比较简单粗暴, 比如当前进行到了Decoder的第$t$步, 那么对应的取Encoder的第$t$步的窗口中心, 然后将其两侧的窗口内的向量用来进行计算.</p>
</li>
<li><p>Predictive alignment</p>
<p><img src="fig_7.png" alt="fig"></p>
<p>先使用一个小网络来进行计算输出一个概率, 再让这个概率乘以Encoder的长度$S$得到窗口中心的位置. 然后在计算得到了Score后, 再乘以一个高斯分布来调整权值, 即靠近窗口中心的权值更大, 反之更小.</p>
</li>
</ul>
</li>
<li><p>General Attention &amp; Self Attention</p>
<p>所谓General Attention, 就是计算Attention Value用到的Query, Key, Value可以来自不同的数据源.</p>
<p>对应的Self Attention, 则是说Query, Key, Value来自于同一数据源. 而前面提到的BERT, 正是用到的Self Attention, 这里不做更多的讲解.</p>
</li>
<li><p>Multi-head Attention</p>
<p>就是说, 将原本的一个Query变成多个Query, 每个Query关注不同的重点.</p>
<script type="math/tex; mode=display">
head_i=Attention(q_i,K,V) \\
MultiHead(Q,K,V)=Concat(head_1,\dots,head_n)W^O</script></li>
<li><p>Pure Attention</p>
<p>Attention的实现可以依附于CNN, RNN这样的模型, 也可以单独用来构建模型, Transformer就是纯粹使用Attention来搭建的.</p>
</li>
</ul>
<p>而对于Query和Key进行计算得到Score的方式, 也是不固定的, 常用的一些方式如下:</p>
<ul>
<li><p>内积:</p>
<script type="math/tex; mode=display">
s(q,k)=q\cdot k</script></li>
<li><p>余弦相似度:</p>
<script type="math/tex; mode=display">
s(q,k)=\frac{q\cdot k}{||q||\cdot||k||}</script></li>
<li><p>线性:</p>
<script type="math/tex; mode=display">
s(q,k)=qWk</script></li>
<li><p>拼接+线性:</p>
<script type="math/tex; mode=display">
s(q,k)=v\cdot[q;k]</script></li>
<li><p>非线性:</p>
<script type="math/tex; mode=display">
s(q,k)=v\cdot \tanh(Wq+Uk)</script></li>
<li><p>拼接+非线性:</p>
<script type="math/tex; mode=display">
s(q,k)=v\cdot\tanh(W[q;k])</script></li>
</ul>
<h1 id="文本分类实例"><a href="#文本分类实例" class="headerlink" title="文本分类实例"></a>文本分类实例</h1><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>在使用LSTM进行文本分类的时候, 一般来说双向的, 即BiLSTM效果会更好一些, 不过这里的重点是看加入了Attention之后, 对于分类效果是否有提升, 所以只用了最简单的单向LSTM. 这里在加载数据前, 已经做好了词表和对原始数据的编码.</p>
<p>为了保证尽可能的公平, 让加入Attention前后的模型参数差不多, 所以在不加入Attention时, 多了一层全连接层.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">LSTM文本分类模型.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskSoftmax</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask)</span>:</span></span><br><span class="line">        tmp = tf.math.exp(inputs) * mask</span><br><span class="line">        tf.print(tf.reduce_sum(tmp, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> tmp / tf.reshape(tf.reduce_sum(tmp, axis=<span class="number">1</span>), (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim=<span class="number">128</span>, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="comment"># input_shape: (batch_size, time_steps, vec_dim)</span></span><br><span class="line">        self.vec_dim = input_shape[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># W矩阵</span></span><br><span class="line">        self.W_layer = keras.layers.Dense(self.vec_dim, use_bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 取出最后的输出向量, 作为ht向量</span></span><br><span class="line">        self.ht_layer = keras.layers.Lambda(<span class="keyword">lambda</span> x: x[:, <span class="number">-1</span>, :], output_shape=(self.vec_dim,))</span><br><span class="line">        <span class="comment"># 带mask的softmax层</span></span><br><span class="line">        self.softmax_layer = MaskSoftmax()</span><br><span class="line">        <span class="comment"># 最后输出层</span></span><br><span class="line">        self.output_layer = keras.layers.Dense(self.output_dim, use_bias=<span class="literal">False</span>, activation=<span class="string">'tanh'</span>)</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: (batch_size, time_steps, vec_dim)</span></span><br><span class="line">        ht = self.ht_layer(inputs)  <span class="comment"># (batch_size, vec_dim)</span></span><br><span class="line">        <span class="comment"># self.W_layer(inputs): (batch_size, time_steps, vec_dim)</span></span><br><span class="line">        score = keras.layers.dot([self.W_layer(inputs), ht], axes=[<span class="number">2</span>, <span class="number">1</span>])  <span class="comment"># (batch_size, time_steps)</span></span><br><span class="line">        weight = self.softmax_layer(score, mask)  <span class="comment"># (batch_size, time_steps)</span></span><br><span class="line">        weight_vec = keras.layers.dot([inputs, weight], axes=[<span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># (batch_size, vec_dim)</span></span><br><span class="line">        concate_vec = keras.layers.concatenate([weight_vec, ht])  <span class="comment"># (batch_size, vec_dim * 2)</span></span><br><span class="line">        outputs = self.output_layer(concate_vec)  <span class="comment"># (batch_size, output_dim)</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, word_dim, embedding=None, is_attention=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.word_dim = word_dim</span><br><span class="line">        self.embedding = embedding</span><br><span class="line">        self.is_attention = is_attention</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_layer = keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.word_dim, mask_zero=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.embedding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding_layer.build((self.vocab_size,))</span><br><span class="line">            self.embedding_layer.set_weights([self.embedding])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_attention:</span><br><span class="line">            self.lstm_layer = keras.layers.LSTM(<span class="number">64</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">            self.attention_layer = Attention(<span class="number">64</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.lstm_layer = keras.layers.LSTM(<span class="number">64</span>)</span><br><span class="line">            self.dense_layer = keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'tanh'</span>)</span><br><span class="line">        self.output_layer = keras.layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>)</span><br><span class="line">        self.softmax_layer = keras.layers.Softmax()</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask)</span>:</span></span><br><span class="line">        x = self.embedding_layer(inputs)</span><br><span class="line">        x = self.lstm_layer(x)</span><br><span class="line">        <span class="keyword">if</span> self.is_attention:</span><br><span class="line">            x = self.attention_layer(x, mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.dense_layer(x)</span><br><span class="line">        x = self.output_layer(x)</span><br><span class="line">        outputs = self.softmax_layer(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    mask = tf.math.logical_not(tf.math.equal(inputs, <span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(mask, tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预处理数据, 准备训练样本.</span></span><br><span class="line"><span class="string">在经过分词, 按出现频次去除部分词后, 得到词表.</span></span><br><span class="line"><span class="string">使用word2vec进行预训练, 得到预训练向量.</span></span><br><span class="line"><span class="string">结合词表, 预训练向量, 得到Embedding矩阵.</span></span><br><span class="line"><span class="string">返回dataset用于训练.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_embedding</span><span class="params">(vocab, model, embedding_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    结合词表和训练的词向量, 来返回Embedding.</span></span><br><span class="line"><span class="string">    @param vocab: 词表字典&#123;word: idx&#125;.</span></span><br><span class="line"><span class="string">    @param model: word2vec模型.</span></span><br><span class="line"><span class="string">    @param embedding_dim: 向量维度.</span></span><br><span class="line"><span class="string">    @return: Embedding矩阵.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocab_size = len(vocab)</span><br><span class="line">    embedding = np.zeros((vocab_size, embedding_dim))</span><br><span class="line">    word2vec_vocab = model.wv.index2word</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> vocab:</span><br><span class="line">        idx = vocab[w]</span><br><span class="line">        <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> word2vec_vocab:</span><br><span class="line">            embedding[idx] = model[w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding[idx] = np.random.uniform(low=<span class="number">-1</span> / embedding_dim, high=<span class="number">1</span> / embedding_dim, size=embedding_dim)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(data_path, batch_size, max_length)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">        res = tf.strings.split(line, <span class="string">'\t'</span>)</span><br><span class="line">        text, label = res[<span class="number">0</span>], res[<span class="number">1</span>]</span><br><span class="line">        text = tf.strings.split(text)</span><br><span class="line">        label = tf.strings.split(label)</span><br><span class="line">        text = tf.cast(tf.strings.to_number(text), tf.int32)</span><br><span class="line">        text = text[: max_length]</span><br><span class="line">        label = tf.cast(tf.strings.to_number(label), tf.int32)</span><br><span class="line">        <span class="comment"># label = tf.expand_dims(tf.cast(tf.strings.to_number(label), tf.int32), axis=0)</span></span><br><span class="line">        <span class="comment"># text = tf.expand_dims(text, axis=0)</span></span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    dataset = tf.data.TextLineDataset(filenames=data_path) \</span><br><span class="line">        .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">        .shuffle(buffer_size=<span class="number">10240</span>, seed=<span class="number">7</span>) \</span><br><span class="line">        .padded_batch(batch_size, padded_shapes=([<span class="number">-1</span>], [<span class="number">-1</span>])) \</span><br><span class="line">        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">配置文件.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前缀路径</span></span><br><span class="line">PREFIX_PATH = <span class="string">'/home/featurize/data/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志保存路径</span></span><br><span class="line">LOG_PATH = <span class="string">'/home/featurize/work/log/attention/attention_20201001.log'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表保存路径</span></span><br><span class="line">VOCAB_PATH = PREFIX_PATH + <span class="string">'attention/vocab.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理后数据保存路径</span></span><br><span class="line">TRAIN_DATA_PATH = PREFIX_PATH + <span class="string">'attention/train.txt'</span></span><br><span class="line">VAL_DATA_PATH = PREFIX_PATH + <span class="string">'attention/val.txt'</span></span><br><span class="line">TEST_DATA_PATH = PREFIX_PATH + <span class="string">'attention/test.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量维度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302240/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">EPOCH = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大长度</span></span><br><span class="line">MAX_LENGTH = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率衰减</span></span><br><span class="line">LR_DECAY_RATE = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多少轮数未降低Loss时学习率衰减</span></span><br><span class="line">NUM_EPOCH_LR_DECAY = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在修改好config后, 运行脚本:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_</span><span class="params">(msg)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_log:</span><br><span class="line">        logging.info(msg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(msg)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成词表</span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(VOCAB_PATH, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            word, idx = line.strip().split()</span><br><span class="line">            vocab[word] = int(idx)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(line)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="comment"># 加载word2vec模型</span></span><br><span class="line">word2vec_model = word2vec.Word2Vec.load(PREFIX_PATH + <span class="string">'attention/gensim_word2vec.model'</span>)</span><br><span class="line"><span class="comment"># 获取embedding</span></span><br><span class="line">embedding = create_embedding(vocab, word2vec_model, EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取训练样本</span></span><br><span class="line">train_dataset = create_dataset(TRAIN_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br><span class="line">val_dataset = create_dataset(VAL_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br><span class="line">test_dataset = create_dataset(TEST_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不使用Attention</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302240/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, embedding, is_attention=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = keras.metrics.Accuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line">val_accuracy = keras.metrics.Accuracy(name=<span class="string">'val_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, MODEL_PATH, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = model(x, mask)</span><br><span class="line">        loss = loss_func(y, pred)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># val step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line">    pred = model(x, mask)</span><br><span class="line">    val_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">        val_step(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">max_val_accuracy = -np.inf</span><br><span class="line">num_epoch = <span class="number">0</span>  <span class="comment"># 当前val_accuracy未提升的连续轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(x, y)</span><br><span class="line"></span><br><span class="line">    evaluate(val_dataset)</span><br><span class="line">    cur_val_accuracy = val_accuracy.result()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_val_accuracy &gt; max_val_accuracy:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        max_val_accuracy = cur_val_accuracy</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减</span></span><br><span class="line">    <span class="keyword">if</span> num_epoch == NUM_EPOCH_LR_DECAY:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        optimizer.lr.assign(optimizer.lr * LR_DECAY_RATE)</span><br><span class="line">        print_(<span class="string">'学习率衰减.'</span>)</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Loss为%.4f'</span> % (epoch + <span class="number">1</span>, train_loss.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, train_accuracy.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮验证集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, cur_val_accuracy))</span><br><span class="line">    print_(<span class="string">'目前验证集最佳Accuracy为%.4f'</span> % max_val_accuracy)</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集评估</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, is_attention=<span class="literal">False</span>)</span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, <span class="string">'/home/featurize/work/model/attention_202009302240/'</span>, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line">    evaluate(test_dataset)</span><br><span class="line">    print_(<span class="string">'测试集最佳Accuracy为%.4f'</span> % val_accuracy.result())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">INFO:root:第100轮训练集Loss为0.0615</span><br><span class="line">INFO:root:第100轮训练集Accuracy为0.9839</span><br><span class="line">INFO:root:第100轮验证集Accuracy为0.9170</span><br><span class="line">INFO:root:目前验证集最佳Accuracy为0.9174</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:已重新加载上一次的模型.</span><br><span class="line">INFO:root:测试集最佳Accuracy为0.9147</span><br></pre></td></tr></table></figure>
<h2 id="Attention-LSTM"><a href="#Attention-LSTM" class="headerlink" title="Attention-LSTM"></a>Attention-LSTM</h2><p>将Attention运用到LSTM的文本分类上面, 具体原理为使用最后一个时刻的输出作为Query, 所有时刻输出的向量作为Key和Value, 其中Query与Key计算Score的方式为$s(q,k)=qWk$. 在通过计算得到Attention Value后, 再与最后一个时刻的输出做拼接, 作为最终的预测向量.</p>
<p>当然, 除了使用最后时刻的输出向量作为Query, 也可以独立出一个额外的向量来作为Query, 都是可以的.</p>
<p>这里要注意到一个问题, 即由于不同的文本长度不一, 会使用PAD进行长度补齐, 但在计算Score时, 本质上只想使用有效长度(非PAD)的输出进行计算, 所以需要加入mask向量来帮助实现.</p>
<p>在代码实现上, 只要将上面的<code>is_attention</code>设置为<code>True</code>即可.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Attention</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302241/'</span></span><br><span class="line"></span><br><span class="line">print_(<span class="string">'\n'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'增加Attention'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, embedding, is_attention=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = keras.metrics.Accuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line">val_accuracy = keras.metrics.Accuracy(name=<span class="string">'val_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, MODEL_PATH, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = model(x, mask)</span><br><span class="line">        loss = loss_func(y, pred)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># val step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line">    pred = model(x, mask)</span><br><span class="line">    val_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">        val_step(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">max_val_accuracy = -np.inf</span><br><span class="line">num_epoch = <span class="number">0</span>  <span class="comment"># 当前val_accuracy未提升的连续轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(x, y)</span><br><span class="line"></span><br><span class="line">    evaluate(val_dataset)</span><br><span class="line">    cur_val_accuracy = val_accuracy.result()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_val_accuracy &gt; max_val_accuracy:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        max_val_accuracy = cur_val_accuracy</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减</span></span><br><span class="line">    <span class="keyword">if</span> num_epoch == NUM_EPOCH_LR_DECAY:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        optimizer.lr.assign(optimizer.lr * LR_DECAY_RATE)</span><br><span class="line">        print_(<span class="string">'学习率衰减.'</span>)</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Loss为%.4f'</span> % (epoch + <span class="number">1</span>, train_loss.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, train_accuracy.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮验证集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, cur_val_accuracy))</span><br><span class="line">    print_(<span class="string">'目前验证集最佳Accuracy为%.4f'</span> % max_val_accuracy)</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"><span class="comment"># 测试集评估</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, is_attention=<span class="literal">True</span>)</span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, <span class="string">'/home/featurize/work/model/attention_202009302241/'</span>, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line">    evaluate(test_dataset)</span><br><span class="line">    print_(<span class="string">'测试集最佳Accuracy为%.4f'</span> % val_accuracy.result())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">INFO:root:第100轮训练集Loss为0.0005</span><br><span class="line">INFO:root:第100轮训练集Accuracy为0.9998</span><br><span class="line">INFO:root:第100轮验证集Accuracy为0.9450</span><br><span class="line">INFO:root:目前验证集最佳Accuracy为0.9458</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:已重新加载上一次的模型.</span><br><span class="line">INFO:root:测试集最佳Accuracy为0.9453</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">0.9147</td>
<td style="text-align:center">0.9147</td>
</tr>
<tr>
<td style="text-align:center">Attention+LSTM</td>
<td style="text-align:center">0.9458</td>
<td style="text-align:center">0.9453</td>
</tr>
</tbody>
</table>
</div>
<p>从结果来看, 加了Attention的LSTM在文本分类上, 表现有明显提升.</p>
<p>同时如果观察整个训练过程的话, 可以看到加入Attention后, 整体收敛更快, 也就是说可以用更少的训练轮数, 达到更好的效果.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>自然语言处理</tag>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title>ELMo</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ELMo/</url>
    <content><![CDATA[<p>ELMo的全称是Embedding from Language Models, 非常直接地说明了主旨, 即从语言模型中得到的词的Embedding.</p>
<p>关于语言模型, 在我的这篇<a href="whitemoonlight.top/2020/08/16/自然语言处理/语言模型/">博客</a>中有介绍.</p>
<p>ELMo的关键之处在于, 它是相对较早提出的一种上下文相关(动态)词向量表示算法.</p>
<a id="more"></a>
<h1 id="ELMo之前"><a href="#ELMo之前" class="headerlink" title="ELMo之前"></a>ELMo之前</h1><p>在我们人类的语言当中, 无论中文还是英文, 都几乎不可避免地有一词多意的现象, 其实想想这是比较符合逻辑的, 如果一旦有一件不同的事物就去取一个不同的词来进行标记或者表示, 那词可能就太多了. 我们在用语言进行交流时, 接收到的不是一个个独立的词, 而是由一个个词有顺序组成的一段话, 对于一词多意的问题, 可以根据其不同的语境, 不同的上下文来进行判断.</p>
<p>之前的一些词向量表示方法, 如独热编码进行表示; 或者使用词的共现关系来得到词向量, 如隐语义模型, 本身并不能较好地表示词的含义与关系.</p>
<p>到了word2vec, 在词的表示上有了质的提升. 不过是一种静态的, 上下文无关的表示方法, 即一个词无论放到哪里, 都对应唯一一个向量. 在一些比较困难的任务中, 会影响最终的结果表现.</p>
<p>结合上面所述, 那么有没有某种算法, 可以让用一个词, 在不同的上下文中, 有不同的向量表示呢? 想想比如在使用LSTM的时候, 每次的输入是预训练好的词向量(如word2vec), 同一个词经过中间的运算, 在不同的语境下会得到变化的输出$h$, 而这个$h$是否可以看成是当前输入单词结合了上文的表示呢? 当然是可以的, 所以看起来思路是比较简单的, 下面就来具体介绍ELMo算法的原理.</p>
<h1 id="ELMo原理"><a href="#ELMo原理" class="headerlink" title="ELMo原理"></a>ELMo原理</h1><p>ELMo是一个RNN Based Model, 其模型结构如下图:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>首先, 其输入是由其它算法(可以是word2vec)预训练好的词向量, 原论文中使用的是CNN-BIG-LSTM得到的预训练词向量.</p>
<p>接着ELMo的核心结构, 是双向LSTM, 而LSTM的层数可以设置多层, 如图中设置的两层.</p>
<p>然后是学习任务, 如其名称Embedding from Language Models, 是通过构建语言模型来学习词向量的参数. 对于前向LSTM, 作为一个语言模型, 根据当前输入, 预测下一个词; 对于后向LSTM, 则反过来, 根据当前输入, 预测前一个词.</p>
<p>用数学公式来对模型进行表示如下:</p>
<script type="math/tex; mode=display">
p_{forward}(w_1,w_2,\dots,w_n)=\prod p(w_i|w_1,w_2,\dots,w_{i-1}) \\
p_{backward}(w_1,w_2,\dots,w_n)=\prod p(w_i|w_{i+1},w_{i+2},\dots,w_n)</script><p>对应的损失函数为:</p>
<script type="math/tex; mode=display">
loss=-\sum_i\log p(w_i|w_1,w_2,\dots,w_{i-1};\theta_x,\overrightarrow\theta_{LSTM},\theta_{softmax})+\log p(w_i|w_{i+1},w_{i+2},\dots,w_n;\theta_x,\overleftarrow\theta_{LSTM},\theta_{softmax})</script><p>其中包含两项, 第一项为前向LSTM的损失, 后面一项为后向LSTM的损失, $\theta_x$表示输入向量, $\overrightarrow\theta_{LSTM}$表示前向LSTM参数, $\overleftarrow\theta_{LSTM}$表示后向LSTM参数, $\theta_{softmax}$表示输出层的softmax参数.</p>
<p>好的, 假设现在已经用大量语料将ELMo模型训练好了, 那么…词向量呢, 这是一个语言模型啊. 其实就和word2vec训练的时候类似, word2vec利用预测是否为上下文词的任务, 来学习词向量, 而ELMo利用语言模型的任务, 来获取词向量.</p>
<p>上面有提到过, 可以用LSTM的输出向量$h$来作为词向量, $h$中包含了上下文信息, 是动态的. 不过现在的问题是, 假设Bi-LSTM有$L$层, 对于每个输入的词来说, 每层有两个$h$(前向与后向)向量, 如果再算上输入向量$\theta_x$, 那么总共就有$(1+2\times L)$个向量, 最终的词向量应该取哪个向量, 或者哪些向量呢?</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>一般来说, 多层的LSTM, 每层的输出向量$h$的表示侧重可能有所不同, 如在两层的Bi-LSTM中, 第一层可能更多表示语法特征, 第二层可能更多表示语义特征.</p>
<p>而在最终使用这些向量来做下游任务(如文本分类, 序列标注)的时候, 最简单的做法, 就是去最后一层的前向与后向LSTM的输出的拼接向量, 即将softmax层去掉以后, 接到下游模型上. 而复杂一些的做法, 就是通过具体的任务, 学习这些向量的权重, 公式如下:</p>
<script type="math/tex; mode=display">
ELMo_i^{task}=\gamma^{task}\sum_js_j^{task}h_{i,j}</script><p>其中$\gamma^{task}$为超参数, 表示对加权和向量的缩放, $s_j^{task}$表示第$j$层(拼接)向量$h_{i,j}$的权重系数, 是学习到的, 而输入向量可以用$h_{i,0}$表示.</p>
<p>作者在不同的任务下进行尝试, 发现不同的任务下输入向量, 第一层的输出向量和第二层的输出向量权重是不一样的:</p>
<p><img src="fig_3.png" alt="fig"></p>
<p>以上, 就是ELMo模型的原理, 其实如果理解语言模型以及LSTM的话, 理解ELMo就非常简单了.</p>
<p>但是ELMo却没有火起来, 原因下面说.</p>
<h1 id="ELMo模型之后"><a href="#ELMo模型之后" class="headerlink" title="ELMo模型之后"></a>ELMo模型之后</h1><p>其实原作者之所以给模型取这个名字, 是因为ELMo是&lt;芝麻街&gt;中的一个角色, 大概长这样:</p>
<p><img src="fig_0.jpeg" alt="fig"></p>
<p>ELMo是RNN Based Model, 在Bi-LSTM上获得动态的上下文相关的词向量表示, 但是一些RNN类模型所具有的缺点, ELMo也有, 比如难以有效捕获长距离相关信息. 比如某个代词, 指代的物品在句子中离它比较远, 那么无论是正向的网络还是反向的网络, 都可能难以发掘其相关性.</p>
<p>此外还有一个计算效率上的缺点, 即难以并行化, 因为RNN本身是串行的模型, 在文本长度较长时, 无论是训练还是预测都会比较慢.</p>
<p>而在ELMo模型之后, 由Transformer衍生出了BERT, GPT. 其中的BERT也是&lt;芝麻街&gt;中的角色(这些人是多喜欢&lt;芝麻街&gt;啊喂), 通过自注意力机制来进行词向量的动态表示, 克服了ELMo的缺点, 把自然语言处理各项任务刷榜了一遍, 后面的博客中会对BERT进行介绍.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>word2vec(二)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/word2vec-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 讲解了word2vec的算法原理, 这一篇中进行代码实践.</p>
<a id="more"></a>
<h1 id="使用Gensim中的word2vec"><a href="#使用Gensim中的word2vec" class="headerlink" title="使用Gensim中的word2vec"></a>使用Gensim中的word2vec</h1><p>Gensim是一个很好用的Python NLP的包, 不光可以用于使用word2vec, 还有很多其它的API可以用. 它封装了Google的C语言版的word2vec. 这里将使用gensim中的word2vec算法来对语料库中的文本进行训练, 然后查看训练结果是否合理.</p>
<p>文本语料为THUCNews的一个子集, 使用了其中的 10 个分类, 每个分类 6500 条数据.</p>
<p>类别包含: 体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br><span class="line"></span><br><span class="line">data_list = train_data_list + val_data_list + test_data_list</span><br><span class="line">label_list = train_label_list + val_label_list + test_label_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_sent</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [x <span class="keyword">for</span> x <span class="keyword">in</span> jieba.cut(sent) <span class="keyword">if</span> x != <span class="string">' '</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">data_list = ordered_multiprocess_task(cut_sent, data_list, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型, 这里语料不大直接进行计算, 若语料大则存入磁盘</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"></span><br><span class="line">model = word2vec.Word2Vec(sentences=data_list,</span><br><span class="line">                          size=<span class="number">64</span>,  <span class="comment"># 向量维度</span></span><br><span class="line">                          alpha=<span class="number">0.025</span>,  <span class="comment"># 学习率</span></span><br><span class="line">                          window=<span class="number">5</span>,  <span class="comment"># 窗口大小</span></span><br><span class="line">                          min_count=<span class="number">5</span>,</span><br><span class="line">                          sample=<span class="number">0.001</span>,</span><br><span class="line">                          seed=<span class="number">7</span>,</span><br><span class="line">                          workers=<span class="number">12</span>,</span><br><span class="line">                          min_alpha=<span class="number">0.0001</span>,</span><br><span class="line">                          sg=<span class="number">1</span>,  <span class="comment"># 使用skip-gram</span></span><br><span class="line">                          hs=<span class="number">0</span>,  <span class="comment"># 使用neg-sample</span></span><br><span class="line">                          negative=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查找与某个词最相似的一些词</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'与"教育"相似的词汇:'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> model.wv.similar_by_word(<span class="string">'教育'</span>, topn=<span class="number">5</span>):</span><br><span class="line">    print(i[<span class="number">0</span>], i[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'与"时尚"相似的词汇:'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> model.wv.similar_by_word(<span class="string">'时尚'</span>, topn=<span class="number">5</span>):</span><br><span class="line">    print(i[<span class="number">0</span>], i[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">与&quot;教育&quot;相似的词汇:</span><br><span class="line">基础教育 0.8181341886520386</span><br><span class="line">高等教育 0.8020640015602112</span><br><span class="line">民办 0.7886888384819031</span><br><span class="line">优秀教师 0.7746903300285339</span><br><span class="line">早教 0.7704285383224487</span><br><span class="line">与&quot;时尚&quot;相似的词汇:</span><br><span class="line">潮流 0.8704825639724731</span><br><span class="line">前卫 0.8683744668960571</span><br><span class="line">时髦 0.8645831942558289</span><br><span class="line">型格 0.8389935493469238</span><br><span class="line">潮品 0.8350383043289185</span><br></pre></td></tr></table></figure>
<p>可以看到效果还不错.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定两个词汇, 查看相似度</span></span><br><span class="line">print(<span class="string">'"足球"与"篮球"'</span>, model.wv.similarity(<span class="string">'足球'</span>, <span class="string">'篮球'</span>))</span><br><span class="line">print(<span class="string">'"游戏"与"教育"'</span>, model.wv.similarity(<span class="string">'游戏'</span>, <span class="string">'教育'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;足球&quot;与&quot;篮球&quot; 0.75202346</span><br><span class="line">&quot;游戏&quot;与&quot;教育&quot; 0.3311203</span><br></pre></td></tr></table></figure>
<p>“足球”与”篮球”相似度较高, “游戏”和”教育”比较低.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定一组词汇, 找出最不同的一个</span></span><br><span class="line"></span><br><span class="line">print(model.wv.doesnt_match(<span class="string">"足球 篮球 乒乓球 睡觉"</span>.split()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">睡觉</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取词表, 以及某个词汇的词向量</span></span><br><span class="line">vocab = model.wv.index2word</span><br><span class="line"></span><br><span class="line">print(<span class="string">'词表大小为%d'</span> % len(vocab))</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> vocab[:<span class="number">5</span>]:</span><br><span class="line">    print(w, model[w])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">词表大小为134564</span><br><span class="line">， [ 0.23444645  0.5052808  -0.07822631  0.10281481  0.21732952  0.3709403</span><br><span class="line">  0.10970413 -0.27258867  0.18322818  0.06661741 -0.18427503  0.20149314</span><br><span class="line">  0.31840575  0.7811422  -0.20637119  0.30055183 -0.09365363  0.28365028</span><br><span class="line">  0.20259145 -0.59974474  0.42180362  0.02262575  0.00870927 -0.13399205</span><br><span class="line">  0.14054394  0.30556074 -0.3716469   0.37428033  0.03954653  0.28264976</span><br><span class="line"> -0.14847441  0.40423253 -0.28393555  0.30066386 -0.19469933  0.07887203</span><br><span class="line"> -0.3646119   0.1710593   0.31207532 -0.044759   -0.11138998 -0.02834223</span><br><span class="line">  0.4666152  -0.19532782 -0.0569643   0.11185751 -0.10793662 -0.35276827</span><br><span class="line">  0.20936984  0.40876475  0.28913507 -0.224972    0.47192773 -0.08726393</span><br><span class="line">  0.04866705 -0.08673578 -0.04979549 -0.00210864  0.15259339  0.23104192</span><br><span class="line"> -0.7081117   0.6581826   0.12544039  0.23255418]</span><br><span class="line">的 [ 0.2148564   0.47956967 -0.09659259  0.10351451  0.16076231  0.25663987</span><br><span class="line">  0.10531557 -0.20816898  0.282943    0.11180176 -0.19441472 -0.01042593</span><br><span class="line">  0.40112293  0.790046    0.1220081   0.03140717  0.09531602  0.24364254</span><br><span class="line">  0.05576983 -0.7218703   0.26664776 -0.07180292  0.00653941 -0.20535158</span><br><span class="line">  0.26897216  0.4055562  -0.27689868  0.3149083  -0.00597505  0.19382124</span><br><span class="line"> -0.18271336  0.40572238 -0.32046267  0.12450676 -0.315657   -0.2470695</span><br><span class="line"> -0.4155292   0.12410802  0.3011421   0.06998867  0.02326411  0.19659767</span><br><span class="line">  0.48717093 -0.19827    -0.04410147  0.12353233  0.1385239  -0.36343998</span><br><span class="line">  0.28934067  0.21699268  0.21682237 -0.31508043  0.4586624   0.00911596</span><br><span class="line">  0.13315779  0.02748186 -0.12713818  0.2303236   0.14492843  0.17951562</span><br><span class="line"> -0.6894058   0.48807827  0.10473885  0.2493376 ]</span><br></pre></td></tr></table></figure>
<h1 id="Tensorflow实现word2vec"><a href="#Tensorflow实现word2vec" class="headerlink" title="Tensorflow实现word2vec"></a>Tensorflow实现word2vec</h1><p>这里使用TensorFlow来实现word2vec的Skip-Gram, 负采样方法. 主要思路为, 先使滑动窗口, 对原始语料进行配对采样, 得到多条样本. 在每一条样本中, 包含两个单词的编号(中心词, 非中心词), 以及是否相关的0-1标签.</p>
<p>在准备好数据以后, 就使用逻辑回归进行计算, 并反向传播优化参数, 具体代码如下.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预处理数据, 准备训练样本.</span></span><br><span class="line"><span class="string">假设初始数据为一段段的文本, 用\n分割.</span></span><br><span class="line"><span class="string">在经过分词, 按出现频次去除部分词后, 得到词表.</span></span><br><span class="line"><span class="string">结合词表, 得到两个同维度矩阵, 作为Embedding.</span></span><br><span class="line"><span class="string">扫描文本, 采样出一条条的样本, 并乱序.</span></span><br><span class="line"><span class="string">返回dataset用于训练.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_vocab</span><span class="params">(data_list, min_num, save_path)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取词表.</span></span><br><span class="line"><span class="string">    @param data_list: 分词后的文本列表, [w_0, w_1, ...]</span></span><br><span class="line"><span class="string">    @param min_num: 最小出现频次, 少于该频次, 会被当成UNK.</span></span><br><span class="line"><span class="string">    @return: 词表, &#123;word: idx&#125;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tmp_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_list:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> tmp_dict:</span><br><span class="line">                tmp_dict[i] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tmp_dict[i] = <span class="number">1</span></span><br><span class="line">    vocab = &#123;<span class="string">'UNK'</span>: [<span class="number">0</span>, <span class="number">0</span>]&#125;</span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> tmp_dict.items():</span><br><span class="line">        <span class="keyword">if</span> v &lt;= min_num:</span><br><span class="line">            vocab[<span class="string">'UNK'</span>][<span class="number">1</span>] += v</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">                vocab[k] = [n, v]</span><br><span class="line">                n += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                vocab[k][<span class="number">1</span>] += v</span><br><span class="line">    <span class="keyword">with</span> open(save_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items():</span><br><span class="line">            f.write(k + <span class="string">' '</span> + str(v[<span class="number">0</span>]) + <span class="string">' '</span> + str(v[<span class="number">1</span>]) + <span class="string">'\n'</span>)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(data_list, vocab, window_size, neg_sample, save_path)</span>:</span></span><br><span class="line">    table = np.zeros(int(<span class="number">1e6</span>))</span><br><span class="line">    power = <span class="number">0.75</span></span><br><span class="line">    norm = sum([np.power(vocab[w][<span class="number">1</span>], power) <span class="keyword">for</span> w <span class="keyword">in</span> vocab])</span><br><span class="line">    cum_p = <span class="number">0</span></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    p_list = []</span><br><span class="line">    idx_list = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        p_list.append(float(np.power(vocab[word][<span class="number">1</span>], power)) / norm)</span><br><span class="line">        idx_list.append(vocab[word][<span class="number">0</span>])</span><br><span class="line">    p_list = [x <span class="keyword">if</span> x &lt; <span class="number">0.001</span> <span class="keyword">else</span> <span class="number">0.001</span> <span class="keyword">for</span> x <span class="keyword">in</span> p_list]</span><br><span class="line">    norm = sum(p_list)</span><br><span class="line">    p_list = [x / norm <span class="keyword">for</span> x <span class="keyword">in</span> p_list]</span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> enumerate(p_list):</span><br><span class="line">        cum_p += p</span><br><span class="line">        <span class="keyword">while</span> idx &lt; <span class="number">1e6</span> <span class="keyword">and</span> idx / <span class="number">1e6</span> &lt; cum_p:</span><br><span class="line">            table[idx] = idx_list[i]</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    print(Counter(table))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sample</span><span class="params">()</span>:</span></span><br><span class="line">        idx = np.random.randint(low=<span class="number">0</span>, high=<span class="number">1e6</span>, size=neg_sample)</span><br><span class="line">        <span class="keyword">return</span> [table[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(save_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(<span class="string">'x_0,x_1,target\n'</span>)</span><br><span class="line">        <span class="keyword">for</span> word_list <span class="keyword">in</span> data_list:</span><br><span class="line">            data_idx = [vocab[x][<span class="number">0</span>] <span class="keyword">if</span> x <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">'UNK'</span>][<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> word_list]</span><br><span class="line">            <span class="keyword">for</span> pos_idx, cent_word_idx <span class="keyword">in</span> enumerate(data_idx):</span><br><span class="line">                <span class="comment"># 确定窗口</span></span><br><span class="line">                context_start = max(<span class="number">0</span>, pos_idx - window_size)</span><br><span class="line">                context_end = min(len(data_idx), pos_idx + window_size)</span><br><span class="line">                context = data_idx[context_start: pos_idx] + data_idx[pos_idx + <span class="number">1</span>: context_end + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> [[cent_word_idx, context_word_idx, <span class="number">1</span>] <span class="keyword">for</span> context_word_idx <span class="keyword">in</span> context] + \</span><br><span class="line">                         [[cent_word_idx, x, <span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> set(_sample()) <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> context <span class="keyword">and</span> x != cent_word_idx]:</span><br><span class="line">                    f.write(<span class="string">','</span>.join([str(int(x)) <span class="keyword">for</span> x <span class="keyword">in</span> i]) + <span class="string">'\n'</span>)</span><br><span class="line">        f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使用TensorFlow实现Skip-Gram NegativeSampling 的word2vec模型.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2vecModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, word_dim)</span>:</span></span><br><span class="line">        super(Word2vecModel, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.word_dim = word_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_layer_0 = keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.word_dim,</span><br><span class="line">                                                        input_length=<span class="number">1</span>,</span><br><span class="line">                                                        embeddings_regularizer=keras.regularizers.l2(<span class="number">0.0001</span>))</span><br><span class="line">        self.embedding_layer_1 = keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.word_dim,</span><br><span class="line">                                                        input_length=<span class="number">1</span>,</span><br><span class="line">                                                        embeddings_regularizer=keras.regularizers.l2(<span class="number">0.0001</span>))</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x_0 = tf.reshape(inputs[<span class="string">'x_0'</span>], (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">        x_1 = tf.reshape(inputs[<span class="string">'x_1'</span>], (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">        x_0 = self.embedding_layer_0(x_0)</span><br><span class="line">        x_1 = self.embedding_layer_1(x_1)</span><br><span class="line">        x_0 = tf.reshape(x_0, (<span class="number">-1</span>, self.word_dim))</span><br><span class="line">        x_1 = tf.reshape(x_1, (<span class="number">-1</span>, self.word_dim))</span><br><span class="line">        outputs = tf.reduce_sum(x_0 * x_1, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># logit = 1. / (1. + tf.exp(-x))</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    loss = keras.losses.binary_crossentropy(y_true, y_pred, from_logits=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">评估</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2VecModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding)</span>:</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.embedding = embedding</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.vocab:</span><br><span class="line">            print(<span class="string">'单词不在词表中.'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx = self.vocab[item][<span class="number">0</span>]</span><br><span class="line">            vec = self.embedding[idx]</span><br><span class="line">            <span class="keyword">return</span> vec</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_similarity</span><span class="params">(w_0, w_1)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(w_0, w_1) / (np.sqrt(np.sum(w_0 ** <span class="number">2</span>)) * np.sqrt(np.sum(w_1 ** <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">top_similarity</span><span class="params">(self, word, n=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.vocab:</span><br><span class="line">            print(<span class="string">'单词不在词表中.'</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        similarity_list = []</span><br><span class="line">        idx = self.vocab[word][<span class="number">0</span>]</span><br><span class="line">        vec = self.embedding[idx]</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.vocab:</span><br><span class="line">            <span class="keyword">if</span> w == word:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> w == <span class="string">'PAD'</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            tmp_vec = self.embedding[self.vocab[w][<span class="number">0</span>]]</span><br><span class="line">            similarity = self._similarity(vec, tmp_vec)</span><br><span class="line">            similarity_list.append((w, similarity))</span><br><span class="line">        similarity_list = list(sorted(similarity_list, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">        <span class="keyword">return</span> similarity_list[: n]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">配置文件.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前缀路径</span></span><br><span class="line">PREFIX_PATH = <span class="string">'/home/shy/work_space/learn/learn_nlp/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志保存路径</span></span><br><span class="line">LOG_PATH = PREFIX_PATH + <span class="string">'log/word2vec/word2vec.log'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集原始数据</span></span><br><span class="line"><span class="comment"># TRAIN_DATA_ORIGIN_PATH = PREFIX_PATH + 'data/word2vec/train.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程分词数量</span></span><br><span class="line"><span class="comment"># TOKEN_MULTI_WORKER = 12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小频次</span></span><br><span class="line">MIN_NUM = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表保存路径</span></span><br><span class="line">VOCAB_PATH = PREFIX_PATH + <span class="string">'data/word2vec/vocab.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 窗口大小</span></span><br><span class="line">WINDOW_SIZE = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 负采样数量</span></span><br><span class="line">NUM_NEG_SAMPLE = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练样本CSV保存路径</span></span><br><span class="line">CSV_SAVE_PATH = PREFIX_PATH + <span class="string">'data/word2vec/train.csv'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量维度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = PREFIX_PATH + <span class="string">'model/word2vec/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">EPOCH = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率衰减</span></span><br><span class="line">LR_DECAY_RATE = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多少轮数未降低Loss时学习率衰减</span></span><br><span class="line">NUM_EPOCH_LR_DECAY = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在修改好config后, 运行脚本:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">is_log = <span class="literal">False</span>  <span class="comment"># 是否保存日志, 否则控制台输出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_log:</span><br><span class="line">    logging.basicConfig(filename=LOG_PATH, level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_</span><span class="params">(msg)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_log:</span><br><span class="line">        logging.info(msg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(msg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词后的结果, 这里用做简单测试, 假设1-12形成一个圆环, 截取其中三段</span></span><br><span class="line"></span><br><span class="line">token_list = [[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>, <span class="string">'7'</span>, <span class="string">'8'</span>, <span class="string">'9'</span>, <span class="string">'10'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表</span></span><br><span class="line">vocab = create_vocab(token_list, MIN_NUM, VOCAB_PATH)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;UNK&apos;: [0, 0],</span><br><span class="line"> &apos;1&apos;: [1, 1],</span><br><span class="line"> &apos;2&apos;: [2, 1],</span><br><span class="line"> &apos;3&apos;: [3, 1],</span><br><span class="line"> &apos;4&apos;: [4, 1],</span><br><span class="line"> &apos;5&apos;: [5, 1],</span><br><span class="line"> &apos;6&apos;: [6, 1],</span><br><span class="line"> &apos;7&apos;: [7, 1],</span><br><span class="line"> &apos;8&apos;: [8, 1],</span><br><span class="line"> &apos;9&apos;: [9, 1],</span><br><span class="line"> &apos;10&apos;: [10, 1]&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取训练样本</span></span><br><span class="line">create_dataset(token_list, vocab, WINDOW_SIZE, NUM_NEG_SAMPLE, CSV_SAVE_PATH)</span><br><span class="line">dataset = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=CSV_SAVE_PATH,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    label_name=<span class="string">"target"</span>,</span><br><span class="line">    num_epochs=<span class="number">100</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">10240</span>, seed=<span class="number">7</span>) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Word2vecModel(len(vocab), EMBEDDING_DIM)</span><br><span class="line">model.build(input_shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, MODEL_PATH, max_to_keep=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = loss_func(y, pred)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line"></span><br><span class="line">print_(<span class="string">'开始训练.'</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">min_loss = np.inf  <span class="comment"># 最低Loss</span></span><br><span class="line">num_epoch = <span class="number">0</span>  <span class="comment"># 当前Loss未降低的连续轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> enumerate(dataset):</span><br><span class="line">        train_step(x, y)</span><br><span class="line">    cur_loss = train_loss.result()</span><br><span class="line">    <span class="keyword">if</span> cur_loss &lt; min_loss:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        min_loss = cur_loss</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减</span></span><br><span class="line">    <span class="keyword">if</span> num_epoch == NUM_EPOCH_LR_DECAY:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        optimizer.lr.assign(optimizer.lr * LR_DECAY_RATE)</span><br><span class="line">        print_(<span class="string">'学习率衰减.'</span>)</span><br><span class="line"></span><br><span class="line">    print_(<span class="string">'第%d轮Loss为%.4f'</span> % (epoch + <span class="number">1</span>, cur_loss))</span><br><span class="line">    print_(<span class="string">'目前最佳Loss为%.4f'</span> % min_loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查输出是否合理</span></span><br><span class="line">    embedding = model.embedding_layer_0.weights[<span class="number">0</span>].numpy()</span><br><span class="line">    w2v_model = Word2VecModel(vocab, embedding)</span><br><span class="line">    print_(<span class="string">'与5相似度大的:'</span>)</span><br><span class="line">    print_(w2v_model.top_similarity(<span class="string">'5'</span>, <span class="number">4</span>))</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">开始训练.</span><br><span class="line">保存模型/home/shy/work_space/learn/learn_nlp/model/word2vec/ckpt-1</span><br><span class="line">第1轮Loss为0.5908</span><br><span class="line">目前最佳Loss为0.5908</span><br><span class="line">与5相似度大的:</span><br><span class="line">[(&apos;4&apos;, 0.6618211), (&apos;6&apos;, 0.6145809), (&apos;3&apos;, 0.41654137), (&apos;7&apos;, 0.21102454)]</span><br><span class="line">##########################################</span><br><span class="line">保存模型/home/shy/work_space/learn/learn_nlp/model/word2vec/ckpt-2</span><br><span class="line">第2轮Loss为0.2660</span><br><span class="line">目前最佳Loss为0.2660</span><br><span class="line">与5相似度大的:</span><br><span class="line">[(&apos;4&apos;, 0.7226397), (&apos;6&apos;, 0.6807906), (&apos;3&apos;, 0.35464686), (&apos;7&apos;, 0.3202437)]</span><br><span class="line">##########################################</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">保存模型/home/shy/work_space/learn/learn_nlp/model/word2vec/ckpt-14</span><br><span class="line">第14轮Loss为0.0011</span><br><span class="line">目前最佳Loss为0.0011</span><br><span class="line">与5相似度大的:</span><br><span class="line">[(&apos;6&apos;, 0.6644588), (&apos;4&apos;, 0.509141), (&apos;7&apos;, 0.35708117), (&apos;3&apos;, 0.24483325)]</span><br><span class="line">##########################################</span><br><span class="line">保存模型/home/shy/work_space/learn/learn_nlp/model/word2vec/ckpt-15</span><br><span class="line">第15轮Loss为0.0008</span><br><span class="line">目前最佳Loss为0.0008</span><br><span class="line">与5相似度大的:</span><br><span class="line">[(&apos;6&apos;, 0.66159034), (&apos;4&apos;, 0.505937), (&apos;7&apos;, 0.35591137), (&apos;3&apos;, 0.24281922)]</span><br><span class="line">##########################################</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到, 在最后通过余弦相似度, 与5最相近的是6, 4, 7, 3, 符合预期.</p>
<p>这里通过对word2vec原理的理解, 使用TensorFlow实现了Skip-Gram, 负采样方法, 并在小例子上表明了有效性, 但是由于实现粗糙, 因此在效率和精度上, 都肯定比不上gensim中的word2vec方法.</p>
<h1 id="使用word2vec进行文本分类"><a href="#使用word2vec进行文本分类" class="headerlink" title="使用word2vec进行文本分类"></a>使用word2vec进行文本分类</h1><h2 id="word2vec-关键词"><a href="#word2vec-关键词" class="headerlink" title="word2vec+关键词"></a>word2vec+关键词</h2><p>这里再尝试直接使用word2vec进行文本分类, 用词汇向量的平均来表示文本.</p>
<p>但是这么做有如下两个问题:</p>
<ul>
<li>如果文本较长, 那么许多词进行平均后, 信息就会被削弱.</li>
<li>文本中存在不少的停用词本身不能提供有效信息.</li>
</ul>
<p>于是, 这里先使用jieba提取关键词, 然后再使用使用关键词对应的word2vec向量进行加权平均, 权重为TF-IDF.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取关键词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">key_word</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jieba.analyse.extract_tags(sent, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">'n'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">'nz'</span>, <span class="string">'vn'</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_key_word_list = ordered_multiprocess_task(key_word, train_data_list, <span class="number">24</span>)</span><br><span class="line">val_key_word_list = ordered_multiprocess_task(key_word, val_data_list, <span class="number">24</span>)</span><br><span class="line">test_key_word_list = ordered_multiprocess_task(key_word, test_data_list, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备入模数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    res = np.zeros(<span class="number">64</span>)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> model:</span><br><span class="line">            n += v</span><br><span class="line">            res += model[k] * v</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> np.zeros(<span class="number">64</span>)</span><br><span class="line">    <span class="keyword">return</span> res / n</span><br><span class="line"></span><br><span class="line">train_data = ordered_multiprocess_task(create_data, train_key_word_list, <span class="number">24</span>)</span><br><span class="line">val_data = ordered_multiprocess_task(create_data, val_key_word_list, <span class="number">24</span>)</span><br><span class="line">test_data = ordered_multiprocess_task(create_data, test_key_word_list, <span class="number">24</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line">train_data = np.array(train_data)</span><br><span class="line">val_data = np.array(val_data)</span><br><span class="line">test_data = np.array(test_data)</span><br><span class="line"></span><br><span class="line">train_data = (train_data - train_data.mean(axis=<span class="number">0</span>)) / train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">val_data = (val_data - val_data.mean(axis=<span class="number">0</span>)) / val_data.std(axis=<span class="number">0</span>)</span><br><span class="line">test_data = (test_data - test_data.mean(axis=<span class="number">0</span>)) / test_data.std(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分类标签编码0-9</span></span><br><span class="line">encode_label_dict = dict(zip([<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>], range(<span class="number">10</span>)))</span><br><span class="line">decode_label_dict = dict(zip(range(<span class="number">10</span>), [<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>]))</span><br><span class="line">train_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> train_label_list]</span><br><span class="line">val_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> val_label_list]</span><br><span class="line">test_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> test_label_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用神经网络进行训练</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks, optimizers</span><br><span class="line"></span><br><span class="line">tf.config.threading.set_intra_op_parallelism_threads(<span class="number">24</span>)</span><br><span class="line">tf.config.threading.set_inter_op_parallelism_threads(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">128</span>,</span><br><span class="line">                 input_shape=(<span class="number">64</span>,),</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line"><span class="comment">#                  kernel_regularizer=regularizers.l1(0.01)</span></span><br><span class="line">                )</span><br><span class="line">)</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">64</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line"><span class="comment">#                  kernel_regularizer=regularizers.l2(0.01)</span></span><br><span class="line">                )</span><br><span class="line">)</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=np.array(train_data),</span><br><span class="line">                    y=np.array(train_label_encode),</span><br><span class="line">                    batch_size=<span class="number">256</span>,</span><br><span class="line">                    validation_data=(np.array(val_data), np.array(val_label_encode)),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_accuracy'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.4265 - accuracy: 0.8843 - val_loss: 0.2388 - val_accuracy: 0.9338</span><br><span class="line">Epoch 2/1000</span><br><span class="line">196/196 [==============================] - 1s 3ms/step - loss: 0.1995 - accuracy: 0.9382 - val_loss: 0.2186 - val_accuracy: 0.9342</span><br><span class="line">Epoch 3/1000</span><br><span class="line">196/196 [==============================] - 1s 3ms/step - loss: 0.1795 - accuracy: 0.9435 - val_loss: 0.2077 - val_accuracy: 0.9372</span><br><span class="line">Epoch 4/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.1669 - accuracy: 0.9479 - val_loss: 0.2088 - val_accuracy: 0.9360</span><br><span class="line">Epoch 5/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.1577 - accuracy: 0.9504 - val_loss: 0.2020 - val_accuracy: 0.9364</span><br><span class="line">Epoch 6/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.9527 - val_loss: 0.1858 - val_accuracy: 0.9440</span><br><span class="line">Epoch 7/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.9550 - val_loss: 0.2088 - val_accuracy: 0.9352</span><br><span class="line">Epoch 8/1000</span><br><span class="line">196/196 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.9557 - val_loss: 0.1820 - val_accuracy: 0.9450</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">print(<span class="string">'训练集结果: %f'</span> % accuracy_score(train_label_encode, model.predict_classes(train_data)))</span><br><span class="line">print(<span class="string">'测试集结果: %f'</span> % accuracy_score(val_label_encode, model.predict_classes(val_data)))</span><br><span class="line">print(<span class="string">'测试集结果: %f'</span> % accuracy_score(test_label_encode, model.predict_classes(test_data)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">训练集结果: 0.959400</span><br><span class="line">测试集结果: 0.945000</span><br><span class="line">测试集结果: 0.941400</span><br></pre></td></tr></table></figure>
<h2 id="word2vec-LSTM"><a href="#word2vec-LSTM" class="headerlink" title="word2vec+LSTM"></a>word2vec+LSTM</h2><p>除了使用word2vec+关键词来进行文本分类, 同样也可以使用word2vec预训练的词向量, 来增强LSTM的模型表现, 下面将对比不使用与使用预训练词向量的区别.</p>
<p>这里已经做好词表, 已经对原始数据进行编码.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">LSTM文本分类模型.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, word_dim, embedding=None)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.word_dim = word_dim</span><br><span class="line">        self.embedding = embedding</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding_layer = keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.word_dim,</span><br><span class="line">                                                      mask_zero=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.embedding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding_layer.build((self.vocab_size,))</span><br><span class="line">            self.embedding_layer.set_weights([self.embedding])</span><br><span class="line">        self.lstm_layer = keras.layers.LSTM(<span class="number">64</span>)</span><br><span class="line">        self.dense_layer = keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'tanh'</span>)</span><br><span class="line">        self.output_layer = keras.layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>)</span><br><span class="line">        self.softmax_layer = keras.layers.Softmax()</span><br><span class="line"></span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.embedding_layer(inputs)</span><br><span class="line">        x = self.lstm_layer(x)</span><br><span class="line">        x = self.dense_layer(x)</span><br><span class="line">        x = self.output_layer(x)</span><br><span class="line">        outputs = self.softmax_layer(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预处理数据, 准备训练样本.</span></span><br><span class="line"><span class="string">在经过分词, 按出现频次去除部分词后, 得到词表.</span></span><br><span class="line"><span class="string">使用word2vec进行预训练, 得到预训练向量.</span></span><br><span class="line"><span class="string">结合词表, 预训练向量, 得到Embedding矩阵.</span></span><br><span class="line"><span class="string">返回dataset用于训练.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_embedding</span><span class="params">(vocab, model, embedding_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    结合词表和训练的词向量, 来返回Embedding.</span></span><br><span class="line"><span class="string">    @param vocab: 词表字典&#123;word: idx&#125;.</span></span><br><span class="line"><span class="string">    @param model: word2vec模型.</span></span><br><span class="line"><span class="string">    @param embedding_dim: 向量维度.</span></span><br><span class="line"><span class="string">    @return: Embedding矩阵.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocab_size = len(vocab)</span><br><span class="line">    embedding = np.zeros((vocab_size, embedding_dim))</span><br><span class="line">    word2vec_vocab = model.wv.index2word</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> vocab:</span><br><span class="line">        idx = vocab[w]</span><br><span class="line">        <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> word2vec_vocab:</span><br><span class="line">            embedding[idx] = model[w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding[idx] = np.random.uniform(low=<span class="number">-1</span> / embedding_dim, high=<span class="number">1</span> / embedding_dim, size=embedding_dim)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(data_path, batch_size, max_length)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">        res = tf.strings.split(line, <span class="string">'\t'</span>)</span><br><span class="line">        text, label = res[<span class="number">0</span>], res[<span class="number">1</span>]</span><br><span class="line">        text = tf.strings.split(text)</span><br><span class="line">        label = tf.strings.split(label)</span><br><span class="line">        text = tf.cast(tf.strings.to_number(text), tf.int32)</span><br><span class="line">        text = text[: max_length]</span><br><span class="line">        label = tf.cast(tf.strings.to_number(label), tf.int32)</span><br><span class="line">        <span class="comment"># label = tf.expand_dims(tf.cast(tf.strings.to_number(label), tf.int32), axis=0)</span></span><br><span class="line">        <span class="comment"># text = tf.expand_dims(text, axis=0)</span></span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    dataset = tf.data.TextLineDataset(filenames=data_path) \</span><br><span class="line">        .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">        .shuffle(buffer_size=<span class="number">10240</span>, seed=<span class="number">7</span>) \</span><br><span class="line">        .padded_batch(batch_size, padded_shapes=([<span class="number">-1</span>], [<span class="number">-1</span>])) \</span><br><span class="line">        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">配置文件.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前缀路径</span></span><br><span class="line">PREFIX_PATH = <span class="string">'/home/featurize/data/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志保存路径</span></span><br><span class="line">LOG_PATH = <span class="string">'/home/featurize/work/log/attention/attention_20201001.log'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表保存路径</span></span><br><span class="line">VOCAB_PATH = PREFIX_PATH + <span class="string">'attention/vocab.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理后数据保存路径</span></span><br><span class="line">TRAIN_DATA_PATH = PREFIX_PATH + <span class="string">'attention/train.txt'</span></span><br><span class="line">VAL_DATA_PATH = PREFIX_PATH + <span class="string">'attention/val.txt'</span></span><br><span class="line">TEST_DATA_PATH = PREFIX_PATH + <span class="string">'attention/test.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量维度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302240/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">EPOCH = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大长度</span></span><br><span class="line">MAX_LENGTH = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率衰减</span></span><br><span class="line">LR_DECAY_RATE = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多少轮数未降低Loss时学习率衰减</span></span><br><span class="line">NUM_EPOCH_LR_DECAY = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在修改好config后, 运行脚本</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_random</span><span class="params">(seed=<span class="number">7</span>)</span>:</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="string">'1'</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.random.set_seed(seed)</span><br><span class="line">    os.environ[<span class="string">'TF_DETERMINISTIC_OPS'</span>] = <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_</span><span class="params">(msg)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_log:</span><br><span class="line">        logging.info(msg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(msg)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 得到词表</span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(VOCAB_PATH, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            word, idx = line.strip().split()</span><br><span class="line">            vocab[word] = int(idx)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(line)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="comment"># 加载训练好的word2vec模型</span></span><br><span class="line">word2vec_model = word2vec.Word2Vec.load(PREFIX_PATH + <span class="string">'attention/gensim_word2vec.model'</span>)</span><br><span class="line"><span class="comment"># 获取embedding</span></span><br><span class="line">embedding = create_embedding(vocab, word2vec_model, EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取训练样本</span></span><br><span class="line">train_dataset = create_dataset(TRAIN_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br><span class="line">val_dataset = create_dataset(VAL_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br><span class="line">test_dataset = create_dataset(TEST_DATA_PATH, BATCH_SIZE, MAX_LENGTH)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">is_log = <span class="literal">True</span>  <span class="comment"># 是否保存日志, 否则控制台输出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_log:</span><br><span class="line">    logging.basicConfig(filename=LOG_PATH, level=logging.DEBUG)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302242/'</span></span><br><span class="line"></span><br><span class="line">print_(<span class="string">'\n'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'原始LSTM'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = keras.metrics.Accuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line">val_accuracy = keras.metrics.Accuracy(name=<span class="string">'val_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, MODEL_PATH, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = model(x, mask)</span><br><span class="line">        loss = loss_func(y, pred)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># val step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line">    pred = model(x, mask)</span><br><span class="line">    val_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">        val_step(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">max_val_accuracy = -np.inf</span><br><span class="line">num_epoch = <span class="number">0</span>  <span class="comment"># 当前val_accuracy未提升的连续轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(x, y)</span><br><span class="line"></span><br><span class="line">    evaluate(val_dataset)</span><br><span class="line">    cur_val_accuracy = val_accuracy.result()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_val_accuracy &gt; max_val_accuracy:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        max_val_accuracy = cur_val_accuracy</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减</span></span><br><span class="line">    <span class="keyword">if</span> num_epoch == NUM_EPOCH_LR_DECAY:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        optimizer.lr.assign(optimizer.lr * LR_DECAY_RATE)</span><br><span class="line">        print_(<span class="string">'学习率衰减.'</span>)</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Loss为%.4f'</span> % (epoch + <span class="number">1</span>, train_loss.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, train_accuracy.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮验证集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, cur_val_accuracy))</span><br><span class="line">    print_(<span class="string">'目前验证集最佳Accuracy为%.4f'</span> % max_val_accuracy)</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集评估</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, is_attention=<span class="literal">False</span>)</span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, <span class="string">'/home/featurize/work/model/attention_202009302242/'</span>, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line">    evaluate(test_dataset)</span><br><span class="line">    print_(<span class="string">'测试集最佳Accuracy为%.4f'</span> % val_accuracy.result())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">INFO:root:第100轮训练集Loss为0.0001</span><br><span class="line">INFO:root:第100轮训练集Accuracy为1.0000</span><br><span class="line">INFO:root:第100轮验证集Accuracy为0.8428</span><br><span class="line">INFO:root:目前验证集最佳Accuracy为0.8612</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:已重新加载上一次的模型.</span><br><span class="line">INFO:root:测试集最佳Accuracy为0.8711</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 构建模型, 使用预训练向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存路径</span></span><br><span class="line">MODEL_PATH = <span class="string">'/home/featurize/work/model/attention_202009302240/'</span></span><br><span class="line"></span><br><span class="line">print_(<span class="string">'\n'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'word2vec+LSTM'</span> + <span class="string">'#'</span> * <span class="number">20</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, embedding)</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line">train_loss = keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = keras.metrics.Accuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line">val_accuracy = keras.metrics.Accuracy(name=<span class="string">'val_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, MODEL_PATH, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = model(x, mask)</span><br><span class="line">        loss = loss_func(y, pred)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># val step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    mask = create_mask(x)</span><br><span class="line">    pred = model(x, mask)</span><br><span class="line">    val_accuracy(y, tf.argmax(pred, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">        val_step(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">max_val_accuracy = -np.inf</span><br><span class="line">num_epoch = <span class="number">0</span>  <span class="comment"># 当前val_accuracy未提升的连续轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    val_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(x, y)</span><br><span class="line"></span><br><span class="line">    evaluate(val_dataset)</span><br><span class="line">    cur_val_accuracy = val_accuracy.result()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_val_accuracy &gt; max_val_accuracy:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        max_val_accuracy = cur_val_accuracy</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减</span></span><br><span class="line">    <span class="keyword">if</span> num_epoch == NUM_EPOCH_LR_DECAY:</span><br><span class="line">        num_epoch = <span class="number">0</span></span><br><span class="line">        optimizer.lr.assign(optimizer.lr * LR_DECAY_RATE)</span><br><span class="line">        print_(<span class="string">'学习率衰减.'</span>)</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Loss为%.4f'</span> % (epoch + <span class="number">1</span>, train_loss.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮训练集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, train_accuracy.result()))</span><br><span class="line">    print_(<span class="string">'第%d轮验证集Accuracy为%.4f'</span> % (epoch + <span class="number">1</span>, cur_val_accuracy))</span><br><span class="line">    print_(<span class="string">'目前验证集最佳Accuracy为%.4f'</span> % max_val_accuracy)</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集评估</span></span><br><span class="line">reset_random()</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = LSTMModel(len(vocab), EMBEDDING_DIM, is_attention=<span class="literal">False</span>)</span><br><span class="line">optimizer = keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, <span class="string">'/home/featurize/work/model/attention_202009302240/'</span>, max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line">    evaluate(test_dataset)</span><br><span class="line">    print_(<span class="string">'测试集最佳Accuracy为%.4f'</span> % val_accuracy.result())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">INFO:root:第100轮训练集Loss为0.0615</span><br><span class="line">INFO:root:第100轮训练集Accuracy为0.9839</span><br><span class="line">INFO:root:第100轮验证集Accuracy为0.9170</span><br><span class="line">INFO:root:目前验证集最佳Accuracy为0.9174</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:已重新加载上一次的模型.</span><br><span class="line">INFO:root:测试集最佳Accuracy为0.9147</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">word2vec+keyword</td>
<td style="text-align:center">0.9450</td>
<td style="text-align:center">0.9414</td>
</tr>
<tr>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">0.8612</td>
<td style="text-align:center">0.8711</td>
</tr>
<tr>
<td style="text-align:center">word2vec+LSTM</td>
<td style="text-align:center">0.9174</td>
<td style="text-align:center">0.9147</td>
</tr>
</tbody>
</table>
</div>
<p>通过对比是否使用word2vec来初始化Embedding, 发现使用以后的模型表现明显优于随机初始化的模型表现.</p>
<p>但是这里如果用这个结果和上面word2vec+关键词的模型结构对比, 发现word2vec+LSTM效果不如word2vec+关键词. 其原因有两点, 一是基于LSTM做文本分类时, 一般来说使用双向会更好; 二是这里的文章相对偏长, 一般都在两千词以上, 部分达到了四千词, 而这里仅仅只截取了五百词作为模型输入, 损失了部分信息. 通过调整模型结构, 增加序列长度以后, 应该是可以达到word2vec+关键词的效果, 甚至更好.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>自然语言处理</tag>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title>word2vec(一)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/word2vec-%E4%B8%80/</url>
    <content><![CDATA[<p>Word2vec是Google在2013年提出的算法, 可以使用稠密的向量来对单词(或者字符)进行表示. Word2vec算法是一种非常重要的算法, 不仅因为其在自然语言处理中应用广泛, 而且在其它一些领域, 也有深远的影响.</p>
<a id="more"></a>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Word2vec算法就是对词向量进行表示, 那么在word2vec之前, 就有原始的词向量表示方法, 即设定向量的维度为整个词汇表的大小, 对于每个具体的词, 将对应位置设为1, 其余位置为0. 这样的方法也称为one hot representation, 其缺点非常明显, 就是词向量本身很长, 很稀疏, 且词与词之间的关系并不能通过这样的向量进行刻画.</p>
<p>而与one hot representation相对的, 是distributed representation, 使用维度不大的向量进行表示, 向量是稠密的, 每个元素是一个浮点数. 这样可以把单词看成向量空间中的点, 而单词之间的关系, 可以通过这样的向量进行一定的刻画. 比如近义词在空间中离得比较近, 或者余弦相似度比较大. 又或者单词向量之间具有一定的逻辑性,  如:</p>
<script type="math/tex; mode=display">
\overrightarrow {king}-\overrightarrow {man}+\overrightarrow {woman}=\overrightarrow {queen}</script><p>那么, 要怎么样才能得到合适的词向量呢? 在word2vec之前, 已经有用神经网络DNN来训练词向量了, 一般可以采用一个三层的神经网络结构(输入层, 隐藏层, 输出层).</p>
<p>核心的思路, 是<strong>单词的共现</strong>, 即上下文中相邻的单词具有相似性, 单词的向量就应该更加接近, 反之亦然.</p>
<p>而输入和输出又是什么呢, 根据输入输出的不同, 可以分为:</p>
<ul>
<li>输入为上下文, 输出为中心词.</li>
<li>输入为中心词, 输出为上下文.</li>
</ul>
<p>然鹅, 经典的DNN并不能很好地完成学习词向量的任务, word2vec在其基础上, 用了一些方法进行优化和改进. 其中CBOW(Continuous Bag-of-Words)与Skip-Gram两种模型, 就对应上面提到的两种模式, 下面进行讲解.</p>
<h1 id="CBOW与Skip-Gram"><a href="#CBOW与Skip-Gram" class="headerlink" title="CBOW与Skip-Gram"></a>CBOW与Skip-Gram</h1><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><p>CBOW模型的训练输入, 是某一个中心词的上下文(周围的词)对应的词向量; 而输出就是这个中心词的词向量.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 取上下文大小为4, 中心词是”learning”, 上下文对应的词有8个, 前后各4个, 这8个词对应向量是模型的输入.</p>
<p>由于CBOW使用的是词袋模型, 因此这8个词都是平等的, 也就是不考虑谁更近一些或者远一些, 谁在前面还是后面，只要在上下文之内即可.</p>
<p><img src="fig_1.jpg" alt="fig"></p>
<p>CBOW的神经网络结构如上图, 上下文单词的词向量平均后, 得到隐藏层的向量, 然后在输出层使用softmax预测中心词. 但是需要注意的是, 这里的隐藏层, 并没有激活函数, 可以看做是想更加直接地学习词向量之间的关系, 而不是通过非线性变换之后的关系.</p>
<h2 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h2><p>Skip-Gram与CBOW相反, 是用中心词, 来预测上下文对应的周围的词.</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>结构如上图所示, 可以想象的是, 相比CBOW, Skip-Gram学习的难度会更大, 更难以预测准确, 但是最终得到的词向量的表示却不一定比CBOW差. 原因是我们原本的目的就不是让模型预测得多么准确, 而是想通过这种方式, 得到对应的词向量, 即神经网络中的部分权重系数, 在这种情况下, 有时候学得更加困难, 反而效果更好. 我个人认为还有一种解释是, CBOW在学习时, 建立的是周围词的平均向量与中心词向量的关系, 而Skip-Gram这里是直接建立中心词与每个周围词向量的关系.</p>
<p>同时, 细心的同学一定也发现了, 前面CBOW计算一次softmax还好, Skip-Gram这里计算多次softmax会不会复杂度比较高. 是的, 其实计算一次softmax的复杂度就已经很高了, 因为词表一般来说很大, 通常在几十万甚至上百万, 这样的情况下想要在大语料库下进行充分学习, 是非常有难度的.</p>
<p>所以, word2vec使用两种方法, Hierarchical Softmax和Negative Sampling进行了优化.</p>
<h1 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h1><p>在将Hierarchical Softmax之前, 需要介绍一种数据结构, 即哈夫曼树(Huffman Tree).</p>
<p>哈夫曼树是一种树型结构, 输入为权值为$w_1,w_2,\dots ,w_n$的$n$个节点., 输出即为哈夫曼树, 其构建过程如下:</p>
<ul>
<li><p>初始状态.</p>
<p>将$n$个节点, 看成$n$棵树的森林, 每棵树只有一个节点.</p>
</li>
<li><p>合并生成新树.</p>
<p>在森林中, 选择根节点权值最小的两棵树进行合并, 得到一棵新树. 这两个棵树分别作为新树的左右子树, 可以约定左子树根节点权值大于等于右子树根节点权值, 同时新树的根节点权值为左右子树根节点权值之和.</p>
</li>
<li><p>更新森林.</p>
<p>将新树加入森林, 对于的删除掉组成新树的两棵子树.</p>
</li>
<li><p>迭代.</p>
<p>重复以上两个步骤, 直到最后仅剩下一棵树, 即为哈夫曼树.</p>
</li>
</ul>
<p>构建过程还是比较简单的, 那么哈夫曼树有什么好处呢? 比如我们想对一些事物(如单词)进行编码(如0-1编码), 使得常用的单词具有较短的编码, 不常用的单词具有相对较长的编码, 那么就可以使用哈夫曼树来实现.</p>
<p>在构建哈夫曼树前, 对每个单词设定权值, 可以正比于出现频率. 在构建好哈夫曼树后, 其叶子节点就是对应的单词, 并且越靠近根节点的叶子节点, 权值越大, 对应单词出现频率越高. 这时候可以从根节点开始进行编码, 如约定往左为1, 往右为0, 那么从根节点到叶子节点的路径形成的0-1串, 就是对应的编码.</p>
<p>在word2vec中, 就是采用的类似的形式, 在构建好词表对应的哈夫曼树后, 其内部节点相当于神经网络隐藏层, 叶子节点相当于输出层.</p>
<p>从根节点开始, 当想要到达某个叶子节点时, 路径是确定且唯一的, 那么以CBOW的形式来说明具体的算法流程, Skip-Gram类似:</p>
<ul>
<li><p>建立哈夫曼树.</p>
<p>根据给定语料库, 建立哈夫曼树. 其中每个内部节点上, 有一个参数向量$\theta_i$, 与词向量维度相同.</p>
</li>
<li><p>周围词向量.</p>
<p>取中心词的周围词的向量的平均, 作为输入.</p>
</li>
<li><p>确定中心词路径.</p>
<p>确定从根节点到中心词对应叶子节点的路径.</p>
</li>
<li><p>沿路径进行计算.</p>
<p>在路径中, 每个内部节点上, 计算一个逻辑回归函数:</p>
<script type="math/tex; mode=display">
p=\frac{1}{1+\exp(-\theta_i\cdot x_w)}</script><p>其中的$x_w$表示周围词向量的平均, $p$表示向左(为1)的概率. 那么从根节点到叶子节点的概率, 可以表示为沿途所有内部节点上概率的乘积.</p>
</li>
<li><p>更新参数.</p>
<p>在有了上面的一条路径的概率后, 就可以使用MLE进行优化. 每次可以使用随机梯度法, 更新一个样本.</p>
</li>
<li><p>终止算法.</p>
<p>当达到终止条件, 如梯度收敛时, 可结束算法.</p>
</li>
</ul>
<p>相比原本为$O(N)$的时间复杂度, 使用哈夫曼树以后, 时间复杂度变成了$O(\log(N))$, $N$为词表大小, 并且由于哈夫曼树中高频词靠近根节点. 实际的平均时间还会更少一些.</p>
<h1 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h1><p>Negative Sampling, 即负采样, 不仅可以使用在word2vec当中, 而且可以当成是一种通用的方法, 在机器学习当中发挥关键作用.</p>
<p>相比哈夫曼树, 负采样的方法显得更加简单. </p>
<p>这里以CBOW的模式来进行说明, Skip-Gram与其类似. 在一个滑动窗口中, 假设窗口大小为$c$, 则存在一个中心词, 与前后文$2c$个周围词, 那么这个中心词和这$2c$个周围词是相关的, 视作一组正样本. 而在这个滑动窗口之外的词, 都可以视作不相关的负样本, 那么可以用某种方式进行采样, 假设采样了$n$个词, 则与周围词一起形成$n$组负样本.</p>
<p>对于每一个样本而言, 有两个参数, 一个是作为词本身的向量$w_i$, 一个是作为周围词的向量$context(w_i)$, 对应一个词在这里有两种角色: 非周围词, 周围词.</p>
<p>通过一个窗口采样得到的$1+n$组样本(对应$2c\times (1+n)$个样本), 可以看做一个二分类问题, 即相关则为1, 不相关则为0, 概率为:</p>
<script type="math/tex; mode=display">
p=\frac{1}{1+\exp(-w_i\cdot context(w_j))}</script><p>然后使用梯度下降进行更新参数即可.</p>
<p>再详细说一下负采样时的具体做法, 每个词对应的长度是不一样的, 高频词对应的概率大, 低频词对应概率小. 在word2vec中, 每个词的采样概率由下式决定:</p>
<script type="math/tex; mode=display">
length\_w=\frac{count(w)^{3/4}}{\sum_{w\in vocab}count(w)^{3/4}}</script><p>然后在一个特别长的数组上, 如$1\times10^8$的数组上, 按词表的累计概率分布, 将词对应到数组的位置上. 在采样时, 每次使用随机数, 在数组上进行采样即可.</p>
<p>为什么要设置这样一个采样机制呢? 这个机制个人认为其实是比较重要的, 回想TF-IDF的原理, 在评判某个单词对于文章的影响时, 即考虑了其在文章出现的频率, 又考虑了单词在其它文章中也经常出现. 在word2vec这里, 对于一些常出现的词, 肯定会频繁地作为其它一些词的周围词, 但是这些词真的与其它一些词相似吗, 它们只是本身经常出现而已. 所以通过上面这个负采样的机制, 可以抑制这种情况的发生, 即让那些不是经常出现, 但却共现的词更加相似.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上介绍了word2vec的算法原理, 可以发现其原理其实并不复杂.</p>
<p>在通过word2vec得到单词(字符)的向量表示后, 可以用来做什么呢? 可以用于自然语言处理中的各项任务, 如在使用RNN进行分类, 标注, 生成的时候, 就可以使用在大语料库上预训练好的word2vec词向量, 来作为每个单词的Embedding. 一般来说在一些困难的任务上, 或者训练样本较少的时候, 相比随机初始化, 可以获得更好的效果, 且收敛更快. 此外, 还可以使用word2vec来对文本进行表示, 如向量平均或者加权平均, 然后可以进行文本分类等.</p>
<p>而除了在自然语言处理, 受到word2vec的启发, 出现了node2vec, item2vec这样的算法, 甚至万物皆可Embedding.</p>
<p>然鹅word2vec也有一些问题, 其中比较明显的就是其向量表示, 是上下文无关的. 诶, 不是说就是训练的时候, 依靠上下文进行训练的吗, 怎么这里又成了上下文无关了呢? 这里的意思是说, 在使用word2vec时, 一个单词无论在什么语境下, 其Embedding都是固定的, 比如Apple这个单词, 可能是水果的苹果, 也可能是企业的苹果, 而一个固定的向量如何表达完全不同的两个意思呢?</p>
<p>所以, word2vec有很多用途, 是一种非常重要的算法, 但是也有缺点存在. 而后续的一些新的算法, 如BERT, 运用新的架构与设计, 可以得到上下文相关的Embedding, 这些以后再讲.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>TextRank</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/TextRank/</url>
    <content><![CDATA[<p>TextRank算法是受到PageRank的启发, 用于文本处理的一种无监督算法. 可以用于摘要生成以及关键词提取, 下面进行介绍.</p>
<a id="more"></a>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>在介绍TextRank之前, 有必要先说一下PageRank, 在我的另一篇博客<a href="whitemoonlight/2020/05/20/图挖掘算法/节点的中心性/">节点中心性</a>中也有讲解. 其主要思想为:</p>
<ul>
<li><p>网页抽象为图中的节点.</p>
</li>
<li><p>网页之间的超链接, 抽象为图中的连边.</p>
</li>
<li><p>边有方向, 指向被链接的网页.</p>
</li>
<li><p>边的值为提供超链接网页的出度的倒数.</p>
</li>
<li><p>给所有网页一个初始值(PageRank值), 阻尼系数.</p>
</li>
<li><p>反复迭代PageRank值, 公式如下:</p>
<script type="math/tex; mode=display">
PR_i=(1-\beta)+\beta(\sum_{j\in I}PR_j/C_j)</script><p>其中, $PR_i$表示节点$i$在某一次迭代中的重要性, $I$表示连接向节点$i$的邻居节点, $C_j$表示节点$j$的出度.</p>
</li>
<li><p>直到达到终止条件时, 停止迭代.</p>
</li>
</ul>
<p>而在TextRank这里, 节点变成了文本/单词, 连边变成了相邻/相似度, 下面进行具体讲解.</p>
<h2 id="摘要抽取"><a href="#摘要抽取" class="headerlink" title="摘要抽取"></a>摘要抽取</h2><p>首先TextRank可以用于文本摘要, 现在一般的文本摘要有两种方式:</p>
<ul>
<li><p>抽取型:</p>
<p>即从原始文本中, 抽取出具有代表性的一些句子, 来作为文本的摘要.</p>
</li>
<li><p>抽象型:</p>
<p>利用算法模型先”理解”文本内容, 然后再使用生成的方式, 生成全新的摘要.</p>
</li>
</ul>
<p>TextRank属于抽取型, 具体的算法流程如下:</p>
<ul>
<li><p>节点:</p>
<p>将一篇文章分割为句子, 作为节点.</p>
</li>
<li><p>连边:</p>
<p>借助一些文本表示方法, 比如使用word2vec, 把一个句子中的单词的word2vec做平均, 得到句子向量.</p>
<p>然后把句子之间的相似性(如余弦相似度)作为连边的劝值, 双向连边.</p>
</li>
<li><p>迭代:</p>
<p>在有了以上的节点与连边后, 就可以直接套用PageRank算法进行计算了.</p>
</li>
<li><p>输出:</p>
<p>得到所有句子的重要性后进行排序, 按设定输出排名靠前的句子即可.</p>
</li>
</ul>
<p>使用TextRank进行摘要抽取, 比较依赖于原始文本中, 是否有能够充当摘要的总结性的句子, 如果没有的话那么效果一般不太好.</p>
<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><p>下面来讲利用TextRank提取关键词的方法.</p>
<p>算法步骤如下:</p>
<ul>
<li><p>节点:</p>
<p>对原始文本进行分词, 词性标注, 停用词过滤, 标点/特殊符号过滤等预处理. 每个词(独特的)就是一个节点.</p>
</li>
<li><p>连边:</p>
<p>设定一个长度的滑动窗口, 对某个中心词来说, 窗口中的词可以算作其相邻词. 相邻词之间存在连边, 连边的权值可以取在滑动窗口中的共现次数.</p>
</li>
<li><p>迭代:</p>
<p>在有了以上的节点与连边后, 就可以直接套用PageRank算法进行计算了.</p>
</li>
<li><p>输出:</p>
<p>得到所有词的重要性后进行排序, 按设定输出排名靠前的词即可.</p>
</li>
</ul>
<p>同时, 对于得到的关键词, 如果在原文中相邻, 则可以组成关键词短语.</p>
<p>使用TextRank进行关键词提取, 对于一些出现频率较高的词, 会给予比较高的重要性, 所以在对原始文本预处理时, 对于停用词/高频词的过滤就显得必要了.</p>
<p>相比使用TF-IDF来进行关键词提取, TextRank使用迭代计算的方式, 在复杂度上更高.</p>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>一般来说使用TextRank算法进行关键词提取更常见一些, 所以这里只展示使用TextRank进行关键词提取.</p>
<p>仍然使用jieba这个包, jieba, 永远滴神!</p>
<p>使用 THUCNews的一个子集进行训练与测试, 使用了其中的 10 个分类, 每个分类 6500 条数据.</p>
<p>类别包含: 体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐.</p>
<p>数据集划分如下:</p>
<ul>
<li>训练集: 5000 * 10</li>
<li>验证集: 500 * 10</li>
<li>测试集: 1000 * 10</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br><span class="line"></span><br><span class="line">data_list = train_data_list + val_data_list + test_data_list</span><br><span class="line">label_list = train_label_list + val_label_list + test_label_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取关键词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">key_word</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jieba.analyse.textrank(sent, topK=<span class="number">20</span>, withWeight=<span class="literal">False</span>, allowPOS=(<span class="string">'n'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">'nz'</span>, <span class="string">'vn'</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">key_word_list = ordered_multiprocess_task(key_word, data_list, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看结果, 体育</span></span><br><span class="line">label_list[<span class="number">0</span>], key_word_list[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&apos;体育&apos;,</span><br><span class="line"> [&apos;训练&apos;,</span><br><span class="line">  &apos;沈阳&apos;,</span><br><span class="line">  &apos;国奥队&apos;,</span><br><span class="line">  &apos;大雨&apos;,</span><br><span class="line">  &apos;热身赛&apos;,</span><br><span class="line">  &apos;队员&apos;,</span><br><span class="line">  &apos;长春&apos;,</span><br><span class="line">  &apos;雨水&apos;,</span><br><span class="line">  &apos;队伍&apos;,</span><br><span class="line">  &apos;球员&apos;,</span><br><span class="line">  &apos;国奥&apos;,</span><br><span class="line">  &apos;女足&apos;,</span><br><span class="line">  &apos;冯萧霆&apos;,</span><br><span class="line">  &apos;马晓旭&apos;,</span><br><span class="line">  &apos;情况&apos;,</span><br><span class="line">  &apos;影响&apos;,</span><br><span class="line">  &apos;殷家&apos;,</span><br><span class="line">  &apos;球队&apos;,</span><br><span class="line">  &apos;伤病&apos;,</span><br><span class="line">  &apos;奥体中心&apos;])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看结果, 娱乐</span></span><br><span class="line">label_list[<span class="number">6500</span>], key_word_list[<span class="number">6500</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&apos;娱乐&apos;,</span><br><span class="line"> [&apos;王菲&apos;,</span><br><span class="line">  &apos;人民币&apos;,</span><br><span class="line">  &apos;代言&apos;,</span><br><span class="line">  &apos;地震&apos;,</span><br><span class="line">  &apos;唐山大&apos;,</span><br><span class="line">  &apos;冯小刚&apos;,</span><br><span class="line">  &apos;广告&apos;,</span><br><span class="line">  &apos;洗发精&apos;,</span><br><span class="line">  &apos;新片&apos;,</span><br><span class="line">  &apos;妈妈&apos;,</span><br><span class="line">  &apos;纪录&apos;,</span><br><span class="line">  &apos;森林&apos;,</span><br><span class="line">  &apos;唱片&apos;,</span><br><span class="line">  &apos;电视广告&apos;,</span><br><span class="line">  &apos;卖场&apos;,</span><br><span class="line">  &apos;陈可辛&apos;,</span><br><span class="line">  &apos;电影&apos;,</span><br><span class="line">  &apos;剧本&apos;,</span><br><span class="line">  &apos;演唱会&apos;,</span><br><span class="line">  &apos;量身&apos;])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>关键词提取</tag>
      </tags>
  </entry>
  <entry>
    <title>TF-IDF</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/TF-IDF/</url>
    <content><![CDATA[<p>TD-IDF(Term Frequency–Inverse Document Frequency), 即词频-逆文档频率, 是一种简单的文本表示方法. 虽然简单, 但是却很有用.</p>
<a id="more"></a>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>在聊TF-IDF之前, 先说一下文本表示方法.</p>
<p>最原始, 最简单的文本表示方法是什么呢? 应该是<strong>词袋模型</strong>了, 即给定一篇文档, 用一个向量对其进行表示, 每个向量元素表示一个词典中的单词, 若文档中出现该单词, 则为1, 否则为0.</p>
<p>词袋模型的方法过于简单, 可以进一步改进, 即用每个单词出现的<strong>频率</strong>来进行表示, 出现次数多的单词更能够表示一篇文档的主旨.</p>
<p>但是在我们人类的语言中, 有一些常用词, 无论在什么场景下都会大量使用, 比如中文里面的”的”, “是”; 此外, 一些出现频率相对低的单词, 不见得就不重要. 所以只考虑单词的频率, 仍然有较大的问题.</p>
<p>于是, TF-IDF出现了. TF-IDF有两层意思, 一层是词频(Term Frequency, 缩写为TF), 另一层是逆文档频率(Inverse Document Frequency, 缩写为IDF).</p>
<p>其中的TF的意思在上面已经说到过了, 那么这里来解释IDF. 直接给出计算公式:</p>
<script type="math/tex; mode=display">
逆文档频率=\log(\frac{语料库文档总数}{包含该词的文档数+1})</script><p>通俗来说, 就是一个单词, 如果在许多篇文档中都出现过, 那么就显得比较平凡; 而如果只在少量文档中出现, 那么就显得比较突出. 分母的加一是为了防止分母为零.</p>
<p>结合TF和IDF, 就可以得到TF-IDF的计算公式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
TF-IDF&=TF\times IDF \\
&=\frac{该词在该文档出现次数}{该文档总词数}\times \log(\frac{语料库文档总数}{包含该词的文档数+1})
\end{aligned}</script><p>总体来说, 如果一个单词在某文档中出现相对较多, 同时在其它文档出现较少, 那么TF-IDF的值就会比较大, 表示这个单词能体现该文档的主旨.</p>
<p>TF-IDF有如下特点:</p>
<ul>
<li>简单快速, 解释性强.</li>
<li>用于文本表示时, 没有考虑序列的位置信息.</li>
</ul>
<h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><p>TF-IDF一开始的出发点是用于表示文本, 而在得到了一篇文本的表示向量后, 可以考虑用来做<strong>聚类</strong>, 或者如果有标签的话, 可以用来做<strong>文本分类</strong>.</p>
<p>此外, 在原理篇说到过, 一篇文档中每个单词的TF-IDF表示其在这篇文档中的重要程度, 因此可以用来做<strong>关键词提取</strong>. 比如在得到每个单词的TF-IDF以后, 可以按其排序, 将最大的一些词作为关键词. 进一步考虑到关键词多为名词, 可以同时做POS(词性标注), 取名词当中TF-IDF较大的作为关键词.</p>
<p>个人感觉自然语言处理中的不少方法, 是具有跨领域用途的, 不一定局限于文本, TF-IDF也可以使用在其它地方. 举个栗子, 对于一些物品集合的表示(用户手机的APP); 再比如序列(用户行为序列). 在使用TF-IDF进行表示后, 再作为特征, 供下游算法模型使用.</p>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>上面介绍了TF-IDF的一些用途, 下面用实际的数据和代码来进行展示.</p>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>使用 THUCNews的一个子集进行训练与测试, 使用了其中的 10 个分类, 每个分类 6500 条数据.</p>
<p>类别包含: 体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐.</p>
<p>数据集划分如下:</p>
<ul>
<li>训练集: 5000 * 10</li>
<li>验证集: 500 * 10</li>
<li>测试集: 1000 * 10</li>
</ul>
<p>可以使用sklearn来转换为TF-IDF表示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取分类标签</span></span><br><span class="line"></span><br><span class="line">train_label_list = []</span><br><span class="line">val_label_list = []</span><br><span class="line">test_label_list = []</span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据集</span></span><br><span class="line">print(<span class="string">'训练集: %d'</span> % len(train_data_list))</span><br><span class="line">print(<span class="string">'验证集: %d'</span> % len(val_data_list))</span><br><span class="line">print(<span class="string">'测试集: %d'</span> % len(test_data_list))</span><br><span class="line">print(<span class="string">'分类: %s'</span> % <span class="string">', '</span>.join(list(set(train_label_list))))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">训练集: 50000</span><br><span class="line">验证集: 5000</span><br><span class="line">测试集: 10000</span><br><span class="line">分类: 时尚, 时政, 家居, 科技, 娱乐, 体育, 教育, 房产, 游戏, 财经</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分类标签编码0-9</span></span><br><span class="line">encode_label_dict = dict(zip([<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>], range(<span class="number">10</span>)))</span><br><span class="line">decode_label_dict = dict(zip(range(<span class="number">10</span>), [<span class="string">'时政'</span>, <span class="string">'财经'</span>, <span class="string">'时尚'</span>, <span class="string">'游戏'</span>, <span class="string">'娱乐'</span>, <span class="string">'科技'</span>, <span class="string">'家居'</span>, <span class="string">'教育'</span>, <span class="string">'房产'</span>, <span class="string">'体育'</span>]))</span><br><span class="line">train_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> train_label_list]</span><br><span class="line">val_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> val_label_list]</span><br><span class="line">test_label_encode = [encode_label_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> test_label_list]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_sent</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(jieba.cut(sent))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">train_data_list = ordered_multiprocess_task(cut_sent, train_data_list, <span class="number">24</span>)</span><br><span class="line">val_data_list = ordered_multiprocess_task(cut_sent, val_data_list, <span class="number">24</span>)</span><br><span class="line">test_data_list = ordered_multiprocess_task(cut_sent, test_data_list, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用sklearn进行tf-idf表示</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">10000</span>, max_df=<span class="number">0.99</span>, min_df=<span class="number">0.01</span>)</span><br><span class="line">train_data_tf_idf = vectorizer.fit_transform(train_data_list).A</span><br><span class="line">val_data_tf_idf = vectorizer.transform(val_data_list).A</span><br><span class="line">test_data_tf_idf = vectorizer.transform(test_data_list).A</span><br><span class="line">train_data_tf_idf.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用神经网络进行训练</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.config.threading.set_intra_op_parallelism_threads(<span class="number">24</span>)</span><br><span class="line">tf.config.threading.set_inter_op_parallelism_threads(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">128</span>,</span><br><span class="line">                 input_shape=(<span class="number">3003</span>,),</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line"><span class="comment">#                  kernel_regularizer=regularizers.l1(0.01)</span></span><br><span class="line">                )</span><br><span class="line">)</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">64</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line"><span class="comment">#                  kernel_regularizer=regularizers.l2(0.01)</span></span><br><span class="line">                )</span><br><span class="line">)</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_data_tf_idf,</span><br><span class="line">                    y=np.array(train_label_encode),</span><br><span class="line">                    batch_size=<span class="number">256</span>,</span><br><span class="line">                    validation_data=(val_data_tf_idf, np.array(val_label_encode)),</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_accuracy'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">print(<span class="string">'训练集结果: %f'</span> % accuracy_score(train_label_encode, model.predict_classes(train_data_tf_idf)))</span><br><span class="line">print(<span class="string">'测试集结果: %f'</span> % accuracy_score(val_label_encode, model.predict_classes(val_data_tf_idf)))</span><br><span class="line">print(<span class="string">'测试集结果: %f'</span> % accuracy_score(test_label_encode, model.predict_classes(test_data_tf_idf)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">训练集结果: 0.985400</span><br><span class="line">测试集结果: 0.946200</span><br><span class="line">测试集结果: 0.942900</span><br></pre></td></tr></table></figure>
<p>从结果来看, 使用TF-IDF来进行文本分类, 效果还不错.</p>
<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><p>中文的话, 可以使用jieba这个包, 来根据TF-IDF进行关键词提取. </p>
<p>Jieba可以指定词性(如名词)来对高TF-IDF关键词的结果进行过滤. 还可以自定义指定IDF语料库和停用词. 其中IDF仍然可以只用sklearn来进行计算, 停用词可以根据具体的场景进行指定.</p>
<p>这里不做太复杂, 只简单指定词性进行过滤关键词.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"></span><br><span class="line">train_data_list = []</span><br><span class="line">val_data_list = []</span><br><span class="line">test_data_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    train_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.val.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    val_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/text_classification/cnews/cnews.test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    test_data_list = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">train_data_list, train_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data_list]</span><br><span class="line">val_data_list, val_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> val_data_list]</span><br><span class="line">test_data_list, test_label_list = [x.split(<span class="string">'\t'</span>)[<span class="number">1</span>].strip() <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list], [x.split(<span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_data_list]</span><br><span class="line"></span><br><span class="line">data_list = train_data_list + val_data_list + test_data_list</span><br><span class="line">label_list = train_label_list + val_label_list + test_label_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取关键词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">key_word</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jieba.analyse.extract_tags(sent, topK=<span class="number">20</span>, withWeight=<span class="literal">False</span>, allowPOS=(<span class="string">'n'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">'nz'</span>, <span class="string">'vn'</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">key_word_list = ordered_multiprocess_task(key_word, data_list, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看结果, 体育</span></span><br><span class="line">label_list[<span class="number">0</span>], key_word_list[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&apos;体育&apos;,</span><br><span class="line"> [&apos;国奥队&apos;,</span><br><span class="line">  &apos;训练&apos;,</span><br><span class="line">  &apos;沈阳&apos;,</span><br><span class="line">  &apos;冯萧霆&apos;,</span><br><span class="line">  &apos;大雨&apos;,</span><br><span class="line">  &apos;球员&apos;,</span><br><span class="line">  &apos;国奥&apos;,</span><br><span class="line">  &apos;长春&apos;,</span><br><span class="line">  &apos;马晓旭&apos;,</span><br><span class="line">  &apos;队员&apos;,</span><br><span class="line">  &apos;热身赛&apos;,</span><br><span class="line">  &apos;雨水&apos;,</span><br><span class="line">  &apos;全队&apos;,</span><br><span class="line">  &apos;队伍&apos;,</span><br><span class="line">  &apos;球队&apos;,</span><br><span class="line">  &apos;殷家&apos;,</span><br><span class="line">  &apos;傅亚雨&apos;,</span><br><span class="line">  &apos;奥体中心&apos;,</span><br><span class="line">  &apos;女足&apos;,</span><br><span class="line">  &apos;草草收场&apos;])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看结果, 娱乐</span></span><br><span class="line">label_list[<span class="number">6500</span>], key_word_list[<span class="number">6500</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&apos;娱乐&apos;,</span><br><span class="line"> [&apos;王菲&apos;,</span><br><span class="line">  &apos;唐山大&apos;,</span><br><span class="line">  &apos;人民币&apos;,</span><br><span class="line">  &apos;代言&apos;,</span><br><span class="line">  &apos;地震&apos;,</span><br><span class="line">  &apos;冯小刚&apos;,</span><br><span class="line">  &apos;新片&apos;,</span><br><span class="line">  &apos;洗发精&apos;,</span><br><span class="line">  &apos;演唱会&apos;,</span><br><span class="line">  &apos;演艺圈&apos;,</span><br><span class="line">  &apos;广告&apos;,</span><br><span class="line">  &apos;成龙&apos;,</span><br><span class="line">  &apos;电影&apos;,</span><br><span class="line">  &apos;妈妈&apos;,</span><br><span class="line">  &apos;唱片&apos;,</span><br><span class="line">  &apos;纪录&apos;,</span><br><span class="line">  &apos;音乐&apos;,</span><br><span class="line">  &apos;新高峰&apos;,</span><br><span class="line">  &apos;演员阵容&apos;,</span><br><span class="line">  &apos;麦当娜&apos;])</span><br></pre></td></tr></table></figure>
<p>从关键词提取的结果来看, 还是具有相关性的.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>文本分类</tag>
        <tag>TF-IDF</tag>
        <tag>关键词提取</tag>
      </tags>
  </entry>
  <entry>
    <title>条件随机场CRF(三)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%89/</url>
    <content><![CDATA[<p>在上一篇中, 比较详细地讲解了条件随机场CRF的原理, 本篇主要针对CRF的实例演示.</p>
<p>将在一个命名体识别的任务上, 分别利用单独的CRF模型, Bi-LSTM, 以及Bi-LSTM+CRF来进行建模, 并简单比较几种方法的结果.</p>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><p>这里使用的数据, 是<a href="https://github.com/CLUEbenchmark/CLUEDatasetSearch" target="_blank" rel="noopener">1998人民日报语料集实体识别标注集</a>, 标注形式为BIO, 共有23061条语料.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将原始数据按样本进行整理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    print(path)</span><br><span class="line">    x_list = []</span><br><span class="line">    y_list = []</span><br><span class="line">    tmp_x_list = []</span><br><span class="line">    tmp_y_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span> line != <span class="string">'\n'</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    char_, target = line.split(<span class="string">' '</span>)</span><br><span class="line">                    tmp_x_list.append(char_)</span><br><span class="line">                    tmp_y_list.append(target.strip())</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'error'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> len(tmp_x_list) &gt; <span class="number">0</span>:</span><br><span class="line">                    x_list.append(tmp_x_list)</span><br><span class="line">                    y_list.append(tmp_y_list)</span><br><span class="line">                    tmp_x_list = []</span><br><span class="line">                    tmp_y_list = []</span><br><span class="line">        f.close()</span><br><span class="line">    print(<span class="string">'done'</span>)</span><br><span class="line">    <span class="keyword">return</span> x_list, y_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_x, train_y = clean_data(<span class="string">'./data/NER/data_train.txt'</span>)</span><br><span class="line">dev_x, dev_y = clean_data(<span class="string">'./data/NER/data_dev.txt'</span>)</span><br><span class="line">test_x, test_y = clean_data(<span class="string">'./data/NER/data_test.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 样本量</span></span><br><span class="line">print(<span class="string">'train x: %d, train y: %d'</span> % (len(train_x), len(train_y)))</span><br><span class="line">print(<span class="string">'dev x: %d, dev y: %d'</span> % (len(dev_x), len(dev_y)))</span><br><span class="line">print(<span class="string">'test x: %d, test y: %d'</span> % (len(test_x), len(test_y)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train x: 11683, train y: 11683</span><br><span class="line">dev x: 3912, dev y: 3912</span><br><span class="line">test x: 3890, test y: 3890</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_x = train_x + dev_x + test_x</span><br><span class="line">all_y = train_y + dev_y + test_y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计有多少不同的字符</span></span><br><span class="line">set_ = set([])</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> all_x:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        set_.add(i)</span><br><span class="line">print(len(set_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计不同的字符的数量</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dict_ = defaultdict(int)</span><br><span class="line"><span class="keyword">for</span> list_ <span class="keyword">in</span> all_y:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_:</span><br><span class="line">        dict_[i] += <span class="number">1</span></span><br><span class="line">print(dict_)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4692</span><br><span class="line">defaultdict(&lt;class &apos;int&apos;&gt;, &#123;&apos;O&apos;: 1785219, &apos;B-PER&apos;: 20242, &apos;I-PER&apos;: 36210&#125;)</span><br></pre></td></tr></table></figure>
<p>常用汉字4千多, 确实是这样的.</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>比较常用的CRF工具包有<a href="https://taku910.github.io/crfpp/" target="_blank" rel="noopener">CRF++</a>, 比较经典的包, 支持命令行模式, 也有Python接口; <a href="https://sklearn-crfsuite.readthedocs.io/en/latest/?badge=latest" target="_blank" rel="noopener">sklearn-crfsuite</a>, 从名字就可以看得出来, 其API是仿sklearn的.</p>
<p>这里使用sklearn-crfsuite, 下面是代码的主要部分.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里仿照sklearn-crfsuite文档中的做法来构造特征</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2features</span><span class="params">(x, i)</span>:</span></span><br><span class="line">    word = x[i]</span><br><span class="line"></span><br><span class="line">    features = &#123;<span class="string">'word'</span>: word&#125;</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">        word_1 = x[i - <span class="number">1</span>]</span><br><span class="line">        features.update(&#123;<span class="string">'-1:word'</span>: word_1&#125;)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        features[<span class="string">'BOS'</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; len(x)<span class="number">-1</span>:</span><br><span class="line">        word_2 = x[i + <span class="number">1</span>]</span><br><span class="line">        features.update(&#123;<span class="string">'+1:word'</span>: word_2&#125;)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        features[<span class="string">'EOS'</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent2features</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [word2features(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))]</span><br><span class="line"></span><br><span class="line">sent2features(train_x[<span class="number">0</span>])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ft = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> train_x]</span><br><span class="line">dev_ft = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> dev_x]</span><br><span class="line">test_ft = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> test_x]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sklearn_crfsuite</span><br><span class="line"><span class="keyword">from</span> sklearn_crfsuite <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">crf = sklearn_crfsuite.CRF(algorithm=<span class="string">'lbfgs'</span>,</span><br><span class="line">                           c1=<span class="number">0.1</span>,</span><br><span class="line">                           c2=<span class="number">0.1</span>,</span><br><span class="line">                           max_iterations=<span class="literal">None</span>,</span><br><span class="line">                           all_possible_transitions=<span class="literal">True</span>,</span><br><span class="line">                          )</span><br><span class="line">crf.fit(train_ft, train_y, X_dev=dev_ft, y_dev=dev_y)</span><br></pre></td></tr></table></figure>
<p>由于sklearn_crfsuite这个包并不支持多线程, 所以开始训练后可以先去干点别的, 比如恰个水果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./model/CRF/crfsuite.model'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(crf, f)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评估</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line">N_LABEL = <span class="number">3</span>  <span class="comment"># 标签数量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">disordered_multiprocess_task</span><span class="params">(func=None, param_list=None, max_process=None)</span>:</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    pool = mp.Pool(max_process)</span><br><span class="line">    res = pool.starmap_async(func=func, iterable=param_list).get()</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    res = [len(y_true)]  <span class="comment"># [total, [true_pos, true_neg, false_pos, false_neg], ...]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N_LABEL):</span><br><span class="line">        y_true_binary = [int(x == i) <span class="keyword">for</span> x <span class="keyword">in</span> y_true]</span><br><span class="line">        y_pred_binary = [int(x == i) <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line"></span><br><span class="line">        true_pos = sum([x * y <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(y_pred_binary, y_true_binary)])</span><br><span class="line">        true_neg = sum([(x - <span class="number">1</span>) * (y - <span class="number">1</span>) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(y_pred_binary, y_true_binary)])</span><br><span class="line">        false_pos = sum([abs(x * (y - <span class="number">1</span>)) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(y_pred_binary, y_true_binary)])</span><br><span class="line">        false_neg = sum([abs((x - <span class="number">1</span>) * y) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(y_pred_binary, y_true_binary)])</span><br><span class="line">        res.append([true_pos, true_neg, false_pos, false_neg])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classification_report</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据真实标签和预测标签, 输出模型评估指标.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    res_list = disordered_multiprocess_task(evaluate, zip(y_true, y_pred), <span class="number">12</span>)</span><br><span class="line">    total = sum([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res_list])</span><br><span class="line">    true_pos_list = []</span><br><span class="line">    true_neg_list = []</span><br><span class="line">    false_pos_list = []</span><br><span class="line">    false_neg_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N_LABEL):</span><br><span class="line">        true_pos_list.append(sum([x[i + <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res_list]))</span><br><span class="line">        true_neg_list.append(sum([x[i + <span class="number">1</span>][<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res_list]))</span><br><span class="line">        false_pos_list.append(sum([x[i + <span class="number">1</span>][<span class="number">2</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res_list]))</span><br><span class="line">        false_neg_list.append(sum([x[i + <span class="number">1</span>][<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res_list]))</span><br><span class="line">    accuracy_list = [(x + y) / total <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(true_pos_list, true_neg_list)]</span><br><span class="line">    precision_list = [x / (x + y) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(true_pos_list, false_pos_list)]</span><br><span class="line">    recall_list = [x / (x + y) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(true_pos_list, false_neg_list)]</span><br><span class="line">    f1_list = [<span class="number">2</span> * x * y / (x + y) <span class="keyword">for</span> x , y <span class="keyword">in</span> zip(precision_list, recall_list)]</span><br><span class="line">    avg_f1 = sum(f1_list) / len(f1_list)</span><br><span class="line">    accuracy_list = [<span class="string">'%.3f'</span> % x <span class="keyword">for</span> x <span class="keyword">in</span> accuracy_list]</span><br><span class="line">    precision_list = [<span class="string">'%.3f'</span> % x <span class="keyword">for</span> x <span class="keyword">in</span> precision_list]</span><br><span class="line">    recall_list = [<span class="string">'%.3f'</span> % x <span class="keyword">for</span> x <span class="keyword">in</span> recall_list]</span><br><span class="line">    f1_list = [<span class="string">'%.3f'</span> % x <span class="keyword">for</span> x <span class="keyword">in</span> f1_list]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Accuracy:'</span>)</span><br><span class="line">    print(<span class="string">'\t'</span>.join(accuracy_list))</span><br><span class="line">    print(<span class="string">'Precision:'</span>)</span><br><span class="line">    print(<span class="string">'\t'</span>.join(precision_list))</span><br><span class="line">    print(<span class="string">'Recall:'</span>)</span><br><span class="line">    print(<span class="string">'\t'</span>.join(recall_list))</span><br><span class="line">    print(<span class="string">'F1:'</span>)</span><br><span class="line">    print(<span class="string">'\t'</span>.join(f1_list))</span><br><span class="line">    print(<span class="string">'Average F1:'</span>)</span><br><span class="line">    print(<span class="string">'%.3f'</span> % avg_f1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评估模型 验证集</span></span><br><span class="line"></span><br><span class="line">encode_dict = &#123;<span class="string">'O'</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">'B-PER'</span>: <span class="number">1</span>,</span><br><span class="line">              <span class="string">'I-PER'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_pred = crf.predict(dev_ft)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [encode_dict[x] <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">y_pred_ecd = [encode(x) <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">dev_y_ecd = [encode(x) <span class="keyword">for</span> x <span class="keyword">in</span> dev_y]</span><br><span class="line"></span><br><span class="line">classification_report(dev_y_ecd, y_pred_ecd)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Accuracy:</span><br><span class="line">0.994	0.998	0.996</span><br><span class="line">Precision:</span><br><span class="line">0.996	0.940	0.924</span><br><span class="line">Recall:</span><br><span class="line">0.998	0.861	0.881</span><br><span class="line">F1:</span><br><span class="line">0.997	0.899	0.902</span><br><span class="line">Average F1:</span><br><span class="line">0.933</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 评估模型 测试集</span><br><span class="line"></span><br><span class="line">encode_dict = &#123;&apos;O&apos;: 0,</span><br><span class="line">              &apos;B-PER&apos;: 1,</span><br><span class="line">              &apos;I-PER&apos;: 2&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_pred = crf.predict(test_ft)</span><br><span class="line"></span><br><span class="line">def encode(data):</span><br><span class="line">    return [encode_dict[x] for x in data]</span><br><span class="line"></span><br><span class="line">y_pred_ecd = [encode(x) for x in y_pred]</span><br><span class="line">test_y_ecd = [encode(x) for x in test_y]</span><br><span class="line"></span><br><span class="line">classification_report(test_y_ecd, y_pred_ecd)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Accuracy:</span><br><span class="line">0.995	0.998	0.996</span><br><span class="line">Precision:</span><br><span class="line">0.996	0.930	0.924</span><br><span class="line">Recall:</span><br><span class="line">0.998	0.858	0.874</span><br><span class="line">F1:</span><br><span class="line">0.997	0.893	0.899</span><br><span class="line">Average F1:</span><br><span class="line">0.930</span><br></pre></td></tr></table></figure>
<p>如果还想提高准确性, 可以尝试添加更多的特征, 并对正则系数进行交叉验证调参.</p>
<p>此外, sklearn_crfsuite还支持查看各个特征, 包括转移特征和发射特征的权重系数, 从中可以看到哪些特征模型是重要的, 是否合理等.</p>
<h1 id="Bi-LSTM-CRF"><a href="#Bi-LSTM-CRF" class="headerlink" title="Bi-LSTM+CRF"></a>Bi-LSTM+CRF</h1><p>在上一篇讲解CRF原理的时候, 说过CRF模型和深度学习模型各有优缺点, 而一种不错的方式就是将它们结合起来使用.</p>
<p>而具体的结构是什么样的呢, 单纯的Bi-LSTM在每个时步上的输出, 可以看成是基于当前给定的序列$X$, 对应目标为$y$的概率, 即$P(y|X)$.</p>
<p>这明显没有考虑到目标序列$Y$本身所蕴含的模式, 所以才在其输出之上, 加了CRF模型. 模型结构如下:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>关于Bi-LSTM+CRF具体的理论, 可以参考这篇<a href="https://www.cnblogs.com/createMoMo/p/7529885.html" target="_blank" rel="noopener">博客</a>, 写得非常详细. 这里只做一个粗略的说明.</p>
<p>可以设定Bi-LSTM的输出层的维度为标签数目, 看做是当前时步的$y$与$X$的分数. 那么假定Bi-LSTM的输出矩阵为$B$, $B_{t,y}$表示时步$t$目标$y$的分数.</p>
<p>在CRF中, 还要考虑到目标序列$Y$的模式, 假设存在一个转移矩阵$A$, $A_{y,y’}$表示两个状态之间分数.</p>
<p>那么给定输入序列$X$, 目标序列$Y$, 定义分数为:</p>
<script type="math/tex; mode=display">
s(X,Y)=\sum_tA_{y_t,y_{t+1}}+\sum_tB_{t,y_t}</script><p>对应的条件概率为:</p>
<script type="math/tex; mode=display">
P(Y|X)=\frac{\exp(s(X,Y))}{\sum_{Y'}\exp (s(X,Y'))}</script><p>到了这里后, 接下来优化与推断就可以结合CRF的方法来做了.</p>
<p>下面是代码的主要部分, 其中的CRF层, 采用了与TensorFlow2.x配合使用的tensorflow_addons.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将原始数据按样本进行整理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    print(path)</span><br><span class="line">    x_list = []</span><br><span class="line">    y_list = []</span><br><span class="line">    tmp_x_list = []</span><br><span class="line">    tmp_y_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span> line != <span class="string">'\n'</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    char_, target = line.split(<span class="string">' '</span>)</span><br><span class="line">                    tmp_x_list.append(char_)</span><br><span class="line">                    tmp_y_list.append(target.strip())</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'error'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> len(tmp_x_list) &gt; <span class="number">0</span>:</span><br><span class="line">                    x_list.append(tmp_x_list)</span><br><span class="line">                    y_list.append(tmp_y_list)</span><br><span class="line">                    tmp_x_list = []</span><br><span class="line">                    tmp_y_list = []</span><br><span class="line">        f.close()</span><br><span class="line">    print(<span class="string">'done'</span>)</span><br><span class="line">    <span class="keyword">return</span> x_list, y_list</span><br><span class="line">train_x, train_y = clean_data(<span class="string">'./data/NER/data_train.txt'</span>)</span><br><span class="line">dev_x, dev_y = clean_data(<span class="string">'./data/NER/data_dev.txt'</span>)</span><br><span class="line">test_x, test_y = clean_data(<span class="string">'./data/NER/data_test.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save2file</span><span class="params">(x, y, path)</span>:</span></span><br><span class="line">    data = zip(x, y)</span><br><span class="line">    res = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> m, n <span class="keyword">in</span> zip(i, j):</span><br><span class="line">            res += m + <span class="string">' '</span> + n + <span class="string">'\n'</span></span><br><span class="line">        res += <span class="string">'\n'</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(res)</span><br><span class="line">        f.close()</span><br><span class="line">save2file(train_x, train_y, <span class="string">'./data/NER/train.txt'</span>)</span><br><span class="line">save2file(dev_x, dev_y, <span class="string">'./data/NER/dev.txt'</span>)</span><br><span class="line">save2file(test_x, test_y, <span class="string">'./data/NER/test.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">模型.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_addons <span class="keyword">as</span> tfa</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFLayer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags)</span>:</span></span><br><span class="line">        super(CRFLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_tags = num_tags</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.trans_params = self.add_weight(</span><br><span class="line">            name=<span class="string">'trans_params'</span>,</span><br><span class="line">            shape=(self.num_tags, self.num_tags))</span><br><span class="line">        self.build = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, seq_len)</span>:</span></span><br><span class="line">        tags, scores = tfa.text.crf_decode(x, self.trans_params, seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags, scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTMCRFModel</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embeddings, vocab_size, config)</span>:</span></span><br><span class="line">        super(BiLSTMCRFModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(</span><br><span class="line">            config[<span class="string">'lstm_dim'</span>],</span><br><span class="line">            return_sequences=<span class="literal">True</span>,</span><br><span class="line">            kernel_regularizer=tf.keras.regularizers.l2(config[<span class="string">'l2'</span>]),</span><br><span class="line">            recurrent_regularizer=tf.keras.regularizers.l2(config[<span class="string">'l2'</span>])))</span><br><span class="line">        self.hidden_layer = tf.keras.layers.Dense(</span><br><span class="line">            config[<span class="string">'lstm_dim'</span>], activation=<span class="string">'tanh'</span>)</span><br><span class="line">        self.final_layer = tf.keras.layers.Dense(config[<span class="string">'n_tags'</span>])</span><br><span class="line">        self.crf_layer = CRFLayer(config[<span class="string">'n_tags'</span>])</span><br><span class="line">        self.embedding_layer = tf.keras.layers.Embedding(</span><br><span class="line">            vocab_size, config[<span class="string">'word_dim'</span>])</span><br><span class="line">        <span class="keyword">if</span> embeddings <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding_layer.build((<span class="literal">None</span>, vocab_size))</span><br><span class="line">            self.embedding_layer.set_weights([embeddings])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, padding_mask)</span>:</span></span><br><span class="line">        x = self.embedding_layer(x)</span><br><span class="line">        x = self.rnn_layer(x, mask=tf.cast(padding_mask, tf.bool))</span><br><span class="line">        x = self.hidden_layer(x)</span><br><span class="line">        logits = self.final_layer(x)</span><br><span class="line"></span><br><span class="line">        true_seq_len = tf.cast(tf.math.reduce_sum(padding_mask, axis=<span class="number">1</span>), tf.int32)</span><br><span class="line">        tags, scores = self.crf_layer(logits, true_seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags, logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span><span class="params">(real, pred, mask, trans_params)</span>:</span></span><br><span class="line">    <span class="comment"># real.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    <span class="comment"># pred.shape == (batch_size, max_seq_len, num_tags)</span></span><br><span class="line">    <span class="comment"># mask.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    true_seq_len = tf.cast(tf.math.reduce_sum(mask, axis=<span class="number">1</span>), tf.int32)</span><br><span class="line">    real = tf.cast(real, tf.int32)</span><br><span class="line">    log_likelihood, _ = tfa.text.crf_log_likelihood(</span><br><span class="line">        pred, real, true_seq_len, trans_params)</span><br><span class="line">    loss = tf.math.reduce_mean(-log_likelihood)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">评估参数.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelMetric</span><span class="params">(tf.keras.metrics.Metric)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_classes, name=<span class="string">'model_metric'</span>, **kwargs)</span>:</span></span><br><span class="line">        super(ModelMetric, self).__init__(name=name, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.true_positives = self.add_weight(</span><br><span class="line">            name=<span class="string">'tp'</span>, shape=(n_classes,), initializer=<span class="string">'zeros'</span>)</span><br><span class="line">        self.false_positives = self.add_weight(</span><br><span class="line">            name=<span class="string">'fp'</span>, shape=(n_classes,), initializer=<span class="string">'zeros'</span>)</span><br><span class="line">        self.true_negatives = self.add_weight(</span><br><span class="line">            name=<span class="string">'tn'</span>, shape=(n_classes,), initializer=<span class="string">'zeros'</span>)</span><br><span class="line">        self.false_negatives = self.add_weight(</span><br><span class="line">            name=<span class="string">'fn'</span>, shape=(n_classes,), initializer=<span class="string">'zeros'</span>)</span><br><span class="line">        self.total = self.add_weight(</span><br><span class="line">            name=<span class="string">'total'</span>, initializer=<span class="string">'zeros'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_state</span><span class="params">(self, y_true, y_pred, sample_weight=None)</span>:</span></span><br><span class="line">        <span class="comment"># sample_weight.shape == (batch_size, max_seq_len)</span></span><br><span class="line">        <span class="comment"># y_pred = tf.cast(y_pred, tf.int64)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            sample_weight = tf.ones_like(y_true, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        true_seq_len = tf.math.reduce_sum(sample_weight)</span><br><span class="line">        self.total.assign_add(true_seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_classes):</span><br><span class="line">            <span class="comment"># i = tf.convert_to_tensor(i, dtype=tf.int64)</span></span><br><span class="line">            binary_true_label = tf.cast(tf.math.equal(y_true, i), tf.float32)</span><br><span class="line">            binary_pred_label = tf.cast(tf.math.equal(y_pred, i), tf.float32)</span><br><span class="line"></span><br><span class="line">            true_pos = tf.math.count_nonzero(</span><br><span class="line">                binary_pred_label * binary_true_label * sample_weight)</span><br><span class="line">            true_pos = tf.cast(true_pos, tf.float32)</span><br><span class="line">            <span class="comment"># true_positives[i] is a Tensor which has no add_assign method.</span></span><br><span class="line">            self.true_positives[i].assign(self.true_positives[i] + true_pos)</span><br><span class="line"></span><br><span class="line">            true_neg = tf.math.count_nonzero(</span><br><span class="line">                (binary_true_label - <span class="number">1</span>) * (binary_pred_label - <span class="number">1</span>) * sample_weight)</span><br><span class="line">            true_neg = tf.cast(true_neg, tf.float32)</span><br><span class="line">            self.true_negatives[i].assign(self.true_negatives[i] + true_neg)</span><br><span class="line"></span><br><span class="line">            false_pos = tf.math.count_nonzero(</span><br><span class="line">                binary_pred_label * (binary_true_label - <span class="number">1</span>) * sample_weight)</span><br><span class="line">            false_pos = tf.cast(false_pos, tf.float32)</span><br><span class="line">            self.false_positives[i].assign(self.false_positives[i] + false_pos)</span><br><span class="line"></span><br><span class="line">            false_neg = tf.math.count_nonzero(</span><br><span class="line">                (binary_pred_label - <span class="number">1</span>) * binary_true_label * sample_weight)</span><br><span class="line">            false_neg = tf.cast(false_neg, tf.float32)</span><br><span class="line">            self.false_negatives[i].assign(self.false_negatives[i] + false_neg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span><span class="params">(self)</span>:</span></span><br><span class="line">        accuracy = tf.math.divide_no_nan(</span><br><span class="line">            self.true_positives + self.true_negatives,</span><br><span class="line">            self.total)</span><br><span class="line">        precision = tf.math.divide_no_nan(</span><br><span class="line">            self.true_positives,</span><br><span class="line">            self.true_positives + self.false_positives)</span><br><span class="line">        recall = tf.math.divide_no_nan(</span><br><span class="line">            self.true_positives,</span><br><span class="line">            self.true_positives + self.false_negatives)</span><br><span class="line">        f1_score = tf.math.divide_no_nan(<span class="number">2</span> * precision * recall,</span><br><span class="line">                                         precision + recall)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the same accuracy with tf.keras.metrics.Accuracy,</span></span><br><span class="line">        <span class="comment"># so it is unnecessary to reimplement it again.</span></span><br><span class="line">        <span class="comment"># accuracy = tf.math.divide_no_nan(</span></span><br><span class="line">        <span class="comment">#     tf.math.reduce_sum(self.true_positives),</span></span><br><span class="line">        <span class="comment">#     self.total)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> accuracy, precision, recall, f1_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_states</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.true_positives.assign(tf.zeros_like(self.true_positives))</span><br><span class="line">        self.false_positives.assign(tf.zeros_like(self.false_positives))</span><br><span class="line">        self.true_negatives.assign(tf.zeros_like(self.true_negatives))</span><br><span class="line">        self.false_negatives.assign(tf.zeros_like(self.false_negatives))</span><br><span class="line">        self.total.assign(tf.zeros_like(self.total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classification_report</span><span class="params">(metric)</span>:</span></span><br><span class="line">    metric_res = metric.result().numpy()</span><br><span class="line">    res = <span class="string">'\taccuracy: '</span> + str(metric_res[<span class="number">0</span>]) + <span class="string">'\n'</span></span><br><span class="line">    res += <span class="string">'\tprecison: '</span> + str(metric_res[<span class="number">1</span>]) + <span class="string">'\n'</span></span><br><span class="line">    res += <span class="string">'\trecall: '</span> + str(metric_res[<span class="number">2</span>]) + <span class="string">'\n'</span></span><br><span class="line">    res += <span class="string">'\tF1 score: '</span> + str(metric_res[<span class="number">3</span>]) + <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据准备.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_vocab</span><span class="params">(file_path, save_path)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据语料初始化词表.</span></span><br><span class="line"><span class="string">    @param file_path: 语料路径, 每一行为"word label", "\n"表示一段文本.</span></span><br><span class="line"><span class="string">    @return: 词表, &#123;'PAD': 0, 'UNK': 1, 'w_0': 2, ...&#125;, 同时保存到指定路径.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocab_dict = &#123;<span class="string">'PAD'</span>: <span class="number">0</span>, <span class="string">'UNK'</span>: <span class="number">1</span>&#125;</span><br><span class="line">    n = <span class="number">2</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span> line != <span class="string">'\n'</span>:</span><br><span class="line">                word, _ = line.strip().split()</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab_dict:</span><br><span class="line">                    vocab_dict[word] = n</span><br><span class="line">                    n += <span class="number">1</span></span><br><span class="line">        f.close()</span><br><span class="line">    res = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> vocab_dict.items():</span><br><span class="line">        res += k + <span class="string">' '</span> + str(v) + <span class="string">'\n'</span></span><br><span class="line">    res = res.strip()</span><br><span class="line">    <span class="keyword">with</span> open(save_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(res)</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">return</span> vocab_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_embedding</span><span class="params">(vec_file, vocab_file)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据词表和预训练向量, 初始化Embedding.</span></span><br><span class="line"><span class="string">    @param vec_file: 预训练向量文件路径.</span></span><br><span class="line"><span class="string">    @param vocab_file: 词表文件路径.</span></span><br><span class="line"><span class="string">    @return: Embedding矩阵, 预训练向量维度.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    char_vec_dict = &#123;&#125;</span><br><span class="line">    <span class="comment"># 读取文件</span></span><br><span class="line">    <span class="keyword">with</span> open(vec_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            data = line.strip().split()</span><br><span class="line">            char_, vec = data[<span class="number">0</span>], data[<span class="number">1</span>:]</span><br><span class="line">            vec = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> vec]</span><br><span class="line">            char_vec_dict[char_] = vec</span><br><span class="line">        f.close()</span><br><span class="line">    size = len(char_vec_dict[char_])</span><br><span class="line"></span><br><span class="line">    vocab_list = []</span><br><span class="line">    <span class="keyword">with</span> open(vocab_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            data = line.strip().split()</span><br><span class="line">            char_, idx = data[<span class="number">0</span>], int(data[<span class="number">1</span>])</span><br><span class="line">            vocab_list.append([char_, idx])</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    vocab_list = sorted(vocab_list, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    vocab_list = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> vocab_list]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据词表, 返回Embedding</span></span><br><span class="line">    embedding = []</span><br><span class="line">    <span class="keyword">for</span> char_ <span class="keyword">in</span> vocab_list:</span><br><span class="line">        <span class="keyword">if</span> char_ <span class="keyword">in</span> char_vec_dict:</span><br><span class="line">            embedding.append(char_vec_dict[char_])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding.append(np.random.uniform(low=<span class="number">-0.5</span> / size, high=<span class="number">0.5</span> / size, size=size))</span><br><span class="line">    <span class="keyword">return</span> np.array(embedding), size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format_data</span><span class="params">(data_path, vocab_file, save_path)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    创建训练用的数据集.</span></span><br><span class="line"><span class="string">    @param data_path: 语料文件路径.</span></span><br><span class="line"><span class="string">    @param vocab_file: 词表文件路径.</span></span><br><span class="line"><span class="string">    @param save_path: 保存文件路径.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocab_list = []</span><br><span class="line">    <span class="keyword">with</span> open(vocab_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            data = line.strip().split()</span><br><span class="line">            char_, idx = data[<span class="number">0</span>], int(data[<span class="number">1</span>])</span><br><span class="line">            vocab_list.append([char_, idx])</span><br><span class="line">        f.close()</span><br><span class="line">    vocab_dict = dict(vocab_list)</span><br><span class="line"></span><br><span class="line">    word_list = []</span><br><span class="line">    tag_list = []</span><br><span class="line">    res = <span class="string">''</span></span><br><span class="line">    <span class="keyword">with</span> open(data_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span> line != <span class="string">'\n'</span>:</span><br><span class="line">                word, tag = line.strip().split()</span><br><span class="line">                tag = tag_dict[tag]</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> vocab_dict:</span><br><span class="line">                    word = vocab_dict[word]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    word = <span class="string">'0'</span></span><br><span class="line">                word_list.append(str(word))</span><br><span class="line">                tag_list.append(str(tag))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res += <span class="string">' '</span>.join(word_list) + <span class="string">'\t'</span> + <span class="string">' '</span>.join(tag_list) + <span class="string">'\n'</span></span><br><span class="line">                word_list = []</span><br><span class="line">                tag_list = []</span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">with</span> open(save_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(res.strip())</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_dataset</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据文件路径返回数据集.</span></span><br><span class="line"><span class="string">    @param path: 经过pre_data后的文件路径.</span></span><br><span class="line"><span class="string">    @return: dataset.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">        res = tf.strings.split(line, <span class="string">'\t'</span>)</span><br><span class="line">        text, label = res[<span class="number">0</span>], res[<span class="number">1</span>]</span><br><span class="line">        text = tf.strings.split(text)</span><br><span class="line">        label = tf.strings.split(label)</span><br><span class="line">        text = tf.cast(tf.strings.to_number(text), tf.int32)</span><br><span class="line">        label = tf.cast(tf.strings.to_number(label), tf.int32)</span><br><span class="line">        <span class="comment"># label = tf.expand_dims(tf.cast(tf.strings.to_number(label), tf.int32), axis=0)</span></span><br><span class="line">        <span class="comment"># text = tf.expand_dims(text, axis=0)</span></span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    dataset = tf.data.TextLineDataset(filenames=path) \</span><br><span class="line">        .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">        .shuffle(buffer_size=<span class="number">10240</span>, seed=<span class="number">7</span>) \</span><br><span class="line">        .padded_batch(train_config[<span class="string">'batch_size'</span>], padded_shapes=([<span class="number">-1</span>], [<span class="number">-1</span>])) \</span><br><span class="line">        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">配置.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">model_config = &#123;<span class="string">'word_dim'</span>: <span class="number">100</span>,</span><br><span class="line">                <span class="string">'lstm_dim'</span>: <span class="number">100</span>,</span><br><span class="line">                <span class="string">'l2'</span>: <span class="number">0.0001</span>,</span><br><span class="line">                <span class="string">'n_tags'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">train_config = &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>,</span><br><span class="line">                <span class="string">'epoch'</span>: <span class="number">100</span>,</span><br><span class="line">                <span class="string">'batch_size'</span>: <span class="number">64</span>,</span><br><span class="line">                <span class="string">'clip_gradients'</span>: <span class="number">5.</span>&#125;</span><br><span class="line"></span><br><span class="line">tag_dict = &#123;<span class="string">'O'</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">'B-PER'</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">'I-PER'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">train_data_path = <span class="string">'./data/NER/train.txt'</span>  <span class="comment"># 训练集路径</span></span><br><span class="line">dev_data_path = <span class="string">'./data/NER/dev.txt'</span>  <span class="comment"># 验证集路径</span></span><br><span class="line"></span><br><span class="line">vocab_path = <span class="string">'./data/NER/vocab.txt'</span>  <span class="comment"># 词表保存路径</span></span><br><span class="line"></span><br><span class="line">train_format_path = <span class="string">'./data/NER/train_pre.txt'</span>  <span class="comment"># 训练集转换格式后路径</span></span><br><span class="line">dev_format_path = <span class="string">'./data/NER/dev_pre.txt'</span>  <span class="comment"># 验证集转换格式后路径</span></span><br><span class="line"></span><br><span class="line">pre_vec_path = <span class="string">'./data/NER/word2vec.txt'</span>  <span class="comment"># 预训练词向量路径</span></span><br><span class="line"></span><br><span class="line">model_save_path = <span class="string">'./model/NER/'</span>  <span class="comment"># 模型保存路径</span></span><br><span class="line"></span><br><span class="line">log_info_path = <span class="string">'./log/ner_train.log'</span>  <span class="comment"># 日志保存路径</span></span><br><span class="line"></span><br><span class="line">is_log = <span class="literal">True</span>  <span class="comment"># 是否保存日志, 否则控制台输出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_log:</span><br><span class="line">    logging.basicConfig(filename=log_info_path, level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_</span><span class="params">(msg)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_log:</span><br><span class="line">        logging.info(msg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(msg)</span><br><span class="line"></span><br><span class="line">vocab = init_vocab(train_data_path, vocab_path)</span><br><span class="line">format_data(train_data_path, vocab_path, train_format_path)</span><br><span class="line">format_data(dev_data_path, vocab_path, dev_format_path)</span><br><span class="line"></span><br><span class="line">train_dataset = init_dataset(train_format_path)</span><br><span class="line">dev_dataset = init_dataset(dev_format_path)</span><br><span class="line"></span><br><span class="line">embedding, embedding_size = init_embedding(pre_vec_path, vocab_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">model = BiLSTMCRFModel(embeddings=embedding, vocab_size=len(vocab), config=model_config)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(lr=train_config[<span class="string">'lr'</span>], clipvalue=train_config[<span class="string">'clip_gradients'</span>])</span><br><span class="line"></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.Accuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line">train_metric = ModelMetric(model_config[<span class="string">'n_tags'</span>])</span><br><span class="line"></span><br><span class="line">dev_loss = tf.keras.metrics.Mean(name=<span class="string">'dev_loss'</span>)</span><br><span class="line">dev_accuracy = tf.keras.metrics.Accuracy(name=<span class="string">'dev_accuracy'</span>)</span><br><span class="line">dev_metric = ModelMetric(model_config[<span class="string">'n_tags'</span>])</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, model_save_path, max_to_keep=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print_(<span class="string">'已重新加载上一次的模型.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成mask, 用以标记有效序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_padding_mask</span><span class="params">(seq)</span>:</span></span><br><span class="line">    mask = tf.math.logical_not(tf.math.equal(seq, <span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(mask, tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(seq, tags)</span>:</span></span><br><span class="line">    <span class="comment"># inp.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    <span class="comment"># tar.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    padding_mask = create_padding_mask(seq)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred, potentials = model(seq, padding_mask)  <span class="comment"># (batch_size, max_seq_len)</span></span><br><span class="line">        loss = loss_function(tags, potentials, padding_mask, model.crf_layer.trans_params)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(tags, pred, padding_mask)</span><br><span class="line">    train_metric(tags, pred, padding_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># eval step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(seq, tags)</span>:</span></span><br><span class="line">    <span class="comment"># inp.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    <span class="comment"># tar.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    padding_mask = create_padding_mask(seq)</span><br><span class="line">    pred, potentials = model(seq, padding_mask)  <span class="comment"># (batch_size, max_seq_len)</span></span><br><span class="line">    loss = loss_function(tags, potentials, padding_mask, model.crf_layer.trans_params)</span><br><span class="line">    dev_loss(loss)</span><br><span class="line">    dev_accuracy(tags, pred, padding_mask)</span><br><span class="line">    dev_metric(tags, pred, padding_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dev_accuracy.reset_states()</span><br><span class="line">    dev_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (seq, tags) <span class="keyword">in</span> enumerate(dataset):</span><br><span class="line">        eval_step(seq, tags)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">best_dev = -np.inf</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_config[<span class="string">'epoch'</span>]):</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    dev_accuracy.reset_states()</span><br><span class="line">    dev_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (seq, tags) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(seq, tags)</span><br><span class="line"></span><br><span class="line">    evaluate(dev_dataset)</span><br><span class="line">    macro_f1 = tf.math.reduce_mean(dev_metric.result()[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> macro_f1 &gt; best_dev:</span><br><span class="line">        best_dev = macro_f1</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print_(<span class="string">'保存模型%s'</span> % ckpt_save_path)</span><br><span class="line"></span><br><span class="line">    print_(<span class="string">'第%d轮, 训练集, 误差为%.4f, 准确率为%.4f'</span> % (epoch + <span class="number">1</span>, train_loss.result(), train_accuracy.result()))</span><br><span class="line">    print_(classification_report(train_metric))</span><br><span class="line">    print_(<span class="string">'第%d轮, 验证集, 误差为%.4f, 准确率为%.4f'</span> % (epoch + <span class="number">1</span>, dev_loss.result(), dev_accuracy.result()))</span><br><span class="line">    print_(classification_report(dev_metric))</span><br><span class="line">    print_(<span class="string">'目前最佳F1为%.4f'</span> % best_dev)</span><br><span class="line">    print_(<span class="string">'#'</span> * <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">print_(<span class="string">'验证集最高F1为%.4f'</span> % best_dev)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">测试.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">test_data_path = <span class="string">'./data/NER/test.txt'</span>  <span class="comment"># 训练集路径</span></span><br><span class="line"></span><br><span class="line">vocab_path = <span class="string">'./data/NER/vocab.txt'</span>  <span class="comment"># 词表保存路径</span></span><br><span class="line"></span><br><span class="line">test_format_path = <span class="string">'./data/NER/test_pre.txt'</span>  <span class="comment"># 训练集转换格式后路径</span></span><br><span class="line"></span><br><span class="line">model_save_path = <span class="string">'./model/NER/'</span>  <span class="comment"># 模型保存路径</span></span><br><span class="line"></span><br><span class="line">log_info_path = <span class="string">'./log/ner_test.log'</span>  <span class="comment"># 日志保存路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">format_data(test_data_path, vocab_path, test_format_path)</span><br><span class="line"></span><br><span class="line">test_dataset = init_dataset(test_format_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span><span class="params">(vocab_path)</span>:</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(vocab_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> f.readlines():</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        f.close()</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = BiLSTMCRFModel(embeddings=<span class="literal">None</span>, vocab_size=vocab_size(vocab_path), config=model_config)</span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(model=model)</span><br><span class="line">ckpt.restore(tf.train.latest_checkpoint(model_save_path))</span><br><span class="line"></span><br><span class="line">test_accuracy = tf.keras.metrics.Accuracy(name=<span class="string">'test_accuracy'</span>)</span><br><span class="line">test_metric = ModelMetric(model_config[<span class="string">'n_tags'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_step</span><span class="params">(seq, tags)</span>:</span></span><br><span class="line">    <span class="comment"># inp.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    <span class="comment"># tar.shape == (batch_size, max_seq_len)</span></span><br><span class="line">    padding_mask = create_padding_mask(seq)</span><br><span class="line">    pred, potentials = model(seq, padding_mask)  <span class="comment"># (batch_size, max_seq_len)</span></span><br><span class="line">    <span class="comment"># loss = loss_function(tags, potentials, padding_mask, model.crf_layer.trans_params)</span></span><br><span class="line">    test_accuracy(tags, pred, padding_mask)</span><br><span class="line">    test_metric(tags, pred, padding_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    test_accuracy.reset_states()</span><br><span class="line">    test_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch, (seq, tags) <span class="keyword">in</span> enumerate(dataset):</span><br><span class="line">        test_step(seq, tags)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始测试</span></span><br><span class="line">evaluate(test_dataset)</span><br><span class="line"></span><br><span class="line">print_(<span class="string">'测试集准确率为%.4f'</span> % test_accuracy.result())</span><br><span class="line">print_(classification_report(test_metric))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO:root:保存模型./model/NER/ckpt-1</span><br><span class="line">INFO:root:第1轮, 训练集, 误差为9.2335, 准确率为0.9638</span><br><span class="line">INFO:root:	accuracy: [0.9653737 0.9861694 0.9760488]</span><br><span class="line">            precison: [0.97286296 0.08592593 0.31056252]</span><br><span class="line">            recall: [0.99191165 0.023213   0.14678898]</span><br><span class="line">            F1 score: [0.9822949  0.03655155 0.19935282]</span><br><span class="line"></span><br><span class="line">INFO:root:第1轮, 验证集, 误差为5.2861, 准确率为0.9717</span><br><span class="line">INFO:root:	accuracy: [0.97374886 0.9897589  0.97993827]</span><br><span class="line">            precison: [0.9775103  0.7192118  0.47381932]</span><br><span class="line">            recall: [0.99583507 0.10900946 0.25839865]</span><br><span class="line">            F1 score: [0.9865876  0.18932354 0.3344203 ]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.5034</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:保存模型./model/NER/ckpt-2</span><br><span class="line">INFO:root:第2轮, 训练集, 误差为4.7165, 准确率为0.9739</span><br><span class="line">INFO:root:	accuracy: [0.97575754 0.9905842  0.9814063 ]</span><br><span class="line">            precison: [0.9794613  0.7741257  0.57181716]</span><br><span class="line">            recall: [0.99584836 0.23565197 0.33704463]</span><br><span class="line">            F1 score: [0.9875869  0.36131567 0.42410827]</span><br><span class="line"></span><br><span class="line">INFO:root:第2轮, 验证集, 误差为4.6032, 准确率为0.9777</span><br><span class="line">INFO:root:	accuracy: [0.9792039  0.99197316 0.98429847]</span><br><span class="line">            precison: [0.9824964  0.79325354 0.66066897]</span><br><span class="line">            recall: [0.99629974 0.3628671  0.40089586]</span><br><span class="line">            F1 score: [0.98934996 0.49795082 0.4989982 ]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.6621</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:保存模型./model/NER/ckpt-3</span><br><span class="line">INFO:root:第3轮, 训练集, 误差为3.7195, 准确率为0.9794</span><br><span class="line">INFO:root:	accuracy: [0.9807477  0.99257445 0.9854882 ]</span><br><span class="line">            precison: [0.9839779  0.808229   0.72045374]</span><br><span class="line">            recall: [0.9963426  0.44969183 0.46668744]</span><br><span class="line">            F1 score: [0.99012166 0.57786465 0.56644773]</span><br><span class="line"></span><br><span class="line">INFO:root:第3轮, 验证集, 误差为4.1696, 准确率为0.9819</span><br><span class="line">INFO:root:	accuracy: [0.98296887 0.9934311  0.9874082 ]</span><br><span class="line">            precison: [0.9862652 0.8071646 0.7572125]</span><br><span class="line">            recall: [0.99630815 0.5271279  0.5216965 ]</span><br><span class="line">            F1 score: [0.9912613  0.6377597  0.61776894]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.7489</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:保存模型./model/NER/ckpt-4</span><br><span class="line">INFO:root:第4轮, 训练集, 误差为3.1638, 准确率为0.9826</span><br><span class="line">INFO:root:	accuracy: [0.98358834 0.9936619  0.988014  ]</span><br><span class="line">            precison: [0.98652077 0.8270354  0.7960759 ]</span><br><span class="line">            recall: [0.9966705 0.555351  0.5511267]</span><br><span class="line">            F1 score: [0.99156964 0.66449577 0.6513329 ]</span><br><span class="line"></span><br><span class="line">INFO:root:第4轮, 验证集, 误差为3.8511, 准确率为0.9843</span><br><span class="line">INFO:root:	accuracy: [0.9852049  0.9941109  0.98919374]</span><br><span class="line">            precison: [0.988604   0.81746846 0.7899527 ]</span><br><span class="line">            recall: [0.9962237 0.5963166 0.6075028]</span><br><span class="line">            F1 score: [0.9923992 0.6895957 0.6868176]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.7896</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:保存模型./model/NER/ckpt-5</span><br><span class="line">INFO:root:第5轮, 训练集, 误差为2.7696, 准确率为0.9847</span><br><span class="line">INFO:root:	accuracy: [0.98550445 0.9942861  0.98953205]</span><br><span class="line">            precison: [0.9882965  0.8405557  0.82722956]</span><br><span class="line">            recall: [0.9968358  0.6101817  0.61263025]</span><br><span class="line">            F1 score: [0.9925478  0.7070772  0.70393777]</span><br><span class="line"></span><br><span class="line">INFO:root:第5轮, 验证集, 误差为3.6003, 准确率为0.9859</span><br><span class="line">INFO:root:	accuracy: [0.98673654 0.9946296  0.9903541 ]</span><br><span class="line">            precison: [0.99010414 0.8326306  0.80784315]</span><br><span class="line">            recall: [0.9962772  0.63887507 0.6632139 ]</span><br><span class="line">            F1 score: [0.9931811  0.7229968  0.72841877]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.8149</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:保存模型./model/NER/ckpt-6</span><br><span class="line">INFO:root:第6轮, 训练集, 误差为2.4751, 准确率为0.9861</span><br><span class="line">INFO:root:	accuracy: [0.98689765 0.9947321  0.99058235]</span><br><span class="line">            precison: [0.98965365 0.85105264 0.8418095 ]</span><br><span class="line">            recall: [0.9968919  0.64716244 0.66050595]</span><br><span class="line">            F1 score: [0.9932596 0.735234  0.7402176]</span><br><span class="line"></span><br><span class="line">INFO:root:第6轮, 验证集, 误差为3.3951, 准确率为0.9870</span><br><span class="line">INFO:root:	accuracy: [0.9877686  0.9950474  0.99114317]</span><br><span class="line">            precison: [0.99115264 0.84459037 0.8182115 ]</span><br><span class="line">            recall: [0.9962772  0.672225   0.70184773]</span><br><span class="line">            F1 score: [0.9937084  0.74861413 0.75557566]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.8326</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">INFO:root:第98轮, 训练集, 误差为0.3043, 准确率为0.9985</span><br><span class="line">INFO:root:	accuracy: [0.9985254  0.99937665 0.9989985 ]</span><br><span class="line">            precison: [0.99898595 0.98472404 0.9801611 ]</span><br><span class="line">            recall: [0.9994918  0.9597375  0.97033936]</span><br><span class="line">            F1 score: [0.99923885 0.9720702  0.97522545]</span><br><span class="line"></span><br><span class="line">INFO:root:第98轮, 验证集, 误差为1.2113, 准确率为0.9949</span><br><span class="line">INFO:root:	accuracy: [0.9951129  0.99803424 0.9966254 ]</span><br><span class="line">            precison: [0.99715203 0.9241255  0.9196023 ]</span><br><span class="line">            recall: [0.9978091  0.89422596 0.906215  ]</span><br><span class="line">            F1 score: [0.9974805  0.9089298  0.91285956]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.9407</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:第99轮, 训练集, 误差为0.2990, 准确率为0.9985</span><br><span class="line">INFO:root:	accuracy: [0.99857515 0.99939656 0.9990338 ]</span><br><span class="line">            precison: [0.9990298 0.9851493 0.9804547]</span><br><span class="line">            recall: [0.99949926 0.9610982  0.971809  ]</span><br><span class="line">            F1 score: [0.9992645  0.97297513 0.9761127 ]</span><br><span class="line"></span><br><span class="line">INFO:root:第99轮, 验证集, 误差为1.2084, 准确率为0.9949</span><br><span class="line">INFO:root:	accuracy: [0.9950965 0.998037  0.9966118]</span><br><span class="line">            precison: [0.9971604 0.9237092 0.9188307]</span><br><span class="line">            recall: [0.9977838  0.8949726  0.90635496]</span><br><span class="line">            F1 score: [0.99747205 0.9091139  0.91255015]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.9407</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:第100轮, 训练集, 误差为0.2939, 准确率为0.9985</span><br><span class="line">INFO:root:	accuracy: [0.99861497 0.99941194 0.99906003]</span><br><span class="line">            precison: [0.9990504 0.9857272 0.9811711]</span><br><span class="line">            recall: [0.9995198  0.9618987  0.97238797]</span><br><span class="line">            F1 score: [0.99928504 0.9736672  0.9767598 ]</span><br><span class="line"></span><br><span class="line">INFO:root:第100轮, 验证集, 误差为1.2056, 准确率为0.9949</span><br><span class="line">INFO:root:	accuracy: [0.99510473 0.9980424  0.99662   ]</span><br><span class="line">            precison: [0.9971688  0.9239661  0.91886526]</span><br><span class="line">            recall: [0.9977838  0.89522153 0.90677494]</span><br><span class="line">            F1 score: [0.99747616 0.9093667  0.91278005]</span><br><span class="line"></span><br><span class="line">INFO:root:目前最佳F1为0.9407</span><br><span class="line">INFO:root:##########################################</span><br><span class="line">INFO:root:验证集最高F1为0.9407</span><br><span class="line">INFO:root:测试集准确率为0.9953</span><br><span class="line">INFO:root:	accuracy: [0.9955521  0.99814945 0.99691033]</span><br><span class="line">            precison: [0.9972337 0.9250421 0.9257107]</span><br><span class="line">            recall: [0.9981939  0.88766164 0.8980649 ]</span><br><span class="line">            F1 score: [0.99771357 0.90596646 0.9116783 ]</span><br><span class="line"></span><br><span class="line">INFO:root:测试集F1为0.9385</span><br></pre></td></tr></table></figure>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集F1</th>
<th style="text-align:center">测试集F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CRF</td>
<td style="text-align:center">0.933</td>
<td style="text-align:center">0.930</td>
</tr>
<tr>
<td style="text-align:center">BiLSTM-CRF</td>
<td style="text-align:center">0.941</td>
<td style="text-align:center">0.939</td>
</tr>
</tbody>
</table>
</div>
<p>从结果可以看到, BiLSTM-CRF相比单纯的CRF, 在F1上高了近一个百分点.</p>
<p>通过前面的原理介绍, 以及这里的代码实践, 对CRF的由来, 具体如何运算有了一个认识. 同时通过与深度学习的结合, 也确实获得了提升. 关于CRF这一篇就告一段落了, 但是关于序列标注, 或者NER还有没有其它一些可能更好的方法呢, 以后再说~</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>条件随机场CRF(二)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%BA%8C/</url>
    <content><![CDATA[<p>这一篇主要来讲条件随机场CRF的原理, 即模型结构, 推断问题以及学习问题, 会比较硬核一些.</p>
<a id="more"></a>
<h1 id="CRF的原理"><a href="#CRF的原理" class="headerlink" title="CRF的原理"></a>CRF的原理</h1><h2 id="Log-linear-model"><a href="#Log-linear-model" class="headerlink" title="Log-linear model"></a>Log-linear model</h2><p>为什么这里会说到log-linear model呢, log-linear model 又是啥呢ヽ(✿ﾟ▽ﾟ)ノ</p>
<p>有木有想过一个问题, 为什么HMM是有向图生成模型, 为什么CRF是无向图判别模型, 有向图与生成, 无向图与判别, 是否有什么联系呢? 是否生成模型一定要拿有向图来做, 判别模型一定要拿无向图来做呢? 答案是否定的, 但是用对了姿势后, 很多东西就会显得比较自然. 一下是我个人的一些理解, 不一定对.</p>
<p>仍然还是这张图:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>在处理非序列数据时, 作为生成模型, 即优化联合概率$P(X,Y)$的朴素贝叶斯模型, 如果在假设中的$X$是序列, 然后将”朴素”变为马尔科夫, 会怎么样呢? 根据概率展开的链式法则, 会得到:</p>
<script type="math/tex; mode=display">
P(X|Y)=\prod P(x_{t+1}|x_t,Y)</script><p>这个形式, 不就与有向图的概率分布类似吗, 因此在由生成模型的朴素贝叶斯, 序列化后, 使用有向图作为模型的结构, 是很自然的做法.</p>
<p>同理, 作为判别模型的逻辑回归$P(Y|X)$, 本质上, 是一个log-linear model的特例, log-linear model的标准形式为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(Y|X;w)&=\frac{\exp(\sum_{i}w_if_i(X,Y))}{Z(X,w)} \\
&=\frac{\prod_i\exp(w_if_i(X,Y))}{Z(X,w)}
\end{aligned}</script><p>之所以叫做”log-linear”, 其实就是因为取$\log$后会变成线性模型.</p>
<p>再联系前面所述的无向图的联合概率的表示, 有没有发现和上面第二个等式后的形式类似呀. 所以, 在从判别模型的逻辑回归, 经过序列化后, 采用无向图来作为模型的结构, 也是很自然的做法.</p>
<p>因此, CRF的具体的概率公式为:</p>
<script type="math/tex; mode=display">
P(Y|X)=\frac{\exp (w\cdot\phi(Y,X))}{Z(X,w)}</script><p>具体的含义与推导, 在后文中会详细讲述.</p>
<h2 id="整体理解"><a href="#整体理解" class="headerlink" title="整体理解"></a>整体理解</h2><p>我想很多刚接触CRF的同学, 一来就看到一大堆各种理论与公式, 八成会灵魂三问, 我是谁, 我在哪, 我要做什么?</p>
<p>所以我认为在推导具体的数学公式之前, 先用少量的公式, 结合比较具体的, 简单的栗子, 来展示CRF的用法, 是一件有必要的事情. 在众多的资料中, 我认为李宏毅老师对于CRF的讲解是很好的, 这部分的内容多数参考了他的课程.</p>
<p>前面说了, CRF属于无向图判别模型, 结合上面所讲的log-linear model, 将其中的$X, Y$换成序列, 对应的无向图联合概率可以用如下数学公式描述:</p>
<script type="math/tex; mode=display">
P(Y, X)=\frac{\exp (w\cdot\phi(Y,X))}{R}</script><p>其中的$R$表示归一项, 是一个常数; $w$表示权重系数向量, 是模型参数; $\phi(Y,X)$是关于该无向图的特征向量, 那么条件概率为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(Y|X)&=\frac{P(Y,X)}{P(X)} \\
&=\frac{P(Y,X)}{\sum\limits_{Y'}P(Y',X)} \\
&=\frac{\exp (w\cdot\phi(Y,X))}{\sum\limits_{Y'}\exp (w\cdot\phi(Y',X))} \\
&=\frac{\exp (w\cdot\phi(Y,X))}{Z(X,w)}
\end{aligned}</script><p>上面的$Z(X)$是条件概率的归一项, 与给定的$X$和$w$有关.</p>
<p>我们知道$\exp (w\cdot\phi(Y,X))$这一项, 表示一个无向图的分数, 但是为什么要写成这样的形式呢? 可以联系HMM得到答案, 在HMM中, 联合概率分布的对数可以简易地表示为:</p>
<script type="math/tex; mode=display">
\log P(Y,X)=\sum_t \log P(y_{t+1}|y_t)+\sum_t\log P(x_t|y_t)</script><p>将该公式进行改写, 转移概率部分改写如下:</p>
<script type="math/tex; mode=display">
\sum_t \log P(y_{t+1}|y_t)=\sum_{y,y'}\log P(y'|y)\times N_{y,y'}(Y,X)</script><p>发射概率部分改写如下:</p>
<script type="math/tex; mode=display">
\sum_t\log P(x_t|y_t)=\sum_{y,x}\log P(x|y)\times N_{y,x}(Y,X)</script><p>也就是说, 将原本按时间顺序求和的形式, 转变为了对所有模式求和的形式, 其中的$N$为整数, 表示当前$X,Y$中, 出现该模式($y\to y’$或$y\to s$)的次数.</p>
<p>由于改写后, 是对不同的特征模式进行求和, 因此每一个子项, 代表了一种独特的模式, 所以可以将$\log P$和$N$分别用向量来表示, 最终结果为两个向量的内积, 即:</p>
<script type="math/tex; mode=display">
\log P(Y,X)=w\cdot \phi(Y,X) \\
P(Y,X)=\exp(w\cdot \phi(Y,X))</script><p>这就和CRF的公式联系了起来, 但是还有一个问题需要注意, 在HMM中$w$等同于条件概率$\log P$, 但在CRF中, $\exp(w)$可能大于1, 这是由于两者本质上一个属于有向图, 一个属于无向图引起的, 所以CRF还有一个归一项$R$:</p>
<script type="math/tex; mode=display">
P(Y, X)=\frac{\exp (w\cdot\phi(Y,X))}{R}</script><p>上面中说的$\phi(Y,X)$就是CRF的特征, $w$就是参数. 那么$\phi$是否一定要取和HMM类似的呢? 是不用的哦, 取和HMM类似的特征, 属于比较简单的CRF, 此外也完全可以取其它各种各样的特征, 比如HMM仅考虑当前状态节点和当前观察节点的关系, 在CRF中, 还可以考虑当前状态节点与前一个观察节点的关系等.</p>
<p>不过虽然理论上可以设计各种不同的复杂特征, 但是存在的问题是, 设计的特征越复杂, 在进行推断或者学习的时候, 就越困难.</p>
<h2 id="推断问题"><a href="#推断问题" class="headerlink" title="推断问题"></a>推断问题</h2><p>对于推断问题来说, 就是给定CRF的模型参数$w$和观察序列$X$, 求解目标序列$Y$. 这里的一个难点是, 对于条件概率分布$P(Y|X;w)$, 要对所有可能的$Y$进行计算, 使用暴力法肯定是不行的, 需要使用动态规划的算法.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat Y&=arg\max_Y P(Y|X;w) \\
&=arg\max_Y \sum_j w_j\times F_j(Y,X) \\
&=arg\max_Y \sum_j w_j\times \sum_{t=2}^Tf_j(y_{t-1},y_t,X) \\
&=arg\max_Y \sum_{t=2}^Tg(y_{t-1},y_t,X) \\
\end{aligned}</script><p>其中, $F_j(Y,X)=\sum_{t=2}^Tf_j(y_{t-1},y_t,X)$, $f_j(y_{t-1},y_t,X)$表示第$j$个特征模式是否在$(y_{t-1},y_t,X)$中存在, 是则为1, 否则为0. </p>
<p>同时最后转换成按时间步求和的形式, 这很重要, 这为运用动态规划算法做准备:</p>
<script type="math/tex; mode=display">
g(y_{t-1},y_t,X)=\sum_j w_j\times f_j(y_{t-1},y_t,X)</script><p>接下来, 再假设一个函数$h(k,v)$, 表示从$t=2$到$t=k$, 且第$k$个状态节点为$v$时, 最大的$\sum_{t=2}^kg_t(y_{t-1},y_t,X)$, 并由此可以得到其递推公式:</p>
<script type="math/tex; mode=display">
h(2,v)=\max_u(g_k(u,v,X)) \\
h(k,v)=\max_u(h(k-1,u)+g_k(u,v,X))</script><p>这样, 最后得到一系列的$h(T,v)$, 选取最大的一个对应的$Y$, 作为最优结果返回即可.</p>
<h2 id="学习问题"><a href="#学习问题" class="headerlink" title="学习问题"></a>学习问题</h2><p>现在已经知道了怎么解决推断问题, 下面进一步来看参数学习的问题.</p>
<p>不同于HMM根据情况既可以使用监督学习的极大似然来做, 也可以使用EM算法来做, CRF只能使用监督学习的方法.</p>
<p>主要的思路是利用梯度下降法, 即要对每个参数$w_j$求偏导, 难点是涉及到配分函数, 即归一化项.</p>
<p>下面来进行推导:</p>
<script type="math/tex; mode=display">
P(Y|X;w)=\frac{\exp\sum_j w_j\times F_j(Y,X)}{Z(X,w)} \\
Z(X,w)=\sum_{Y'}\exp\sum_j w_j\times F_j(Y',X)</script><p>对参数$w_i$求偏导:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial\log P(Y|X;w)}{\partial w_i}&=\frac{\partial}{\partial w_i}(\sum_j w_j\times F_j(Y,X)-\log Z(X,w)) \\
&=F_i(Y,X)-\frac{1}{Z(X,w)}\frac{\partial}{\partial w_i}Z(X,w) \\
&=F_i(Y,X)-\frac{1}{Z(X,w)}\frac{\partial}{\partial w_i}(\sum_{Y'}\exp\sum_j w_j\times F_j(Y',X)) \\
&=F_i(Y,X)-\frac{1}{Z(X,w)}\sum_{Y'}\frac{\partial}{\partial w_i}(\exp\sum_j w_j\times F_j(Y',X)) \\
&=F_i(Y,X)-\frac{1}{Z(X,w)}\sum_{Y'}[(\exp\sum_j w_j\times F_j(Y',X))\times F_i(Y,X)]\\
&=F_i(Y,X)-\sum_{Y'}[F_i(Y,X)\times \frac{(\exp\sum_j w_j\times F_j(Y',X))}{Z(X,w)}]\\
&=F_i(Y,X)-\sum_{Y'}F_i(Y,X)\times P(Y'|X;w) \\
&=F_i(Y,X)- \mathbb{E}_{P(Y'|X;w)}F_i(Y,X) \\
\end{aligned}</script><p>以上就是对数条件概率求偏导的结果, 其结果可以看做是若该特征模式在本条样本中出现较多, 而在其它可能的$Y’$中出现较少, 则增加$w_i$, 反之亦然.</p>
<p>现在的关键是$Z(X,w)$, 下面利用动态规划来进行计算:</p>
<script type="math/tex; mode=display">
\begin{aligned}
Z(X,w)&=\sum_{Y'}\exp\sum_j w_j\times F_j(Y',X) \\
&=\sum_{Y'}\exp\sum_j w_j\times \sum_{t=2}^Tf_j(y_{t-1},y_t,X) \\
&=\sum_{Y'}\exp \sum_{t=2}^Tg(y_{t-1},y_t,X)
\end{aligned}</script><p>那么类似HMM那里的前向法, 设定一个函数$\alpha(k,v)$, 表示在$t=k$时状态节点为$v$的上面公式的和$\sum_{Y’}\exp \sum_{t=2}^kg_t(y_{t-1},y_t,X)$, 对应的递推公式为:</p>
<script type="math/tex; mode=display">
\alpha(2,v)=\sum_{u}\exp g(u,v,X) \\
\alpha(k+1,v)=\sum_u\alpha(k,u)\times\exp g(u,v)</script><p>由此, 最后可以通过对所有的$\alpha(T,v)$求和, 得到$Z(X,w)$.</p>
<p>如果利用后向法, 则可以设定一个函数$\beta(k,v)$, 表示在$t=k$时状态节点为$v$的上面公式的和$\sum_{Y’}\exp \sum_{t=k}^Tg(y_{t-1},y_t,X)$.</p>
<p>这样, 配分函数可以利用前向函数和后向函数进行表示:</p>
<script type="math/tex; mode=display">
Z(X,w)=\sum_u\alpha(k,u)\beta(k,u)</script><p>在有了以上的结果后, 进一步改写求偏导的公式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial\log P(Y|X;w)}{\partial w_i}&=F_i(Y,X)- \mathbb{E}_{P(Y'|X;w)}F_i(Y,X) \\
&=F_i(Y,X)- \mathbb{E}_{P(Y'|X;w)}[\sum_{t=2}^Tf_i(y_{t-1},y_t,X)] \\
&=F_i(Y,X)- \sum_{t=2}^T\mathbb{E}_{P(y_{t-1},y_t|X;w)}[f_i(y_{t-1},y_t,X)] \\
&=F_i(Y,X)- \sum_{t=2}^T\sum_{y_{t-1}}\sum_{y_t}f_i(y_{t-1},y_t,X)\times P(y_{t-1},y_t|X;w) \\
&=F_i(Y,X)- \sum_{t=2}^T\sum_{y_{t-1}}\sum_{y_t}f_i(y_{t-1},y_t,X)\times \frac{\alpha(t-1,y_{t-1})\times \exp[g(y_{t-1},y_t,X)]\times \beta(t,y_t)}{Z(X,w)} \\
\end{aligned}</script><p>是的, 上面就是最终可以通过动态规划, 得到的最终的对数条件概率$\log P(Y|X;w)$对参数$w_i$求偏导的结果. 有了这个结果, 再使用梯度下降法就可以进行学习了.</p>
<h1 id="CRF与其它模型"><a href="#CRF与其它模型" class="headerlink" title="CRF与其它模型"></a>CRF与其它模型</h1><h2 id="CRF与HMM"><a href="#CRF与HMM" class="headerlink" title="CRF与HMM"></a>CRF与HMM</h2><p>在前面的文章中讲解HMM时, 就提到过HMM由于其模型本身的假设, 导致其存在一些问题, 其中相对而言比较严重的问题是, HMM只是让已知的正确序列的联合概率变大, 但不能保证正确序列的联合概率大于其它序列的联合概率, 即:</p>
<script type="math/tex; mode=display">
P(Y,O)?\ge P(Y',O)</script><p>但是CRF可以很大程度上解决这个问题, 通过CRF学习过程中, 参数更新的公式就可以看出, CRF在更新参数$w$时, 若正确(存在)序列中有对应特征, 则$w$增加, 若错误(不存在)序列中有对应特征, 则$w$减少. 通过调整$w$, CRF可以尽可能地保证$P(Y,O)\ge P(Y’,O)$.</p>
<p>此外, CRF模型的复杂度, 可以根据特征的设计而调整, 若同样设计序列$Y$的马尔科夫转移特征, 以及序列$Y$与$X$之间的发射特征, 那么CRF与HMM就非常相似, 这是上文中也提到的.</p>
<p>作为判别模型的CRF, 相比生成模型的HMM, 虽然不能模拟数据分布, 生成序列样本, 但是通过有监督学习获得的模型参数, 一般也可能让CRF获得更好的表现.</p>
<h2 id="CRF与RNN"><a href="#CRF与RNN" class="headerlink" title="CRF与RNN"></a>CRF与RNN</h2><p>这里与其说比较CRF与RNN, 不如说以它们作为代表模型, 比较经典序列模型(如HMM, CRF, Structured Perceptron等), 与深度学习序列模型(如RNN, LSTM, GRU).</p>
<p>那么经典序列模型, 相比深度学习序列模型, 有哪些优点呢? 我认为可以有如下一些优点:</p>
<p><strong>*</strong> 利用动态规划算法, 考虑到了整个序列的信息, 而一般的RNN只能考虑单向信息(BiLSTM一定程度上可以考虑双向信息).</p>
<p><strong>*</strong> 想学习和推断时, 除了考虑特征序列$X$, 也会考虑目标序列$Y$, 学习目标序列$Y$的内在模式, 会在所有可能的目标序列$Y$中, 挑选出最优解. 而类RNN模型, 在学习和推断时, 是没有考虑到目标序列$Y$的内在模式的, 仅仅学习了$X$与$Y$的关系, 这样在一些时候, 可能会推断出一些明显不合理的结果.</p>
<p><strong>*</strong> 由于基于统计理论, 随着优化的进行, 可以保证模型的表现确实是会逐渐变好. 但是深度学习模型在这里并不能保证随着损失函数的优化, 模型表现总是会变好.</p>
<p>再说深度学习序列模型的优点:</p>
<p><strong>*</strong> 模型学习与表达能力强, 在数据足够多的时候, 可以获得很好的表现.</p>
<p>说了这么多, 如果单独分别用经典序列模型与深度学习序列模型进行建模, 谁的表现好呢? 不出意外是深度学习序列模型Ｏ(≧口≦)Ｏ</p>
<p>纳尼???扯了这么多, 然后还是深度学习模型厉害Σ(っ °Д °;)っ</p>
<p>那我们为什么还要费这么大劲学这堆过时的玩意（╯‵□′）╯︵┴─┴</p>
<p>来跟我一起大喊: 深度学习, 永远滴神!</p>
<p>其实…还是有用的, 虽然单独拿出来, 是深度学习模型占优, 但是经典序列模型的优点也是确实存在不能忽视的. 所以, 一种比较好的做法是, 将两者进行结合, 比如利用Bi-LSTM+CRF的结构, 能够同时获得两类模型的优点, 获得更好的性能, 这个将会在下一篇中进行讲解.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>条件随机场CRF(一)</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%80/</url>
    <content><![CDATA[<p>好, 啊今天来康康传说中的条件随机场CRF(Conditional Random Fields).</p>
<p>有一说一, 我认为CRF是自然语言处理, 甚至机器学习算法中, 比较难的一个算法了. 当初在学习李航的&lt;统计机器学习&gt;时, 我真的看不懂它在干什么QAQ</p>
<p>后来在一个学习群(你们懂的)聊天时, 我说CRF很难, 一位群友说”CRF? 特别简单好吧”, 只能说群友藏龙卧虎, 我菜得真实QAQ.</p>
<a id="more"></a>
<p>本篇主要介绍CRF的由来, 能够解决的问题. 理论推导(主要包括推断和学习), 以及和HMM, RNN等模型的联系将在下一篇讲解.</p>
<p>同时, 由于CRF涉及到的知识面其实很广, 我不可能在这里面面俱到, 一些地方可能会比较简略(我也没懂透QAQ</p>
<p>这里还推荐两篇论文, 会非常系统地讲解CRF有关的方方面面, 一篇叫做<a href="https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf" target="_blank" rel="noopener">An Introduction to Conditional Random Fields</a>, 还有一篇叫做<a href="https://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf" target="_blank" rel="noopener">Log-linear models and conditional random fields</a>, 有空的同学可以阅读.</p>
<h1 id="CRF简介"><a href="#CRF简介" class="headerlink" title="CRF简介"></a>CRF简介</h1><p>这一节没啥公式, 主要介绍一些基础概念.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>首先来康康上面这张图, 在这张图中, 主要描述的是各个模型之间的关系.</p>
<p>第一排从左到右, 分别是朴素贝叶斯, 序列化后, 变成HMM, 进一步泛化后, 变成生成有向图模型.</p>
<p>由第一排条件(判别)化后, 可以得到第二排的模型, 从左到右分别是逻辑回归, 序列化后, 变成线性链CRF, 进一步泛化后, 变成判别无向图模型.</p>
<p>这里涉及到两个概念, 一个是有向图/无向图, 另一个是生成/判别模型, 下面分别进行简要介绍.</p>
<h2 id="有向图与无向图"><a href="#有向图与无向图" class="headerlink" title="有向图与无向图"></a>有向图与无向图</h2><p>图可以看成是一种数据结构, 在统计当中, 可以用于表示随机变量之间的关系, 由节点和连边组成.</p>
<p>有向图, 指的是节点之间的关系存在方向性, 如HMM. 而在计算有向图所对应的概率的时候, 是相对比较简单和直观的. 回想HMM的联合概率计算方式:</p>
<script type="math/tex; mode=display">
P(X,Y)=\prod A_{i\to j}B_{j\to k}</script><p>即从初始节点对应的先验概率开始, 根据节点的依赖关系, 连乘以后验概率, 就可以得到有向图的联合概率分布.</p>
<p>无向图, 指的是节点之间存在关系, 但没有方向, CRF就属于无向图模型. 那么在计算无向图对应的概率分布时, 又应该怎么做呢? </p>
<p>是的, 无向图对应的概率分布没有有向图的那么直观和简单.一般来说, 会先将无向图划分为一些团, 每个团包含相邻的一些节点, 每个节点属于一个或者多个团.</p>
<p>而对于每个无向图的团$i$, 根据其不同的分布模式, 可以给出不同的分数(由参数决定)$s_i$. 那么对于一个无向图来说, 整体的分数等于所有团的分数$S_i=\prod s_i$. 由此, 无向图对应的概率分布为:</p>
<script type="math/tex; mode=display">
P(graph_i)=\frac{S_i}{Z} \\
Z=\sum\limits_{j}S_j</script><p>上面的$Z$又被称为配分函数, 表示归一化项. 无向图模型在一些问题上, 较有向图模型有更好的表现, 但是也正是因为$Z$的存在, 需要考虑所有可能存在的图的分布, 因此在计算难度上, 相比有向图要难一些. 关于$Z$的计算, 通常来说可以采用动态规划算法, 或者统计上的一些采样法等.</p>
<p>这里可能没有将概率图模型讲得很清楚, 感兴趣的同学可以另行谷歌, 或者康康一开始推荐的两篇文章.</p>
<h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><p>仍然回想HMM, 或者朴素贝叶斯模型, 其对应的联合概率分布为$P(X,Y)$, 在模型学习的过程中, 对其进行优化; 在推断的过程中, 可以利用贝叶斯公式进行推断:</p>
<script type="math/tex; mode=display">
P(Y|X)=\frac{P(Y,X)}{P(X)}</script><p>而判别模型, 如逻辑回归, CRF, 对应的都是条件概率分布$P(Y|X)$或者$Y=f(X)$, 在学习时针对这个条件概率分布进行优化.</p>
<p>对比生成模型与判别模型, 生成模型有以下特点:</p>
<ul>
<li>通过贝叶斯公式可以转变为判别模型.</li>
<li>能够学习到数据分布, 进而生成数据样本.</li>
</ul>
<p>判别模型有以下特点:</p>
<ul>
<li>针对目标变量优化, 往往可以取得更好的模型效果.</li>
</ul>
<p>用一个栗子来对生成模型和判别模型进行描述. 比如现在有一堆关于猫和狗的数据样本, 如果让生成模型来学习, 那么生成模型会学习它们的<strong>特征分布</strong>, 即猫有哪些特点, 狗有哪些特点. 然后在推断时, 给定$X$, 可以比较猫和狗哪个的联合概率分布$P(Y,X)$更大; 此外, 可以利用学习到的联合概率分布$P(Y,X)$, 生成一些关于猫或者狗的数据:</p>
<script type="math/tex; mode=display">
P(X|Y)=\frac{P(Y,X)}{P(Y)}</script><p>对于判别模型来说, 可以看做是学习的是猫和狗之间的<strong>区别</strong>(如判别边界). 那么在推断时, 给定$X$, 可以比较猫和狗哪个的条件概率$P(Y|X)$更大; 但是若要问判别模型猫或者狗长什么样, 判别模型会说无可奉告（○｀ 3′○）</p>
<h2 id="CRF的由来"><a href="#CRF的由来" class="headerlink" title="CRF的由来"></a>CRF的由来</h2><p>那么在介绍了有向图/无向图, 生成模型/判别模型后, 现在来介绍条件随机场CRF.</p>
<p>首先来康康神马是随机场, “随机场”的名字取得比较玄乎, 其实就可以看做一个无向图, 嗯, 问题不大.</p>
<p>那么再来说马尔科夫随机场, 一看到”马尔科夫”这个词, 其实大概就能明白一些意思了. 马尔科夫随机场是随机场的特例, 它假设在随机场中, 某一个节点的值(状态), 仅仅与和它相邻的节点有关. 比如在HMM中, 当前状态节点与前一个时刻的状态节点有关, 而在随机场这里, 还可以与其它更多相邻的节点有关.</p>
<p>最后, 就到了条件随机场了CRF了, CRF是马尔科夫随机场的特例, 它假设马尔科夫随机场中, 有两种类型的变量$X$与$Y$, $X$一般是给定的, 而$Y$一般是在给定$X$的条件下的输出. 这样马尔科夫随机场就特化成了条件随机场. 用数学语言进行描述: 设$X$与$Y$是随机变量, $P(Y|X)$是给定$X$时$Y$的条件概率分布, 若随机变量$Y$构成的是一个马尔科夫随机场, 则称条件概率分布$P(Y|X)$对应的模型是条件随机场.</p>
<p>需要注意的是, 在CRF的定义中, 并没有要求$X$与$Y$有相同的结构, 而实际当中, 一般要求$X$与$Y$有相同的结构, 即:</p>
<script type="math/tex; mode=display">
X=(X_1,X_2,\dots,X_T) \\
Y=(Y_1,Y_2,\dots,Y_T)</script><p>可以表示为如下图结构:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>这样的条件随机场称为线性链条件随机场(Linear chain Conditional Random Fields), 简称linear-CRF.</p>
<p>哦对了, 还没有说CRF可以用来干嘛呢, 其实与HMM类似, 仍然是主要用于序列标注一类的问题, 如词性标注(POS Tagging), 命名体识别(NER)等. </p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>隐马尔科夫模型HMM</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM/</url>
    <content><![CDATA[<p>隐马尔科夫模型(Hidden Markov Model, 以下简称HMM), 是比较经典的机器学习模型了. 它在自然语言处理, 模式识别等领域得到广泛的应用. </p>
<p>当然, 随着目前深度学习的崛起, HMM的地位有所下降. 但是作为一个经典的模型, 学习HMM的模型和对应算法, 对我们解决问题建模的能力提高以及算法思路的拓展还是很好的.</p>
<a id="more"></a>
<h1 id="HMM介绍"><a href="#HMM介绍" class="headerlink" title="HMM介绍"></a>HMM介绍</h1><p>首先看看什么样的问题解决可以用HMM模型, 可以使用HMM模型的问题一般有如下两个特征:</p>
<p>​    <strong>*</strong> 问题是<strong>基于序列</strong>的, 比如时间序列, 语言序列.</p>
<p>​    <strong>*</strong> 问题中有<strong>两类数据</strong>, 一类序列数据是可以观测到的, 即<strong>观测序列</strong>; 而另一类数据是不能观察到的, 即隐藏状态序列, 简称<strong>状态序列</strong>, 这也是模型名称中”隐”的由来.</p>
<p>一般来说, HMM经常用于解决<strong>序列标注问题(Sequence Labeling Problem)</strong>.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 给到一个可观测序列$X$, 推断出隐藏序列$Y$. 具体说来, 比如要处理词性标注(POS tagging)问题, 即给出一段语句, 对语句中的各个词汇标注词性(如动词, 名词等). 那如果让HMM来做, 是如何处理这样的问题的呢?</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>可以有这样的一个假设, 当我们在说一句话的时候, 我们第一步是通过语法规则, 生成一个词性序列, 然后第二步是基于这个词性序列与词典, 生成句子.</p>
<p>HMM本身的模型结构是一个有向图, 隐藏节点之间具有马尔可夫性, 即当前状态节点仅依赖于上一个节点, 同时当前可观测节点仅依赖于当前状态节点.</p>
<p>那么在这个模型结构之下, 整个过程(生成状态序列, 生成可观测序列)的联合概率计算方式如下图:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>联合概率的计算主要由两部分组成, 状态序列之间转移的部分称为转移概率(Transition probability), 从状态序列到观察序列的部分称为发射概率(Emission probability). 可以对应到两个参数矩阵, 马尔科夫链的状态转移矩阵$A$, 观察序列的生成(发射)矩阵$B$.</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>到了这里, 大概知道HMM长什么样了, 同时也知道可以用来处理哪一类的问题. </p>
<p>假设模型参数为$\lambda=\{A,B\}$, 初始分布算作$A_{start}$, 观察序列为$O=\{o_1,o_2,\dots o_n\}$.</p>
<p>现在有了下面几个问题:</p>
<p>​    <strong>*</strong> 估计观察序列概率. 在已知模型参数$\lambda$和观察序列$O$的情况下, 如何计算观察序列出现的概率$P(O|\lambda)$. 这个问题相对简单.</p>
<p>​    <strong>*</strong> 预测/解码. 已知模型参数$\lambda$和观察序列$O$的情况下, 求最有可能出现的隐藏状态序列, 会用到维特比算法, 要稍微难一些.</p>
<p>​    <strong>*</strong> 模型参数学习问题. 这个问题如果给定隐藏状态序列, 那么会非常简单, 直接统计就可以得到$\lambda$. 但是如果只给出观察序列$O$, 估计$\lambda$, 使得$P(O|\lambda)$最大, 就比较复杂了, 需要用到EM算法.</p>
<h1 id="估计观察序列概率"><a href="#估计观察序列概率" class="headerlink" title="估计观察序列概率"></a>估计观察序列概率</h1><p>假设现在已经知道了HMM的参数$\lambda$, 观测序列$O$, 求解$P(O|\lambda)$.</p>
<p>这个问题是比较简单的, 首先从直观的暴力法的角度来看. 如果能够列举出所有可能的长度为$T$的隐藏序列$H=\{h_1,h_2,\dots,h_T\}$, 那么对于每个可能的隐藏序列, 可以得到联合概率分布$P(O,H|\lambda)$, 这样就可以通过对所有可能求和, 得到边缘分布$P(O|\lambda)$:</p>
<script type="math/tex; mode=display">
P(O,H|\lambda)=P(H|\lambda)P(O|H,\lambda) \\
P(O|\lambda)=\sum_IP(O,H|\lambda)</script><p>对于暴力法来说, 其时间复杂度取决于序列长度$T$和隐藏状态数$N$, 为$O(TN^T)$.</p>
<p>OK, 现在有了暴力法, 那么接下来就只差简答的优化了ヽ(✿ﾟ▽ﾟ)ノ</p>
<p>优化方法仍然是属于动态规划方法(诶为什么说又), 叫做前向法. 有前向法就有后向法, 不过两者原理类似.</p>
<p>动态规划算法通常来说有几个要点:</p>
<ul>
<li>定义好子问题.</li>
<li>找到递推公式.</li>
<li>有一个初始解.</li>
</ul>
<p>那么在这里, 我们要求$P(O|\lambda)$, 不妨定义一个前向概率:</p>
<script type="math/tex; mode=display">
\alpha_t(i)=P(o_1,o_2,\dots,o_t,h_t=i|\lambda)</script><p>表示在$t$时隐藏状态为$i$, 同时观测序列为$O$的联合概率.</p>
<p>在$t=1$时, $\alpha_{1}(i)=A_{start\to i}\times B_{i\to o_1}$.</p>
<p>在$t&gt;1$时, $\alpha_{t}(j)=(\sum\limits_{i=1}^N \alpha_{t-1}(i) A_{i\to j})\times B_{j\to o_1}$. 这一步递推是核心, 即每一时刻的前向概率, 可以通过前一个时刻所有隐藏状态的前向概率求和得到.</p>
<p>在$t=T$时, 得到$\alpha_T(i)$, 而想要求解的$P(O|\lambda)=\sum_i\alpha_T(i)$</p>
<p>利用前向法估计观察序列的概率, 时间复杂度为$O(TN^2)$.</p>
<p>类似的, 定义后向概率:</p>
<script type="math/tex; mode=display">
\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T| h_t =i , \lambda)</script><p>对应的递推公式为:</p>
<script type="math/tex; mode=display">
\beta_{t}(i) = \sum\limits_{j=1}^{N}A_{i\to j}\times B_{j\to o_{t+1}}\times\beta_{t+1}(j)</script><p>基于上面两个前向与后向概率, 可以进一步得到下面几个概率公式, 它们将在HMM使用EM学习参数时发挥作用.</p>
<ul>
<li><p>给定参数$\lambda$和观察序列$O$, $h_t=i$的概率:</p>
<script type="math/tex; mode=display">
\gamma_t(i) = P(h_t = i | O,\lambda) = \frac{P(h_t = i ,O|\lambda)}{P(O|\lambda)}</script><p>由前向后向概率可得:</p>
<script type="math/tex; mode=display">
P(h_t = i ,O|\lambda) = \alpha_t(i)\beta_t(i)</script><p>所以:</p>
<script type="math/tex; mode=display">
\gamma_t(i) = \frac{ \alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N \alpha_t(j)\beta_t(j)}</script></li>
<li><p>给定参数$\lambda$和观察序列$O$, $h_t=i,h_{t+1}=j$的概率:</p>
<script type="math/tex; mode=display">
\xi_t(i,j) = P(h_t = i, h_{t+1}=j | O,\lambda) = \frac{ P(h_t = i, h_{t+1}=j , O|\lambda)}{P(O|\lambda)}</script><p>由前向后向概率可得:</p>
<script type="math/tex; mode=display">
P(h_t = i, h_{t+1}=j , O|\lambda) = \alpha_t(i)A_{i\to j}B_{j\to O_{t+1}}\beta_{t+1}(j)</script><p>所以:</p>
<script type="math/tex; mode=display">
\xi_t(i,j) = \frac{\alpha_t(i)A_{i\to j}B_{j\to O_{t+1}}\beta_{t+1}(j)}{\sum\limits_{k=1}^N\sum\limits_{l=1}^N\alpha_t(k)A_{k\to l}B_{l\to O_{t+1}}\beta_{t+1}(l)}</script></li>
</ul>
<h1 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h1><p>解码, 或者预测, 指的是已知模型参数$\lambda$和观察序列$O$的情况下, 求最有可能出现的隐藏状态序列.</p>
<p>使用与分词那里类似的维特比算法, 找到合适的子问题, 以及递推关系进行逐步求解.</p>
<p>定义一个函数$\delta_t(h)$, 表示在$t$时状态节点为$i$的最大路径(概率):</p>
<script type="math/tex; mode=display">
\delta_t(h)=\max\limits_{h_1,h_2,\dots,h_{t-1}}P(o_1,o_2,\dots, o_t,h_1,h_2,\dots,h_{t-1},h_t=h|\lambda)</script><p>其中$h\in\{1,2,\dots,N\}$.</p>
<p>对应的递推表达式为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\delta_{t+1}(h)&=\max\limits_{h_1,h_2,\dots,h_{t}}P(o_1,o_2,\dots, o_{t+1},h_1,h_2,\dots,h_{t},h_{t+1}=h|\lambda) \\
&=\max\limits_{1\le i\le N}(\delta_{t}(i)\times A_{i\to h})B_{h\to o_{t+1}}
\end{aligned}</script><p>有了上面的递推表达式, 可以从第一个状态节点$\delta_{1}(h)=A_{start\to h}B_{h\to o_{1}}$逐步往后计算, 同时需要记录下每个时刻下每个状态的上一个状态, 即路径.</p>
<p>在计算到$\delta_{T}(h)$时, 在$h\in\{1,2,\dots,N\}$中找到最大的状态, 其对应的路径, 既是需要求解的隐藏状态序列$H$.</p>
<h1 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h1><p>在模型参数估计时, 如果给定隐藏状态序列, 那么会非常简单, 直接统计就可以得到$\lambda$. 但是如果只给出观察序列$O$, 估计$\lambda$, 使得$P(O|\lambda)$最大, 就比较复杂了, 需要用到EM算法</p>
<h2 id="直接统计频次"><a href="#直接统计频次" class="headerlink" title="直接统计频次"></a>直接统计频次</h2><p>在一些情况下, 比如词性标注, 通常会有供学习的语料, 这些语料是已经标注好的, 即同时提供了观测序列$O$和状态序列$H$, 此时估计参数矩阵$A$和$B$的方式, 可以采用直接统计频次.</p>
<p>对于转移矩阵$A$来说:</p>
<script type="math/tex; mode=display">
A_{i\to i'}=\frac{count(i\to i')}{count(i)}</script><p>对于发射矩阵$B$来说:</p>
<script type="math/tex; mode=display">
B_{i\to j}=\frac{count(i\to j)}{count(i)}</script><p>没错, 就是这么简单.</p>
<h2 id="利用EM算法"><a href="#利用EM算法" class="headerlink" title="利用EM算法"></a>利用EM算法</h2><p>如果对EM算法不太了解的同学, 可以在我的博客的搜索页中, 查找一下”EM算法”的关键词, 即可看到专门对EM算法的介绍.</p>
<p>那么这里假设已经对EM算法流程熟悉了, 直接开搞.</p>
<p>按照EM算法, 需要分两步, 在E步的时候求出联合分布$P(O,H|\lambda)$基于条件概率$P(H|O;\lambda’)$的期望, 其中$\lambda’$为当前的模型参数; 然后在M步利用MLE最大化这个期望, 得到更新的模型参数$\lambda$, 作为下一轮迭代时的$\lambda’$, 直到最终收敛.</p>
<p>假设我们的训练数据为$\{(O_1, H_1), (O_2, H_2), …(O_D, H_D)\}$, 其中任意一个观测序列为$O_d = \{o_1^{(d)}, o_2^{(d)},\dots,o_T^{(d)}\}$, 其对应的隐藏序列为$H_d = \{h_1^{(d)}, h_2^{(d)},\dots,h_T^{(d)}\}$.</p>
<p>联合分布表达式为:</p>
<script type="math/tex; mode=display">
P(O,H|\lambda)=\prod_{d=1}^D (A_{start\to h_1^{(d)}}B_{h_1^{(d)}\to o_1^{(d)}})\times\cdots\times(A_{h_{T-1}^{(d)}\to h_T^{(d)}}B_{h_T^{(d)}\to o_T^{(d)}})</script><p>在E步的期望表达式为:</p>
<script type="math/tex; mode=display">
L(\lambda, \lambda') = \sum\limits_{
H}P(H|O,\lambda')\log P(O,H|\lambda)</script><p>在M步极大化上式, 由于$P(H|O,\lambda’) = P(H,O|\lambda’)/P(O|\lambda’)$, 且$P(O|\lambda’)$是常数, 因此要极大化的式子等价于:</p>
<script type="math/tex; mode=display">
L(\lambda, \lambda') = \sum\limits_{
H}P(O, H|\lambda')\log P(O,H|\lambda)</script><p>将$P(O,H|\lambda)$带入, 可得:</p>
<script type="math/tex; mode=display">
\lambda'=arg\max_\lambda \sum_{d=1}^D\sum\limits_{
H}P(O, H|\lambda')(\sum_{t=1}^{T}\log A_{h_{T-1}^{(d)}\to h_T^{(d)}}+\sum_{t=1}^{T}\log B_{h_T^{(d)}\to o_T^{(d)}})</script><p>首先求解$A$, 此时式子可整理为:</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum\limits_{
H}\sum_{t=1}^{T}P(O, H|\lambda')\log A_{h_{t-1}^{(d)}\to h_t^{(d)}}=\sum_{d=1}^D\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T}P(O, h_{t-1}^{(d)}=i,h_{t}^{(d)}=j|\lambda')\log A_{i\to j}</script><p>同时由于$A_{i\to j}$满足约束条件$\sum\limits_{j}A_{i\to j}=1$, 所以利用拉格朗日乘子法, 得到拉格朗日函数, 对$A_{i\to j}$求偏导, 并令其等于0, 可得到:</p>
<script type="math/tex; mode=display">
A_{i\to j}=\frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O^{(d)}, h_{t-1}^{(d)} = i, h_{t}^{(d)} = j|\lambda')}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O^{(d)}, h_{t-1}^{(d)} = i|\lambda')} \tag{1}</script><p>然后求解$B$, 与求解$A$一样:</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum\limits_{
H}\sum_{t=1}^{T}P(O, H|\lambda')\log B_{h_{t}^{(d)}\to o_t^{(d)}}=\sum_{d=1}^D\sum_{i=1}^N\sum_{t=1}^{T}P(O, h_{t}^{(d)}=i|\lambda')\log B_{i\to o_t^{(d)}}</script><p>并且$B_{i\to j}$满足$\sum\limits_{j}B_{i\to j}=1$, 同样使用拉格朗日乘子法可得:</p>
<script type="math/tex; mode=display">
B_{i\to j}=\frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O^{(d)}, h_{t}^{(d)} = i|\lambda')I(o_t^{(d)}=j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O^{(d)}, h_{t}^{(d)} = i|\lambda')} \tag{2}</script><p>其中$I(o_t^{(d)}=j)$当且仅当$o_t^{(d)}=j$时为1, 否则为0.</p>
<p>上面式$(1)$和式$(2)$即为最后用于迭代的公式, 其中涉及到的运算在前面几小节中也有涉及到, 包括$\gamma_t(i)$与$\xi_t(i,j)$.</p>
<h1 id="HMM的局限性"><a href="#HMM的局限性" class="headerlink" title="HMM的局限性"></a>HMM的局限性</h1><p>那么, HMM有没有啥缺点呢, 或者说HMM有没有啥局限性呢?</p>
<p>当然是有的, 从HMM的模型结构与假设出发, 就比较容易能够看出其局限性.</p>
<p>首先是状态序列的马尔科夫性, 当前节点仅依赖过去节点, 这某种意义上类似于Bigram, 属于考虑了序列信息, 但只包含了局部信息.</p>
<p>然后就是观测序列, 每个时刻的观测序列仅依赖于当前状态序列, 这可能也是有一些问题的, 为什么不能依赖于当前节点之前或者之后的节点呢?</p>
<p>以上两个不足, 可能会带来一些问题. 例如, 通过现有的参数$\lambda$, 得到状态$a$到状态$b$的转移概率$P_{a\to b}$较大, 同时状态$b$到观测$c$的发射概率$P_{b\to c}$也较大, 那么通过HMM就可以推断$P_{a\to b\to c}$的概率较大. 但是, 实际上在真实数据中$P_{a\to b\to c}$概率可能很小, 只是因为HMM局部地学习了$P_{a\to b}$和$P_{b\to c}$, 导致出现了这样的错误.</p>
<p>针对这样的问题, 条件随机场CRF等模型做出来改进.</p>
<p>尽管如此, HMM仍然在不少任务中可以使用, 特别是在训练数据不多的时候.</p>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><p>如果要体验HMM模型的使用, 可以尝试一个叫hmmlearn的Python包, 从名字就可以看出来, 与sklearn的API格式是比较类似的, 所以使用起来也比较简单.</p>
<p>在hmmlearn中, 实现了三种HMM模型, 其中包括:</p>
<p>​    <strong>*</strong> MultinomialHMM, 就是上面讲到的HMM模型, 其观察序列是离散的.</p>
<p>​    <strong>*</strong> GaussianHMM, 其观察序列是连续的, 服从高斯分布, 此时状态序列与观察序列之间的关系不能再使用发射矩阵$B$描述, 而使用均值与协方差描述.</p>
<p>​    <strong>*</strong> GMMHMM, 其观察序列服从混合高斯分布, 相对复杂一些, 一般使用GaussianHMM即可.</p>
<p>这里主要对MultinomialHMM的解码, 生成, 训练做一个展示, GaussianHMM与GMMHMM的使用是类似的.</p>
<p>对于MultinomialHMM模型, 使用比较简单, <code>startprob_</code>参数表示状态序列初始分布$A_{start}$, <code>transmat_</code>表示状态转移矩阵$A$, <code>emissionprob_</code>表示发射矩阵$B$.</p>
<p>这里假设有两枚硬币, 每次随机选取一枚(初始分布为$[0.5, 0.5]$), 硬币1的则转移概率为$[0.7, 0.3]$, 硬币2的转移概率为$[0.5, 0.5]$, 硬币1翻正反面概率为$[0.5,0.5]$, 硬币2翻正反面概率为$[0.8,0.2]$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> hmmlearn <span class="keyword">import</span> hmm</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置观察与状态的离散值</span></span><br><span class="line">states = [<span class="string">'硬币1'</span>, <span class="string">'硬币2'</span>]</span><br><span class="line">n_states = len(states)</span><br><span class="line"></span><br><span class="line">observations = [<span class="string">'正面'</span>, <span class="string">'反面'</span>]</span><br><span class="line">n_observations = len(observations)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置初始分布, 转移矩阵, 发射矩阵</span></span><br><span class="line">init_prob = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">A = np.array([[<span class="number">0.7</span>, <span class="number">0.3</span>],</span><br><span class="line">              [<span class="number">0.3</span>, <span class="number">0.7</span>]])</span><br><span class="line">B = np.array([[<span class="number">0.2</span>, <span class="number">0.8</span>],</span><br><span class="line">              [<span class="number">0.8</span>, <span class="number">0.2</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造模型</span></span><br><span class="line">model = hmm.MultinomialHMM(n_components=n_states)</span><br><span class="line">model.startprob_ = init_prob</span><br><span class="line">model.transmat_ = A</span><br><span class="line">model.emissionprob_ = B</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算观察序列概率</span></span><br><span class="line">seen_list = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">np.exp(model.score(seen_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.026785999999999994</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解码 decode或predict方法都可以</span></span><br><span class="line">log_prob_list, decode_list = model.decode(seen_list, algorithm=<span class="string">"viterbi"</span>)</span><br><span class="line">print(<span class="string">'观察序列: %s'</span> % <span class="string">', '</span>.join(map(<span class="keyword">lambda</span> x: observations[x[<span class="number">0</span>]], seen_list)))</span><br><span class="line">print(<span class="string">'状态序列: %s'</span> % <span class="string">', '</span>.join(map(<span class="keyword">lambda</span> x: states[x], decode_list)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">观察序列: 正面, 正面, 反面, 反面, 正面</span><br><span class="line">状态序列: 硬币2, 硬币2, 硬币1, 硬币1, 硬币2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用上面的模型, 生成观察序列和状态序列</span></span><br><span class="line"><span class="comment"># 每次生成不同的长度, 并记录每次的序列长度, 用于接下来的训练</span></span><br><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">seen_list = <span class="literal">None</span></span><br><span class="line">len_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    tmp_len = np.random.randint(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">    tmp_seen_list, _ = model.sample(tmp_len)</span><br><span class="line">    len_list.append(tmp_len)</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        seen_list = tmp_seen_list</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        seen_list = np.concatenate([seen_list, tmp_seen_list])</span><br><span class="line">seen_list[: <span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[0],</span><br><span class="line">       [1],</span><br><span class="line">       [0],</span><br><span class="line">       [1],</span><br><span class="line">       [1]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model = hmm.MultinomialHMM(n_components=n_states, random_state=<span class="number">7</span>, n_iter=<span class="number">1000</span>)</span><br><span class="line">model.fit(seen_list, lengths=len_list)</span><br><span class="line">print(model.startprob_)</span><br><span class="line">print(model.transmat_)</span><br><span class="line">print(model.emissionprob_)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0.44676585 0.55323415]</span><br><span class="line">[[0.65659885 0.34340115]</span><br><span class="line"> [0.2695147  0.7304853 ]]</span><br><span class="line">[[0.14879228 0.85120772]</span><br><span class="line"> [0.77509473 0.22490527]]</span><br></pre></td></tr></table></figure>
<p>对比输出结果和一开始的参数设置, 发现是非常接近的, 奥利给!</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>自然语言处理</tag>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title>分词</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%88%86%E8%AF%8D/</url>
    <content><![CDATA[<p>在做文本挖掘的时候, 一般来说首先要做的就是分词.</p>
<p>英文单词天然有空格, 容易分词, 但有时候也需要把多个单词视作一个单词, 如一些名词”New York”.</p>
<p>中文没有空格, “句读之不知, 惑之不解”, 相比古文, 如今的中文分词相对容易, 不过仍是一个需要专门去解决的问题.</p>
<a id="more"></a>
<p>分词的方法有很多种, 但常用的基本都是基于词典的分词, 下面讲解最大匹配法, 以及结合语言模型的维特比算法. HMM和CRF使用基于序列标注的方法也能够用来分词, 不过这里不做介绍.</p>
<h1 id="最大匹配法-Maximum-Matching"><a href="#最大匹配法-Maximum-Matching" class="headerlink" title="最大匹配法(Maximum Matching)"></a>最大匹配法(Maximum Matching)</h1><p>最大匹配法非常简单, 就是以词典为依据, 依次从最长的单词开始进行扫描, 试图获得单词颗粒度较大的划分.</p>
<p>主要包括正向最大匹配法, 反向最大匹配法, 以及双向最大匹配法.</p>
<p>只要明白了正向最大匹配法的原理, 反向最大匹配法就是换了一个扫描方向而已. 双向最大匹配法指的是各自执行一次正向/反向最大匹配法, 然后比较两者结果, 取大颗粒度的词越多, 非词典词和单字词越少的划分结果, 作为最终结果.</p>
<p>所以, 下面主要就说明正向最大匹配法的原理, 举个栗子来说明.</p>
<p>比如现在有一个待分词的句子”我们在野生动物园玩”.</p>
<p>同时有一个词典{“我们”, “在”, “在野”, “生动”, “野生”, “动物园”, “野生动物园”, “物”, “玩”}.</p>
<p>词典最大长度为5.</p>
<p>第一轮扫描:</p>
<ul>
<li>“我们在野生”, 扫描词典, 无.</li>
<li>“我们在野”, 扫描词典, 无.</li>
<li>“我们在”, 扫描词典, 无.</li>
<li>“我们”, 扫描词典, 有.</li>
</ul>
<p>第一轮扫描结束, 划分出”我们”, 将”我们”排除后, 进行第二轮扫描.</p>
<p>第二轮扫描:</p>
<ul>
<li>“在野生动物”, 扫描词典, 无.</li>
<li>“在野生动”, 扫描词典, 无.</li>
<li>“在野生”, 扫描词典, 无.</li>
<li>“在野”, 扫描词典, 有.</li>
</ul>
<p>第二轮扫描结束, 划分出”在野”, 将”我们在野”排除后, 进行第三轮扫描.</p>
<p>……</p>
<p>最终我们得到划分结果为”我们/在野/生动/物/园/玩”, 挺离谱的…</p>
<p>在这里用同样的语句和词典, 使用反向最大匹配法, 得到的划分结果为”我们/在/野生动物园/玩”, 完全正确!</p>
<p>那是不是就说反向最大匹配法比正向匹配法好呢, 当然不一样, 所以实际使用时可以尝试双向最大匹配法.</p>
<p>最大匹配法有如下一些特点:</p>
<ul>
<li>优点:<ul>
<li>算法流程简单, 设计合理可以并行化. 如将不同长度的词典分开, 在扫描时根据长度选择不同词典.</li>
</ul>
</li>
<li>缺点:<ul>
<li>需要给定词典, 存在OOV, 即未登录词的问题.</li>
<li>词典中词较少, 则影响准确率; 词典中词较多, 则影响运行效率.</li>
</ul>
</li>
</ul>
<h1 id="结合语言模型的维特比算法"><a href="#结合语言模型的维特比算法" class="headerlink" title="结合语言模型的维特比算法"></a>结合语言模型的维特比算法</h1><p>在上一篇中, 介绍了语言模型, 其作用通俗来说, 就是可以判断给定的一段语言序列, 是否像是”人话”; 或者也可以对于给的两段语言序列, 判断哪一句更像是”人话”.</p>
<p>于是, 对于一段待分词的句子, 我们可以穷尽所有可能的划分方式, 然后让语言模型从中找出联合概率分布最大的划分方式, 作为最优的划分.</p>
<p>然鹅, 我们都知道, 暴力法是不可取的(或许未来量子计算机可以这么玩), 所以需要有更低时间复杂度的算法, 来在众多划分方式中, 搜索得到语言模型认为最优的划分方式.</p>
<p>算法的名称, 就是大名鼎鼎的<strong>维特比算法</strong>, 是一种动态规划算法, 不仅在这里, 在其它一些与序列相关, 寻找最长/最短路径的场景下, 也能看到它的身影.</p>
<p>为了清楚地说明维特比算法如何应用于分词, 仍然是举栗子. 这里分别以基于Unigram语言模型, 和Bigram语言模型来讲解.</p>
<h2 id="基于Unigram"><a href="#基于Unigram" class="headerlink" title="基于Unigram"></a>基于Unigram</h2><p>现在给定一个句子, “人生如梦境”, 正确的划分方式, 应该是”人生/如/梦境”.</p>
<p>词典为{“人”, “人生”, “生”, “如”, “如梦”, “梦”, “梦境”}.</p>
<p>语言模型为Unigram.</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>如上图, 首先根据词典以及语言模型构建一个有向无环图, 其中的连边, 代表了连边首尾节点之间的词的概率(图上是随意设置的).</p>
<p>既然我们想要找到联合概率最大的划分方式, 那么在这里等价于找到一条路径, 使得其对应的长度(概率)最长.</p>
<p>使用维特比算法, 使用动态规划的方式. 采取如下步骤:</p>
<ul>
<li><p>假设到节点6, 最大路径长度为$f(6)$, 其中$f$表示以某节点为最后节点时, 最优划分方式对应的最大路径长度(概率).</p>
</li>
<li><p>节点6可以由节点5与节点4到达, 那么可以得到递归方程:</p>
<script type="math/tex; mode=display">
f(6)={\rm min}\{f(4)\times 0.02,\ f(5)\times 0.03\}</script></li>
<li><p>接下来查看$f(5)$, 节点5可以由节点4和节点3到达, 又可以得到递归方程:</p>
<script type="math/tex; mode=display">
f(5)={\rm min}\{f(3)\times 0.04,\ f(4)\times 0.06\}</script></li>
<li><p>逐步向前递归……</p>
</li>
<li><p>当碰到$f(1)$时, 返回$f(1)=1$, 从而可以求解出$f(2)\sim f(6)$. 同时, 每次记录到达的节点.</p>
</li>
<li><p>对于节点6, 通过反推其到达的节点(节点4, 节点3, 节点1), 即可得到划分结果, “人生/如/梦境”.</p>
</li>
</ul>
<p>以上就是维特比算法的过程, 当然, 细心的同学可能发现了, 万一语言模型给出的概率特别小, 再连乘很多次……所以, 可以先将概率取负对数, 这样可以避免连乘结果趋于零的问题, 不过从原本求最长路径, 变成了求最短路径, 递归方程里面的$\times$需要变成$+$, 并且$f(1)$此时等于0.</p>
<h2 id="基于Bigram"><a href="#基于Bigram" class="headerlink" title="基于Bigram"></a>基于Bigram</h2><p>在明白了基于Unigram语言模型的维特比算法后, 基于Bigram的算法是类似的, 这里就不做过多讲解了.</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>如上图, 发现基于Bigram的有向无环图与基于Unigram的不太一样, 多了起始符$&lt;{\rm start}&gt;$和结束符$&lt;{\rm end}&gt;$, 同时字词本身变成了节点. 其实虽然看起来不一样, 本质上都是有向无环图, 连边表示语言模型的概率, 对于Bigram来说这样分析更加方便.</p>
<p>在使用维特比算法时, 从$f(&lt;{\rm end}&gt;)$开始向前递归, 然后记录每个节点的最优到达路径, 最后也就得到了最优划分.</p>
<h1 id="jieba分词"><a href="#jieba分词" class="headerlink" title="jieba分词"></a>jieba分词</h1><p>分词工具有不少, 这里介绍<strong>jieba分词</strong>.</p>
<blockquote>
<p>“结巴”中文分词：做最好的 Python 中文分词组件</p>
</blockquote>
<h2 id="核心算法"><a href="#核心算法" class="headerlink" title="核心算法"></a>核心算法</h2><ul>
<li>基于前缀词典(一种树状的数据结构)实现高效的词图扫描, 生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG).</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合. 即与上文所讲算法一样.</li>
<li>对于未登录词, 采用了基于汉字成词能力的 HMM 模型处理. 利用HMM对未出现在词典中的词, 进行序列标注(词首, 词中, 词尾)来进行分词.</li>
</ul>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>主要支持四种分词模式, 精确模式, 全模式, 搜索引擎模式, paddle模式. 其中paddle模式需要安装额外的Python包, 这里不做介绍.</p>
<p>精确模式, 试图将句子最精确地切开, 适合文本分析.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Signature: jieba.cut(sentence, cut_all=False, HMM=True, use_paddle=False)</span><br><span class="line">Docstring:</span><br><span class="line">The main function that segments an entire sentence that contains</span><br><span class="line">Chinese characters into separated words.</span><br><span class="line"></span><br><span class="line">Parameter:</span><br><span class="line">    - sentence: The str(unicode) to be segmented.</span><br><span class="line">    - cut_all: Model type. True for full pattern, False for accurate pattern.</span><br><span class="line">    - HMM: Whether to use the Hidden Markov Model.</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认即为精确模式</span></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我/来到/北京/清华大学</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 包含"杭研"未登录词时, 关闭HMM的新词发现功能</span></span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>, HMM=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">他/来到/了/网易/杭/研/大厦</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 包含"杭研"未登录词时, 打开HMM的新词发现功能</span></span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>, HMM=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">他/来到/了/网易/杭研/大厦</span><br></pre></td></tr></table></figure>
<p>全模式, 把句子中所有的可以成词的词语都扫描出来, 速度非常快, 但是不能解决歧义.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 全模式</span></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我/来到/北京/清华/清华大学/华大/大学</span><br></pre></td></tr></table></figure>
<p>搜索引擎模式, 在精确模式的基础上, 对长词再次切分, 提高召回率, 适合用于搜索引擎分词.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Signature: jieba.cut_for_search(sentence, HMM=True)</span><br><span class="line">Docstring: Finer segmentation for search engines.</span><br><span class="line">File:      ~/opt/anaconda3/lib/python3.7/site-packages/jieba/__init__.py</span><br><span class="line">Type:      method</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索引擎模式</span></span><br><span class="line">seg_list = jieba.cut_for_search(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我/来到/北京/清华/华大/大学/清华大学</span><br></pre></td></tr></table></figure>
<h2 id="自定义词典"><a href="#自定义词典" class="headerlink" title="自定义词典"></a>自定义词典</h2><p>可以载<strong>入自定义词典</strong>, 以便包含jieba词库里没有的词. 虽然jieba有新词识别能力, 但是自行添加新词可以保证更高的正确率.</p>
<p>用法为<code>jieba.load_userdict(file_name)</code> , 其中<code>file_name</code>为文件类对象或自定义词典的路径. 或者使用<code>jieba.set_dictionary(file_name)</code>进行永久替换.</p>
<p>词典格式为一个词占一行, 每一行分三部分: 词语, 词频(可省略), 词性(可省略), 用空格隔开, 顺序不可颠倒. <code>file_name</code> 若为路径或二进制方式打开的文件, 则文件必须为 UTF-8 编码. 例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">云计算 5</span><br><span class="line">李小福 2 nr</span><br><span class="line">创新办 3 i</span><br><span class="line">easy_install 3 eng</span><br><span class="line">好用 300</span><br><span class="line">韩玉赏鉴 3 nz</span><br><span class="line">八一双鹿 3 nz</span><br><span class="line">台中</span><br><span class="line">凱特琳 nz</span><br><span class="line">Edu Trust认证 2000</span><br></pre></td></tr></table></figure>
<p>词频省略时使用自动计算的能保证分出该词的词频.</p>
<p>此外, 还可以使用<code>add_word(word, freq=None, tag=None)</code>和<code>del_word(word)</code>可在程序中动态修改词典.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Signature: jieba.add_word(word, freq=None, tag=None)</span><br><span class="line">Docstring:</span><br><span class="line">Add a word to dictionary.</span><br><span class="line"></span><br><span class="line">freq and tag can be omitted, freq defaults to be a calculated value</span><br><span class="line">that ensures the word can be cut out.</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">jieba.del_word(<span class="string">'清华大学'</span>)</span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">jieba.add_word(<span class="string">'清华大学'</span>)</span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我/来到/北京/清华大学</span><br><span class="line">我/来到/北京/清华/大学</span><br><span class="line">我/来到/北京/清华大学</span><br></pre></td></tr></table></figure>
<p>使用<code>suggest_freq(segment, tune=True)</code>可调节单个词语的词频, 使其能(或不能)被分出来. 需要注意的是, 该方法有时候对HMM的结果无效, 所以若分词结果与调整不符, 尝试关闭HMM.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Signature: jieba.suggest_freq(segment, tune=False)</span><br><span class="line">Docstring:</span><br><span class="line">Suggest word frequency to force the characters in a word to be</span><br><span class="line">joined or splitted.</span><br><span class="line"></span><br><span class="line">Parameter:</span><br><span class="line">    - segment : The segments that the word is expected to be cut into,</span><br><span class="line">                If the word should be treated as a whole, use a str.</span><br><span class="line">    - tune : If True, tune the word frequency.</span><br><span class="line"></span><br><span class="line">Note that HMM may affect the final result. If the result doesn&apos;t change,</span><br><span class="line">set HMM=False.</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">'我来到'</span>, tune=<span class="literal">True</span>)</span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>)</span><br><span class="line">print(<span class="string">"/"</span>.join(seg_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我/来到/北京/清华大学</span><br><span class="line">我来到/北京/清华大学</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>分词</tag>
      </tags>
  </entry>
  <entry>
    <title>语言模型</title>
    <url>/2020/08/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>我们平时在与人交流的时候, 会根据我们想表达的意图(信息), 组织成一段话, 然后按顺序讲出来; 写文章的时候也是, 在构思好如何行文后, 一个字一个字地打出来.</p>
<p>在自然语言处理中, 语言模型(Language Model)的存在就是为了处理这么一件事情.</p>
<a id="more"></a>
<h1 id="语言模型定义"><a href="#语言模型定义" class="headerlink" title="语言模型定义"></a>语言模型定义</h1><p>通俗地来说, 语言模型的主要任务就是衡量, 给定一个语言序列, 判断其是否是一个正常的序列, 即是否属于”人话”, 例如:</p>
<script type="math/tex; mode=display">
p(我\ 想\ 你)>p(你\ 我\ 想)</script><p>标准的定义是, 对于语言序列$w_1, w_2,…,w_n$, 语言模型就是计算该序列的概率, 即$p(w_1, w_2,…,w_n)$.</p>
<p>从机器学习的角度来看, 语言模型就是对语言序列的概率分布进行建模.</p>
<h1 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h1><h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>由定义可知, 我们的目的是想得到$p(w_1, w_2,…,w_n)$, 那么利用条件概率的链式法则可以得到:</p>
<script type="math/tex; mode=display">
p(w_1, w_2,...,w_n)=p(w_1)p(w_2|w_1)\cdots p(w_n|w_1, w_2,...,w_{n-1})</script><p>而其中的每一项, 在统计语言模型中, 可以采用极大似然来进行估计, 即:</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w_i|w_1, w_2,...,w_{i-1})&=\frac{C(w_1, w_2,...,w_i)}{\sum_w C(w_1, w_2,...,w_{i-1},w)} \\
&=\frac{C(w_1, w_2,...,w_i)}{C(w_1, w_2,...,w_{i-1})}
\end{aligned}</script><p>这里的$C(\cdot)$表示子序列在语料库(训练集)中出现的频次. 同时, 在上式中由第一个等号到第二个等号的转换, 其实是有一定限制的, 这个后文会说到.</p>
<p>聪明的小伙伴肯定已经想到了, 对于任意长的语句, 根据极大似然直接计算$p(w_i|w_1, w_2,…,w_{i-1})$是不现实的, 本身复杂度极高, 而且依赖越长, 在有限的语料库下, 估计也越不准确.</p>
<p>为了解决这个问题, 引入了经典的<strong>马尔科夫假设(Markov assumption)</strong>, 即假设当前词的出现概率, 仅仅依赖于最邻近的$n$个词, 于是可以将上面的条件概率进行简化:</p>
<script type="math/tex; mode=display">
p(w_i|w_1, w_2,...,w_{i-1})=p(w_i|w_{i-n},...,w_{i-1})</script><p>基于上式, 当$n$取不同的值的时候, 可以得到对应的N-gram模型:</p>
<script type="math/tex; mode=display">
\begin{aligned}
{\rm Unigram}\ n&=1:\quad p(w_1, w_2,...,w_n)=\prod p(w_i) \\
{\rm Bigram}\ n&=2:\quad p(w_1, w_2,...,w_n)=\prod p(w_i|w_{i-1}) \\
{\rm Trigram}\ n&=3:\quad p(w_1, w_2,...,w_n)=\prod p(w_i|w_{i-2},w_{i-1}) \\
\cdots
\end{aligned}</script><p>其中, 当$n&gt;1$时, 为了使句首的条件概率有意义, 需要给原序列加上一个起始符, 如$&lt;{\rm start}&gt;$, 可以说起始符的作用就是为了表征句首词出现的条件概率.</p>
<p>同时, 一般来说也需要加上一个结束符, 如$&lt;{\rm end}&gt;$, 而结束符的作用又是什么呢? 我认为有两点: 一是让上面基于统计的极大似然估计等式成立, 如果不加结束符, 不能满足假设里面$p(w_1, w_2,…,w_n)$是所以可能序列的概率分布(所有可能的概率和为1); 二是在一些时候, 利用语言模型进行生成或者预测时, 当得到下一个字符为结束符时, 表明可以结束这句话.</p>
<h2 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h2><p>在使用基于统计的语言模型时, 一个可能存在的问题, 就是会出现<strong>未登录词(OOV)</strong>, 即在未来(或者测试集)出现了在语料库(或者训练集)中没有出现过的词(或词的组合), 而导致语言模型计算出的概率为零.</p>
<p>若是分子为零, 还好一些, 但是如果分母为零, 那就连一个概率也计算不出来.</p>
<p>为了缓解这种情况, 就出现了平衡技术. 具体的平衡技术有多种, 这里介绍简单的两种方法.</p>
<p>一种是Add-one Smoothing, 也叫做Laplace Smoothing. 举个栗子, 对于Bigram来说:</p>
<script type="math/tex; mode=display">
p_{add\ one}(w_i|w_{i-1})=\frac{C(w_1, w_2,...,w_i)+1}{C(w_1, w_2,...,w_{i-1})+V}</script><p>其中的$V$表示词库的大小.</p>
<p>另一种是Add-one Smoothing的升级版, Add-K Smoothing. 仍然举Bigram的栗子:</p>
<script type="math/tex; mode=display">
p_{add\ k}(w_i|w_{i-1})=\frac{C(w_1, w_2,...,w_i)+k}{C(w_1, w_2,...,w_{i-1})+kV}</script><p>如果想选取合适的$k$, 则可以通过交叉验证的方式, 而评估的指标后文会进行介绍.</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>基于统计的N-gram语言模型有如下特点:</p>
<ul>
<li>优点:<ul>
<li>采用极大似然, 参数容易训练($n$不是很大时).</li>
<li>完全包含了前$n$个词的信息.</li>
<li>可解释性强.</li>
</ul>
</li>
<li>缺点:<ul>
<li>缺乏长程依赖, $n$通常不能取较大.</li>
<li>随着$n$的增长, 参数空间指数增长.</li>
<li>难免会出现OOV的问题.</li>
<li>单词依赖统计频次, 泛化能力差.</li>
</ul>
</li>
</ul>
<h1 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h1><p>从基于统计的语言模型中可以看出, 其关键点在于估计$p(w_i|w_1, w_2,…,w_{i-1})$, 那么除了利用统计频次去估计以外, 还有没有其它的方式也能近似地去估计呢?</p>
<h2 id="基于前馈神经网络"><a href="#基于前馈神经网络" class="headerlink" title="基于前馈神经网络"></a>基于前馈神经网络</h2><p>Bengio大神在2003年, 提出了如下图所示的前馈神经网络结构:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>先给每一个词赋予一个词向量, 再通过神经网络去学习这种分布式的表示. 利用神经网络去学习当前词的出现概率与前$n$个词之间的关系.</p>
<p>显然这种方式相比N-gram具有更好的泛化能力, 只要词向量表示得好, 一定程度上降低了数据稀疏带来的问题.</p>
<p>但是其缺点是仍然仅包含了有限的前文信息, 且不易训练.</p>
<h2 id="基于循环神经网络"><a href="#基于循环神经网络" class="headerlink" title="基于循环神经网络"></a>基于循环神经网络</h2><p>Mikolov在2010年, 提出了强大的RNN模型, 揭开了自然语言处理的新篇章.</p>
<p>关于RNN的原理, 以及特点这里不做详细讲解. 将其作为语言模型来使用, 仍然举一个栗子来说明建模过程:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>假设序列为<strong>我想你</strong>, 添加起始符和结束符后, 序列为<strong>$&lt;{\rm start}&gt;$ 我 想 你 $&lt;{\rm end}&gt;$</strong>. 按顺序将其作为网络的输入, 那么在输出层, 可以看做分别是在计算条件概率$p(w|&lt;{\rm start}&gt;)$, $p(w|&lt;{\rm start}&gt;\ 我)$, $p(w|&lt;{\rm start}&gt;\ 我\ 想)$, $p(w|&lt;{\rm start}&gt;\ 我\ 想\ 你)$. 而训练的目标, 就是使期望词对应的条件概率尽可能的大.</p>
<p>相比前馈神经网络, 具有隐状态$h_t$的传递, 使得RNN语言模型原则上可以捕捉前向序列的大部分信息(理想情况下). </p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>神经网络语言模型, 通过构建神经网络的方式, 来学习语言序列的内在规则与依赖关系. 相比基于统计的语言模型, 基于神经网络的语言模型有如下特点:</p>
<ul>
<li>优点:<ul>
<li>可以存着长距离依赖.</li>
<li>一定程度上缓解了OOV的问题.</li>
<li>好的词向量可以提高模型泛化能力.</li>
</ul>
</li>
<li>缺点:<ul>
<li>需要大量语料与计算资源进行训练.</li>
<li>可解释性差.</li>
</ul>
</li>
</ul>
<h1 id="语言模型评价指标"><a href="#语言模型评价指标" class="headerlink" title="语言模型评价指标"></a>语言模型评价指标</h1><p>在评估一个语言模型的好坏时, 一种直观的方法是, 在某个应用语言模型的具体任务上, 比较使用不同语言模型后的效果. 比如一些用到了语言模型的机器翻译, 语音识别等任务上进行比较.</p>
<p>而更加通用的评估指标, 是<strong>困惑度(Perplexity)</strong>. </p>
<p>困惑度的定义与交叉熵有关, 交叉熵可以描述两个分布之间的差距, 在这里可以看成给定一句真实的语句, 希望语言模型给出的概率较大.</p>
<script type="math/tex; mode=display">
H(W)=H(w_1, w_2,...,w_n)=-\frac{1}{N}{\rm log}\ p(w_1, w_2,...,w_n)</script><p>其中除以$N$是考虑到不同句子长度带来的影响.</p>
<p>于是困惑度的定义为:</p>
<script type="math/tex; mode=display">
Perplexity(W)=2^{H(W)}=\frac{1}{\sqrt[N]{p(w_1, w_2,...,w_n)}}</script><p>由公式可以看成, 困惑度指标越小越好, 即语言模型不仅正确, 且”自信”.</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-图像数据建模</title>
    <url>/2020/08/15/TensorFlow/TensorFlow-%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>AI的发展离不开对视觉的探索与研究, 包括图像与视频这样的数据. 在深度学习出现之前, 几乎难以有成熟的落地应用, 而当深度学习, 尤其是CNN的出现, 让计算机视觉这一领域发生了质的改变.<br>这里主要通过TensorFlow, 来做一个简单的图像数据建模的示例.</p>
<a id="more"></a>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>在这里用到的图像数据, 是经典的cifar10数据的子集, 仅仅包括两个类别airplane和automobile.<br>在训练集中airplane和automobile各5000张, 测试集中airplane和automobile各1000张.<br>我们的任务当然就是构建一个模型, 来对以上两类图像进行分类.<br>数据存放的格式如下:</p>
<ul>
<li>train<ul>
<li>0_airplane: fig_0, fig_1, …</li>
<li>1_automobile: fig_0, fig_1, …</li>
</ul>
</li>
<li>test<ul>
<li>0_airplane: fig_0, fig_1, …</li>
<li>1_automobile: fig_0, fig_1, …</li>
</ul>
</li>
</ul>
<p>在TensorFlow中准备图像数据的常用方案有两种:</p>
<ul>
<li>使用<code>tf.keras</code>中的<code>ImageDataGenerator</code>工具构建图像数据生成器.</li>
<li>使用<code>tf.data</code>搭配<code>tf.image</code>中的一些图像处理方法构建数据管道.</li>
</ul>
<p>第一种方法相对简单, 第二种方法为TensorFlow的原生方法, 更加灵活, 在一些时候可以获得更好的性能.<br>这里采用第二种方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = tf.constant(<span class="number">1</span>, tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(</span><br><span class="line">        img_path, <span class="string">".*automobile.*"</span>) <span class="keyword">else</span> tf.constant(<span class="number">0</span>, tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size) / tf.constant(<span class="number">255.</span>)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>) \</span><br><span class="line">    .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>).batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/test/*/*.jpg"</span>) \</span><br><span class="line">    .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看部分样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds_train.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_5_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape, y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(128, 32, 32, 3) (128,)
</code></pre><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models, regularizers, callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2D, MaxPool2D, Dropout, Flatten, Dense, Input</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), kernel_regularizer=regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(</span><br><span class="line">    Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), kernel_regularizer=regularizers.l2(<span class="number">0.001</span>)))</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(rate=<span class="number">0.1</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1601      
=================================================================
Total params: 53,761
Trainable params: 53,761
Non-trainable params: 0
_________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(<span class="string">'adam'</span>, <span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
79/79 [==============================] - 2s 30ms/step - loss: 0.5348 - accuracy: 0.7618 - val_loss: 0.3987 - val_accuracy: 0.8565
Epoch 2/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.4062 - accuracy: 0.8453 - val_loss: 0.4038 - val_accuracy: 0.8395
Epoch 3/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3869 - accuracy: 0.8524 - val_loss: 0.3396 - val_accuracy: 0.8785
Epoch 4/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3459 - accuracy: 0.8734 - val_loss: 0.3118 - val_accuracy: 0.8845
Epoch 5/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3215 - accuracy: 0.8848 - val_loss: 0.2912 - val_accuracy: 0.8980
Epoch 6/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.3041 - accuracy: 0.8931 - val_loss: 0.2995 - val_accuracy: 0.8875
Epoch 7/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2984 - accuracy: 0.8950 - val_loss: 0.2724 - val_accuracy: 0.9015
Epoch 8/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2846 - accuracy: 0.9028 - val_loss: 0.2654 - val_accuracy: 0.9025
Epoch 9/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2714 - accuracy: 0.9061 - val_loss: 0.2534 - val_accuracy: 0.9125
Epoch 10/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2591 - accuracy: 0.9151 - val_loss: 0.2515 - val_accuracy: 0.9100
Epoch 11/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2522 - accuracy: 0.9159 - val_loss: 0.2497 - val_accuracy: 0.9085
Epoch 12/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2512 - accuracy: 0.9165 - val_loss: 0.2413 - val_accuracy: 0.9180
Epoch 13/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2403 - accuracy: 0.9193 - val_loss: 0.2409 - val_accuracy: 0.9145
Epoch 14/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2322 - accuracy: 0.9245 - val_loss: 0.2320 - val_accuracy: 0.9215
Epoch 15/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2326 - accuracy: 0.9256 - val_loss: 0.2292 - val_accuracy: 0.9270
Epoch 16/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2272 - accuracy: 0.9260 - val_loss: 0.2340 - val_accuracy: 0.9200
Epoch 17/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2269 - accuracy: 0.9260 - val_loss: 0.2283 - val_accuracy: 0.9260
Epoch 18/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2150 - accuracy: 0.9348 - val_loss: 0.2324 - val_accuracy: 0.9260
Epoch 19/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.2145 - accuracy: 0.9339 - val_loss: 0.2311 - val_accuracy: 0.9255
Epoch 20/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.2110 - accuracy: 0.9351 - val_loss: 0.2470 - val_accuracy: 0.9160
Epoch 21/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1990 - accuracy: 0.9415 - val_loss: 0.2238 - val_accuracy: 0.9290
Epoch 22/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1920 - accuracy: 0.9450 - val_loss: 0.2216 - val_accuracy: 0.9280
Epoch 23/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1876 - accuracy: 0.9474 - val_loss: 0.2264 - val_accuracy: 0.9280
Epoch 24/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1855 - accuracy: 0.9465 - val_loss: 0.2191 - val_accuracy: 0.9320
Epoch 25/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1860 - accuracy: 0.9475 - val_loss: 0.2285 - val_accuracy: 0.9250
Epoch 26/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1799 - accuracy: 0.9520 - val_loss: 0.2275 - val_accuracy: 0.9290
Epoch 27/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1808 - accuracy: 0.9484 - val_loss: 0.2213 - val_accuracy: 0.9280
Epoch 28/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1707 - accuracy: 0.9561 - val_loss: 0.2161 - val_accuracy: 0.9320
Epoch 29/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1689 - accuracy: 0.9573 - val_loss: 0.2140 - val_accuracy: 0.9350
Epoch 30/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1659 - accuracy: 0.9565 - val_loss: 0.2122 - val_accuracy: 0.9340
Epoch 31/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1667 - accuracy: 0.9576 - val_loss: 0.2154 - val_accuracy: 0.9365
Epoch 32/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1653 - accuracy: 0.9614 - val_loss: 0.2142 - val_accuracy: 0.9305
Epoch 33/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1653 - accuracy: 0.9582 - val_loss: 0.2153 - val_accuracy: 0.9345
Epoch 34/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1600 - accuracy: 0.9606 - val_loss: 0.2107 - val_accuracy: 0.9350
Epoch 35/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1603 - accuracy: 0.9618 - val_loss: 0.2163 - val_accuracy: 0.9335
Epoch 36/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1601 - accuracy: 0.9620 - val_loss: 0.2113 - val_accuracy: 0.9365
Epoch 37/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1584 - accuracy: 0.9604 - val_loss: 0.2105 - val_accuracy: 0.9355
Epoch 38/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1571 - accuracy: 0.9623 - val_loss: 0.2120 - val_accuracy: 0.9335
Epoch 39/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1570 - accuracy: 0.9635 - val_loss: 0.2109 - val_accuracy: 0.9375
Epoch 40/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1565 - accuracy: 0.9632 - val_loss: 0.2104 - val_accuracy: 0.9350
Epoch 41/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1543 - accuracy: 0.9638 - val_loss: 0.2102 - val_accuracy: 0.9360
Epoch 42/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1553 - accuracy: 0.9638 - val_loss: 0.2091 - val_accuracy: 0.9345
Epoch 43/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1546 - accuracy: 0.9632 - val_loss: 0.2089 - val_accuracy: 0.9355
Epoch 44/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1540 - accuracy: 0.9631 - val_loss: 0.2102 - val_accuracy: 0.9365
Epoch 45/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1526 - accuracy: 0.9643 - val_loss: 0.2101 - val_accuracy: 0.9370
Epoch 46/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1521 - accuracy: 0.9647 - val_loss: 0.2137 - val_accuracy: 0.9325
Epoch 47/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1501 - accuracy: 0.9655 - val_loss: 0.2115 - val_accuracy: 0.9335
Epoch 48/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1489 - accuracy: 0.9680 - val_loss: 0.2082 - val_accuracy: 0.9370
Epoch 49/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1483 - accuracy: 0.9675 - val_loss: 0.2090 - val_accuracy: 0.9370
Epoch 50/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1483 - accuracy: 0.9655 - val_loss: 0.2127 - val_accuracy: 0.9350
Epoch 51/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1502 - accuracy: 0.9646 - val_loss: 0.2081 - val_accuracy: 0.9360
Epoch 52/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1475 - accuracy: 0.9678 - val_loss: 0.2086 - val_accuracy: 0.9350
Epoch 53/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1475 - accuracy: 0.9671 - val_loss: 0.2088 - val_accuracy: 0.9370
Epoch 54/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1469 - accuracy: 0.9681 - val_loss: 0.2108 - val_accuracy: 0.9335
Epoch 55/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1459 - accuracy: 0.9681 - val_loss: 0.2078 - val_accuracy: 0.9380
Epoch 56/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1452 - accuracy: 0.9694 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 57/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1469 - accuracy: 0.9669 - val_loss: 0.2076 - val_accuracy: 0.9360
Epoch 58/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1454 - accuracy: 0.9692 - val_loss: 0.2072 - val_accuracy: 0.9375
Epoch 59/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1451 - accuracy: 0.9692 - val_loss: 0.2082 - val_accuracy: 0.9345
Epoch 60/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1447 - accuracy: 0.9685 - val_loss: 0.2075 - val_accuracy: 0.9380
Epoch 61/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1444 - accuracy: 0.9683 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 62/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1446 - accuracy: 0.9695 - val_loss: 0.2071 - val_accuracy: 0.9375
Epoch 63/1000
79/79 [==============================] - 2s 28ms/step - loss: 0.1450 - accuracy: 0.9683 - val_loss: 0.2070 - val_accuracy: 0.9380
Epoch 64/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1452 - accuracy: 0.9681 - val_loss: 0.2071 - val_accuracy: 0.9380
Epoch 65/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1442 - accuracy: 0.9692 - val_loss: 0.2071 - val_accuracy: 0.9375
Epoch 66/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1449 - accuracy: 0.9687 - val_loss: 0.2072 - val_accuracy: 0.9375
Epoch 67/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1443 - accuracy: 0.9687 - val_loss: 0.2072 - val_accuracy: 0.9370
Epoch 68/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1432 - accuracy: 0.9708 - val_loss: 0.2076 - val_accuracy: 0.9350
Epoch 69/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1429 - accuracy: 0.9688 - val_loss: 0.2073 - val_accuracy: 0.9380
Epoch 70/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1426 - accuracy: 0.9700 - val_loss: 0.2073 - val_accuracy: 0.9360
Epoch 71/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1431 - accuracy: 0.9696 - val_loss: 0.2073 - val_accuracy: 0.9365
Epoch 72/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1443 - accuracy: 0.9696 - val_loss: 0.2073 - val_accuracy: 0.9370
Epoch 73/1000
79/79 [==============================] - 2s 29ms/step - loss: 0.1428 - accuracy: 0.9690 - val_loss: 0.2072 - val_accuracy: 0.9380
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_14_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么梯度是上升最快的方向</title>
    <url>/2020/08/15/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E4%B8%8A%E5%8D%87%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/</url>
    <content><![CDATA[<p>我们在使用一些优化算法, 比如梯度下降法时, 会选择损失函数的负梯度来作为参数的更新方向, 因为梯度表示一个函数上升最快的方向.</p>
<p>那么, 为什么呢?</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在部分机器学习算法中, 以及大部分深度学习算法中, 基于梯度下降的优化算法可以说是最常用的算法了. 其核心思想, 就是在对参数进行优化时, 基于损失函数对参数的梯度.</p>
<p>一般来说, 可以表示为如下形式:</p>
<script type="math/tex; mode=display">
\vec g=\nabla_{\vec w_t}Loss \\[7mm]
\vec w_{t+1}=\vec w_{t}-\eta \vec g</script><p>而梯度的具体形式, 可以表示为:</p>
<script type="math/tex; mode=display">
\begin{align}
\vec g&=\nabla_{\vec w_t}Loss  \\
&=\bigg[\frac{\partial Loss}{\partial x},\frac{\partial Loss}{\partial y},\frac{\partial Loss}{\partial z},...\bigg]^T
\end{align}</script><p>表示用各个方向(对各个参数)的偏导, 组成的向量, 来指向的方向.</p>
<p>下面从理论的角度, 来解释为什么要选择负梯度作为优化方向的问题.</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>在我的<a href="whitemoonlight.top/2020/10/10/传统机器学习/GBDT-二/">这一篇</a>讲解GBDT的文章中, 就提到了泰勒展开公式, 这里再说一下, 这是非常重要的公式:</p>
<script type="math/tex; mode=display">
\begin{align*}
f(x)&=\sum_{n=0}^\infty\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \\
&=f(x_0)+f^1(x_0)(x-x_0)+\frac{f^2(x_0)}{2}(x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\end{align*}</script><p>假设现在已知一个函数的形式$f(x)$, 并且这个函数在某个点$x_0$处可导, 有对应的各阶导数$f^{(n)}(x_0)$, 那么其邻近的一点的值$f(x_0+\Delta x)$可以通过多项式的形式近似得到, 并且多项式的阶数越多, 那么就可以更加准确.</p>
<p>如果只考虑一阶展开, 可得:</p>
<script type="math/tex; mode=display">
\begin{align*}
f(\vec x)&=f(\vec x_0+\Delta \vec x) \\[7mm]
&\approx f(\vec x_0)+\nabla f(\vec x_0)\cdot \Delta \vec x
\end{align*}</script><p>这里的$\Delta \vec x$表示往某个方向移动一段距离, 对应可以得到函数的该变量:</p>
<script type="math/tex; mode=display">
\begin{align*}
\Delta y&= f(\vec x_0+\Delta \vec x)-f(\vec x_0) \\[7mm]
&\approx \nabla f(\vec x_0)\cdot \Delta \vec x
\end{align*}</script><p>那么在往某个方向, 移动一个单位距离时, 对应的函数改变量为:</p>
<script type="math/tex; mode=display">
\begin{align*}
\Delta y&\approx \nabla f(\vec x_0)\cdot \frac{\Delta \vec x}{||\Delta \vec x||_2}
\end{align*}</script><p>这时候在向量空间中, 给定一个向量$\nabla f(\vec x_0)$ , 找寻一个单位向量, 使其在该向量上的投影最大, 那么很明显, 只能是两个向量重合.</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>上面用比较简短的篇幅, 讲解了为什么梯度是上升最快的方向, 其实只要熟悉泰勒公式, 那么理解起来是非常简单的.</p>
<p>虽然简单, 但是这也算是机器学习理论中非常重要的基础了, 只有确定了它的正确性, 才能够有效地对模型进行优化.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>L1与L2正则</title>
    <url>/2020/08/15/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/L1%E4%B8%8EL2%E6%AD%A3%E5%88%99/</url>
    <content><![CDATA[<p>机器学习中, 为了抑制过拟合, 在一些模型(线性回归)中, 采用L1或者L2正则, 是常见的方法.</p>
<p>那么, 为什么L1与L2正则可以有抑制过拟合的作用, 它们之间又有什么差别呢, 下面来进行具体的阐述.</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在机器学习中, 过拟合是一个常见的问题, 所谓过拟合, 通俗来讲, 就是一个模型在训练集上训练后, 在训练集上可以有很好的表现, 但是在验证集或者测试集上表现很差.</p>
<p>评判过拟合一般也没有什么特别明确的标准, 可以用一些评估指标来进行判别. 假设我们在训练集上训练模型, 在验证集上调整超参数, 最后在测试集上看模型表现, 在一份数据上, 两次训练得到的结果(评估指标为准确率)如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">训练集</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">模型一</td>
<td style="text-align:center">0.90</td>
<td style="text-align:center">0.70</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">模型二</td>
<td style="text-align:center">0.75</td>
<td style="text-align:center">0.70</td>
<td style="text-align:center">0.65</td>
</tr>
</tbody>
</table>
</div>
<p>单独看测试集, 当然是模型二更好, 这没得说. 然鹅比较科学的做法, 是在确定模型训练得不错以后, 再到测试集上看模型表现. 那么如果在只看训练集和验证集的评估指标时, 如何判定哪个模型”大概率”更好呢? 上面的数据虽然是编的, 但是在本宝宝有限的建模经验中, 也可以说是经常遇到这种情况了, 即两个模型验证集上准确率相当(差距很小), 但训练集上差距较大. 这时候怎么选, 如果是没有太理解过拟合的同学, 可能会选择模型一, 验证集准确率一样, 训练集准确率高的更好不是很自然的事情吗♪(^∇^*) 但是这就是一个典型的过拟合表现, 即训练集和验证集的差距(gap)过大. 所以, 当两个模型在验证集上表现相当时, 选训练集上评估指标小的那个, 对应过拟合程度更小, 才是比较稳妥的做法, </p>
<p>上面通过一个小例子, 大致说明了过拟合的表现, 那么, 如何抑制过拟合呢? 其实我一直在用”抑制”这个词, 而没有用”防止”或者”消除”这样的词, 因为过拟合在很多时候是难以避免的, 主要原因有二:</p>
<ul>
<li><p>原因一:</p>
<p>造成过拟合的源头可能有多个, 比如数据本身, 模型结构, 训练过程等, 难以把各方面都做得完美.</p>
</li>
<li><p>原因二:</p>
<p>过拟合的反面就是欠拟合, 通过一些比较极端的方法, 确实可以消除过拟合, 但此时很可能达到了欠拟合的状态.</p>
<p>而通常稍微过拟合的状态下, 比欠拟合状态下的模型表现要好一些.</p>
</li>
</ul>
<p>一般有如下一些方法可以抑制过拟合:</p>
<ul>
<li><p>增大数据量.</p>
<p>增大训练样本总是没错的, 但是有时候不是想增加就能增加的.</p>
</li>
<li><p>简化模型结构.</p>
<p>比如在GBDT中用更少的树, 在MLP中用更少的隐藏层.</p>
</li>
<li><p>调节正则参数.</p>
<p>比较常见的, 就是L1和L2正则参数了.</p>
</li>
</ul>
<p>先说一下神马是L1和L2正则参数:</p>
<script type="math/tex; mode=display">
Loss=L(y,f(x))+\Omega(f)</script><p>上式是损失函数的一般形式, 其中第一项为关于学习目标$y$与模型预测$f(x)$的误差, 第二项为关于模型的正则损失项$\Omega(f)$.</p>
<p>这里假设$f$为线性模型$f(x)=w_1x_1+w_2x_2+\cdots$, $\Omega$为L1正则, 那么:</p>
<script type="math/tex; mode=display">
Loss=L(y,f(x))+\lambda\sum_i|w_i|</script><p>若$\Omega$为L2正则:</p>
<script type="math/tex; mode=display">
Loss=L(y,f(x))+\frac{1}{2}\lambda\sum_iw_i^2</script><p>下面就来说为什么L1和L2正则, 可以抑制过拟合.</p>
<h1 id="抑制过拟合"><a href="#抑制过拟合" class="headerlink" title="抑制过拟合"></a>抑制过拟合</h1><p>首先来观察L1与L2正则在损失函数中的表达式:</p>
<script type="math/tex; mode=display">
\lambda\sum_i|w_i|,\quad \frac{1}{2}\lambda\sum_iw_i^2</script><p>单看正则损失项, 对于单个参数$w_i$来说, L1正则损失为一个关于$y$轴的V型函数, L2正则损失为一个二次凸函数. 它们都有一个共同点, 即$w_i$越靠近0, 正则损失越小.</p>
<p>也就是说, 由于L1和L2正则项的存在, 会使得原本的参数有向0靠拢的趋势, 而这为什么能抑制过拟合呢? 这里仍然举一个栗子来进行说明, 假设有一个线性回归模型:</p>
<script type="math/tex; mode=display">
y=w_0+w_1x_1+w_2x_2  \tag{1}</script><p>并假设原本的数据是人为产生的, 产生的式子为:</p>
<script type="math/tex; mode=display">
y=2x_1+\epsilon \\[7mm]
x_2=x_1+\epsilon'</script><p>即只需要一个变量$x_1$就能够拟合$y$, $x_1$与$x_2$为高度相关. 此时若仍然使用(1)式去进行拟合, 且只使用平方误差作为损失函数(不添加正则损失项), 会得到怎么样的结果呢? 可能会完全正确拟合:</p>
<script type="math/tex; mode=display">
y=2x_1  \tag{2}</script><p>也可能得到:</p>
<script type="math/tex; mode=display">
y=x_1+x_2  \tag{3}</script><p>还有可能得到:</p>
<script type="math/tex; mode=display">
y=10x_1-8x_2  \tag{4}</script><p>这里就不更多地列举了, 想说的是, 当线性回归的特征中存在相关性(或多重共线性)时, 可以有多种结果可以达到相似的损失函数值.</p>
<p>从结果来看, 上面三个模型, 好像都能够达到相同的结果呀, 没问题呀.</p>
<p>但是在真实的场景中, 数据往往是不完全稳定的, 可能会随着时间发生一些变化, 而且可能会出现一些奇异值(远离数据中心). 从统计的角度来看, $x_1$与$x_2$都可以看做随机变量, 对于(1)式和(2)式, 由于随机变量系数较小, 对应的, 其结果$y$的方差也会较小; 而(3)式由于系数较大, 尽管期望是一致的, 但方差却成倍地被放大.</p>
<p>方差大意味着什么呢, 其实就是不稳定. 这里举一个实际的栗子, 假如在测试集中, 某个样本的$y$为$2.0$, 对应$x_1$为$1.1$, $x_2$为$1.0$, 使用平方误差$(1/2)(y-\hat{y})^2$那么对应不同模型的结果与误差为:</p>
<script type="math/tex; mode=display">
\begin{align*}
&y=2x_1=2.2&    &Loss=0.5\times0.2^2=0.02 \\[7mm]
&y=x_1+x_2=2.1&    &Loss=0.5\times0.1^2=0.005 \\[7mm]
&y=10x_1-8x_2=3.0&    &Loss=0.5\times3.0^2=4.5 \\[7mm]
\end{align*}</script><p>也就是说, 如果在未来出现了一些偏离训练集模式的数据, 那么这种误差对于(2)式和(3)式来说, 还在可接受范围内, 而对于(4)式来说, 误差会被成倍放大.</p>
<p>所以, 这里知道了, 虽然在训练集上可能三个模型的损失误差是相当的, 对应它们的模型期望相同, 但是前面两个模型的方差更小, 更加稳定, 或者说, (4)式对应的模型存在比较严重的过拟合.</p>
<p>现在来看看, 如果加入正则损失项, 会有什么变化吧. 假设现在训练集中有一个样本, $y$为$2.0$, $x_1$为$1.0$, $x_2$为$1.0$.</p>
<p>先试试加入L1正则损失项:</p>
<script type="math/tex; mode=display">
\begin{align*}
&y=2x_1=2.0&    &Loss=|2.0|+|0.0|=2.0 \\[7mm]
&y=x_1+x_2=2.0&    &Loss=|1.0|+|1.0|=2.0 \\[7mm]
&y=10x_1-8x_2=2.0&    &Loss=|10.0|+|8.0|=18.0 \\[7mm]
\end{align*}</script><p>再来看加入L2正则损失项:</p>
<script type="math/tex; mode=display">
\begin{align*}
&y=2x_1=2.0&    &Loss=0.5\times(4.0+0.0)=2.0 \\[7mm]
&y=x_1+x_2=2.0&    &Loss=0.5\times(1.0+1.0)=1.0 \\[7mm]
&y=10x_1-8x_2=2.0&    &Loss=0.5\times(100.0+64.0)=82.0 \\[7mm]
\end{align*}</script><p>从上面可以很容易看出, 在加了正则损失项以后, 即使各模型能够得拟合得到一样的结果, 但是系数更小(向0靠拢)的模型, 整体损失会更小.</p>
<p>综上, 结合具体的栗子, 可以知道怎样的模型, 会容易过拟合. 同时也说明了使用正则损失项以后, 能够有效地避免这样的情况, 即模型参数向0靠拢, 那么进一步, L1和L2有什么特点和区别呢, 下面来进行讲解.</p>
<h1 id="L1与L2的由来"><a href="#L1与L2的由来" class="headerlink" title="L1与L2的由来"></a>L1与L2的由来</h1><p>在上一节中, 是将L1和L2正则的形式, 直接搬了出来, 其实在一些经典的统计模型(如逻辑回归)中, 是可以通过一些基本假设, 从而推导得到的, 下面就用逻辑回归来进行演示.</p>
<p>关于线性模型的一些相关理论, 可以看一下我的<a href="whitemoonlight.top/2020/10/10/传统机器学习/逻辑回归/">这篇</a>讲解. 在统计中, 可以分为频率学派和贝叶斯学派, 其中频率学派讲求从观察到的现象, 来总结规律, 通俗来讲, 看到的数据是什么样的, 那就是什么样的. 而贝叶斯学派表示, 不能完全相信已观察的数据, 因为现有数据可能并不能代表总体数据, 需要给出先验信息.</p>
<p>具体说来, 比如对逻辑回归模型进行优化, 如果是频率学派来做, 那么就会首先写出模型的似然函数, 然后根据MLE(极大似然估计), 来进行模型参数的估计和学习.</p>
<script type="math/tex; mode=display">
h_w(x)=P(y=1|x;w) \\[7mm]
P(y|x;w)=h_w(x)^y\cdot \big(1-h_w(x)\big)^{1-y}</script><p>现在换贝叶斯学派来做, 说这样还不够, 模型参数本身应该满足一些先验分布$P(w)$, 再根据大名鼎鼎的贝叶斯公式:</p>
<script type="math/tex; mode=display">
P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>可得:</p>
<script type="math/tex; mode=display">
P(w|x;y)\sim P(y|x;w)P(w)</script><p>由于在优化过程中$P(x;y)$可以看做常量, 故省去. 上式即为MAP(最大后验估计), 在原本的似然分布的基础上, 增加了一个先验分布$P(w)$, 对应$P(w|x;y)$称为后验分布.</p>
<p>同时, 若计算得到的后验分布的形式与先验分布, 在形式上一直(比如都是正态分布), 那么称该先验分布为对应似然分布的共轭分布. 这里稍微说一下共轭分布的好处, 由于先验分布与后验分布形式一致, 那么这一次由一批样本经过计算得到的后验分布, 在下一次另一批样本中, 可以当做其先验分布, 从而实现了可以增量计算. 常见的共轭分布, 比如有伯努利分布与贝塔分布, 多项式分布与狄利克雷分布.</p>
<p>现在来看一个具体的栗子, 假设逻辑回归的先验分布为正态分布, 则:</p>
<script type="math/tex; mode=display">
\begin{align*}
J(w)&=-\ln L(w) \\
&=-\ln P(y|x;w)P(w) \\
&=-\ln P(y|x;w)-\ln P(w) \\
&=-\ln P(y|x;w)-\ln \alpha \exp\frac{-w^2}{\beta} \\
&=-\ln P(y|x;w)+\lambda w^2+constant
\end{align*}</script><p>可以看到在对MAP做了负对数后, 就是在原本没有加先验信息对应的损失函数后, 增加了$\lambda w^2$这一项, 就是平时见到的L2正则项.</p>
<p>而如果假设先验分布是拉普拉斯分布, 则:</p>
<script type="math/tex; mode=display">
\begin{align*}
J(w)&=-\ln P(y|x;w)-\ln P(w) \\
&=-\ln P(y|x;w)-\ln \alpha \exp\frac{-|w|}{\beta} \\
&=-\ln P(y|x;w)+\lambda |w|+constant
\end{align*}</script><p>这就是L1正则项的形式.</p>
<p>上面通过介绍贝叶斯学派的做法, 由MLE到MAP, 再通过具体的栗子说明了L1与L2正则的由来, 下面具体讲解L1与L2正则的特点与区别.</p>
<h1 id="L1倾向于稀疏解"><a href="#L1倾向于稀疏解" class="headerlink" title="L1倾向于稀疏解"></a>L1倾向于稀疏解</h1><p>前面已经说到了, L1和L2都能够使得模型参数向0靠拢, 但是要说它们之间最大的区别, 可能就是L1相比L2更加能够使得模型参数具有稀疏性.</p>
<p>先问是不是, 在考虑为什么.</p>
<p>这里仍然使用一个线性回归的小栗子, 来说明L1相比L2会使得模型参数稀疏(等于0).</p>
<p>仍然仿照之前的做法, 假设有一个线性回归模型:</p>
<script type="math/tex; mode=display">
y=w_0+w_1x_1+w_2x_2  \tag{5}</script><p>并假设原本的数据是人为产生的, 产生的式子为:</p>
<script type="math/tex; mode=display">
y=3x_1+\epsilon \\[7mm]
x_2=2x_1+\epsilon'</script><p>即只需要一个变量$x_1$就能够拟合$y$, $x_1$与$x_2$为高度相关. 此时若仍然使用(5)式去进行拟合, 且只使用平方误差作为损失函数, 并分别使用L1与L2正则, 会得到怎么样的结果呢? </p>
<p>首先考虑L2正则, 在拟合$y$值完全正确的情况下, $x_1$与$x_2$的系数有多种组合:</p>
<script type="math/tex; mode=display">
\begin{align*}
&y=3x_1&    &Loss=0.5\times(9.0+0.0)=4.5 \\[7mm]
&y=x_1+x_2&    &Loss=0.5\times(1.0+1.0)=1.0 \\[7mm]
&y=0.6x_1+1.2x_2&    &Loss=0.5\times(0.36+1.44)=0.9 \\[7mm]
&y=1.5x_2&    &Loss=0.5\times(0.0+2.25)=1.125 \\[7mm]
\end{align*}</script><p>从上面的几个模型对应的损失, 可以发现, 当系数集中在某个变量上时, 损失较大, 而当系数分摊到各个变量时, 损失较小. 其实$y=0.6x_1+1.2x_2$应该就是这里损失最小的模型, $x_1$与$x_2$高度正相关, 单位$x_2$对$y$造成的影响是$x_1$的两倍, 那么对应其在模型中的权重系数, $x_2$就是$x_1$的两倍.</p>
<p>是的, L2的特点是让模型参数向0靠拢, 同时会尽可能地按照各个自变量对因变量的贡献(影响), 来将权重系数(参数)分配给它们, 使得权重系数比较均匀.</p>
<p>接着来考虑L1正则, 同上, 在拟合$y$值完全正确的情况下, $x_1$与$x_2$的系数有多种组合:</p>
<script type="math/tex; mode=display">
\begin{align*}
&y=3x_1&    &Loss=|3.0|+0.0=3.0 \\[7mm]
&y=x_1+x_2&    &Loss=|1.0|+|1.0|=2.0 \\[7mm]
&y=0.6x_1+1.2x_2&    &Loss=|0.6|+|1.2|=1.8 \\[7mm]
&y=1.5x_2&    &Loss=|0.0|+|1.5|=1.5 \\[7mm]
\end{align*}</script><p>先说结论, $y=1.5x_2$是这里L1下的最优模型. 在L1的眼中, 如果发现某个变量($x_1$), 能够被一个更强的变量$x_2$代替掉, 那么它不会像L2那样仍然分一些权重系数给相对弱的变量, 而是会直接将其系数变为0, 以使得正则损失最小. 当然, L1正则将某个参数变为0, 也不一定是这个参数有一个上位替代, 也可能是这个参数对应的变量本身不显著.</p>
<p>从上面的小栗子可以看得出来, L1好像相比L2确实更容易使得模型参数稀疏, 那么为什么呢? 这里从三个角度来进行解释.</p>
<h2 id="先验分布"><a href="#先验分布" class="headerlink" title="先验分布"></a>先验分布</h2><p>在上文中已经说过, L1正则可以由拉普拉斯分布得到, L2正则可以由正态分布得到, 而正态分布一般来说是长这样的:</p>
<p><img src="fig_0.png" alt="fig"></p>
<p>拉普拉斯分布长这样:</p>
<p><img src="fig_1.png" alt="fig"></p>
<p>对比这两个分布, 可以发现, 在靠近期望(比如0)时, 对于正态分布来说, 相对比较平缓, 也就是说在期望的附近, 仍然有不小的概率密度分布.</p>
<p>但是对于拉普拉斯分布来说, 期望附近的概率密度分布比较陡峭, 更多的概率集中在期望值那里, 往两侧则迅速下降.</p>
<p>所以, 从先验分布来看, L1正则相比L2正则更容易使模型参数趋向于0.</p>
<h2 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h2><p>几何解释也是相对直观的一种方法:</p>
<p><img src="fig_2.png" alt="fig"></p>
<p>上图中, 左边对应L2正则的损失函数, 右边对应L1正则的损失函数. 蓝色误差等高线表示模型损失项, 橙色图形表示正则损失项, $w_1$和$w_2$表示模型参数.</p>
<p>先看蓝色部分, 如果不考虑正则损失, 那么图中的蓝色中心点, 应该是最优模型参数.</p>
<p>再看橙色部分, 对于L2对应的正态分布来说, 正则损失的误差等高线是一个中心在原点的圆; 对于L1对应的拉普拉斯分布来说, 正则损失的误差等高线是一个中心在原点的正方形(侧转45度).</p>
<p>而最优模型参数在哪呢, 或者说可能出现在哪呢? 只可能出现在蓝色的误差等高线和橙色的误差等高线的切点上. 所谓切点, 即两个图像在边缘处有且只有一个公共点(交点).</p>
<p>为什么必须是切点呢, 因为如果假设不是切点(有两个交点), 那么固定其中一个误差等高线不动, 将另外一个误差等高线往其中心缩小, 那么在这个过程中, 整体的损失函数必然是在下降的, 所以这就是必须是切点的原因.</p>
<p>现在明白了最优点可能出现在两个误差等高线的切点处, 分别来看两个正则项的差异. 对L2正则项来说, 正则损失的误差等高线是一个光滑的圆, 其切点出现的位置并不带有任何偏向性, 结合模型损失项的误差等高线, 切点可以出现在各个方向上. 而对于L1正则项来说, 正则损失的误差等高线是”不光滑”的, 在轴的方向上更为突出, 这就会导致切点出现的位置, 更有可能在某个轴上, 对应的就会有其它一些轴的变量参数为0.</p>
<p>其实这里几何解释, 本质上也是两种分布的差异带来的结果.</p>
<h2 id="数学解释"><a href="#数学解释" class="headerlink" title="数学解释"></a>数学解释</h2><p>最后, 再用偏数学的方法, 来对L1容易形成稀疏性进行解释.</p>
<p>在给出带正则项的损失函数后, 使用常用的梯度下降法进行优化.</p>
<p>对于L2正则:</p>
<script type="math/tex; mode=display">
Loss=L(y,f(x))+\frac{1}{2}\lambda\sum_iw_i^2 \\[7mm]
\frac{\partial Loss}{\partial w_i}=\frac{\partial L}{\partial w_i}+\lambda w_i \\[7mm]
w_i'=w_i-\eta\frac{\partial Loss}{\partial w_i}=(1-\eta\lambda)w_i-\eta\frac{\partial L}{\partial w_i}</script><p>一般来说, 正则参数$\lambda$比较小, 学习率$\eta$也小于0, 所以前面一项$(1-\eta\lambda)w_i$可以看做是在对$w_i$做一个缩小, 且$\lambda$越大, 缩小幅度越大, 但并不会直接将其变成0.</p>
<p>再来看L1正则, 严格意义上$|w_i|$在0点不可导, 所以采用分段优化:</p>
<script type="math/tex; mode=display">
Loss=L(y,f(x))+\lambda\sum_i|w_i| \\[7mm]</script><p>当$w_i\le0$时:</p>
<script type="math/tex; mode=display">
\frac{\partial Loss}{\partial w_i}=\frac{\partial L}{\partial w_i}-\lambda \\[7mm]
w_i'=\min\bigg(0,w_i-\eta\frac{\partial Loss}{\partial w_i}\bigg)=\min\bigg(0,w_i+\eta\lambda-\eta\frac{\partial L}{\partial w_i}\bigg)</script><p>当$w_i&gt;0$时:</p>
<script type="math/tex; mode=display">
\frac{\partial Loss}{\partial w_i}=\frac{\partial L}{\partial w_i}+\lambda \\[7mm]
w_i'=\max\bigg(0,w_i-\eta\frac{\partial Loss}{\partial w_i}\bigg)=\max\bigg(0,w_i-\eta\lambda-\eta\frac{\partial L}{\partial w_i}\bigg)</script><p>观察上面的结果, 发现相比L2时每次迭代进行缩放, 在L1这里每次迭代会直接用相加或相减正则参数, 来向0靠拢. 同时由于分段优化的条件, 当模型损失项的梯度作用小于正则项梯度, 参数更新越过0值时, 会被直接置为0, 即损失函数极小值(最小值)出现在边界0处. 此后, 若模型损失项的梯度的绝对值一直小于正则项梯度绝对值时, 对应模型参数将会被固定在0处:</p>
<script type="math/tex; mode=display">
-\lambda\le-\frac{\partial L}{\partial w_i}\le\lambda,\quad w_i=0</script><p>以上, 就是关于L1为什么相比L2更容易形成稀疏性的数学解释了. 这里再用通俗的话来表述一遍, 对于L1正则来说, 就好像在0附近, 有一块强力磁铁, 这块磁铁比较特殊, 它对于任何位置的参数, 吸引力是一样的, 但是一旦当参数被吸附到0点, 除非有比较强大的动能(模型损失梯度), 来帮助参数脱离磁铁, 否则难以脱身. 这样说, 会不会更加通俗易懂了呢♪(^∇^*)</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上, 便是L1与L2正则的全部内容了, 通过介绍机器学习中的过拟合现象, 引入L1与L2正则方法来抑制过拟合. 然后进一步阐述了L1与L2的特点与区别, 并详细说明了为什么L1相比L2, 更容易得到稀疏解(模型参数).</p>
<p>在真实的场景中, L1和L2是可以混用的, 并且由于在大数据场景下, 由于特征非常多, 模型可能也很复杂, 这时候使用L1使得模型参数稀疏化, 会带来一些工程上的便利. 不过L1正则也不是在仍和场景下, 都能够给模型参数带来稀疏性, 而这个问题, 将会在以后介绍FTRL时再进行讨论.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
        <tag>L1正则</tag>
      </tags>
  </entry>
  <entry>
    <title>概率校准</title>
    <url>/2020/08/13/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A6%82%E7%8E%87%E6%A0%A1%E5%87%86/</url>
    <content><![CDATA[<p>很多时候, 比如我们用机器学习模型做二分类问题时, 模型会对每个样本输出一个数值, 这个数值通常在0~1之间. 那么这个数值代表真实概率吗, 即该样本属于正样本的概率, 不一定.</p>
<p>不过我们通常也不关心这个数值是否等于属于正样本的概率, 因为我们用来处理<strong>排序</strong>任务时, 只要模型给出各样本数值的相对顺序不变, 排序效果就不会变.</p>
<p>但是偶尔当我们需要用模型来预测真实概率时, 比如想用模型来预测一个人的真实逾期率, 用以估计整体风险; 比如用模型来预测一个人对一个推送(如广告)的点击率, 即CTR, 来估算ROI(投入产出比). 此时, 我们需要概率校准.</p>
<a id="more"></a>
<h1 id="基于逻辑回归的概率校准"><a href="#基于逻辑回归的概率校准" class="headerlink" title="基于逻辑回归的概率校准"></a>基于逻辑回归的概率校准</h1><p>首先说具体的方法流程, 非常简单:</p>
<ul>
<li>得到原模型对样本的输出数值.</li>
<li>利用该模型的输出作为特征, 结合标签再训练一个逻辑回归模型.</li>
<li>新的逻辑回归模型的输出结果, 即为概率校准结果.</li>
</ul>
<p>那么, 为什么逻辑回归模型会有概率校准的效果呢? 这其实与逻辑回归模型的损失函数有关:</p>
<script type="math/tex; mode=display">
loss=-\sum y_i{\rm log}\ \hat y_i+(1-y_i){\rm log}(1-\hat y_i)</script><p>对于逻辑回归来说, 就是要让最终估计的结果, 尽可能接近真实的标签. 通过对原始输入进行拉伸/压缩, 再通过逻辑回归函数进行非线性变换, 可以实现概率校准.</p>
<p>基于逻辑回归的概率校准方法的优点是用少量样本训练即可, 一般不会过拟合. 但缺点是, 并不是万能的, 对于一些模型”奇异”的输出, 简单的逻辑回归难以进行有效的概率校准.</p>
<h1 id="基于保序回归的概率校准"><a href="#基于保序回归的概率校准" class="headerlink" title="基于保序回归的概率校准"></a>基于保序回归的概率校准</h1><p>神马是保序回归(Isotonic Regression)呢? 保序回归并不单单使用于模型校准, 在很多其他领域应用都很广泛.</p>
<p>其主要目的, 是将原本的一组非递增/递减的数组, 变换为递增/递减. 举几个栗子来说明算法会更好理解, 这里只考虑递增的情况:</p>
<ul>
<li>一组数为$<1,2,3,4>$, 本身递增, 无需操作.</1,2,3,4></li>
<li>一组数为$<1,3,2,8>$, 从左到右遍历发现$3&gt;2$, 将这两个数转换为均值, 即$<1,2.5,2.5,8>$.</1,2.5,2.5,8></1,3,2,8></li>
<li>一组数为$<1,3,2,2,5>$, 发现$3&gt;2$, 转换后得到$<1,2.5,2.5,2,5>$, 继续遍历时又发现$2.5&gt;2$, 于是将$3,2,2$转换为均值, 得到$<1,2.3,2.3,2.3,5>$.</1,2.3,2.3,2.3,5></1,2.5,2.5,2,5></1,3,2,2,5></li>
</ul>
<p>那么如何将保序回归用于概率校准呢? 如当前数据集样本为$(\textbf{x}_i,y_i)$, 待校准模型为$f(\textbf{x})$, 可得到数据集$(f(\textbf{x}_i), y_i)$. 然后按$f(\textbf{x}_i)$进行(递增)排序, 得到新的序列$(f(\text{x}_j),y_j)$, 对序列$y_j$进行保序回归即可完成校准.</p>
<p>相比基于逻辑回归的概率校准, 基于保序回归的概率校准更加强大, 但风险是容易过拟合. 所以, 当数据量较小时, 优先尝试基于逻辑回归的概率校准, 效果不好的情况下再使用基于保序回归的概率校准.</p>
<h1 id="负采样后的模型校准"><a href="#负采样后的模型校准" class="headerlink" title="负采样后的模型校准"></a>负采样后的模型校准</h1><p>在做二分类排序任务时, 有时候负样本数量远多于正样本数量, 我们可能会采取负样本下采样, 简称负采样的方法, 来优化建模过程. 不过这样带来的后果, 其实就是会导致预测的数值偏大, 若要回归真实的预测概率, 则需要进行概率校准.</p>
<p>在这种情况下进行概率校准, 不要依赖上述模型进行学习什么的, 直接利用公式即可:</p>
<script type="math/tex; mode=display">
p_r=\frac{p_s}{p_s+(1-p_s)/w}</script><p>其中, $p_r$表示校准后的概率, $p_s$表示负采样后的概率, $w$为负采样比例.</p>
<p>证明如下, 假设原本正样本数量为$N^+$, 负样本数量为$N^-$, 采样后的负样本数量$N^-_s=N^-\times w$, 此时正负样本的比例为:</p>
<script type="math/tex; mode=display">
\frac{N^+}{N^-}=\frac{p_r}{1-p_r}</script><p>采样后, 正负样本比例为:</p>
<script type="math/tex; mode=display">
\frac{N^+}{N^-\times w}=\frac{p_s}{1-p_s}</script><p>结合上两式, 可得到:</p>
<script type="math/tex; mode=display">
\frac{N^+}{N^-}=\frac{p_r}{1-p_r}=\frac{p_s\times w}{1-p_s}</script><p>由此可以推导出概率校准公式.</p>
<h1 id="概率校准评估"><a href="#概率校准评估" class="headerlink" title="概率校准评估"></a>概率校准评估</h1><p>一般使用Brier score来进行概率校准效果好坏的评估, 或者说, 模型输出数值是否接近真实概率的评估.</p>
<p>数学公式如下:</p>
<script type="math/tex; mode=display">
Brier\_score=\frac{1}{N}\sum(y_i-\hat y)^2 \\[3mm]
y_i\in \{0,1\} \quad real\ class\ label\\[3mm]
\hat y\in [0,1]\quad estimated\ default\ probability</script><h1 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h1><p>在sklearn中, 有专门的针对概率校准的方法, 可供使用.</p>
<p>让我们一起大喊: sklearn, 永远滴神!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss, roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> CalibratedClassifierCV, calibration_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一份二分类样本, 少量包含信息的特征, 多少冗余特征</span></span><br><span class="line">X, y = datasets.make_classification(n_samples=<span class="number">100000</span>,</span><br><span class="line">                                    n_features=<span class="number">20</span>,</span><br><span class="line">                                    n_informative=<span class="number">2</span>,</span><br><span class="line">                                    n_redundant=<span class="number">10</span>,</span><br><span class="line">                                    random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了确保统计上的稳定性, 将大量样本留作测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,</span><br><span class="line">                                                    y,</span><br><span class="line">                                                    test_size=<span class="number">0.99</span>,</span><br><span class="line">                                                    random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_calibration_curve</span><span class="params">(est, name, fig_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    绘制模型概率校准前后的的分布图</span></span><br><span class="line"><span class="string">    est: 模型</span></span><br><span class="line"><span class="string">    name: 模型名称</span></span><br><span class="line"><span class="string">    fig_index: 图标号, 在多次使用函数画图时, 区分在不同的图上画</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用基于保序回归的概率校准</span></span><br><span class="line">    isotonic = CalibratedClassifierCV(est, cv=<span class="number">2</span>, method=<span class="string">'isotonic'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用基于逻辑回归的概率校准</span></span><br><span class="line">    sigmoid = CalibratedClassifierCV(est, cv=<span class="number">2</span>, method=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以逻辑回归模型作为基准进行对比</span></span><br><span class="line">    lr = LogisticRegression(C=<span class="number">1.</span>, solver=<span class="string">'lbfgs'</span>)</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(fig_index, figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    ax1 = plt.subplot2grid((<span class="number">3</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">0</span>), rowspan=<span class="number">2</span>)</span><br><span class="line">    ax2 = plt.subplot2grid((<span class="number">3</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 画基准线</span></span><br><span class="line">    ax1.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>)</span><br><span class="line">    <span class="keyword">for</span> clf, name <span class="keyword">in</span> [(lr, <span class="string">'Logistic'</span>), </span><br><span class="line">                      (est, name),</span><br><span class="line">                      (isotonic, name + <span class="string">' + Isotonic'</span>),</span><br><span class="line">                      (sigmoid, name + <span class="string">' + Sigmoid'</span>)]:</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        y_pred = clf.predict(X_test)</span><br><span class="line">        <span class="keyword">if</span> hasattr(clf, <span class="string">"predict_proba"</span>):</span><br><span class="line">            prob_pos = clf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 使用决策函数(边界)</span></span><br><span class="line">            prob_pos = clf.decision_function(X_test)</span><br><span class="line">            prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用各指标进行评估</span></span><br><span class="line">        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())</span><br><span class="line">        print(<span class="string">"%s:"</span> % name)</span><br><span class="line">        print(<span class="string">"\tBrier: %1.3f"</span> % (clf_score))</span><br><span class="line">        print(<span class="string">"\tAUC: %1.3f"</span> % roc_auc_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        ax1.plot(mean_predicted_value,</span><br><span class="line">                 fraction_of_positives,</span><br><span class="line">                 <span class="string">"s-"</span>,</span><br><span class="line">                 label=<span class="string">"%s (%1.3f)"</span> % (name, clf_score))</span><br><span class="line"></span><br><span class="line">        ax2.hist(prob_pos,</span><br><span class="line">                 range=(<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">                 bins=<span class="number">10</span>,</span><br><span class="line">                 label=name,</span><br><span class="line">                 histtype=<span class="string">"step"</span>,</span><br><span class="line">                 lw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    ax1.set_ylabel(<span class="string">"Fraction of positives"</span>)</span><br><span class="line">    ax1.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">    ax1.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">    ax1.set_title(<span class="string">'Calibration plots  (reliability curve)'</span>)</span><br><span class="line"></span><br><span class="line">    ax2.set_xlabel(<span class="string">"Mean predicted value"</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="string">"Count"</span>)</span><br><span class="line">    ax2.legend(loc=<span class="string">"upper center"</span>, ncol=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画朴素贝叶斯模型的概率校准图</span></span><br><span class="line">plot_calibration_curve(GaussianNB(), <span class="string">"Naive Bayes"</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Logistic:</span><br><span class="line">	Brier: 0.099</span><br><span class="line">	AUC: 0.863</span><br><span class="line">Naive Bayes:</span><br><span class="line">	Brier: 0.118</span><br><span class="line">	AUC: 0.865</span><br><span class="line">Naive Bayes + Isotonic:</span><br><span class="line">	Brier: 0.098</span><br><span class="line">	AUC: 0.863</span><br><span class="line">Naive Bayes + Sigmoid:</span><br><span class="line">	Brier: 0.109</span><br><span class="line">	AUC: 0.865</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig"></p>
<p>可以看到, 在同一份样本上, 逻辑回归的输出是符合真实概率的, 而朴素贝叶斯不是. 同时对比校准结果, 发现基于逻辑回归的效果不好, 而基于保序回归的效果很好.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画线性支持向量机(回归)的概率校准图</span></span><br><span class="line">plot_calibration_curve(LinearSVC(), <span class="string">"SVC"</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Logistic:</span><br><span class="line">	Brier: 0.099</span><br><span class="line">	AUC: 0.863</span><br><span class="line">SVC:</span><br><span class="line">	Brier: 0.163</span><br><span class="line">	AUC: 0.863</span><br><span class="line">SVC + Isotonic:</span><br><span class="line">	Brier: 0.100</span><br><span class="line">	AUC: 0.863</span><br><span class="line">SVC + Sigmoid:</span><br><span class="line">	Brier: 0.099</span><br><span class="line">	AUC: 0.863</span><br></pre></td></tr></table></figure>
<p><img src="fig_1.png" alt="fig"></p>
<p>在这里, 对线性支持向量机的输出结果, 利用逻辑回归与保序回归都能得到不错的概率校准效果.</p>
<p>最后, 概率校准技术本身不会对模型输出结果的排序造成影响, 在一个数组外面套上一个逻辑回归函数, 或者使用保序回归, 不会改变数组的相对大小顺序. 但是, 有没有其它用法, 可以使用概率校准这个技术, 来提升模型表现呢?</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-文本数据建模</title>
    <url>/2020/08/13/TensorFlow/TensorFlow-%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>在现实世界中, 文本这种原生的数据类型的数量是很多的, 因为语言是我们交换信息的渠道. 而相比传统的机器学习方法, 基于深度学习的方法在自然语言处理上, 取得了巨大的成功.<br>本篇的主要目的不是深入探讨文本处理的细节, 而是通过TensorFlow来展示基础的文本建模流程.</p>
<a id="more"></a>
<h1 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h1><p>这里使用经典的IMDB数据集, 来进行文本数据建模, 做情感分析. 首先, 对原始数据进行一些简单的分析.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">train_sent_list = []</span><br><span class="line">train_label_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/imdb/train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        label, sent = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        train_sent_list.append(sent)</span><br><span class="line">        train_label_list.append(int(label))</span><br><span class="line">    f.close()</span><br><span class="line">len(train_sent_list)</span><br></pre></td></tr></table></figure>
<pre><code>20000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(train_label_list)</span><br></pre></td></tr></table></figure>
<pre><code>10062
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sent_list[: <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&quot;It really boggles my mind when someone comes across a movie like this and claims it to be one of the worst slasher films out there. This is by far not one of the worst out there, still not a good movie, but not the worst nonetheless. Go see something like Death Nurse or Blood Lake and then come back to me and tell me if you think the Night Brings Charlie is the worst. The film has decent camera work and editing, which is way more than I can say for many more extremely obscure slasher films.&lt;br /&gt;&lt;br /&gt;The film doesn&#39;t deliver on the on-screen deaths, there&#39;s one death where you see his pruning saw rip into a neck, but all other deaths are hardly interesting. But the lack of on-screen graphic violence doesn&#39;t mean this isn&#39;t a slasher film, just a bad one.&lt;br /&gt;&lt;br /&gt;The film was obviously intended not to be taken too seriously. The film came in at the end of the second slasher cycle, so it certainly was a reflection on traditional slasher elements, done in a tongue in cheek way. For example, after a kill, Charlie goes to the town&#39;s &#39;welcome&#39; sign and marks the population down one less. This is something that can only get a laugh.&lt;br /&gt;&lt;br /&gt;If you&#39;re into slasher films, definitely give this film a watch. It is slightly different than your usual slasher film with possibility of two killers, but not by much. The comedy of the movie is pretty much telling the audience to relax and not take the movie so god darn serious. You may forget the movie, you may remember it. I&#39;ll remember it because I love the name.\n&quot;,
 &quot;Mary Pickford becomes the chieftain of a Scottish clan after the death of her father, and then has a romance. As fellow commenter Snow Leopard said, the film is rather episodic to begin. Some of it is amusing, such as Pickford whipping her clansmen to church, while some of it is just there. All in all, the story is weak, especially the recycled, contrived romance plot-line and its climax. The transfer is so dark it&#39;s difficult to appreciate the scenery, but even accounting for that, this doesn&#39;t appear to be director Maurice Tourneur&#39;s best work. Pickford and Tourneur collaborated once more in the somewhat more accessible &#39;The Poor Little Rich Girl,&#39; typecasting Pickford as a child character.\n&quot;]
</code></pre><p>可以看到, 训练集有20000条样本, 正负样本各占一半, 接下来再看测试集样本.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_sent_list = []</span><br><span class="line">test_label_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/imdb/test.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        label, sent = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        test_sent_list.append(sent)</span><br><span class="line">        test_label_list.append(int(label))</span><br><span class="line">    f.close()</span><br><span class="line">len(test_sent_list)</span><br></pre></td></tr></table></figure>
<pre><code>5000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sum(test_label_list)</span><br></pre></td></tr></table></figure>
<pre><code>2438
</code></pre><p>测试集有5000条样本, 正负样本也是各占一半.<br>下面再进行一下简单的句子长度统计和词频统计.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计句子长度</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">all_sent_list = train_sent_list + test_sent_list</span><br><span class="line"></span><br><span class="line">sent_len_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> all_sent_list:</span><br><span class="line">    sent_len_list.append(len(sent.split(<span class="string">' '</span>)))</span><br><span class="line"></span><br><span class="line">counter = Counter(sent_len_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制句子长度直方图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.hist(sent_len_list, bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_11_1.png" alt="png"></p>
<p>可以看到, 大部分的句子长度小于500, 集中于200附近, 由此后续将文本截断长度设置为200.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计词频</span></span><br><span class="line">word_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> all_sent_list:</span><br><span class="line">    word_list += sent.split(<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line">counter = Counter(word_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 总的词汇(包括标点, 其它字符)有约29万个</span></span><br><span class="line">len(counter)</span><br></pre></td></tr></table></figure>
<pre><code>290691
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按词频排序, 查看出现次数最多的词汇, 发现个别词汇出现次数很多</span></span><br><span class="line">counter_sort = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">print(counter_sort[: <span class="number">30</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;the&#39;, 287012), (&#39;a&#39;, 155089), (&#39;and&#39;, 152645), (&#39;of&#39;, 142970), (&#39;to&#39;, 132566), (&#39;is&#39;, 103225), (&#39;in&#39;, 85576), (&#39;that&#39;, 64553), (&#39;I&#39;, 64024), (&#39;this&#39;, 57166), (&#39;it&#39;, 54404), (&#39;/&gt;&lt;br&#39;, 50935), (&#39;was&#39;, 46697), (&#39;as&#39;, 42507), (&#39;with&#39;, 41717), (&#39;for&#39;, 41065), (&#39;but&#39;, 33780), (&#39;The&#39;, 33095), (&#39;on&#39;, 30765), (&#39;movie&#39;, 30480), (&#39;are&#39;, 28497), (&#39;his&#39;, 27686), (&#39;film&#39;, 27389), (&#39;have&#39;, 27124), (&#39;not&#39;, 26258), (&#39;be&#39;, 25509), (&#39;you&#39;, 25108), (&#39;he&#39;, 21674), (&#39;by&#39;, 21422), (&#39;at&#39;, 21295)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不包括出现频次特别高的词汇, 绘制词频直方图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.hist(list(map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], counter_sort[<span class="number">5000</span>:])), bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(array([1.78939e+05, 3.55410e+04, 1.64970e+04, 9.91400e+03, 6.79800e+03,
        4.93300e+03, 0.00000e+00, 3.69100e+03, 3.04000e+03, 2.48100e+03,
        2.08100e+03, 1.82100e+03, 0.00000e+00, 1.53300e+03, 1.32100e+03,
        1.16800e+03, 1.06300e+03, 8.84000e+02, 0.00000e+00, 8.43000e+02,
        7.69000e+02, 6.78000e+02, 6.56000e+02, 6.00000e+02, 0.00000e+00,
        5.21000e+02, 5.12000e+02, 4.59000e+02, 4.19000e+02, 4.15000e+02,
        3.73000e+02, 0.00000e+00, 3.56000e+02, 3.35000e+02, 2.78000e+02,
        3.09000e+02, 3.08000e+02, 0.00000e+00, 2.83000e+02, 2.47000e+02,
        2.40000e+02, 2.47000e+02, 2.07000e+02, 0.00000e+00, 2.14000e+02,
        2.02000e+02, 1.94000e+02, 1.95000e+02, 1.64000e+02, 0.00000e+00,
        1.63000e+02, 1.72000e+02, 1.48000e+02, 1.61000e+02, 1.53000e+02,
        1.47000e+02, 0.00000e+00, 1.34000e+02, 1.42000e+02, 1.12000e+02,
        1.33000e+02, 1.15000e+02, 0.00000e+00, 1.14000e+02, 1.09000e+02,
        1.03000e+02, 1.04000e+02, 9.70000e+01, 0.00000e+00, 9.50000e+01,
        1.11000e+02, 9.70000e+01, 7.10000e+01, 9.20000e+01, 0.00000e+00,
        9.20000e+01, 7.50000e+01, 7.40000e+01, 8.70000e+01, 7.00000e+01,
        7.80000e+01, 0.00000e+00, 6.00000e+01, 5.80000e+01, 5.30000e+01,
        7.10000e+01, 7.00000e+01, 0.00000e+00, 7.10000e+01, 6.30000e+01,
        4.40000e+01, 7.00000e+01, 5.60000e+01, 0.00000e+00, 6.10000e+01,
        5.10000e+01, 5.70000e+01, 5.40000e+01, 5.30000e+01, 2.10000e+01]),
 array([ 1.  ,  1.84,  2.68,  3.52,  4.36,  5.2 ,  6.04,  6.88,  7.72,
         8.56,  9.4 , 10.24, 11.08, 11.92, 12.76, 13.6 , 14.44, 15.28,
        16.12, 16.96, 17.8 , 18.64, 19.48, 20.32, 21.16, 22.  , 22.84,
        23.68, 24.52, 25.36, 26.2 , 27.04, 27.88, 28.72, 29.56, 30.4 ,
        31.24, 32.08, 32.92, 33.76, 34.6 , 35.44, 36.28, 37.12, 37.96,
        38.8 , 39.64, 40.48, 41.32, 42.16, 43.  , 43.84, 44.68, 45.52,
        46.36, 47.2 , 48.04, 48.88, 49.72, 50.56, 51.4 , 52.24, 53.08,
        53.92, 54.76, 55.6 , 56.44, 57.28, 58.12, 58.96, 59.8 , 60.64,
        61.48, 62.32, 63.16, 64.  , 64.84, 65.68, 66.52, 67.36, 68.2 ,
        69.04, 69.88, 70.72, 71.56, 72.4 , 73.24, 74.08, 74.92, 75.76,
        76.6 , 77.44, 78.28, 79.12, 79.96, 80.8 , 81.64, 82.48, 83.32,
        84.16, 85.  ]),
 &lt;a list of 100 Patch objects&gt;)
</code></pre><p><img src="output_16_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> counter:</span><br><span class="line">    <span class="keyword">if</span> counter[k] == <span class="number">1</span>:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">num</span><br></pre></td></tr></table></figure>
<pre><code>178939
</code></pre><p>从上面的词频直方图可以看到, 有非常多词频低于20, 词频为1的词汇就有约180000. 在不使用其它预训练好的词向量时, 这样的词汇并不能给模型带来多少信息, 所以在进行编码时, 可以将其编码为”其它”, 即设置最大词汇量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> counter:</span><br><span class="line">        <span class="keyword">if</span> counter[k] == i:</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'词频小于等于%d, 数量%d'</span> % (i, num))</span><br></pre></td></tr></table></figure>
<pre><code>词频小于等于1, 数量178939
词频小于等于2, 数量214480
词频小于等于3, 数量230977
词频小于等于4, 数量240891
词频小于等于5, 数量247689
词频小于等于6, 数量252622
词频小于等于7, 数量256313
词频小于等于8, 数量259353
词频小于等于9, 数量261834
词频小于等于10, 数量263915
词频小于等于11, 数量265736
词频小于等于12, 数量267269
词频小于等于13, 数量268590
词频小于等于14, 数量269758
词频小于等于15, 数量270821
词频小于等于16, 数量271705
词频小于等于17, 数量272548
词频小于等于18, 数量273317
词频小于等于19, 数量273995
词频小于等于20, 数量274651
</code></pre><p>可以看到, 词频小于等于20有约27万, 联系到总词汇量约29万, 所以后续将最大词汇量设置为1万.</p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>利用TensorFlow完成文本数据预处理的常用方法一般可以有如下两种.</p>
<ul>
<li>使用<code>tf.keras.preprocessing</code>中的<code>Tokenizer</code>词典构建工具, 和<code>tf.keras.utils.Sequence</code>构建文本数据生成器管道.</li>
<li>使用<code>tf.data.Dataset</code>搭配<code>tf.keras.layers.experimental.preprocessing.TextVectorization</code>预处理层, 有点类似于结构化数据那里的<code>DenseFeatures</code>层.<br>第二种方式是TensorFlow的原生方式, 相对简单一些, 这里使用第二种方式.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_path = <span class="string">'./data/imdb/train.txt'</span></span><br><span class="line">test_path = <span class="string">'./data/imdb/test.txt'</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 指定最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个句子保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    res = tf.strings.split(line, <span class="string">'\t'</span>)</span><br><span class="line">    label, text = res[<span class="number">0</span>], res[<span class="number">1</span>]</span><br><span class="line">    label = tf.expand_dims(tf.cast(tf.strings.to_number(label), tf.int32),</span><br><span class="line">                           axis=<span class="number">0</span>)</span><br><span class="line">    text = tf.expand_dims(text, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.TextLineDataset(filenames=train_path) \</span><br><span class="line">    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.TextLineDataset(filenames=test_path) \</span><br><span class="line">    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">    .shuffle(buffer_size=<span class="number">1024</span>) \</span><br><span class="line">    .batch(BATCH_SIZE) \</span><br><span class="line">    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建词向量化层, 包括预处理(小写, 去除特殊字符/标点, 按空格分词), 构建词典, 构建指定长度(截断/填充)的向量</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="keyword">import</span> TextVectorization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 小写转换</span></span><br><span class="line">    text = tf.strings.lower(text)</span><br><span class="line">    <span class="comment"># 去掉特殊符号</span></span><br><span class="line">    text = tf.strings.regex_replace(text, <span class="string">'&lt;br /&gt;'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 去掉标点符号</span></span><br><span class="line">    text = tf.strings.regex_replace(text,</span><br><span class="line">                                    <span class="string">'[%s]'</span> % re.escape(string.punctuation), <span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text_vec_layer = TextVectorization(standardize=clean_text,</span><br><span class="line">                                   split=<span class="string">'whitespace'</span>,</span><br><span class="line">                                   max_tokens=MAX_WORDS,</span><br><span class="line">                                   output_mode=<span class="string">'int'</span>,</span><br><span class="line">                                   output_sequence_length=MAX_LEN)</span><br><span class="line">text_vec_layer.adapt(ds_train.map(<span class="keyword">lambda</span> text, label: text))</span><br><span class="line"></span><br><span class="line">text_vec_layer.get_vocabulary()[<span class="number">0</span>: <span class="number">20</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;&#39;,
 &#39;[UNK]&#39;,
 &#39;the&#39;,
 &#39;and&#39;,
 &#39;a&#39;,
 &#39;of&#39;,
 &#39;to&#39;,
 &#39;is&#39;,
 &#39;in&#39;,
 &#39;it&#39;,
 &#39;i&#39;,
 &#39;this&#39;,
 &#39;that&#39;,
 &#39;was&#39;,
 &#39;as&#39;,
 &#39;for&#39;,
 &#39;with&#39;,
 &#39;movie&#39;,
 &#39;but&#39;,
 &#39;film&#39;]
</code></pre><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>针对文本分类, 在深度学习中有不少方法与模型, 这里使用比较简单的双向LSTM模型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models, regularizers, callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Embedding, LSTM, Bidirectional, Dropout, Dense</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(text_vec_layer)</span><br><span class="line">model.add(Embedding(MAX_WORDS, <span class="number">128</span>, input_length=MAX_LEN))</span><br><span class="line">model.add(Bidirectional(LSTM(<span class="number">64</span>, kernel_regularizer=regularizers.l2(<span class="number">0.01</span>))))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(<span class="string">'adam'</span>, <span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
157/157 [==============================] - 50s 320ms/step - loss: 1.3471 - accuracy: 0.7157 - val_loss: 0.4484 - val_accuracy: 0.8360
Epoch 2/1000
157/157 [==============================] - 47s 300ms/step - loss: 0.3424 - accuracy: 0.8825 - val_loss: 0.3506 - val_accuracy: 0.8664
Epoch 3/1000
157/157 [==============================] - 47s 299ms/step - loss: 0.2467 - accuracy: 0.9196 - val_loss: 0.3507 - val_accuracy: 0.8638
Epoch 4/1000
157/157 [==============================] - 46s 294ms/step - loss: 0.2021 - accuracy: 0.9379 - val_loss: 0.3884 - val_accuracy: 0.8478
Epoch 5/1000
157/157 [==============================] - 45s 288ms/step - loss: 0.1690 - accuracy: 0.9534 - val_loss: 0.4779 - val_accuracy: 0.8516
Epoch 6/1000
157/157 [==============================] - 45s 289ms/step - loss: 0.1319 - accuracy: 0.9667 - val_loss: 0.4618 - val_accuracy: 0.8546
Epoch 7/1000
157/157 [==============================] - 45s 288ms/step - loss: 0.1159 - accuracy: 0.9714 - val_loss: 0.5181 - val_accuracy: 0.8494
Epoch 8/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0965 - accuracy: 0.9798 - val_loss: 0.4833 - val_accuracy: 0.8556
Epoch 9/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0753 - accuracy: 0.9867 - val_loss: 0.5316 - val_accuracy: 0.8510
Epoch 10/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0680 - accuracy: 0.9883 - val_loss: 0.5539 - val_accuracy: 0.8504
Epoch 11/1000
157/157 [==============================] - 45s 287ms/step - loss: 0.0632 - accuracy: 0.9888 - val_loss: 0.5177 - val_accuracy: 0.8482
Epoch 12/1000
157/157 [==============================] - 45s 289ms/step - loss: 0.0572 - accuracy: 0.9912 - val_loss: 0.5414 - val_accuracy: 0.8508
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_33_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_34_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-结构化数据建模</title>
    <url>/2020/08/13/TensorFlow/TensorFlow-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>结构化数据是比较基础, 但也是很重要, 很常用的一类数据.<br>这里示范一下完整的从结构化数据处理, 到构建模型, 再到训练评估的过程.</p>
<a id="more"></a>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>使用经典的titanic数据集, 目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存.<br>结构化数据可以使用pandas中的DataFrame进行预处理, 包括缺失值填充等. 这里主要目的是展示数据处理和建模流程, 因此不做更多的特征工程.<br>字段说明:</p>
<ul>
<li>Survived: 0代表死亡, 1代表存活 (y标签)</li>
<li>Pclass: 乘客所持票类, 有三种值(1, 2, 3) (embedding)</li>
<li>Name: 乘客姓名 (舍去)</li>
<li>Sex: 乘客性别 (转换成bool特征)</li>
<li>Age: 乘客年龄(有缺失) (数值特征, 添加“年龄是否缺失”作为辅助特征)</li>
<li>SibSp: 乘客兄弟姐妹/配偶的个数(整数值) (数值特征)</li>
<li>Parch: 乘客父母/孩子的个数(整数值) (数值特征)</li>
<li>Ticket: 票号(字符串) (舍去)</li>
<li>Fare: 乘客所持票的价格(浮点数, 0-500不等) (数值特征)</li>
<li>Cabin: 乘客所在船舱(有缺失) (添加“所在船舱是否缺失”作为辅助特征)</li>
<li>Embarked: 乘客登船港口: S、C、Q(有缺失) (embedding)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'./data/titanic/test.csv'</span>)</span><br><span class="line">train.shape, test.shape</span><br></pre></td></tr></table></figure>
<pre><code>((891, 13), (418, 13))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 13 columns):
Age            714 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
Fare           891 non-null float64
Name           891 non-null object
Parch          891 non-null int64
PassengerId    891 non-null int64
Pclass         891 non-null int64
Sex            891 non-null object
SibSp          891 non-null int64
Survived       891 non-null float64
Ticket         891 non-null object
id             891 non-null object
dtypes: float64(3), int64(4), object(6)
memory usage: 90.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 13 columns):
Age            332 non-null float64
Cabin          91 non-null object
Embarked       418 non-null object
Fare           417 non-null float64
Name           418 non-null object
Parch          418 non-null int64
PassengerId    418 non-null int64
Pclass         418 non-null int64
Sex            418 non-null object
SibSp          418 non-null int64
Survived       418 non-null int64
Ticket         418 non-null object
id             418 non-null object
dtypes: float64(2), int64(5), object(6)
memory usage: 42.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_df = train.append(test)</span><br><span class="line">all_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 13 columns):
Age            1046 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
Fare           1308 non-null float64
Name           1309 non-null object
Parch          1309 non-null int64
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Sex            1309 non-null object
SibSp          1309 non-null int64
Survived       1309 non-null float64
Ticket         1309 non-null object
id             1309 non-null object
dtypes: float64(3), int64(4), object(6)
memory usage: 143.2+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ft_cols_list = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Pclass</span></span><br><span class="line">cat_list = all_df[<span class="string">'Pclass'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line">ft_cols_list.append(</span><br><span class="line">    feature_column.embedding_column(</span><br><span class="line">        feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">            <span class="string">'Pclass'</span>, vocabulary_list=cat_list), <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[3, 1, 2]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Sex</span></span><br><span class="line">cat_list = all_df[<span class="string">'Sex'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line">all_df[<span class="string">'Sex'</span>] = all_df[<span class="string">'Sex'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">'male'</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Sex'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;male&#39;, &#39;female&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Age</span></span><br><span class="line">mean_ = all_df[<span class="string">'Age'</span>].mean()</span><br><span class="line">print(mean_)</span><br><span class="line">all_df[<span class="string">'is_age_null'</span>] = all_df[<span class="string">'Age'</span>].apply(<span class="keyword">lambda</span> x: int(pd.isnull(x)))</span><br><span class="line">all_df[<span class="string">'Age'</span>] = all_df[<span class="string">'Age'</span>].fillna(mean_)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'is_age_null'</span>))</span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Age'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>29.881137667304014
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SibSp</span></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'SibSp'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parch</span></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Parch'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Fare</span></span><br><span class="line">mean_ = all_df[<span class="string">'Fare'</span>].mean()</span><br><span class="line">print(mean_)</span><br><span class="line">all_df[<span class="string">'Fare'</span>] = all_df[<span class="string">'Fare'</span>].fillna(mean_)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'Fare'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>33.2954792813456
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cabin</span></span><br><span class="line"></span><br><span class="line">all_df[<span class="string">'is_cabin_null'</span>] = all_df[<span class="string">'Cabin'</span>].apply(<span class="keyword">lambda</span> x: int(pd.isnull(x)))</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(feature_column.numeric_column(<span class="string">'is_cabin_null'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Embarked</span></span><br><span class="line"></span><br><span class="line">print(all_df[<span class="string">'Embarked'</span>].value_counts())</span><br><span class="line">all_df[<span class="string">'Embarked'</span>] = all_df[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line"></span><br><span class="line">cat_list = all_df[<span class="string">'Embarked'</span>].unique().tolist()</span><br><span class="line">print(cat_list)</span><br><span class="line"></span><br><span class="line">ft_cols_list.append(</span><br><span class="line">    feature_column.embedding_column(</span><br><span class="line">        feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">            <span class="string">'Embarked'</span>, vocabulary_list=cat_list), <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<pre><code>S    914
C    270
Q    123
Name: Embarked, dtype: int64
[&#39;S&#39;, &#39;C&#39;, &#39;Q&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 1309 entries, 0 to 417
Data columns (total 15 columns):
Age              1309 non-null float64
Cabin            295 non-null object
Embarked         1309 non-null object
Fare             1309 non-null float64
Name             1309 non-null object
Parch            1309 non-null int64
PassengerId      1309 non-null int64
Pclass           1309 non-null int64
Sex              1309 non-null int64
SibSp            1309 non-null int64
Survived         1309 non-null float64
Ticket           1309 non-null object
id               1309 non-null object
is_age_null      1309 non-null int64
is_cabin_null    1309 non-null int64
dtypes: float64(3), int64(7), object(5)
memory usage: 163.6+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = all_df.iloc[: <span class="number">891</span>]</span><br><span class="line">test = all_df.iloc[<span class="number">891</span>:]</span><br><span class="line">train.to_csv(<span class="string">'./data/titanic/pre_train.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">test.to_csv(<span class="string">'./data/titanic/pre_test.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">ds_train = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=<span class="string">"./data/titanic/pre_train.csv"</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">''</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=<span class="string">"./data/titanic/pre_test.csv"</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">''</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, models, regularizers, callbacks</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.DenseFeatures(ft_cols_list))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">100</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(</span><br><span class="line">    layers.Dense(<span class="number">50</span>,</span><br><span class="line">                 activation=<span class="string">'relu'</span>,</span><br><span class="line">                 kernel_regularizer=regularizers.l2(<span class="number">0.01</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'AUC'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,</span><br><span class="line">                    validation_data=ds_test,</span><br><span class="line">                    epochs=<span class="number">1000</span>,</span><br><span class="line">                    callbacks=[</span><br><span class="line">                        callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>,</span><br><span class="line">                                                patience=<span class="number">10</span>,</span><br><span class="line">                                                restore_best_weights=<span class="literal">True</span>),</span><br><span class="line">                        callbacks.ReduceLROnPlateau(factor=<span class="number">0.5</span>, patience=<span class="number">3</span>)</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1000
7/7 [==============================] - 2s 333ms/step - loss: 2.2648 - AUC: 0.5301 - val_loss: 2.1298 - val_AUC: 0.6524
Epoch 122/1000
7/7 [==============================] - 0s 14ms/step - loss: 0.6692 - AUC: 0.8731 - val_loss: 0.7278 - val_AUC: 0.7968
</code></pre><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span> + metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span> + metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span> + metric, <span class="string">'val_'</span> + metric])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_26_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history, <span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_27_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-特征列</title>
    <url>/2020/08/12/TensorFlow/TensorFlow-%E7%89%B9%E5%BE%81%E5%88%97/</url>
    <content><![CDATA[<p>特征列<code>tf.feature_column</code>通常用于对结构化数据实施特征工程时候使用, 可以让你将各种不同的原始数据转化为模型可用的格式, 图像或者文本数据一般不会用到特征列.</p>
<a id="more"></a>
<h1 id="特征列介绍"><a href="#特征列介绍" class="headerlink" title="特征列介绍"></a>特征列介绍</h1><p><img src="fig_0.jpg" alt="fig"></p>
<p>对于经过一些简单清洗的结构化数据来说, 数据的类型有各种各样的, 不过如果从计算的角度来说的话, 可以大体分成两类, 即<strong>数值(numeric)特征</strong>, 与<strong>类别(categorical)特征</strong>.</p>
<p><img src="fig_1.jpg" alt="fig"></p>
<p>特征列的作用, 类似于连接原始数据与模型之间的一座桥梁, 原始的结构化数据在经过特征列以后, 可以以更好的形式方便地送入模型当做, 进行训练与预测.</p>
<p><img src="fig_2.jpg" alt="fig"></p>
<p>而具体说来, <code>tf.feature_column</code>的API如上, 有两个类分别是Categorical-Column与Dense-Column, 各自代表了类别与类别与数值特征的相关类, 以及各自的数据处理方法. 其中的<code>bucketized_column</code>方法作用于数值特征, 但转换得到的结果与类别特征一致, 所以图上同时连接到两个类.</p>
<p>下面逐一展示各个方法的主要用法.</p>
<h1 id="数值型方法"><a href="#数值型方法" class="headerlink" title="数值型方法"></a>数值型方法</h1><h2 id="numeric-column"><a href="#numeric-column" class="headerlink" title="numeric_column"></a>numeric_column</h2><p><code>numeric_column</code>是数值型方法中常用且简单的方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数值变量的名称(这里是x)输入, 得到特征列, 告诉模型这是一个数值特征</span></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_cols_list = [ft_col]  <span class="comment"># 需要加入列表, 列表可以包含多个特征列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过DenseFeatures进行转换表示, 且输入需要是字典(需要特征名称信息)</span></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=</span><br><span class="line">array([[1.],</span><br><span class="line">       [2.],</span><br><span class="line">       [3.],</span><br><span class="line">       [4.],</span><br><span class="line">       [5.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="bucketized-column"><a href="#bucketized-column" class="headerlink" title="bucketized_column"></a>bucketized_column</h2><p><img src="fig_3.jpg" alt="fig"></p>
<p>如上图, 在使用<code>bucketized_column</code>时, 需要设置分割区间, 假设输入区间按大小顺序包含3个数值, 则可得到4个个分箱.</p>
<p><img src="fig_4.jpg" alt="fig"></p>
<p>在得到分箱后, <code>bucketized_column</code>会进一步将其进行独热编码表示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要先转换为numeric型</span></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_col = feature_column.bucketized_column(ft_col, boundaries=[<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从结果来看, 分割区间是左闭右开的</span></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">5</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="embedding-column"><a href="#embedding-column" class="headerlink" title="embedding_column"></a>embedding_column</h2><p>一般来说, 类别特征或者离散化的数值特征会做独热编码, 这样就可以直接输入模型进行训练. 不过若是对深度学习有更多了解的同学, 知道可能先进行embedding, 再参与运算会好一些. 这里<code>tf.embedding_column</code>就是做的这件事情.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line">ft_col = feature_column.numeric_column(<span class="string">'x'</span>)</span><br><span class="line">ft_col = feature_column.bucketized_column(ft_col, boundaries=[<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="comment"># 利用上面得到的独热编码, 转换为embedding, 并设置向量维度</span></span><br><span class="line">ft_col = feature_column.embedding_column(ft_col, <span class="number">3</span>)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=</span><br><span class="line">array([[ 0.76490587, -0.27499318,  0.26791143],</span><br><span class="line">       [ 0.0151019 , -0.84910244, -0.02452291],</span><br><span class="line">       [ 0.0151019 , -0.84910244, -0.02452291],</span><br><span class="line">       [-0.95111394,  0.39225465, -0.36392847],</span><br><span class="line">       [-0.95111394,  0.39225465, -0.36392847]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h1 id="类别型方法"><a href="#类别型方法" class="headerlink" title="类别型方法"></a>类别型方法</h1><h2 id="categorical-column-with-vocabulary-list"><a href="#categorical-column-with-vocabulary-list" class="headerlink" title="categorical_column_with_vocabulary_list"></a>categorical_column_with_vocabulary_list</h2><p><img src="fig_5.jpg" alt="fig"></p>
<p>如上图所示, 将类别特征转换为独热编码.</p>
<p><code>categorical_column_with_vocabulary_list</code>与<code>categorical_column_with_vocabulary_file</code>方法类似, 只是前者利用列表指定需要编码的属性, 后者利用文件(将属性写入文件)来指定编码.</p>
<p>同时, 可能细心的同学注意到了, 前面的数值型方法中没有<code>indicator_column</code>方法, 这是因为即便是类别型特征, 在入模前也需要转换成数值型, 而<code>indicator_column</code>的作用就是这个.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在vocabulary_list中指定需要编码的属性</span></span><br><span class="line">ft_col = feature_column.categorical_column_with_vocabulary_list(<span class="string">'x'</span>, vocabulary_list=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line"><span class="comment"># 利用indicator_column再进行转换</span></span><br><span class="line">ft_col = feature_column.indicator_column(ft_col)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=</span><br><span class="line">array([[1., 0., 0.],</span><br><span class="line">       [0., 1., 0.],</span><br><span class="line">       [0., 0., 1.],</span><br><span class="line">       [1., 0., 0.],</span><br><span class="line">       [0., 1., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="categorical-column-with-hash-bucket"><a href="#categorical-column-with-hash-bucket" class="headerlink" title="categorical_column_with_hash_bucket"></a>categorical_column_with_hash_bucket</h2><p><img src="fig_6.jpg" alt="fig"></p>
<p>在一些时候, 类别特征中包含的属性数量过多, 一来指定编码会比较麻烦, 而来可能造成过拟合. 此时一个解决方法是利用hash映射, 将较多的属性映射到不同的较少的分箱. 带来的问题是会存在hash冲突.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">data_dict = &#123;<span class="string">'x'</span>: data&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要指定分箱数量</span></span><br><span class="line">ft_col = feature_column.categorical_column_with_hash_bucket(<span class="string">'x'</span>, <span class="number">2</span>)</span><br><span class="line">ft_col = feature_column.indicator_column(ft_col)</span><br><span class="line">ft_cols_list = [ft_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 2), dtype=float32, numpy=</span><br><span class="line">array([[0., 1.],</span><br><span class="line">       [1., 0.],</span><br><span class="line">       [1., 0.],</span><br><span class="line">       [0., 1.],</span><br><span class="line">       [1., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="crossed-column"><a href="#crossed-column" class="headerlink" title="crossed_column"></a>crossed_column</h2><p>对于线性模型来说, 由于表达能力较差, 有时候需要手动将特征进行交叉组合, 来提高学习效果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dict = &#123;<span class="string">'x'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">'y'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]&#125;</span><br><span class="line"></span><br><span class="line">cross_col = feature_column.crossed_column([<span class="string">'x'</span>, <span class="string">'y'</span>], hash_bucket_size=<span class="number">5</span>)</span><br><span class="line">cross_col = feature_column.indicator_column(cross_col)</span><br><span class="line"></span><br><span class="line">ft_cols_list = [cross_col]</span><br><span class="line"></span><br><span class="line">tf.keras.layers.DenseFeatures(ft_cols_list)(data_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=</span><br><span class="line">array([[1., 0., 0., 0., 0.],</span><br><span class="line">       [0., 0., 0., 0., 1.],</span><br><span class="line">       [0., 0., 0., 1., 0.],</span><br><span class="line">       [1., 0., 0., 0., 0.],</span><br><span class="line">       [0., 1., 0., 0., 0.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas读写大文件</title>
    <url>/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE/pandas%E8%AF%BB%E5%86%99%E5%A4%A7%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>在日常处理数据时, 可能会遇到一份数据比较大, 不能整个读进内存. 同时又不方便用一些”正规军”(如spark)来进行处理, 那应该怎么办呢?</p>
<p>这时候仍然可以使用pandas进行处理, 注意一些细节就行. </p>
<p>让我们一起喊出来, pandas, 永远滴神╰(<em>°▽°</em>)╯</p>
<a id="more"></a>
<p>利用pandas处理大数据, 总体来说有以下三点需要注意:</p>
<ul>
<li>分批读入数据, 以保证内存够用.</li>
<li>多进程处理数据, 以保证数据处理速度.</li>
<li>分批保存数据, 可保存成多份文件, 也可以保存到一份文件.</li>
</ul>
<h1 id="分批读入数据"><a href="#分批读入数据" class="headerlink" title="分批读入数据"></a>分批读入数据</h1><h2 id="chunksize"><a href="#chunksize" class="headerlink" title="chunksize"></a>chunksize</h2><p>使用<code>chunksize</code>后<code>pd.read_csv</code>将会返回一个可以迭代的TextFileReader对象. <code>chunksize</code>的值代表了每次迭代对象的数量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, chunksize=<span class="number">5</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;pandas.io.parsers.TextFileReader at 0x7fb4c10bf790&gt;</span><br></pre></td></tr></table></figure>
<p>当我们需要从这个TextFileReader对象中取值时, 可以使用for循环. </p>
<p>值得注意的是, 当最后剩余数据条数小于<code>chunksize</code>时, 仅返回剩余数据, 在一些时候要防止由此出现的bug.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> df:</span><br><span class="line">    print(type(data))</span><br><span class="line">    print(data.iloc[:, : <span class="number">3</span>])</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">   PassengerId  Survived  Pclass</span><br><span class="line">0            1         0       3</span><br><span class="line">1            2         1       1</span><br><span class="line">2            3         1       3</span><br><span class="line">3            4         1       1</span><br><span class="line">4            5         0       3</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">   PassengerId  Survived  Pclass</span><br><span class="line">5            6         0       3</span><br><span class="line">6            7         0       1</span><br><span class="line">7            8         0       3</span><br><span class="line">8            9         1       3</span><br><span class="line">9           10         1       2</span><br></pre></td></tr></table></figure>
<h2 id="iterator"><a href="#iterator" class="headerlink" title="iterator"></a>iterator</h2><p>与<code>chunksize</code>类似, 返回的也是TextFileReader 对象, 但是使用的时候和<code>chunksize</code>参数略有不同, 需要使用<code>get_chunk</code>方法获取数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, iterator=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;pandas.io.parsers.TextFileReader at 0x7fb4c2487050&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.get_chunk(<span class="number">2</span>).iloc[:, : <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.get_chunk(<span class="number">2</span>).iloc[:, : <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
</div>
<h2 id="dtype"><a href="#dtype" class="headerlink" title="dtype"></a>dtype</h2><p>有时候会发现生成并保存了了一个csv文件, 最后读取的时候发现总是内存不足不能读取. 该现象的出现是由于pandas中的<code>read_csv</code>方法必须要读取所有行才能确定每一类的数据类型<code>dtype</code>, 据说在读取的时候默认以字符串格式读入, 当读取完最后一行后才会更改其类型.</p>
<p>以字符串格式读入时, 内存占用比较大, 在这时候可能会出现内存不足的问题. 所以我们只需要让pandas在读取的时候就按照对应的格式读取就不会出现该问题.</p>
<p>可以直接传入pandas支持的类型, 具体有哪些请见 <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes" target="_blank" rel="noopener">pandas官方文档</a>, 也可以传入numpy支持的类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">dtype_dict = &#123;</span><br><span class="line">    <span class="string">'PassengerId'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Survived'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Pclass'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Name'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Sex'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Age'</span>: <span class="string">'float64'</span>,</span><br><span class="line">    <span class="string">'SibSp'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Parch'</span>: <span class="string">'int64'</span>,</span><br><span class="line">    <span class="string">'Ticket'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Fare'</span>: <span class="string">'float64'</span>,</span><br><span class="line">    <span class="string">'Cabin'</span>: <span class="string">'object'</span>,</span><br><span class="line">    <span class="string">'Embarked'</span>: <span class="string">'object'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, dtype=dtype_dict)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 12 columns):</span><br><span class="line">PassengerId    891 non-null int64</span><br><span class="line">Survived       891 non-null int64</span><br><span class="line">Pclass         891 non-null int64</span><br><span class="line">Name           891 non-null object</span><br><span class="line">Sex            891 non-null object</span><br><span class="line">Age            714 non-null float64</span><br><span class="line">SibSp          891 non-null int64</span><br><span class="line">Parch          891 non-null int64</span><br><span class="line">Ticket         891 non-null object</span><br><span class="line">Fare           891 non-null float64</span><br><span class="line">Cabin          204 non-null object</span><br><span class="line">Embarked       889 non-null object</span><br><span class="line">dtypes: float64(2), int64(5), object(5)</span><br><span class="line">memory usage: 83.7+ KB</span><br></pre></td></tr></table></figure>
<h2 id="usecols"><a href="#usecols" class="headerlink" title="usecols"></a>usecols</h2><p><code>usecols</code>可以指定读取的列, 其中的参数可以是列名或者列序号.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">ft_list = [<span class="string">'PassengerId'</span>, <span class="string">'Survived'</span>, <span class="string">'Pclass'</span>]</span><br><span class="line">df = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>, usecols=ft_list)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
</tr>
</tbody>
</table>
</div>
<h2 id="nrows"><a href="#nrows" class="headerlink" title="nrows"></a>nrows</h2><p><code>nrows</code>可以指定读取的行数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;./data/titanic/train.csv&apos;, nrows=5)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">PassengerId</th>
<th style="text-align:right">Survived</th>
<th style="text-align:right">Pclass</th>
<th style="text-align:right">Name</th>
<th style="text-align:right">Sex</th>
<th style="text-align:right">Age</th>
<th style="text-align:right">SibSp</th>
<th style="text-align:right">Parch</th>
<th style="text-align:right">Ticket</th>
<th style="text-align:right">Fare</th>
<th style="text-align:right">Cabin</th>
<th style="text-align:right">Embarked</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Braund, Mr. Owen Harris</td>
<td style="text-align:right">male</td>
<td style="text-align:right">22</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">A/5 21171</td>
<td style="text-align:right">7.2500</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">Cumings, Mrs. John Bradley (Florence Briggs Th…</td>
<td style="text-align:right">female</td>
<td style="text-align:right">38</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">PC 17599</td>
<td style="text-align:right">71.2833</td>
<td style="text-align:right">C85</td>
<td style="text-align:right">C</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Heikkinen, Miss. Laina</td>
<td style="text-align:right">female</td>
<td style="text-align:right">26</td>
<td style="text-align:right">0</td>
<td style="text-align:right">0</td>
<td style="text-align:right">STON/O2. 3101282</td>
<td style="text-align:right">7.9250</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td style="text-align:right">female</td>
<td style="text-align:right">35</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">113803</td>
<td style="text-align:right">53.1000</td>
<td style="text-align:right">C123</td>
<td style="text-align:right">S</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">Allen, Mr. William Henry</td>
<td style="text-align:right">male</td>
<td style="text-align:right">35</td>
<td style="text-align:right">0</td>
<td style="text-align:right">0</td>
<td style="text-align:right">373450</td>
<td style="text-align:right">8.0500</td>
<td style="text-align:right">NaN</td>
<td style="text-align:right">S</td>
</tr>
</tbody>
</table>
</div>
<h1 id="多进程处理数据"><a href="#多进程处理数据" class="headerlink" title="多进程处理数据"></a>多进程处理数据</h1><p>当数据量多的时候, 或者处理过程比较复杂的时候, 单进程的处理会显得非常吃力. 这时候除了要尽可能优化处理方法代码本身, 一个更通用的方法就是利用多进程. 而pandas本身不带多进程功能, 所以需要我们自己实现多进程.</p>
<p>以我的经验来说, 当样本量在100万以下时, 直接对pandas的列进行处理, 时间在可接受范围内, 而当样本量达到百万量级时, 优先考虑多进程加速.</p>
<h2 id="多进程函数"><a href="#多进程函数" class="headerlink" title="多进程函数"></a>多进程函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多进程函数.</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordered_multiprocess_task</span><span class="params">(func=None, data=None, max_process=None)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    func: 任务函数, 必需的参数为数据(单个).</span></span><br><span class="line"><span class="string">    data: 数据, 类list.</span></span><br><span class="line"><span class="string">    max_process: 最大进程数.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    n_cpu = mp.cpu_count()</span><br><span class="line">    <span class="keyword">if</span> max_process <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_process = n_cpu</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> max_process &gt; n_cpu:</span><br><span class="line">            max_process = n_cpu</span><br><span class="line">    <span class="keyword">with</span> mp.Pool(max_process) <span class="keyword">as</span> pool:</span><br><span class="line">        res = pool.map(func, data)</span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line">        pool.terminate()</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多进程简单示例.</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">ordered_multiprocess_task(square, data, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1, 4, 9, 16, 25]</span><br></pre></td></tr></table></figure>
<h2 id="多进程读取"><a href="#多进程读取" class="headerlink" title="多进程读取"></a>多进程读取</h2><p>有时候原始数据能够全部读入内存, 且需要读取的文件数据分散为许多份零碎的数据时, 也可以使用多进程读取.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取单份文件.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_csv</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pd.read_csv(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历返回文件路径列表.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_file_list</span><span class="params">(path)</span>:</span></span><br><span class="line">    file_list = []</span><br><span class="line">    <span class="keyword">for</span> parents, dirnames, filenames <span class="keyword">in</span> os.walk(path):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            file_list.append(os.path.join(parents, filename))</span><br><span class="line">    <span class="keyword">return</span> file_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程读取数据.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mp_read_csv</span><span class="params">(path)</span>:</span></span><br><span class="line">    file_list = get_file_list(path)</span><br><span class="line">    res = ordered_multiprocess_task(func=read_csv, data=file_list)</span><br><span class="line">    df = pd.concat(res, axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">df = mp_read_csv(file_path)</span><br></pre></td></tr></table></figure>
<h2 id="多进程处理"><a href="#多进程处理" class="headerlink" title="多进程处理"></a>多进程处理</h2><p>多进程处理单列/多列数据.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多进程.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mp_func</span><span class="params">(func, df, n_split=<span class="number">4</span>, max_process=None)</span>:</span></span><br><span class="line">    <span class="comment"># 切分数据.</span></span><br><span class="line">    n_sample = df.shape[<span class="number">0</span>]</span><br><span class="line">    n_sample_per_split = n_sample // n_split + <span class="number">1</span></span><br><span class="line">    data_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_split):</span><br><span class="line">        data_list.append(df.iloc[i * n_sample_per_split: (i + <span class="number">1</span>) * n_sample_per_split])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多进程处理.</span></span><br><span class="line">    res = ordered_multiprocess_task(func, data_list, max_process)</span><br><span class="line"></span><br><span class="line">    df = pd.concat(res, axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程函数.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ...</span><br><span class="line">  </span><br><span class="line">df = mp_func(func, df)</span><br></pre></td></tr></table></figure>
<h1 id="分批保存数据"><a href="#分批保存数据" class="headerlink" title="分批保存数据"></a>分批保存数据</h1><p>在分批读取, 分批处理完数据后, 需要进行分批保存.</p>
<p>可以将每次分批处理的数据以不同的文件名分别保存, 也可以仍然统一保存到一个文件里. 这里主要介绍第二种方法, 主要就是利用<code>to_csv</code>方法中的<code>mode</code>参数.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!rm ./data/tmp.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'a'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'b'</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;)</span><br><span class="line">df.to_csv(<span class="string">'./data/tmp.csv'</span>, index=<span class="literal">False</span>, mode=<span class="string">'a'</span>)</span><br><span class="line">df.to_csv(<span class="string">'./data/tmp.csv'</span>, index=<span class="literal">False</span>, mode=<span class="string">'a'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">df_read = pd.read_csv(<span class="string">'./data/tmp.csv'</span>)</span><br><span class="line">df_read</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right">a</th>
<th style="text-align:right">b</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
</tr>
</tbody>
</table>
</div>
<p>与Python的文件读写一样, <code>mode</code>默认参数为<code>w</code>, 表示重写, 而<code>a</code>表示接着写. 但要注意要正确地将<code>header</code>参数设置为<code>None</code>, 否则列名将会被当成普通数据重复写入.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-数据管道</title>
    <url>/2020/08/08/TensorFlow/TensorFlow-%E6%95%B0%E6%8D%AE%E7%AE%A1%E9%81%93/</url>
    <content><![CDATA[<p>如果需要训练的数据大小不大, 那么可以直接全部读入内存中进行训练, 这样一般效率最高.<br>但如果需要训练的数据很大, 无法一次载入内存, 那么通常需要在训练的过程中分批读入.<br>使用<code>tf.data</code>的API可以构建数据输入管道, 轻松处理大量的数据, 不同的数据格式, 以及不同的数据转换.</p>
<a id="more"></a>
<h1 id="构建数据管道"><a href="#构建数据管道" class="headerlink" title="构建数据管道"></a>构建数据管道</h1><h2 id="从numpy-array构建数据管道"><a href="#从numpy-array构建数据管道" class="headerlink" title="从numpy array构建数据管道"></a>从numpy array构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从numpy array构建数据管道</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((iris[<span class="string">"data"</span>], iris[<span class="string">"target"</span>]))</span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(features, label)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
</code></pre><h2 id="从pandas-DataFrame构建数据管道"><a href="#从pandas-DataFrame构建数据管道" class="headerlink" title="从pandas DataFrame构建数据管道"></a>从pandas DataFrame构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从 pandas DataFrame构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">df = pd.DataFrame(iris[<span class="string">"data"</span>], columns=iris[<span class="string">'feature_names'</span>])</span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((df.to_dict(<span class="string">"list"</span>), iris[<span class="string">"target"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> ds.take(<span class="number">3</span>):</span><br><span class="line">    print(features, label)</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.1&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.5&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.9&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
{&#39;sepal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.7&gt;, &#39;sepal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.2&gt;, &#39;petal length (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.3&gt;, &#39;petal width (cm)&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.2&gt;} tf.Tensor(0, shape=(), dtype=int64)
</code></pre><h2 id="从Python-gennerator构建数据管道"><a href="#从Python-gennerator构建数据管道" class="headerlink" title="从Python gennerator构建数据管道"></a>从Python gennerator构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Python generator构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个从文件中读取图片的generator</span></span><br><span class="line">image_generator = ImageDataGenerator(rescale=<span class="number">1.0</span> / <span class="number">255</span>).flow_from_directory(</span><br><span class="line">    <span class="string">"./data/cifar2/test/"</span>,</span><br><span class="line">    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">classdict = image_generator.class_indices</span><br><span class="line">print(classdict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features, label <span class="keyword">in</span> image_generator:</span><br><span class="line">        <span class="keyword">yield</span> features, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,</span><br><span class="line">                                    output_types=(tf.float32, tf.int32))</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
{&#39;0_airplane&#39;: 0, &#39;1_automobile&#39;: 1}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_8_0.png" alt="png"></p>
<h2 id="从csv文件构建数据管道"><a href="#从csv文件构建数据管道" class="headerlink" title="从csv文件构建数据管道"></a>从csv文件构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line">ds = tf.data.experimental.make_csv_dataset(</span><br><span class="line">    file_pattern=[<span class="string">"./data/titanic/train.csv"</span>, <span class="string">"./data/titanic/test.csv"</span>],</span><br><span class="line">    batch_size=<span class="number">3</span>,</span><br><span class="line">    label_name=<span class="string">"Survived"</span>,</span><br><span class="line">    na_value=<span class="string">""</span>,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    ignore_errors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> ds.take(<span class="number">2</span>):</span><br><span class="line">    print(data, label)</span><br></pre></td></tr></table></figure>
<pre><code>OrderedDict([(&#39;PassengerId&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([185, 948, 331], dtype=int32)&gt;), (&#39;Pclass&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 3], dtype=int32)&gt;), (&#39;Name&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=
array([b&#39;Kink-Heilmann, Miss. Luise Gretchen&#39;, b&#39;Cor, Mr. Bartol&#39;,
       b&#39;McCoy, Miss. Agnes&#39;], dtype=object)&gt;), (&#39;Sex&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;female&#39;, b&#39;male&#39;, b&#39;female&#39;], dtype=object)&gt;), (&#39;Age&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 4., 35.,  0.], dtype=float32)&gt;), (&#39;SibSp&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 2], dtype=int32)&gt;), (&#39;Parch&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 0], dtype=int32)&gt;), (&#39;Ticket&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;315153&#39;, b&#39;349230&#39;, b&#39;367226&#39;], dtype=object)&gt;), (&#39;Fare&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([22.025 ,  7.8958, 23.25  ], dtype=float32)&gt;), (&#39;Cabin&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;&#39;, b&#39;&#39;, b&#39;&#39;], dtype=object)&gt;), (&#39;Embarked&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;S&#39;, b&#39;S&#39;, b&#39;Q&#39;], dtype=object)&gt;)]) tf.Tensor([1. 0. 1.], shape=(3,), dtype=float32)
OrderedDict([(&#39;PassengerId&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1228,   57, 1014], dtype=int32)&gt;), (&#39;Pclass&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 1], dtype=int32)&gt;), (&#39;Name&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=
array([b&#39;de Brito, Mr. Jose Joaquim&#39;, b&#39;Rugg, Miss. Emily&#39;,
       b&#39;Schabert, Mrs. Paul (Emma Mock)&#39;], dtype=object)&gt;), (&#39;Sex&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;male&#39;, b&#39;female&#39;, b&#39;female&#39;], dtype=object)&gt;), (&#39;Age&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([32., 21., 35.], dtype=float32)&gt;), (&#39;SibSp&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)&gt;), (&#39;Parch&#39;, &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)&gt;), (&#39;Ticket&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;244360&#39;, b&#39;C.A. 31026&#39;, b&#39;13236&#39;], dtype=object)&gt;), (&#39;Fare&#39;, &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([13.  , 10.5 , 57.75], dtype=float32)&gt;), (&#39;Cabin&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;&#39;, b&#39;&#39;, b&#39;C28&#39;], dtype=object)&gt;), (&#39;Embarked&#39;, &lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;S&#39;, b&#39;S&#39;, b&#39;C&#39;], dtype=object)&gt;)]) tf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)
</code></pre><h2 id="从文本文件构建数据管道"><a href="#从文本文件构建数据管道" class="headerlink" title="从文本文件构建数据管道"></a>从文本文件构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从文本文件构建数据管道</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.TextLineDataset(</span><br><span class="line">    filenames=[<span class="string">"./data/titanic/train.csv"</span>, </span><br><span class="line">               <span class="string">"./data/titanic/test.csv"</span>]).skip(<span class="number">1</span>)  <span class="comment"># 略去第一行header</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;1,0,3,&quot;Braund, Mr. Owen Harris&quot;,male,22,1,0,A/5 21171,7.25,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;2,1,1,&quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;,female,38,1,0,PC 17599,71.2833,C85,C&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;3,1,3,&quot;Heikkinen, Miss. Laina&quot;,female,26,0,0,STON/O2. 3101282,7.925,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;4,1,1,&quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;,female,35,1,0,113803,53.1,C123,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;5,0,3,&quot;Allen, Mr. William Henry&quot;,male,35,0,0,373450,8.05,,S&#39;, shape=(), dtype=string)
</code></pre><h2 id="从文件路径构建数据管道"><a href="#从文件路径构建数据管道" class="headerlink" title="从文件路径构建数据管道"></a>从文件路径构建数据管道</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> ds.take(<span class="number">5</span>):</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;./data/cifar2/train/0_airplane/1625.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/3325.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/3621.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/0_airplane/1615.jpg&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;./data/cifar2/train/1_automobile/4356.jpg&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,</span><br><span class="line">                                             <span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment"># 注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(ds.map(load_image).take(<span class="number">2</span>)):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    plt.imshow((img / <span class="number">255.0</span>).numpy())</span><br><span class="line">    plt.title(<span class="string">"label = %d"</span> % label)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
<p><img src="output_15_1.png" alt="png"></p>
<h2 id="从TFRecord构建数据管道"><a href="#从TFRecord构建数据管道" class="headerlink" title="从TFRecord构建数据管道"></a>从TFRecord构建数据管道</h2><p>TFRecord是一种二进制格式, 可以高效地读取/传输数据, 当一些数据在预处理好以后, 可能会重复使用时, 可以考虑TFRecord格式.</p>
<p>TFRecord需要首先将原始数据序列化保存, 然后在读取的时候在进行相应的解析.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(value, type(tf.constant(<span class="number">0</span>))):</span><br><span class="line">        value = value.numpy()  <span class="comment"># BytesList won't unpack a string from an EagerTensor.</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_float_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a float_list from a float / double."""</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns an int64_list from a bool / enum / int / uint."""</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_list_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">    value = tf.io.serialize_tensor(value).numpy()</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 原始数据</span></span><br><span class="line"></span><br><span class="line">data_set = &#123;</span><br><span class="line">    <span class="string">'a'</span>: np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]),</span><br><span class="line">    <span class="string">'b'</span>: np.array([<span class="number">.1</span>, <span class="number">.2</span>, <span class="number">.3</span>, <span class="number">.4</span>]),</span><br><span class="line">    <span class="string">'c'</span>: np.array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]),</span><br><span class="line">    <span class="string">'d'</span>: np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_set = tf.data.Dataset.from_tensor_slices(data_set)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 序列化并保存为tfrecord</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize_example</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  Creates a tf.Example message ready to be written to a file.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">    <span class="comment"># Create a dictionary mapping the feature name to the tf.Example-compatible</span></span><br><span class="line">    <span class="comment"># data type.</span></span><br><span class="line">    feature = &#123;</span><br><span class="line">        <span class="string">'a'</span>: _int64_feature(x[<span class="string">'a'</span>]),</span><br><span class="line">        <span class="string">'b'</span>: _float_feature(x[<span class="string">'b'</span>]),</span><br><span class="line">        <span class="string">'c'</span>: _bytes_feature(x[<span class="string">'c'</span>]),</span><br><span class="line">        <span class="string">'d'</span>: _list_feature(x[<span class="string">'d'</span>]),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a Features message using tf.train.Example.</span></span><br><span class="line"></span><br><span class="line">    example_proto = tf.train.Example(features=tf.train.Features(</span><br><span class="line">        feature=feature))</span><br><span class="line">    <span class="keyword">return</span> example_proto.SerializeToString()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features <span class="keyword">in</span> data_set:</span><br><span class="line">        <span class="keyword">yield</span> serialize_example(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">serialized_features_dataset = tf.data.Dataset.from_generator(</span><br><span class="line">    generator, output_types=tf.string, output_shapes=())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入tfrecord文件</span></span><br><span class="line">filename = <span class="string">'test.tfrecord'</span></span><br><span class="line">writer = tf.data.experimental.TFRecordWriter(filename)</span><br><span class="line">writer.write(serialized_features_dataset)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取tfrecord文件</span></span><br><span class="line"></span><br><span class="line">filenames = [filename]</span><br><span class="line">raw_dataset = tf.data.TFRecordDataset(filenames)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析</span></span><br><span class="line">feature_description = &#123;</span><br><span class="line">    <span class="string">'a'</span>: tf.io.FixedLenFeature([], tf.int64, default_value=<span class="number">0</span>),</span><br><span class="line">    <span class="string">'b'</span>: tf.io.FixedLenFeature([], tf.float32, default_value=<span class="number">0.0</span>),</span><br><span class="line">    <span class="string">'c'</span>: tf.io.FixedLenFeature([], tf.string, default_value=<span class="string">''</span>),</span><br><span class="line">    <span class="string">'d'</span>: tf.io.FixedLenFeature([], tf.string, default_value=<span class="string">''</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="comment"># Parse the input `tf.Example` proto using the dictionary above.</span></span><br><span class="line">    feature = tf.io.parse_single_example(example_proto, feature_description)</span><br><span class="line">    feature[<span class="string">'d'</span>] = tf.io.parse_tensor(feature[<span class="string">'d'</span>], tf.int64)</span><br><span class="line">    <span class="keyword">return</span> feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parsed_dataset = raw_dataset.map(_parse_function)</span><br><span class="line"><span class="keyword">for</span> parsed_record <span class="keyword">in</span> parsed_dataset.take(<span class="number">10</span>):</span><br><span class="line">    tf.print(parsed_record)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;a&apos;: 1, &apos;b&apos;: 0.1, &apos;c&apos;: &quot;a&quot;, &apos;d&apos;: [1 2 3]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 2, &apos;b&apos;: 0.2, &apos;c&apos;: &quot;b&quot;, &apos;d&apos;: [2 3 4]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 3, &apos;b&apos;: 0.3, &apos;c&apos;: &quot;c&quot;, &apos;d&apos;: [3 4 5]&#125;</span><br><span class="line">&#123;&apos;a&apos;: 4, &apos;b&apos;: 0.4, &apos;c&apos;: &quot;d&quot;, &apos;d&apos;: [4 5 6]&#125;</span><br></pre></td></tr></table></figure>
<h1 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h1><p>Dataset数据结构应用非常灵活, 因为它本质上是一个Sequece序列, 其每个元素可以是各种类型, 例如可以是张量, 列表, 字典, 也可以是Dataset.<br>Dataset包含了非常丰富的数据转换功能.</p>
<ul>
<li>map: 将转换函数映射到数据集每一个元素.</li>
<li>flat_map: 将转换函数映射到数据集的每一个元素, 并将嵌套的Dataset压平.</li>
<li>interleave: 效果类似flat_map, 但可以将不同来源的数据夹在一起.</li>
<li>filter: 过滤掉某些元素.</li>
<li>zip: 将两个长度相同的Dataset横向铰合.</li>
<li>concatenate: 将两个Dataset纵向连接. </li>
<li>reduce: 执行归并操作.</li>
<li>batch: 构建批次, 每次放一个批次. 比原始数据增加一个维度. 其逆操作为unbatch.</li>
<li>padded_batch: 构建批次, 类似batch, 但可以填充到相同的形状.</li>
<li>window: 构建滑动窗口, 返回Dataset of Dataset.</li>
<li>shuffle: 数据顺序洗牌.</li>
<li>repeat: 重复数据若干次, 不带参数时, 重复无数次.</li>
<li>shard: 采样, 从某个位置开始隔固定距离采样一个元素.</li>
<li>take: 采样, 从开始位置取前几个元素.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># map: 将转换函数映射到数据集每一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_map = ds.map(<span class="keyword">lambda</span> x: tf.strings.split(x, <span class="string">" "</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([b&#39;hello&#39; b&#39;world&#39;], shape=(2,), dtype=string)
tf.Tensor([b&#39;hello&#39; b&#39;China&#39;], shape=(2,), dtype=string)
tf.Tensor([b&#39;hello&#39; b&#39;Beijing&#39;], shape=(2,), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flat_map: 将转换函数映射到数据集的每一个元素, 并将嵌套的Dataset压平</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_flatmap = ds.flat_map(</span><br><span class="line">    <span class="keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x, <span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_flatmap:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;world&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interleave: 效果类似flat_map, 但可以将不同来源的数据夹在一起</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    [<span class="string">"hello world"</span>, <span class="string">"hello China"</span>, <span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_interleave = ds.interleave(</span><br><span class="line">    <span class="keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x, <span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_interleave:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;world&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># filter: 过滤掉某些元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line"><span class="comment"># 找出含有字母a或B的元素</span></span><br><span class="line">ds_filter = ds.filter(<span class="keyword">lambda</span> x: tf.strings.regex_full_match(x, <span class="string">".*[a|B].*"</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_filter:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;hello China&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;hello Beijing&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># zip: 将两个长度相同的Dataset横向铰合。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">ds3 = tf.data.Dataset.range(<span class="number">6</span>, <span class="number">9</span>)</span><br><span class="line">ds_zip = tf.data.Dataset.zip((ds1, ds2, ds3))</span><br><span class="line"><span class="keyword">for</span> x, y, z <span class="keyword">in</span> ds_zip:</span><br><span class="line">    print(x.numpy(), y.numpy(), z.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>0 3 6
1 4 7
2 5 8
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># condatenate: 将两个Dataset纵向连接。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">ds_concat = tf.data.Dataset.concatenate(ds1, ds2)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_concat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reduce: 执行归并操作。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5.0</span>])</span><br><span class="line">result = ds.reduce(<span class="number">0.0</span>, <span class="keyword">lambda</span> x, y: tf.add(x, y))</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=15.0&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># batch: 构建批次, 每次放一个批次. 比原始数据增加一个维度. 其逆操作为unbatch</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">11</span>)</span><br><span class="line">ds_batch = ds.batch(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)
tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)
tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># padded_batch: 构建批次, 类似batch, 但可以填充到相同的形状</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>]]</span><br><span class="line">ds = tf.data.Dataset.from_generator(<span class="keyword">lambda</span>: iter(elements), tf.int32)</span><br><span class="line"></span><br><span class="line">ds_padded_batch = ds.padded_batch(<span class="number">2</span>, padded_shapes=[<span class="number">4</span>,])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_padded_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(
[[1 2 0 0]
 [3 4 5 0]], shape=(2, 4), dtype=int32)
tf.Tensor(
[[6 7 0 0]
 [8 0 0 0]], shape=(2, 4), dtype=int32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># window: 构建滑动窗口, 返回Dataset of Dataset</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># window返回的是Dataset of Dataset, 可以用flat_map压平</span></span><br><span class="line">ds_window = ds.window(</span><br><span class="line">    <span class="number">3</span>, shift=<span class="number">1</span>).flat_map(<span class="keyword">lambda</span> x: x.batch(<span class="number">3</span>, drop_remainder=<span class="literal">True</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_window:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7fcb59acb9e0&gt; and will run it as-is.
Cause: could not parse the source code:

    3, shift=1).flat_map(lambda x: x.batch(3, drop_remainder=True))

This error may be avoided by creating the lambda in a standalone statement.

WARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7fcb59acb9e0&gt; and will run it as-is.
Cause: could not parse the source code:

    3, shift=1).flat_map(lambda x: x.batch(3, drop_remainder=True))

This error may be avoided by creating the lambda in a standalone statement.

tf.Tensor([0 1 2], shape=(3,), dtype=int64)
tf.Tensor([1 2 3], shape=(3,), dtype=int64)
tf.Tensor([2 3 4], shape=(3,), dtype=int64)
tf.Tensor([3 4 5], shape=(3,), dtype=int64)
tf.Tensor([4 5 6], shape=(3,), dtype=int64)
tf.Tensor([5 6 7], shape=(3,), dtype=int64)
tf.Tensor([6 7 8], shape=(3,), dtype=int64)
tf.Tensor([7 8 9], shape=(3,), dtype=int64)
tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)
tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shuffle: 数据顺序洗牌.</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shuffle = ds.shuffle(buffer_size=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shuffle:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(6, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(8, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
tf.Tensor(9, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
tf.Tensor(11, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># repeat: 重复数据若干次, 不带参数时, 重复无数次</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">3</span>)</span><br><span class="line">ds_repeat = ds.repeat(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_repeat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shard: 采样, 从某个位置开始隔固定距离采样一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shard = ds.shard(<span class="number">3</span>, index=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shard:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># take: 采样, 从开始位置取前几个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_take = ds.take(<span class="number">3</span>)</span><br><span class="line">abs</span><br><span class="line">list(ds_take.as_numpy_iterator())</span><br></pre></td></tr></table></figure>
<pre><code>[0, 1, 2]
</code></pre><h1 id="提升管道性能"><a href="#提升管道性能" class="headerlink" title="提升管道性能"></a>提升管道性能</h1><p>训练深度学习模型常常会非常耗时.<br>模型训练的耗时主要来自于两个部分, 一部分来自数据准备, 另一部分来自参数迭代.<br>参数迭代过程的耗时通常依赖于GPU来提升.<br>而数据准备过程的耗时则可以通过构建高效的数据管道进行提升.<br>以下是一些构建高效数据管道的建议.</p>
<ul>
<li>使用<code>prefetch</code>方法让数据准备和参数迭代两个过程相互并行.</li>
<li>使用<code>interleave</code>方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起.</li>
<li>使用<code>map</code>时设置<code>num_parallel_calls</code>让数据转换过程多进行执行.</li>
<li>使用<code>cache</code>方法让数据在第一个<code>epoch</code>后缓存到内存中, 仅限于数据集不大情形.</li>
<li>使用<code>map</code>转换时, 先<code>batch</code>, 然后采用向量化的转换方法对每个<code>batch</code>进行转换</li>
</ul>
<h2 id="prefetch-方法"><a href="#prefetch-方法" class="headerlink" title="prefetch 方法"></a>prefetch 方法</h2><p>使用<code>prefetch</code>方法让数据准备和参数迭代两个过程相互并行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts % (<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts // <span class="number">3600</span> + <span class="number">8</span>, tf.int32) % tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts % <span class="number">3600</span>) // <span class="number">60</span>, tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts % <span class="number">60</span>), tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>, m)) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"0&#123;&#125;"</span>, m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (tf.strings.format(<span class="string">"&#123;&#125;"</span>, m))</span><br><span class="line"></span><br><span class="line">    timestring = tf.strings.join(</span><br><span class="line">        [timeformat(hour),</span><br><span class="line">         timeformat(minite),</span><br><span class="line">         timeformat(second)],</span><br><span class="line">        separator=<span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span> * <span class="number">3</span>, end=<span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备和参数迭代两个过程默认情况下是串行的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要1s</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练过程预计耗时 10 * 2 + 10 * 1 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">    train_step()</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================00:31:27
start training...
==============================00:31:57
end training...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 max(10 * 2, 10 * 1) = 20s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training with prefetch..."</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE):</span><br><span class="line">    train_step()</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================00:33:49
start training with prefetch...
==============================00:34:10
end training...
</code></pre><h2 id="interleave"><a href="#interleave" class="headerlink" title="interleave"></a>interleave</h2><p>使用<code>interleave</code>方法可以让数据读取过程多进程执行, 并将不同来源数据夹在一起.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.flat_map(<span class="keyword">lambda</span> x: tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">4</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;892,0.0,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;893,1.0,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47.0,1,0,363272,7.0,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;894,0.0,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62.0,0,0,240276,9.6875,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;895,0.0,3,&quot;Wirz, Mr. Albert&quot;,male,27.0,0,0,315154,8.6625,,S&#39;, shape=(), dtype=string)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.interleave(<span class="keyword">lambda</span> x: tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">8</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(b&#39;1,0,3,&quot;Braund, Mr. Owen Harris&quot;,male,22,1,0,A/5 21171,7.25,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;892,0.0,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;2,1,1,&quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;,female,38,1,0,PC 17599,71.2833,C85,C&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;893,1.0,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47.0,1,0,363272,7.0,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;3,1,3,&quot;Heikkinen, Miss. Laina&quot;,female,26,0,0,STON/O2. 3101282,7.925,,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;894,0.0,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62.0,0,0,240276,9.6875,,Q&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;4,1,1,&quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;,female,35,1,0,113803,53.1,C123,S&#39;, shape=(), dtype=string)
tf.Tensor(b&#39;895,0.0,3,&quot;Wirz, Mr. Albert&quot;,male,27.0,0,0,315154,8.6625,,S&#39;, shape=(), dtype=string)
</code></pre><h2 id="num-parallel-calls"><a href="#num-parallel-calls" class="headerlink" title="num_parallel_calls"></a>num_parallel_calls</h2><p>使用<code>map</code>时设置<code>num_parallel_calls</code>让数据转换过程多进行执行.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path, size=<span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,</span><br><span class="line">                                             <span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img)  <span class="comment"># 注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, size)</span><br><span class="line">    <span class="keyword">return</span> img, label</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 单进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map = ds.map(load_image)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================11:02:15
start transformation...
==============================11:02:19
end transformation...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start parallel transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map_parallel = ds.map(load_image,</span><br><span class="line">                         num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map_parallel:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end parallel transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================11:02:52
start parallel transformation...
==============================11:02:53
end parallel transformation...
</code></pre><h2 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h2><p>使用<code>cache</code>方法让数据在第一个<code>epoch</code>后缓存到内存中, 仅限于数据集不大情形.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 假设每一步训练需要0s</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5 * 2 + 5 * 0) * 3 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()</span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>, epoch, <span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:17:35
start training...
==============================18:17:45
epoch = 0  ended
==============================18:17:56
epoch = 1  ended
==============================18:18:06
epoch = 2  ended
==============================18:18:06
end training...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cache方法让数据在第一个epoch后缓存到内存中, 仅限于数据集不大情形</span></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator, output_types=(tf.int32)).cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 假设每一步训练需要0s</span></span><br><span class="line">    time.sleep(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5 * 2 + 5 * 0) + (5 * 0 + 5 * 0) * 2 = 10s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()</span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>, epoch, <span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:19:13
start training...
==============================18:19:23
epoch = 0  ended
==============================18:19:23
epoch = 1  ended
==============================18:19:23
epoch = 2  ended
==============================18:19:23
end training...
</code></pre><h2 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h2><p>使用<code>map</code>转换时, 先<code>batch</code>, 然后采用向量化的转换方法对每个<code>batch</code>进行转换.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先map后batch</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_map_batch = ds.map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>).batch(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start scalar transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map_batch:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end scalar transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:20:50
start scalar transformation...
==============================18:20:53
end scalar transformation...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先batch后map</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_batch_map = ds.batch(<span class="number">20</span>).map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start vector transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end vector transformation..."</span>))</span><br></pre></td></tr></table></figure>
<pre><code>==============================18:21:20
start vector transformation...
==============================18:21:20
end vector transformation...
</code></pre>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-计算图</title>
    <url>/2020/08/07/TensorFlow/TensorFlow-%E8%AE%A1%E7%AE%97%E5%9B%BE/</url>
    <content><![CDATA[<p>在TensorFlow1.x的时代, 采用的是<strong>静态计算图</strong>, 需要首先使用TensorFlow的各种算子创建计算图, 然后再开启一个会话Session, 显式执行计算图. 在静态图构建完成之后, 几乎全在TensorFlow内核上使用C++代码执行, 效率较高. 此外静态图还会对计算步骤进行一定的优化, 略去和结果无关的步骤.</p>
<a id="more"></a>
<p>而在TensorFlow2.x时代, 使用的是<strong>动态计算图</strong>, 即每使用一个算子后, 该算子都会被动态地加入到隐含的默认计算图中, 立即执行得到结果, 而无需开启Session. 使用动态计算图即Eager Excution的好处是方便调试程序, 它会让TensorFlow代码的表现和Python原生代码的表现一样, 写起来就像写numpy一样, 各种日志打印, 控制流全部都是可以使用. 不过动态图的缺点是运行效率会相对低一些, 因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信.</p>
<p>如果需要在TensorFlow2.x中使用静态图, 可以使用<code>@tf.function</code>装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码. 运行该函数就相当于在TensorFlow1.x中用Session执行代码. 使用tf.function构建静态图的方式叫做 Autograph.</p>
<p>在TensorFlow2.x中, 有三种计算图的构建方式: 静态计算图, 动态计算图, 以及Autograph.</p>
<h1 id="计算图简介"><a href="#计算图简介" class="headerlink" title="计算图简介"></a>计算图简介</h1><p>计算图由节点(nodes)和边(edges)组成. 节点表示操作符Operator, 或者称之为算子, 边表示算子之间的依赖关系.</p>
<p>实线表示有数据传递, 即张量. 虚线表示控制依赖, 即先后顺序.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<h1 id="静态计算图"><a href="#静态计算图" class="headerlink" title="静态计算图"></a>静态计算图</h1><p>TensorFlow2.x为了确保对TensorFlow1.x项目的兼容性, 在tf.compat.v1子模块中保留了对TensorFlow1.x那种静态计算图构建风格的支持. 可称之为怀旧版静态计算图, 不推荐使用.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">"join"</span>,separator = <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&apos;hello world&apos;</span><br></pre></td></tr></table></figure>
<h1 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h1><p>在TensorFlow2.x中, 使用的是动态计算图和Autograph. 动态计算图已经不区分计算图的定义和执行了, 而是定义后立即执行. 因此称之为 Eager Excution, Eager这个英文单词的原意是”迫不及待的”, 也就是立即执行的意思.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建, 构建后立即执行</span></span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="string">'hello'</span>)</span><br><span class="line">y = tf.constant(<span class="string">'world'</span>)</span><br><span class="line">z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出封装成函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">res = hello_world(tf.constant(<span class="string">'hello'</span>), tf.constant(<span class="string">'world'</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.Tensor(b&apos;hello world&apos;, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h1 id="Autograph"><a href="#Autograph" class="headerlink" title="Autograph"></a>Autograph</h1><p>动态计算图运行效率相对较低, 可以用<code>@tf.function</code>装饰器将普通Python函数转换成和TensorFlow1.x对应的静态计算图构建代码.</p>
<p>在TensorFlow1.x中, 使用计算图分两步, 第一步<strong>定义计算图</strong>, 第二步在会话中<strong>执行计算图</strong>.</p>
<p>在TensorFlow2.x中, 如果采用Autograph的方式使用计算图, 第一步定义计算图变成了<strong>定义函数</strong>, 第二步执行计算图变成了<strong>调用函数</strong>. 不需要使用会话了, 一切都像原始的Python语法一样自然.</p>
<p>实践中, 一般会先用动态计算图调试代码, 然后在需要提高性能的的地方利用<code>@tf.function</code>切换成Autograph获得更高的效率.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用autograph创建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    z = tf.strings.join([x, y], separator=<span class="string">' '</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">res = hello_world(tf.constant(<span class="string">'hello'</span>), tf.constant(<span class="string">'world'</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.Tensor(b&apos;hello world&apos;, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">logdir = <span class="string">'./data/autograph/test'</span></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行autograph</span></span><br><span class="line">res = hello_world(<span class="string">'hello'</span>, <span class="string">'world'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(name=<span class="string">'autograph'</span>, </span><br><span class="line">                            step=<span class="number">0</span>, </span><br><span class="line">                            profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在jupyter中用魔法命令启动tensorboard</span><br><span class="line">%reload_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动tensorboard</span><br><span class="line">%tensorboard --logdir &apos;./data/autograph/&apos;</span><br></pre></td></tr></table></figure>
<h2 id="AutoGraph的使用规范"><a href="#AutoGraph的使用规范" class="headerlink" title="AutoGraph的使用规范"></a>AutoGraph的使用规范</h2><p>Autograph机制能够转换的代码并不是没有任何约束的, 有一些编码规范需要遵循, 否则可能会转换失败或者不符合预期.</p>
<p>总体来说, 使用AutoGraph需要遵循如下规则:</p>
<ul>
<li>被<code>@tf.function</code>修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数. 例如使用<code>tf.print</code>而不是<code>print</code>, 使用<code>tf.range</code>而不是<code>range</code>, 使用<code>tf.constant(True)</code>而不是<code>True</code>.</li>
<li>避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>.</li>
<li>被<code>@tf.function</code>修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = np.random.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    tf.print(a)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.random.normal((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#np_random每次执行都是一样的结果。</span></span><br><span class="line">np_random()</span><br><span class="line">np_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 0.61978529,  0.48959305],</span><br><span class="line">       [-0.1440161 ,  1.72774268]])</span><br><span class="line">array([[ 0.61978529,  0.48959305],</span><br><span class="line">       [-0.1440161 ,  1.72774268]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tf_random每次执行都会有重新生成随机数。</span></span><br><span class="line">tf_random()</span><br><span class="line">tf_random()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0.37242806 -1.14954114]</span><br><span class="line"> [-0.806226909 1.40349495]]</span><br><span class="line">[[0.226775587 1.14588106]</span><br><span class="line"> [0.647506893 1.2036798]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 避免在@tf.function修饰的函数内部定义tf.Variable.</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outer_var</span><span class="params">()</span>:</span></span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">outer_var()</span><br><span class="line">outer_var()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_var</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> (x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#执行将报错</span></span><br><span class="line"><span class="comment"># inner_var()</span></span><br><span class="line"><span class="comment"># inner_var()</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量</span></span><br><span class="line"></span><br><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;tf.Tensor &apos;x:0&apos; shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="AutoGraph的机制原理"><a href="#AutoGraph的机制原理" class="headerlink" title="AutoGraph的机制原理"></a>AutoGraph的机制原理</h2><p><strong>当我们使用<code>@tf.function</code>装饰一个函数的时候, 后面到底发生了什么呢?</strong></p>
<p>例如我们写下如下代码.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(autograph=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myadd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">        tf.print(i)</span><br><span class="line">    c = a + b</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>后面什么都没有发生. 仅仅是在Python堆栈中记录了这样一个函数的签名.</p>
<p><strong>当我们第一次调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"hello"</span>), tf.constant(<span class="string">"world"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>发生了两件事情.</p>
<p><strong>第一件事情是创建计算图.</strong> 即创建一个静态计算图, 跟踪执行一遍函数体中的Python代码, 确定各个变量的Tensor类型, 并根据执行顺序将算子添加到计算图中. 在这个过程中, 如果开启了<code>autograph=True</code>(默认开启), 会将Python控制流转换成TensorFlow图内控制流. 主要是将<code>if</code>语句转换成 <code>tf.cond</code>算子表达, 将<code>while</code>和<code>for</code>循环语句转换成<code>tf.while_loop</code>算子表达, 并在必要的时候添加 <code>tf.control_dependencies</code>指定执行顺序依赖关系.</p>
<p><strong>第二件事情是执行计算图.</strong></p>
<p>因此我们先看到的是第一个步骤的结果: 即Python调用标准输出流打印”tracing”语句. 然后看到第二个步骤的结果: TensorFlow调用标准输出流打印0, 1, 2.</p>
<p><strong>当我们再次用相同的输入参数类型调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"good"</span>), tf.constant(<span class="string">"morning"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>只会发生一件事情, 那就是上面步骤的第二步, 执行计算图. 所以这一次我们没有看到打印”tracing”的结果.</p>
<p><strong>当我们再次用不同的的输入参数类型调用这个被<code>@tf.function</code>装饰的函数时, 后面到底发生了什么?</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>由于输入参数的类型已经发生变化, 已经创建的计算图不能够再次使用.</p>
<p>需要重新做两件事情: 创建新的计算图, 执行计算图, </p>
<p>所以我们又会先看到的是第一个步骤的结果: 即Python调用标准输出流打印”tracing”语句. 然后再看到第二个步骤的结果: TensorFlow调用标准输出流打印0, 1, 2.</p>
<p><strong>需要注意的是, 如果调用被<code>@tf.function</code>装饰的函数时输入的参数不是Tensor类型, 则每次都会重新创建计算图.</strong></p>
<p>例如我们写下如下代码, 两次都会重新创建计算图. 因此, 一般建议调用<code>@tf.function</code>时应传入Tensor类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(<span class="string">"hello"</span>, <span class="string">"world"</span>)</span><br><span class="line">myadd(<span class="string">"good"</span>, <span class="string">"morning"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">tracing</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>了解了以上Autograph的机制原理, 我们也就能够理解Autograph编码规范的三条建议了.</p>
<ul>
<li><p>被<code>@tf.function</code>修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数. 例如使用<code>tf.print</code>而不是<code>print</code>, 使用<code>tf.range</code>而不是<code>range</code>, 使用<code>tf.constant(True)</code>而不是<code>True</code>.</p>
<p>解释: Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用, 普通Python函数是无法嵌入到静态计算图中的. 所以在计算图构建好之后再次调用的时候, 这些Python函数并没有被计算. 而TensorFlow中的函数则可以嵌入到计算图中. </p>
</li>
<li><p>避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>.</p>
<p>解释: 我们创建静态计算图的目的, 是反复使用其进行正向传播, 然后计算梯度后反向传播. 而如果在静态计算图中, 有创建<code>tf.Variable</code>的操作存在, 那将与上面的目的相违背, 因为每次正向传播时, 都会再初始化一个变量张量.</p>
</li>
<li><p>被<code>@tf.function</code>修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量.</p>
<p>静态计算图是被编译成C++代码在TensorFlow内核中执行的. Python中的列表和字典等数据结构变量是无法嵌入到计算图中, 它们仅仅能够在创建计算图时被读取, 在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的. </p>
</li>
</ul>
<h2 id="AutoGraph和tf-Module"><a href="#AutoGraph和tf-Module" class="headerlink" title="AutoGraph和tf.Module"></a>AutoGraph和tf.Module</h2><p>前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在<code>@tf.function</code>修饰的函数内部定义<code>tf.Variable</code>. 但是如果在函数外部定义<code>tf.Variable</code>的话, 又会显得这个函数有外部变量依赖, 封装不够完美.</p>
<p>一种简单的思路是定义一个类, 并将相关的<code>tf.Variable</code>创建放在类的初始化方法中. 而将函数的逻辑放在其他方法中. TensorFlow提供了一个基类<code>tf.Module</code>, 通过继承它构建子类, 可以非常方便地管理变量, 还可以非常方便地管理它引用的其它Module. 最重要的是, 我们能够利用<code>tf.saved_model</code>保存模型并实现跨平台部署使用.</p>
<p>实际上, <code>tf.keras.models.Model</code>, <code>tf.keras.layers.Layer</code>都是继承自<code>tf.Module</code>的, 提供了方便的变量管理和所引用的子模块管理的功能.</p>
<p><strong>因此, 利用<code>tf.Module</code>提供的封装, 再结合TensoFlow丰富的低阶API, 实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型), 并实现跨平台部署使用.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个简单的function</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在tf.function中用input_signature限定输入张量的签名类型：shape和dtype</span></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_print</span><span class="params">(a)</span>:</span></span><br><span class="line">    x.assign_add(a)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_print(tf.constant(<span class="number">3.0</span>))</span><br><span class="line"><span class="comment">#add_print(tf.constant(3))  # 输入不符合张量签名的参数将报错</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.Module的子类化将其封装一下</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoModule</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, init_value=tf.constant<span class="params">(<span class="number">0.0</span>)</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.x = tf.Variable(init_value, dtype=tf.float32, trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.x.assign_add(a)</span><br><span class="line">        tf.print(self.x)</span><br><span class="line">        <span class="keyword">return</span> self.x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 执行</span></span><br><span class="line">demo = DemoModule(init_value=tf.constant(<span class="number">1.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模块中的全部变量和全部可训练变量</span></span><br><span class="line">print(demo.variables)</span><br><span class="line">print(demo.trainable_variables)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&lt;tf.Variable &apos;Variable:0&apos; shape=() dtype=float32, numpy=6.0&gt;,)</span><br><span class="line">(&lt;tf.Variable &apos;Variable:0&apos; shape=() dtype=float32, numpy=6.0&gt;,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模块中的全部子模块</span></span><br><span class="line">demo.submodules</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用tf.saved_model保存模型, 并指定需要跨平台部署的方法</span></span><br><span class="line">tf.saved_model.save(demo,</span><br><span class="line">                    <span class="string">"./model/demo/1"</span>,</span><br><span class="line">                    signatures=&#123;<span class="string">"serving_default"</span>: demo.addprint&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:Assets written to: ./model/demo/1/assets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">demo2 = tf.saved_model.load(<span class="string">"./model/demo/1"</span>)</span><br><span class="line">demo2.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看模型文件相关信息, 红框标出来的输出信息在模型部署和跨平台使用时有可能会用到</span><br><span class="line">!saved_model_cli show --dir ./model/demo/1 --all</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MetaGraphDef with tag-set: &apos;serve&apos; contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[&apos;__saved_model_init_op&apos;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&apos;__saved_model_init_op&apos;] tensor_info:</span><br><span class="line">        dtype: DT_INVALID</span><br><span class="line">        shape: unknown_rank</span><br><span class="line">        name: NoOp</span><br><span class="line">  Method name is: </span><br><span class="line"></span><br><span class="line">signature_def[&apos;serving_default&apos;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&apos;a&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: ()</span><br><span class="line">        name: serving_default_a:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&apos;output_0&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: ()</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure>
<p>除了利用<code>tf.Module</code>的子类化实现封装, 我们也可以通过给<code>tf.Module</code>添加属性和方法进行封装.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mymodule = tf.Module()</span><br><span class="line">mymodule.x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(a)</span>:</span></span><br><span class="line">    mymodule.x.assign_add(a)</span><br><span class="line">    tf.print(mymodule.x)</span><br><span class="line">    <span class="keyword">return</span> (mymodule.x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mymodule.addprint = addprint</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-自动微分机制</title>
    <url>/2020/08/05/TensorFlow/TensorFlow-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>神经网络通常依赖反向传播求梯度来更新网络参数, 求梯度过程通常是一件非常复杂而容易出错的事情. 而深度学习框架最主要的功能之一, 就是可以帮助我们自动地完成这种求梯度运算.</p>
<p>TensorFlow一般使用梯度磁带<code>tf.GradientTape</code>来记录正向运算过程, 然后反播磁带自动得到梯度值. 这种利用<code>tf.GradientTape</code>求微分的方法叫做Tensorflow的自动微分机制.</p>
<a id="more"></a>
<h1 id="利用梯度磁带求导数"><a href="#利用梯度磁带求导数" class="headerlink" title="利用梯度磁带求导数"></a>利用梯度磁带求导数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a * x**<span class="number">2</span> + b * x + c  <span class="comment"># tf.GradientTape会自动记录变量张量的传播.</span></span><br><span class="line">dy_dx = tape.gradient(y, x)</span><br><span class="line"></span><br><span class="line">print(dy_dx)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(-2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对常量张量也可以求导，需要增加watch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([a, b, c])</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">dy_dx, dy_da, dy_db, dy_dc = tape.gradient(y, [x, a, b, c])</span><br><span class="line"></span><br><span class="line">print(dy_da)</span><br><span class="line">print(dy_dc)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(0.0, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(1.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以求二阶导数</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape1.gradient(y, x)</span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx, x)</span><br><span class="line"></span><br><span class="line">print(dy2_dx2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(2.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自变量转换成tf.float32</span></span><br><span class="line">    x = tf.cast(x, tf.float32)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch(x)</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape.gradient(y, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dy_dx, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(f(tf.constant(<span class="number">0.0</span>)))</span><br><span class="line">tf.print(f(tf.constant(<span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(-2, 1)</span><br><span class="line">(0, 0)</span><br></pre></td></tr></table></figure>
<h1 id="利用梯度磁带和优化器求最小值"><a href="#利用梯度磁带和优化器求最小值" class="headerlink" title="利用梯度磁带和优化器求最小值"></a>利用梯度磁带和优化器求最小值</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    dy_dx = tape.gradient(y, x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx, x)])</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>, y, <span class="string">"; x ="</span>, x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y = 0 ; x = 0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"><span class="comment"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意f()无参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f, [x])</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>, f(), <span class="string">"; x ="</span>, x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y = 0 ; x = 0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(<span class="number">1000</span>):  <span class="comment">#注意autograph时使用tf.range(1000)而不是range(1000)</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">        dy_dx = tape.gradient(y, x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx, x)])</span><br><span class="line"></span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(minimizef())</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">0.999998569</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"x"</span>, dtype=tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a * tf.pow(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):</span><br><span class="line">        optimizer.minimize(f, [x])</span><br><span class="line">    <span class="keyword">return</span> f()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(train(<span class="number">1000</span>))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">0.999998569</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-张量</title>
    <url>/2020/08/05/TensorFlow/TensorFlow-%E5%BC%A0%E9%87%8F/</url>
    <content><![CDATA[<p>程序 = 数据结构 + 算法</p>
<p>TensorFlow程序 = 张量数据结构 + 计算图算法</p>
<p>张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似.</p>
<p>从行为特性来看, 有两种类型的张量, 分别是常量<strong>constant</strong>和变量<strong>Variable</strong>. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值.</p>
<p>张量的操作主要包括张量的<strong>结构操作</strong>和张量的<strong>数学运算</strong>. 张量结构操作诸如: 张量创建, 索引切片, 维度变换, 合并分割. 张量数学运算主要有: 标量运算, 向量运算, 矩阵运算.</p>
<a id="more"></a>
<h1 id="常量张量"><a href="#常量张量" class="headerlink" title="常量张量"></a>常量张量</h1><p>张量的数据类型基本和numpy.array的数据类型对应.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">1</span>)  <span class="comment"># 默认整数tf.int32</span></span><br><span class="line">b = tf.constant(<span class="number">1</span>, dtype=tf.int64)  <span class="comment"># tf.int64</span></span><br><span class="line">c = tf.constant(<span class="number">5.20</span>)  <span class="comment"># 默认小数tf.float32</span></span><br><span class="line">d = tf.constant(<span class="number">5.20</span>, dtype=tf.double)  <span class="comment">#tf.double</span></span><br><span class="line">e = tf.constant(<span class="string">'are you ok?'</span>)  <span class="comment"># tf.string</span></span><br><span class="line">f = tf.constant(<span class="literal">True</span>)  <span class="comment"># tf.bool</span></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64)</span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode)  <span class="comment"># 不等价</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>张量可以有各种不同的维度, 标量是0维的张量, 向量是1维的张量, 矩阵是2维的张量, 彩色图像有rgb三个通道, 可以表示为3维张量, 视频有时间维, 可以表示为4维张量. 通俗来说, 有几层中括号, 就是几维张量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment"># 标量, 0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank与numpy的ndim方法查看具体维度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(0, shape=(), dtype=int32)</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 向量, 1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector))</span><br><span class="line">print(vector.numpy().ndim)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(1, shape=(), dtype=int32)</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                     [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 矩阵, 2维张量</span></span><br><span class="line">print(tf.rank(matrix))</span><br><span class="line">print(np.ndim(matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(2, shape=(), dtype=int32)</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], </span><br><span class="line">                       [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line"></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[[1 2]</span><br><span class="line">  [3 4]]</span><br><span class="line"></span><br><span class="line"> [[5 6]</span><br><span class="line">  [7 8]]], shape=(2, 2, 2), dtype=int32)</span><br><span class="line">tf.Tensor(3, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>可以用<code>tf.cast</code>来来改变张量中的数据类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.cast(a, tf.float32)</span><br><span class="line"></span><br><span class="line">print(a.dtype, b.dtype)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dtype: &apos;int32&apos;&gt; &lt;dtype: &apos;float32&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以用numpy中的方法, 将TensorFlow中的张量转化为numpy中的张量. 用shape方法查看张量的尺寸.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">print(a.numpy())  <span class="comment"># 转换成np.array</span></span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 2]</span><br><span class="line">(2,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="string">'你好啊!'</span>)</span><br><span class="line"></span><br><span class="line">print(a.numpy())</span><br><span class="line">print(a.numpy().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&apos;\xe4\xbd\xa0\xe5\xa5\xbd\xe5\x95\x8a!&apos;</span><br><span class="line">你好啊!</span><br></pre></td></tr></table></figure>
<h1 id="变量张量"><a href="#变量张量" class="headerlink" title="变量张量"></a>变量张量</h1><p>模型中需要被训练的参数一般被设置为变量.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 常量值不可改变, 常量的重新赋值相当于创造新的内存空间.</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a = a + tf.constant([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(c)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.Tensor([1 2], shape=(2,), dtype=int32)</span><br><span class="line">140680756570256</span><br><span class="line">tf.Tensor([2 3], shape=(2,), dtype=int32)</span><br><span class="line">140680756571312</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变, 可以通过assign, assign_add等方法进行赋值.</span></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a.assign_add([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br></pre></td></tr></table></figure>
<h1 id="张量的结构操作"><a href="#张量的结构操作" class="headerlink" title="张量的结构操作"></a>张量的结构操作</h1><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的许多方法和numpy中创建array的方法很像.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>, <span class="number">10</span>, delta=<span class="number">2</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 3 5 7 9]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">100</span>)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0 0.101010099 0.202020198 ... 9.79797935 9.89899 10]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.ones([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.zeros_like(a, dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 1]</span><br><span class="line"> [1 1]]</span><br><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[5 5]</span><br><span class="line"> [5 5]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[8.34453773 2.33366609 8.79651928 0.466492176 8.03496838]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-2.27333117 -1.65921044]</span><br><span class="line"> [-0.263356805 -0.809234142]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机，对两倍方差进行截断</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">2</span>, <span class="number">2</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1.57495022 -0.52902168]</span><br><span class="line"> [-0.704079211 -0.923798501]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print()</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 对角阵</span></span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 0]</span><br><span class="line"> [0 1]]</span><br><span class="line"></span><br><span class="line">[[1 0 0]</span><br><span class="line"> [0 2 0]</span><br><span class="line"> [0 0 3]]</span><br></pre></td></tr></table></figure>
<h2 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h2><p>张量的索引切片方式和numpy几乎是一样的, 切片时支持缺省参数和省略号. </p>
<p>对于<code>tf.Variable</code>可以通过索引和切片对部分元素进行修改.</p>
<p>对于提取张量的连续子区域, 也可以使用<code>tf.slice</code>.</p>
<p>此外, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p><code>tf.boolean_mask</code>功能最为强大, 它可以实现<code>tf.gather</code>, <code>tf.gather_nd</code>的功能, 并且<code>tf.boolean_mask</code>还可以实现布尔索引.</p>
<p>如果要通过修改张量的某些元素得到新的张量, 可以使用<code>tf.where</code>, <code>tf.scatter_nd</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>, <span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[4 5 7 8 4]</span><br><span class="line"> [7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]</span><br><span class="line"> [4 6 5 9 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[4 5 7 8 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[4 6 5 9 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :])</span><br><span class="line">tf.print(tf.slice(t, [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">3</span>, <span class="number">5</span>]))  <span class="comment">#tf.slice(input, begin_vector, size_vector)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :<span class="number">4</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[7 8]</span><br><span class="line"> [0 7]</span><br><span class="line"> [0 2]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对变量来说，还可以使用索引和切片修改部分元素</span></span><br><span class="line">x = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">x[<span class="number">1</span>, :].assign(tf.constant([<span class="number">0.0</span>, <span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1 2]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[0 4]</span><br><span class="line">  [7 2]]</span><br><span class="line"></span><br><span class="line"> [[4 0]</span><br><span class="line">  [7 5]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 省略号可以表示多个冒号</span></span><br><span class="line">tf.print(a[..., <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[4 2]</span><br><span class="line"> [0 5]]</span><br></pre></td></tr></table></figure>
<p>以上切片方式相对规则, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p>考虑班级成绩册的例子, 有4个班级, 每个班级5个学生, 每个学生6门科目成绩. 可以用一个(4, 5, 6)的张量来表示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), minval=<span class="number">0</span>, maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.print(scores)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [22 67 12 59 33 61]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [43 94 21 25 69 53]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [77 34 29 78 64 96]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [14 79 72 81 90 86]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [5 18 37 63 36 27]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [79 62 29 13 62 3]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [19 51 84 98 81 97]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [89 69 94 84 13 54]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生, 第2个学生, 第4个学生的第1门课程, 第3门课程, 第5门课程成绩</span></span><br><span class="line">q = tf.gather(tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>), [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[84 14 4]</span><br><span class="line">  [29 94 60]</span><br><span class="line">  [1 10 58]]</span><br><span class="line"></span><br><span class="line"> [[31 1 29]</span><br><span class="line">  [74 14 45]</span><br><span class="line">  [70 41 12]]</span><br><span class="line"></span><br><span class="line"> [[24 80 70]</span><br><span class="line">  [32 36 55]</span><br><span class="line">  [22 2 10]]</span><br><span class="line"></span><br><span class="line"> [[66 16 56]</span><br><span class="line">  [87 29 71]</span><br><span class="line">  [73 41 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取第1个班级第1个学生, 第2个班级的第2个学生, 第3个班级的第3个学生的全部成绩</span></span><br><span class="line"><span class="comment"># indices的长度为采样样本的个数, 每个元素为采样位置的坐标</span></span><br><span class="line">s = tf.gather_nd(scores, indices=[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]</span><br><span class="line"> [89 69 94 84 13 54]]</span><br></pre></td></tr></table></figure>
<p>以上<code>tf.gather</code>和<code>tf.gather_nd</code>的功能也可以用<code>tf.boolean_mask</code>来实现.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores, [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取第0个班级第0个学生, 第1个班级的第1个学生, 第2个班级的第2个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(</span><br><span class="line">    scores,</span><br><span class="line">    [[<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[28 84 29 14 65 4]</span><br><span class="line"> [77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.boolean_mask可以实现布尔索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到矩阵中小于0的元素</span></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">tf.print(c, <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">tf.print(tf.boolean_mask(c, c &lt; <span class="number">0</span>), <span class="string">"\n"</span>)</span><br><span class="line">tf.print(c[c &lt; <span class="number">0</span>])  <span class="comment"># 布尔索引, 为boolean_mask的语法糖形式</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-1 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 -3 3]] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3]</span><br></pre></td></tr></table></figure>
<p>以上这些方法仅能提取张量的部分元素值, 但不能更改张量的部分元素值得到新的张量.</p>
<p>如果要通过修改张量的部分元素值得到新的张量, 可以使用<code>tf.where</code>和<code>tf.scatter_nd</code>.</p>
<p><code>tf.where</code>可以理解为<code>if</code>的张量版本, 此外它还可以用于找到满足条件的所有元素的位置坐标.</p>
<p><code>tf.scatter_nd</code>的作用和<code>tf.gather_nd</code>有些相反, <code>tf.gather_nd</code>用于收集张量的给定位置的元素, 而<code>tf.scatter_nd</code>可以将某些值插入到一个给定shape的全0的张量的指定位置处.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找到张量中小于0的元素, 将其换成np.nan得到新的张量</span></span><br><span class="line"><span class="comment"># tf.where和np.where作用类似, 可以理解为if的张量版本</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">d = tf.where(c &lt; <span class="number">0</span>, np.nan, c)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[nan 1 nan]</span><br><span class="line"> [2 2 nan]</span><br><span class="line"> [3 nan 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果where只有一个参数, 将返回所有满足条件的位置坐标</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(indices)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 2]</span><br><span class="line"> [1 2]</span><br><span class="line"> [2 1]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量</span></span><br><span class="line"></span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>]], [c[<span class="number">0</span>, <span class="number">0</span>], c[<span class="number">2</span>, <span class="number">1</span>]], c.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 0 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># scatter_nd的作用和gather_nd有些相反</span></span><br><span class="line"><span class="comment"># 可以将某些值插入到一个给定shape的全0的张量的指定位置处</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(tf.scatter_nd(indices, tf.gather_nd(c, indices), c.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-1 0 -1]</span><br><span class="line"> [0 0 -2]</span><br><span class="line"> [0 -3 0]]</span><br></pre></td></tr></table></figure>
<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>维度变换相关函数主要有 <code>tf.reshape</code>, <code>tf.squeeze</code>, <code>tf.expand_dims</code>, <code>tf.transpose</code>.</p>
<p><code>tf.reshape</code>可以改变张量的形状. 可以改变张量的形状, 但是其本质上不会改变张量元素的存储顺序, 所以该操作实际上非常迅速, 并且是可逆的.</p>
<p><code>tf.squeeze</code>可以减少维度.</p>
<p><code>tf.expand_dims</code>可以增加维度.</p>
<p><code>tf.transpose</code>可以交换维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">255</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改成(3, 6)形状的张量</span></span><br><span class="line"></span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.print(b.shape)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 6])</span><br><span class="line">[[95 119 125 231 120 219]</span><br><span class="line"> [179 202 37 124 246 167]</span><br><span class="line"> [211 93 165 94 31 189]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改回成(1,3,3,2)形状的张量</span></span><br><span class="line"></span><br><span class="line">c = tf.reshape(b, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p>如果张量在某个维度上只有一个元素, 利用<code>tf.squeeze</code>可以消除这个维度.</p>
<p>和<code>tf.reshape</code>相似, 它本质上不会改变张量元素的存储顺序. 张量的各个元素在内存中是线性存储的, 其一般规律是, 同一层级中的相邻元素的物理地址也相邻.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = tf.squeeze(a)</span><br><span class="line">tf.print(s.shape)</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 3, 2])</span><br><span class="line">[[[95 119]</span><br><span class="line">  [125 231]</span><br><span class="line">  [120 219]]</span><br><span class="line"></span><br><span class="line"> [[179 202]</span><br><span class="line">  [37 124]</span><br><span class="line">  [246 167]]</span><br><span class="line"></span><br><span class="line"> [[211 93]</span><br><span class="line">  [165 94]</span><br><span class="line">  [31 189]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.expand_dims(s, axis=<span class="number">0</span>)  <span class="comment"># 在第0维插入长度为1的一个维度</span></span><br><span class="line">tf.print(d.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p><code>tf.transpose</code>可以交换张量的维度, 与<code>tf.reshape</code>不同, 它会改变张量元素的存储顺序.</p>
<p><code>tf.transpose</code>常用于图片存储格式的变换上.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Batch, Height, Width, Channel</span></span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>, <span class="number">600</span>, <span class="number">600</span>, <span class="number">4</span>],</span><br><span class="line">                      minval=<span class="number">0</span>,</span><br><span class="line">                      maxval=<span class="number">255</span>,</span><br><span class="line">                      dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Channel, Height, Width, Batch</span></span><br><span class="line">s = tf.transpose(a, perm=[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">tf.print(s.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([100, 600, 600, 4])</span><br><span class="line">TensorShape([4, 600, 600, 100])</span><br></pre></td></tr></table></figure>
<h2 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h2><p>和numpy类似, 可以用<code>tf.concat</code>和<code>tf.stack</code>方法对多个张量进行合并, 可以用<code>tf.split</code>方法把一个张量分割成多个张量.</p>
<p><code>tf.concat</code>和<code>tf.stack</code>有略微的区别, <code>tf.concat</code>是连接, 不会增加维度, 而<code>tf.stack</code>是堆叠, 会增加维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(6, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [ 3.,  4.],</span><br><span class="line">       [ 5.,  6.],</span><br><span class="line">       [ 7.,  8.],</span><br><span class="line">       [ 9., 10.],</span><br><span class="line">       [11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.concat([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 6), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.,  5.,  6.,  9., 10.],</span><br><span class="line">       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 3.,  4.]],</span><br><span class="line"></span><br><span class="line">       [[ 5.,  6.],</span><br><span class="line">        [ 7.,  8.]],</span><br><span class="line"></span><br><span class="line">       [[ 9., 10.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 5.,  6.],</span><br><span class="line">        [ 9., 10.]],</span><br><span class="line"></span><br><span class="line">       [[ 3.,  4.],</span><br><span class="line">        [ 7.,  8.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.split</code>是<code>tf.concat</code>的逆运算, 可以指定分割份数平均分割, 也可以通过指定每份的记录数量进行分割.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf.split(value, num_or_size_splits, axis)</span></span><br><span class="line">tf.print(tf.split(c, <span class="number">3</span>, axis=<span class="number">0</span>))  <span class="comment"># 指定分割份数, 平均分割</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.split(c, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], axis=<span class="number">0</span>))  <span class="comment"># 指定每份的记录数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<h1 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h1><p>张量的数学运算符可以分为标量运算符, 向量运算符, 以及矩阵运算符.</p>
<h2 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h2><p>加减乘除乘方, 以及三角函数, 指数, 对数等常见函数, 逻辑比较运算符等都是标量运算符.</p>
<p>标量运算符的特点是对张量实施逐元素运算.</p>
<p>有些标量运算符对常用的数学运算符进行了重载. 并且支持类似numpy的广播特性.</p>
<p>许多标量运算符都在<code>tf.math</code>模块下.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">-3</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">a + b  <span class="comment"># 运算符重载</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 6.,  8.],</span><br><span class="line">       [ 4., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a - b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ -4.,  -4.],</span><br><span class="line">       [-10.,  -4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a * b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[  5.,  12.],</span><br><span class="line">       [-21.,  32.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a / b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.2       ,  0.33333334],</span><br><span class="line">       [-0.42857143,  0.5       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  4.],</span><br><span class="line">       [ 9., 16.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a**<span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1.       , 1.4142135],</span><br><span class="line">       [      nan, 2.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a % <span class="number">3</span>  <span class="comment"># mod的运算符重载, 等价于m = tf.math.mod(a, 3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [-0.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a // <span class="number">3</span>  <span class="comment"># 地板除法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.,  0.],</span><br><span class="line">       [-1.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a &gt;= <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) &amp; (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) | (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[ True,  True],</span><br><span class="line">       [ True,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a == <span class="number">5</span>  <span class="comment"># tf.equal(a, 5)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False, False],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>, <span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>, <span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.maximum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[5 8]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(tf.minimum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 6]</span><br></pre></td></tr></table></figure>
<h2 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h2><p>向量运算符只在一个特定轴上运算, 将一个向量映射到一个标量或者另外一个向量. 许多向量运算符都以<code>reduce</code>开头.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a))</span><br><span class="line">tf.print(tf.reduce_mean(a))</span><br><span class="line">tf.print(tf.reduce_max(a))</span><br><span class="line">tf.print(tf.reduce_min(a))</span><br><span class="line">tf.print(tf.reduce_prod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">45</span><br><span class="line">5</span><br><span class="line">9</span><br><span class="line">1</span><br><span class="line">362880</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[6]</span><br><span class="line"> [15]</span><br><span class="line"> [24]]</span><br><span class="line">[[12 15 18]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p))</span><br><span class="line">tf.print(tf.reduce_any(q))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a, b: a + b, tf.range(<span class="number">10</span>))</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">45</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a))</span><br><span class="line">tf.print(tf.math.cumprod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1 3 6 ... 28 36 45]</span><br><span class="line">[1 2 6 ... 5040 40320 362880]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a))</span><br><span class="line">tf.print(tf.argmin(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">8</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values, indices = tf.math.top_k(a, <span class="number">3</span>, sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values)</span><br><span class="line">tf.print(indices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[8 7 5]</span><br><span class="line">[5 2 3]</span><br></pre></td></tr></table></figure>
<h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><p>矩阵必须是二维的, 类似<code>tf.constant([1, 2, 3])</code>这样的不是矩阵.</p>
<p>矩阵运算包括: 矩阵乘法, 矩阵转置, 矩阵逆, 矩阵求迹, 矩阵范数, 矩阵行列式, 矩阵求特征值, 矩阵分解等运算.</p>
<p>除了一些常用的运算外, 大部分和矩阵有关的运算都在<code>tf.linalg</code>子包中.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">a @ b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span><br><span class="line">array([[2, 4],</span><br><span class="line">       [6, 8]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1., 3.],</span><br><span class="line">       [2., 4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵逆, 必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3.0</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[-2.0000002 ,  1.0000001 ],</span><br><span class="line">       [ 1.5000001 , -0.50000006]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.trace(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.477226&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵特征值</span></span><br><span class="line">tf.linalg.eigvalsh(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵qr分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">q, r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q @ r)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-0.316227794 -0.948683321]</span><br><span class="line"> [-0.948683321 0.316227734]]</span><br><span class="line">[[-3.1622777 -4.4271884]</span><br><span class="line"> [0 -0.632455349]]</span><br><span class="line">[[1.00000012 1.99999976]</span><br><span class="line"> [3 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵svd分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">v, s, d = tf.linalg.svd(a)</span><br><span class="line">tf.matmul(tf.matmul(s, tf.linalg.diag(v)), d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[0.9999996, 1.9999996],</span><br><span class="line">       [2.9999998, 4.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><p>TensorFlow的广播规则和numpy是一样的:</p>
<ul>
<li>如果张量的维度不同, 将维度较小的张量进行扩展, 直到两个张量的维度都一样.</li>
<li>如果两个张量在某个维度上的长度是相同的, 或者其中一个张量在该维度上的长度为1, 那么我们就说这两个张量在该维度上是相容的.</li>
<li>如果两个张量在所有维度上都是相容的, 它们就能使用广播.</li>
<li>广播之后, 每个维度的长度将取两个张量在该维度长度的较大值.</li>
<li>在任何一个维度上, 如果一个张量的长度为1, 另一个张量长度大于1, 那么在该维度上, 就好像是对第一个张量进行了复制.</li>
</ul>
<p><code>tf.broadcast_to</code>以显式的方式按照广播机制扩展张量的维度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment"># 等价于 b + tf.broadcast_to(a, b.shape)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [2, 3, 4],</span><br><span class="line">       [3, 4, 5]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.broadcast_to(a, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [1, 2, 3],</span><br><span class="line">       [1, 2, 3]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 静态形状, TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([3, 3])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 动态形状, Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c), tf.shape(d))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 广播效果</span></span><br><span class="line">c + d  <span class="comment"># 等价于 tf.broadcast_to(c, [3, 3]) + tf.broadcast_to(d, [3, 3])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[2, 3, 4],</span><br><span class="line">       [3, 4, 5],</span><br><span class="line">       [4, 5, 6]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是推荐系统</title>
    <url>/2020/07/25/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E4%BB%80%E4%B9%88%E6%98%AF%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>这个系列的文章, 算是对王喆大佬的新书&lt;深度学习推荐系统&gt;的学习笔记, 大佬的书写得非常非常好, 好在哪呢? 首先是对于推荐系统有一个非常完整地介绍, 几乎包含了与推荐系统相关的一切; 其次整体结构非常有逻辑, 从推荐系统简介, 到经典的方法, 深度学习的方法, 推荐系统的一些难题, 工程问题等, 逐渐进行讲解; 最后, 对于涉及到的关键方法, 能够用精练的语言进行描述, 并加入自己独到的见解, 看完让人不至于陷入细枝末节, 而是有一个较为宏观的认识.</p>
<p>如果还有没有阅读过的同学, 强烈推荐嗷~</p>
<a id="more"></a>
<h1 id="增长引擎"><a href="#增长引擎" class="headerlink" title="增长引擎"></a>增长引擎</h1><p>记得还在我很小的时候, 我有时候会异想天开地想, 要是全中国每个人给我一块钱, 那我就有十多亿了…</p>
<p>后来我才慢慢发现, 互联网的出现, 正在让这一”胡思乱想”成为现实. 互联网让信息的传播成本降低到趋于零, 将原本天涯海角的人联系起来, 并且可以由量变发生质变, 在互联网的基础上, 派生出了许多商业奇迹.</p>
<p>对于互联网, 或者说互联网企业来说, 什么是重要的呢? 是技术吗, 是产品吗, 可以是, 但是光有技术与产品, 没有用户或者说流量, 那也是一个空架子.</p>
<p>那么, 如何获取用户呢? 如果是传统的商店, 企业, 可能会上街发传单, 上门搞推销, 但是这样的方法有两个比较大的问题:</p>
<ul>
<li>成本高, 发传单的人, 搞推销的人, 传单本身, 推销时的物品消耗…</li>
<li>效率低, 多数时候只能一对一服务, 同时你在这推销半天, 人家最后走了, 等于零.</li>
</ul>
<p>所以, 互联网依托于信息传播与其它技术, 在获取客户与流量方面, 有自己的方法. 而在众多方法中, 有一种方法可以被称为互联网的<strong>增长引擎</strong>, 那就是<strong>推荐系统</strong>.</p>
<h1 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h1><p>有没有发现, 逐渐的, 我们的生活很多地方都被推荐系统影响着:</p>
<ul>
<li>想买东西时, 推荐系统可以帮助你挑选满意的商品.</li>
<li>想了解近期的时事, 资讯, 推荐系统可以将感兴趣的内容优先放到你面前.</li>
<li>想看看有趣的视频, 推荐系统也会把你觉得不错的视频奉上.</li>
<li>想随便听听歌, 推荐系统也会为你挑选合适的音乐.</li>
</ul>
<p>在当今环境下, 推荐系统的作用于意义可言从用户与公司两个角度来进行说明:</p>
<ul>
<li>从用户的角度, 很多时候面临着<strong>信息过载</strong>的问题, 而用户想快速地获取感兴趣的信息.</li>
<li>从公司的角度, 在一个好的产品的基础上, 推荐系统可以使其能够更好地<strong>吸引用户</strong>, <strong>存留用户</strong>, 增加用户<strong>活跃度</strong>, <strong>转化率</strong>. 最终帮助公司商业目标增长.</li>
</ul>
<p>具体来说, 比如我平时喜欢逛哔哩哔哩(老二次元了), 哔哩哔哩的核心产品是视频, 各种各样的视频, 动画, 游戏, 舞蹈, 教学…在如今哔哩哔哩壮大后, 由于UP主的活跃, 每日可以新增许许多多的视频. 而我作为一个用户, 如果让我自己找自己想看的, 其实有时候我都不知道我想看啥; 每次只给我推热点视频, 里面很可能大多数都是我不感兴趣的. 作为哔哩哔哩大会员, 即便没有推荐系统给我推荐有趣的视频, 我大概率仍然会使用, 但是有了推荐系统, 可以让我花更多的时间看视频.</p>
<p>就是说, 对于类似的这种内容平台, 通过推荐系统, 可以将原本只是路过, 只是一面之缘的用户, 吸引成老用户, 而将原本的老用户, 培养成骨灰级用户. 当许多人花许多时间在平台上时, 就会吸引更多的优质UP主, 产生更多的优质视频, 这一切形成一个良性循环, 最终要实现盈利, 实现流量变现, 就非常简单了. 不过我个人喜欢哔哩哔哩的原因之一, 其实是它的广告和营销比较节制, 不会让人感到不适, 希望继续保持~</p>
<p>对于内容平台来说, 如果说推荐系统主要负责引流, 增长, 那么对于电商这样的平台来说, 推荐系统可能直接和营收挂钩, 一个优秀的推荐系统, 可以非常显著地帮助公司创造利润.</p>
<h1 id="推荐系统架构"><a href="#推荐系统架构" class="headerlink" title="推荐系统架构"></a>推荐系统架构</h1><p>推荐系统连接的两端, 分别是<strong>信息</strong>与<strong>用户</strong>, 因而对于推荐系统来说, 可用的数据类型总体可分为三类:</p>
<ul>
<li>物品信息, 如视频, 新闻, 商品等.</li>
<li>用户信息, 如用户的性别, 年龄, 历史行为, 关系网络等.</li>
<li>场景信息, 或者称为上下文信息, 即当前用户所处的环境, 如时间, 地点, 用户状态等.</li>
</ul>
<p>所以, 简单的推荐系统的框架为:</p>
<ul>
<li>输入一: 用户信息, 物品信息, 场景信息.</li>
<li>输入二: 候选物品库.</li>
<li>推荐模型: $f(U, I, C)$, 其中的参数分别对应输入一中的数据.</li>
<li>输出: 推荐物品列表.</li>
</ul>
<p>更近一步, 面对上面比较抽象的框架, 进行具体化和工程化. 可以分成两大板块, 分别是<strong>数据部分</strong>与<strong>模型部分</strong>.</p>
<h2 id="数据部分"><a href="#数据部分" class="headerlink" title="数据部分"></a>数据部分</h2><p>主要负责用户, 物品, 场景的信息收集与处理. 按平台实时性的强弱排序, 依次为”客户端及服务器端实时数据处理”, “流处理平台准实时数据处理”, “大数据平台离线数据处理”, 同时对应的海量数据处理能力由弱到强.</p>
<p>经过数据处理后, 数据出口主要有三个:</p>
<ul>
<li>生成推荐模型的样本数据, 用于模型训练与评估.</li>
<li>生成模型服务所需的特征, 用于线上推断.</li>
<li>生成系统监控, 商业智能系统所需的统计数据.</li>
</ul>
<h2 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h2><p>在模型部分, 整体的结构可以由以下组成:</p>
<ul>
<li>召回层: 利用高效的规则, 算法, 简单的模型, 快速从海量候选集中召回用户可能感兴趣的物品.</li>
<li>排序层: 利用排序模型, 对初筛的候选集进行精排序.</li>
<li>补充策略与算法层: 也被称为”再排序层”, 可以在原有的排序结果上, 为了兼顾”多样性”, “流行度”, “新鲜度”等指标, 结合一些补充的策略和算法进行调整, 形成最终的推荐列表.</li>
</ul>
<p>以上过程被称为模型服务过程, 在线环境进行模型服务前, 需要先进行模型训练与评估. 模型训练可分为<strong>离线训练</strong>与<strong>在线更新</strong>两种. 其中离线训练课利用全量的样本与特征, 使模型接近全局最优. 在线更新可以准实时地利用新的样本进行训练, 学习数据的变化趋势, 满足实时性需求.</p>
<p>此外, 为了方便模型的迭代优化, 需要用到<strong>离线评估</strong>与线上<strong>A/B测试</strong>等多种评估模块, 得到线下线上评估指标, 指导模型迭代优化.</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-模型评估</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/</url>
    <content><![CDATA[<p>利用机器学习算法得到模型, 对模型的评估是必不可少的关键一步.</p>
<p>因为首先, 通过模型评估, 可以知道模型到底效果好不好, 有多好.</p>
<p>其次如果模型不好, 大不了咱们不使用就行了嘛ヾ(´∀`o)+</p>
<p>再者, 通过模型评估过程, 可以发现一些问题, 进而可以尝试进行针对性的改动.</p>
<a id="more"></a>
<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>在不同的场景, 不同的任务下, 评估模型的指标是不一样的.</p>
<p>在众多的指标中, 我认为总体说来可以分为两大类:</p>
<ul>
<li><p>适合训练模型的指标.</p>
<p>啥是适合训练模型的指标, 比如在二分类问题中, 训练模型时大部分默认的损失函数是logloss. 但是如果直接给我们展示一个logloss, 我们会对它有什么感受吗, 是大还是小呢? 这需要比较才能知道.</p>
</li>
<li><p>方便人们比较的指标.</p>
<p>一些方便人们直观感受的指标, 比如准确率accuracy, 曲线下面积AUC. 就拿accuracy来说, 我们知道其范围是0-1, 越大越好.</p>
</li>
</ul>
<p>除此之外, 同样的任务下, 根据样本的分布, 可以采取不同的评估指标. </p>
<p>仍然以二分类为例, 若正负样本均衡, 接近1:1的比例, 那这个时候用accuracy就可以. 但是若正负样本不均衡, 比如1:10的比例, 这时候再使用accuracy可能就不合适了. 可以考虑其它一些指标如F1, AUC. 而通常来说AUC更好一些, 因为相比F1, AUC指标更关注模型本身的区分度, 倘若样本中正负样本的比例发生变化(分层抽样), 那么F1会跟着变化, 而AUC会几乎保持不变.</p>
<p>常用的一些评估指标如下:</p>
<ul>
<li>分类.<ul>
<li>accuracy</li>
<li>AUC</li>
<li>F1</li>
<li>precision</li>
<li>recall</li>
<li>logloss</li>
</ul>
</li>
<li>回归.<ul>
<li>MSE</li>
<li>RMSE</li>
<li>MAE</li>
<li>R-squared</li>
</ul>
</li>
</ul>
<h1 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h1><p>过拟合是一个老生常谈的话题了, 几乎每一个学习机器学习的同学, 在一开始就会被灌输这样的知识. 相比欠拟合, 过拟合更容易出现, 带来的后果也可能更严重.</p>
<p>而在这里, 不会严谨地去说明过拟合的概念呀神马的, 而是从我个人的经验上来进行一些思考, 不一定正确, 建议用批判性的方式阅读（○｀ 3′○）</p>
<ul>
<li><p>什么是过拟合.</p>
<p>过拟合通俗来讲, 指的是模型在训练集或者与训练集完全同分布的数据集上表现很好, 但是泛化性太差, 可能样本分布稍微变一点, 模型表现就大打折扣.</p>
</li>
<li><p>怎么判断过拟合.</p>
<p>判断是否过拟合, 有多种方法, 这里说一下我自己常用的方法. 一般来说, 我们建模的时候会有训练集, 验证集以及测试集. 通过观察对比模型在三份数据集, 或者仅仅观察训练集与验证集的表现, 来定性地进行是否过拟合判断.</p>
<p>举个栗子, 在二分类问题中, 以AUC为评估指标. 一个过拟合的模型, 可能在训练集上AUC为0.9, 验证集上0.7, 测试集上0.6. 而另一个模型, 在训练集上AUC为0.75, 验证集0.7, 测试集0.68. 对比之下, 肯定是后者更好.</p>
<p>也就是说, 通过对比不同数据集之间评估指标的差异, 可以对是否过拟合做出一定的判断.</p>
<p>同时, 在一些时候, 很难”刚刚好”做到所谓的不过拟合, 一定程度的轻微过拟合也是可以接受的. 或许尝试各种办法, 把训练集和验证集的评估指标变得几乎一致, 不过拟合了, 但此时可能处于欠拟合状态.</p>
</li>
<li><p>为什么会造成过拟合.</p>
<p>有很多原因可能会造成过拟合, 这里以GBDT模型为例, 来列举一些会让训练集/验证集/测试集之间评估指标差距变大的原因:</p>
<ul>
<li><p>树模型超参数设置不合理. 在树模型中, 对结果影响较大的参数, 比如最大树深度, 当取值比较大时(例如7层), 会发生什么情况呢? 树模型会尽其所能去寻找特征与特征之间的交互, 以求优化目标函数. 但这样存在的问题是, 越深的层数, 越复杂的特征之间的交互, 其有效性可能只存在于训练集, 部分存在于验证集, 不存在于测试集. 简而言之, 模型复杂度较高, 学到了一些”假知识”.</p>
</li>
<li><p>提前停止参数设置过大. 除了最大树深度, 对于GBDT而言, 树的棵树也是很重要的参数. 而提前停止这一方法, 可以较好的寻找最佳树的棵树. 但是, 当此参数设置过大时, 也会造成过拟合的情况. </p>
<p>比如原本学到某个树的棵树时, 已经比较合适了. 而由于提前停止设置过大, 只要验证集在某一刻变好一点, 就会继续增加树木棵树, 训练集上评估指标会一直变好, 验证集上评估指标会发生波动. 训练集与验证集上的评估指标差距拉大, 造成过拟合.</p>
</li>
<li><p>有一些”噪音”特征也会造成过拟合. 比如GBDT在训练的时候, 有一些特征原本对问题没有真正贡献, 但在”偶然”的情况下, 参与了学习. 其带来的效果是会抬高模型在训练集的评估指标, 但通常在验证集与测试集上都会降低评估指标.</p>
<p>若结合验证集使用提前停止方法, 那模型可能学一些冗余的规则, 来抵消掉”噪音”特征带来的降低的效果. 最终结果, 相比没有”噪音”特征造成过拟合的情况, 可能是训练集上评估指标很高, 验证集差距不大, 测试集评估指标较低Σ(っ °Д °;)っ 因为除了”噪音”特征, 那些冗余的规则对于测试集来说可能也会带来反效果.</p>
</li>
</ul>
</li>
<li><p>如何应对过拟合.</p>
<p>针对不同的场景不同的模型, 有不同的应对过拟合的方法, 这里针对上面提到的一些可能造成过拟合原因, 列出一些方法.</p>
<ul>
<li>尽可能尝试相对简单的模型结构φ(≧ω≦*)♪, 本着能简单绝不复杂的态度. 比如在超参数选择的时候, 两组超参数对应的验证集上评估指标差距很小, 那优先选择更简单(如最大深度更小)的那一组超参数.</li>
<li>同样, 在不影响验证集评估指标的情况下, 适当调低提前停止参数.</li>
<li>对于”噪音”特征, 在之前的篇章&lt;特征选择&gt;中, 有一些方法可以一定程度上进行处理(๑•̀ㅂ•́)و✧</li>
</ul>
</li>
</ul>
<h1 id="模型解释性"><a href="#模型解释性" class="headerlink" title="模型解释性"></a>模型解释性</h1><p>在一些场景下, 我们需要尽可能地确保模型的解释性.</p>
<p>那什么是模型的解释性呢? </p>
<p>首先是入模特征的解释性, 即对某个特征, 是否了解其含义, 其分布是否符合经验认知. 举个栗子, 有个特征是”性别”, 含义是很清晰的(&gt;▽&lt;) 假设标签是”是否爱玩游戏”, 经过分析发现, 性别为女的爱玩游戏的概率更大(=′ー`) 这时候好像和我们的认知有那么点冲突, 可以进一步分析, 这里的”游戏”是&lt;英雄联盟&gt;, 还是&lt;奇迹暖暖&gt; ヽ(✿ﾟ▽ﾟ)</p>
<p>然后是模型学成啥样了. 要想知道模型学成啥样了, 最直观的模型莫过于线性模型. 对于线性模型来说, 我们可以知道它各个特征系数的大小, 是正还是负, 很好理解. 然后是决策树一类的模型, 虽然层数多了以后比较复杂, 但是仍然有明了的结构. 而更复杂的模型, 如GBDT, 神经网络等, 想像线性模型一样, 直接观察其参数去估计模型学成啥样了, 是不现实的.</p>
<p>那有没有方法对复杂的模型, 也能够从模型解释性的角度对其进行分析呢? 当然是可以的, 其实模型解释性是一个专门的方向, 一直以来人们提出了很多的方法. 这里着重介绍两种方法.</p>
<ul>
<li><p>PDP(Partial Dependence Plot).</p>
<p>具体的做法, 就是对于训练好的模型, 对某一个特征, 每次只取一个值, 将该特征的全部样本都赋予这个值, 然后利用模型进行预测, 对预测得到的分数取平均, 得到一个(特征值, 平均分数). 多次操作后, 可以得到多组这样的值, 将他们画到一幅图上, 可以观察到随着该特征的变化, 会对模型产生怎样的影响.</p>
<p>仍然以上文中的”性别”为例, 假设0代表女, 1代表男, 并且性别男在统计上更爱玩游戏. 那么通过PDP得到的结果, 可能也是性别为1时, 模型分均值更高(表示更爱玩游戏). </p>
<p>但是也有可能PDP给出相反的情况, 此时不要惊慌φ(≧ω≦*)♪ 造成这中情况的原因有多种. 比如过拟合; 有其它特征与”性别”强相关; 或者在树模型中, 经过一些分割后的样本中, 就是呈现这种模式. 而要应对这种情况, 也有不同的方法, 一般来说, 如果这样的特征重要性靠后, 可以考虑不管; 如果特征重要性靠前, 同时有着强烈的先验知识作为支撑, 在GBDT中可以用”单调性限制”进行强行扭转; 实在看不惯, 且对模型整体没啥影响的话, 也可以删除该特征(。・∀・)ノ</p>
</li>
<li><p>Shap.</p>
<p>这里有个GitHub的仓库<a href="https://github.com/slundberg/shap" target="_blank" rel="noopener">Shap</a>, 快一万收藏了. 关于Shap的原理以及应用实例, 以后应该会单独进行介绍, 这里进行简单说明.</p>
<p>相比PDP, Shap可以进行更加细粒度的分析. 比如对于某个样本, 模型最后给到了一个比较高的预测值, 那么各个特征对其起到了怎样的作用呢? Shap可以将每个特征, 用线性加权的方式连接起来, 得到最后的模型预测值, 类似线性模型那样(每个样本对应一个线性模型), 通过观察每个特征的系数, 可以知道每个特征在每个样本中,起到了怎样的作用, 是大还是小, 是正向还是负向.</p>
</li>
</ul>
<p>总之, 模型解释性可以让我们增加对模型的认识和理解, 若模型效果不错, 也在解释性上没有明细不足, 那这样将给予我们信心把模型应用到生产实践中.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-特征选择</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
    <content><![CDATA[<blockquote>
<p>有时候, 选择比努力更加重要.</p>
</blockquote>
<p>如果在前一篇中, 我还扯”特征工程不那么重要”, 那这里我会说”特征选择一直很重要”（○｀ 3′○）</p>
<p>这里我会根据自己有限的经验, 介绍一些特征选择的方法.</p>
<a id="more"></a>
<h1 id="为什么要进行特征选择"><a href="#为什么要进行特征选择" class="headerlink" title="为什么要进行特征选择"></a>为什么要进行特征选择</h1><p>特征选择有不少的好处, 主要来说有以下几点:</p>
<ul>
<li>减少特征数量, 让模型更加简单. 同样的模型效果, 一个模型用了1000个特征, 一个模型用了100个特征, 在使用的时候会选择哪个模型呢? 我想大多数人应该都会选择第二个相对简单的模型.</li>
<li>提升模型稳定性, 模型的稳定很大程度上依赖于特征的稳定, 在一个随时间变化的环境中, 模型的稳定性有时候是最重要的.</li>
<li>提升模型表现. 并不是所有特征对模型都是正向提升的, 零提升, 甚至负提升的特征也经常存在. 正因如此, 特征选择才会如此重要. 想像一下, 耗费了大量精力衍生出上千个特征, 自信满满地跑一个模型, 结果发现测试集评估指标没啥变化, 隐约…还下降了(っ*´Д`)っ这时候的心态是有点崩的, 因为衍生的特征中, 有大量零提升与少量负提升的特征.</li>
</ul>
<h1 id="特征选择的两个方面"><a href="#特征选择的两个方面" class="headerlink" title="特征选择的两个方面"></a>特征选择的两个方面</h1><p>在进一步讨论特征选择之前, 先来讨论一下数据集的划分. 在训练模型时, 一个比较科学地做法, 是首先按照OOT(out of time)的方式, 划分出测试集. 之所以这样, 是因为在实际生产环境中, 多数时候是用当前数据训练模型, 去预测未来的事情, 而为了模拟这一情况, 测试集的划分通常选择整体样本中时间段靠后的一匹样本. 然后在训练集中分离出验证集, 或者在训练集中做交叉验证, 在样本量大的时候两种方式基本等价, 这里假设是采用验证集的方式.</p>
<p>那么问题就来了, 由于时间跨度的不同, 样本数据特征的分布就很可能不同. 举个栗子, 某个在训练集上的强特征(与标签有较高的相关性), 到了测试集上变弱了. 如果不考虑这一情况, 仍旧照常训练, 后果就是模型也许在验证集上表现不错, 但是到了测试集就明显拉胯了o(TヘTo)</p>
<p>基于此, 我认为特征选择主要可以分为两个方面:</p>
<ul>
<li>特征稳定性.</li>
<li>特征有效性.</li>
</ul>
<p>下面进行具体地说明.</p>
<h1 id="特征稳定性"><a href="#特征稳定性" class="headerlink" title="特征稳定性"></a>特征稳定性</h1><p>先说稳定性, 是因为我认为应该先做特征稳定性的检验, 只有特征是相对稳定的情况下, 再评估特征的对模型的提升才有意义.</p>
<p>那么怎么评估特征的稳定性呢? 主要的思路是观察对比特征在不同的时间段上的表现是否一致. 如果看到了特征随时间的分布发生了一些变化, 有时候并不是特征本身不稳定, 而是样本本身发生了偏移. 但有些特征, 即使在样本发生偏移时, 仍能够保持较好的稳定性, 这样的特征能够确保我们的模型在未来变化的环境中, 能够长期保持较为稳定的性能.</p>
<p>具体的的做法, 有以下一些, 并不代表全部可行的做法, 也不代表一定好使(≧∇≦)ﾉ:</p>
<ul>
<li><p>PSI.</p>
<p>如果不了解PSI的同学, 可以谷歌一下, 原理很简单这里就不多说了, 计算公式如下:</p>
<script type="math/tex; mode=display">
PSI=\sum_i(A_i-E_i)\times {\rm ln}(A_i/E_i)</script><p>在多个时间段的样本上, 计算特征的PSI, 如果特征的PSI较大, 超过某阈值(一般超过0.1), 则可以考虑舍弃该特征.</p>
<p>不过我个人认为PSI指标更多说明的是样本本身的变化, 可以用来辅助判断特征的稳定性, 但如果过于依赖PSI的话可能会有问题.</p>
</li>
<li><p>基于模型验证稳定性.</p>
<p>可以在训练集上, 按时间将其分成两份数据集, 在其中一份数据集(通常选时间靠前的一份)上, 对每个特征单独进行建模, 得到交叉验证的评估, 然后在另一份数据集上进行预测评估, 对比前后评估结果是否差距过大.</p>
<p>比如在二分类问题中, 采用AUC作为评估指标, 在第一份数据集上的交叉验证结果为0.6, 第二份数据集上为0.58, 那么认为特征稳定性还不错. 而如果第一份上0.6, 第二份上0.5, 那么很明显该特征是不稳定的.</p>
</li>
<li><p>基于分箱验证稳定性.</p>
<p>这里与上面基于模型验证稳定性是比较类似的. </p>
<p>举个栗子, 在二分类问题中, 对某个特征采用等频分箱, 分成10箱. 然后按时间前后分成两部分, 在两份数据上, 分别计算出各分箱的标签均值(比如标签为是否逾期, 则表示逾期率). 然后可以将两份数据上各箱的逾期率, 以折线图的形式, 画到图上, 比较前后两份数据集上该特征的表现是否一致. 若一致则可认为稳定, 否则不稳定. 除了画图人为观察判断以外, 还可以用一些其它方法, 比如对两个分箱后的标签均值列表, 计算其相关性(皮尔森相关性较好), 通过相关性来判断特征前后表现是否一致.</p>
<p>此外, 比如当分成10箱时发现特征前后表现不一致, 可以尝试减少分箱数, 如5箱, 虽然损失了信息, 但有可能可以提高特征稳定性.</p>
</li>
<li><p>对抗验证.</p>
<p>这个方法的原理是这样的, 有时候测试集与训练集之间, 存在分布上的差异, 这个差异导致在验证集上获得的模型表现, 在测试集不成立.</p>
<p>为了定量衡量这种分布差异, 可以给训练集标签为0, 测试集标签为1, 进行二分类建模.</p>
<p>建模后, 可以观察模型的AUC, 如果训练集与测试集同分布, 且模型训练得当, 则AUC一定在0.5附近. 若AUC比较大比如0.6, 则说明两者的分布是不一致的.</p>
<p>若AUC较大时, 原本对抗验证的处理方法, 是将训练集中, 模型预测值最低(与测试集分布差异最大)的一批样本删除, 或者将预测值最高(与测试集分布差异最小)的一批样本作为验证集. 而在一些时候样本是比较珍贵的, 可以先根据模型得到的特征重要性, 找到模型认为对分布差异贡献最大的一些特征, 可以考虑删除这些特征, 来使训练集与测试集分布趋于一致.</p>
</li>
</ul>
<p>以上提到的一些方法, 在进行特征选择时, 可以结合起来使用, 这样能够从多个不同的维度去筛选出稳定的特征, 为后续进一步筛选特征做准备.</p>
<h1 id="特征有效性"><a href="#特征有效性" class="headerlink" title="特征有效性"></a>特征有效性</h1><p>现在假设特征的稳定性已经较好了, 此时一般来说有一个现象, 就是在验证集上获得的模型表现, 在测试集上也能用于近似的结果.</p>
<p>那么进一步, 要在这些特征的中挑选出能够让模型学习得最好的那一组特征. 举个栗子, 就好比一个学生要考试了, 摆在他面前的有一大堆学习资料, 如果其中掺杂了一些不好的资料, 那么不但会耽误学生学习其它优秀的资料, 而且可能还会形成误导.</p>
<p>筛选特征的方法有很多, 我这里列举一些比较常用的做法.</p>
<h2 id="基于简单统计信息"><a href="#基于简单统计信息" class="headerlink" title="基于简单统计信息"></a>基于简单统计信息</h2><p>比如基于特征的缺失率, 当一个特征的缺失率高达95%时, 也许对当前模型表现还有那么一丢丢贡献, 但是实际生产环境中这样的特征更倾向于舍弃.</p>
<p>同理, 特征中的某个数值, 占据了绝大部分比例, 比如0占了98%, 那么这样的特征在一些情况下也是可以考虑直接删除的.</p>
<h2 id="基于相关性"><a href="#基于相关性" class="headerlink" title="基于相关性"></a>基于相关性</h2><p>在一般的分类, 回归问题中, 可以进行特征与标签的相关性评估.</p>
<p>常用的相关性有以下一些:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">相关性指标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">连续 &amp; 连续</td>
<td style="text-align:center">皮尔森, 斯皮尔曼</td>
</tr>
<tr>
<td style="text-align:center">连续 &amp; 离散</td>
<td style="text-align:center">方差分析</td>
</tr>
<tr>
<td style="text-align:center">离散 &amp; 离散</td>
<td style="text-align:center">互信息</td>
</tr>
</tbody>
</table>
</div>
<p>基于统计相关性来进行评估特征, 优点是很快, 在一些场景下需要用到线性模型, 或者特征里有很多强特征时, 是个还不错的方法. 但缺点是有时候不能较好地评估非线性相关的特征, 比如皮尔森相关性.</p>
<p>因此, 在使用相关性来评估特征时, 要结合具体场景进行谨慎考虑.</p>
<h2 id="L1正则"><a href="#L1正则" class="headerlink" title="L1正则"></a>L1正则</h2><p>利用含有L1正则的线性模型来挑选特征, 根据不同的正则系数, 会将不同数量的特征的系数变成0.</p>
<p>选择合适的正则系数, 可以较快地在大量特征中选择出一组较好的特征.</p>
<p>不过由于是基于线性模型进行选择, 所以如果原本特征中, 存在一些与标签关系为非线性的特征, 可能也会被当做无用特征, 系数变为0.</p>
<h2 id="特征重要性"><a href="#特征重要性" class="headerlink" title="特征重要性"></a>特征重要性</h2><p>此方法的核心思想是, 先利用一些方法, 对特征的重要性进行排序, 然后根据排序保留靠前的特征, 舍弃靠后的特征.</p>
<p>常用的衡量特征重要性的方法有以下一些:</p>
<ul>
<li>直接基于树模型衡量特征重要性. 比如在GBDT模型中, 可以根据某特征被分割次数, 或者提供的信息增益衡量其重要性.</li>
<li>基于随机排列衡量特征重要性. 这一类方法的主要思想是, 一个特征原本有一个重要性, 而在另外一种情况下(比如将该特征随机排列), 又得到一个重要性, 通过对比前后两个重要性, 来判断该特征是否有效.<ul>
<li>Null Importance: 此方法是保持特征不变, 随机排列标签, 来对比前后特征重要性的变化. 若前后特征重要性变化大, 则说明该特征是真实有效的; 若特征重要性前后几乎差不多, 甚至还提高了o(*≧▽≦)ツ 那这个特征极大可能是噪音.</li>
<li>Eli5: 这里叫做这个名称, 是因为在一个叫<code>eli5</code>的包中有实现这种方法~此方法是在训练集上正常训练, 然后在验证集上预测时计算评估指标(如AUC), 不过考虑两种情况, 一种是真实的特征, 一种是对某个特征进行随机排列, 对比前后评估指标的变化. 同样若前后差距较大, 则说明该特征是重要的.</li>
<li>Boruta: 此方法是通过在训练时, 把原本的所有特征复制一份, 比如原本的10个特征变成20个特征. 然后对复制的所有特征进行随机排列. 接下来多次训练模型, 在每次训练模型后, 整体考虑真实特征与影子(随机排列)特征的重要性, 将不合格的特征逐一去除, 保留最后较好的一组特征.</li>
</ul>
</li>
<li>Shap. 此方法原本是用于模型解释性的, 但是其同样可以输出特征重要性的评估. 这里不细讲, 以后应该会单独介绍.</li>
</ul>
<p>而在获得了各个特征的重要性后, 一般来说, 有两种处理方式:</p>
<ul>
<li><p>直接按某阈值切分, 按比例保留一部分高重要性特征, 舍弃其余低重要性特征. 在考虑切分阈值时, 可以画出按重要性排序后, 重要性的累计图, 找到累计重要性变化较小的地方(即剩下特征对整体贡献很小), 作为阈值.</p>
</li>
<li><p>RFE(recursive feature elimination), 即不使用重要性进行一刀切, 而是逐步地剔除重要性较小的特征. 比如原本100个特征, 每次训练模型后, 剔除当前重要性最小的5个特征, 同时记录当前模型在验证集或者交叉验证上的表现.</p>
<p>在最后停止时, 会得到在不同数量特征组合下, 模型的表现, 此时可以选择其中表现最好的特征组合. 也可以缩小范围, 比如发现在30~40特征数量上表现不错, 那在这个区间每次只剔除重要性最小的特征, 进行细致地搜索.</p>
</li>
</ul>
<p>一般来说, RFE效果不错, 且耗费计算资源在可接受范围内, 是一种较好的方法.</p>
<h2 id="逐步法"><a href="#逐步法" class="headerlink" title="逐步法"></a>逐步法</h2><p>逐步法的核心思想是, 从一组初始特征开始, 逐渐加入或者逐渐舍弃特征. 在每次加入或者舍弃时, 一般需要所有可能进行建模比较效果, 选择最优的情况.</p>
<ul>
<li><p>前向法.</p>
<p>从指定特征或者0特征开始, 每次对原特征加上其它某一个特征进行建模, 遍历一遍后, 将表现最好的特征加入, 再进行下一轮循环. 直到达到停止条件, 如指定最大特征数量, 或者评估指标不再上升.</p>
</li>
<li><p>后向法.</p>
<p>与前向法相反, 从全部特征开始, 每次考虑剔除一个特征, 遍历后剔除那个剔除后让模型效果提升最大的特征. 直到达到最小特征数量, 或者评估指标不再上升.</p>
</li>
<li><p>前向后向法.</p>
<p>结合前向法与后向法, 比如先用前向法, 当评估指标不再上升后, 再使用后向法, 如此循环.</p>
</li>
</ul>
<p>逐步法的好处是也许可能找到一组非常好的特征, 但是缺点也很明显, 算法时间复杂度很高, 当特征很多, 且模型较为需要较多训练时间(如GBDT)时, 会显得很慢. 可以在特征不是很多, 且使用线性模型这样训练速度较快的模型时使用.</p>
<h2 id="搜索法"><a href="#搜索法" class="headerlink" title="搜索法"></a>搜索法</h2><p>考虑到特征选择其实是一件玄学的事情, 所以为什么不用玄学对抗玄学呢(≧∇≦)ﾉ</p>
<ul>
<li><p>暴力搜索法.</p>
<p>遍历所有可能的特征组合. 如果算力够的话, 也不失为一种办法…</p>
</li>
<li><p>随机搜索法.</p>
<p>每次随机抽取一部分特征, 相比暴力搜索法, 更加可控一些, 可以指定搜索次数, 防止程序一直跑下去.</p>
</li>
<li><p>启发式搜索法.</p>
<p>使用一些启发式算法来优化搜索过程. 比如基因算法, 每个特征是否在当前组合中出现, 可以看出是一组基因序列, 1即出现, 0即不出现.</p>
<p>在某一代的组群中, 表现较好的特征组合用于更多的机会进行交叉组合与变异.</p>
<p>通过这样的方式, 既可以避免暴力搜索的超高复杂度, 也可以尽量避免随机搜索进行的无效搜索.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-特征工程</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<p>也许有一些刚开始接触传统机器学习相关算法, 在学习建模的时候, 会觉得除了算法本身外, <strong>特征工程</strong>是最重要的, 能够化腐朽为神奇, 能够让自己打比赛拿第一ヾ(≧▽≦*)o</p>
<p>比赛能不能拿好成绩我不知道, 也许通过一些特征工程手法, 在最后提升了几个千分点从而击败其他选手. 但是, 特征工程并没有那么重要, 且不说深度学习, 在传统机器学习里, <strong>数据本身的质量决定了模型的性能</strong>.</p>
<blockquote>
<p>数据决定了上限, 我们做的其他事情只是在逼近这个上限而已.</p>
</blockquote>
<a id="more"></a>
<h1 id="什么是特征工程"><a href="#什么是特征工程" class="headerlink" title="什么是特征工程"></a>什么是特征工程</h1><p>特征工程没有太过明确的定义, 在结构化数据的范畴中, 如下一些操作可以算进特征工程</p>
<ul>
<li><p>特征预处理.</p>
<p>一般来说, 采取不同的模型, 根据模型的特点, 对特征做一些适当的操作, 可以明显提升模型效果.</p>
</li>
<li><p>特征提取/衍生.</p>
<p>原本的一些特征可能无法很好地表达信息, 而在经过一些如特征提取, 特征组合(两两相乘)得到新特征后, 可以让模型学得更好.</p>
</li>
</ul>
<p>在后续的内容中, 会进行更加详细的叙述.</p>
<h1 id="不那么重要的特征工程"><a href="#不那么重要的特征工程" class="headerlink" title="不那么重要的特征工程"></a>不那么重要的特征工程</h1><p>如一开始所说, 特征工程不那么重要, 这里就来说明为啥不那么重要.</p>
<ul>
<li><p>现在的一些”高级”模型, 可以更好地捕获数据信息</p>
<p>同样一份数据, 同样基于决策树的模型, 单棵决策树, 随机森林, GBDT系列(如XGB, LGB), 建模得到的效果经常是有明显差异的, 通常是GBDT系列的模型效果最好.</p>
<p>因为GBDT模型有更高的复杂度, 同时学习到了更多有效的信息(不过拟合的情况下).</p>
</li>
<li><p>在有些时候, 特征工程前后提升较小.</p>
<p>特征工程的效果, 有时候还得数据”配合”. 更通俗一点, 数据越原始, 特征工程的空间也就越大, 提升空间也越大.</p>
<p>比如手里的数据是一些日志记录, 需要先经过清洗整理为结构化信息, 然后发掘其中更有效的特征, 提升模型效果, 这时候往往相比刚清洗好的特征, 经过进一步处理的特征会更好地提升模型效果.</p>
<p>还有一些时候, 拿到的数据是已经经过别人精心设计处理过后的, 此时再做更多的尝试, 收效甚微.</p>
</li>
<li><p>过于复杂的特征工程, 不利于实际生产环境部署.</p>
<p>举个栗子, 一个原本只有一百个特征的数据集, 经过一顿操作, 衍生出了各种各样的特征, 变成了几千个特征, 但是带来的收益呢, 经常是不成正比的.</p>
<p>而在实际生产环境中, 有些时候甚至不会那么在意那一点点模型效果的差距, 一个简单的模型的优先级会高于一个过于复杂的模型.</p>
</li>
</ul>
<h1 id="重要的特征工程"><a href="#重要的特征工程" class="headerlink" title="重要的特征工程"></a>重要的特征工程</h1><p>喂前面才说特征工程不重要, 为什么这里又要说它重要呢(＠￣ー￣＠)</p>
<p>啊…这…因为确实重要呀ヾ(≧▽≦*)o</p>
<ul>
<li><p>即使是”高级模型”, 特征工程也能够带来显著提升.</p>
<p>诚然”高级模型”一些时候更加能够捕获数据信息, 但是一些实际的经验显示, 有时候也许就是那么一个衍生特征的差别(如两个特征相除), 前后模型评估指标就有显著差距(如AUC相差2个百分点).</p>
</li>
<li><p>对一些”不太友好”的数据, 合适的特征工程非常重要.</p>
<p>比如原始数据是一些短文本, 需要将其转换成结构化数据; 如原始数据非常稀疏, 能不能变得稠密一些? 原始数据中有大量高基数的类别特征, 怎么处理好一些?</p>
<p>此时好的特征工程方法, 确实可以带来不一样的表现.</p>
</li>
<li><p>对不同模型, 采取不同的方法.</p>
<p>如果是一些基于决策树的模型, 那其实可以省事很多, 有时候甚至可以不做啥处理.</p>
<p>而另外一些模型, 比如线性模型, 神经网络, KNN等, 需要了解模型特性, 进而选择对应合适的方法处理数据. 正确的处理方法相比不正确的处理方法, 差异很大.</p>
</li>
</ul>
<h1 id="一些常见的特征工程方法"><a href="#一些常见的特征工程方法" class="headerlink" title="一些常见的特征工程方法"></a>一些常见的特征工程方法</h1><p>在进行特征工程前, 做好EDA是很有好处了, 通过在数据分析时获得的信息, 再加上我们的脑洞, 也许可以创造出所谓的”魔法特征”哟~</p>
<h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>常见的标准化方式, 是将特征转变到以0为均值, 1为标准差的分布上. 或者可以更进一步实施BOXCOX, 将分布尽量转化为正态分布.</p>
<p>一般来说, 哪些模型需要做标准化呢? 总体说来有两类:</p>
<ul>
<li>基于点与点(样本与样本)之间的距离. 比如KNN, SVM等.</li>
<li>依赖类似梯度下降法训练的模型. 比如线性回归, 神经网络等.</li>
</ul>
<p>在合适的时候标准化, 可以加速模型训练, 同时获得更好的模型效果.</p>
<h2 id="缺失值填充"><a href="#缺失值填充" class="headerlink" title="缺失值填充"></a>缺失值填充</h2><p>通常, 一些模型如线性回归, 必须要进行缺失值填充. 基于决策树的模型可以不进行缺失值填充, 但是同样可以进行尝试, 因为也许在一些情况下可以让模型学习得更好.</p>
<p>一般在缺失值填充时, 还可以考虑是否生成额外的缺失值指示列, 即用于表明原本某个特征是否缺失(如1表示缺失, 0表示未缺失).</p>
<p>常用的缺失值填充方式有以下一些:</p>
<ul>
<li><p>固定值填充.</p>
<p>这个比较无脑, 比如用0这样的值来对缺失值进行填充. 但这种方法在多数情况下不是很好.</p>
</li>
<li><p>统计值填充.</p>
<p>对于连续型特征, 可以考虑用均值, 中位数进行填充; 对于离散型特征, 可以考虑用中位数, 众数进行填充.</p>
</li>
<li><p>结合标签信息填充.</p>
<p>比如在二分类问题中, 标签为某个样本是否发生逾期, 那么对某一个特征, 可以计算其缺失部分的逾期率.</p>
<p>对非缺失部分, 进行分箱(通常等频分箱更好), 计算每箱的逾期率.</p>
<p>用缺失部分的逾期率与非缺失部分的每箱进行对比, 选择逾期率最接近的那一箱的特征统计值(如均值)来填充缺失值.</p>
<p>这个方法个人觉得在一些特征为强特征时可以使用, 所谓强特征, 即特征与标签高相关. 而在一些时候, 可能会有一定的过拟合风险.</p>
</li>
<li><p>利用其它特征.</p>
<p>如果处理过经典的泰坦尼克数据集的同学, 应该看到过用每个人的称谓来估计他们缺失年龄的做法, 这就是一个非常典型的利用其它特征来进行缺失值填充的例子.</p>
<p>如果利用模型来进行填充, 总体思路是使用半监督学习的做法. 即对于某个特征, 将其非缺失部分作为标签, 结合其它对应的特征来训练模型, 然后对其缺失部分数据进行预测.</p>
<p>比较简单的做法, 是一个特征构建一个模型, 比如使用KNN(需要做一些额外操作来应对其它特征的缺失值).</p>
<p>复杂一些的方法, 是循环构建模型:</p>
<ol>
<li>确定所选模型(如随机森林), 对特征按一定顺序进行排序.</li>
<li>按顺序对每个特征进行学习. 把目标特征外的特征, 按统计值(如均值)进行填充, 对于已经学习过的特征使用模型预测值填充. 以目标特征为标签, 训练模型.</li>
<li>可以多次循环第2步. 比如总共有10个特征, 遍历3轮, 则最终可以得到30个模型(每个特征对应3个模型).</li>
<li>在<code>transform</code>时, 会按照和<code>fit</code>时同样的方法和顺序进行填充.</li>
</ol>
<p>这个方法的优点是填充值充分利用到了其它特征的信息, 缺点是无论是训练还是预测填充的过程, 都比较慢, 在一些需要快响应的场景中不适合.</p>
</li>
</ul>
<h2 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h2><p>对于异常值, 这里暂时只考虑单个特征分布上的异常, 不考虑其它一些例如”异常检测”中所指的异常. 具体说来就是某个特征上, 少数样本的值严重偏离整体分布的异常值.</p>
<p>在处理异常值之前, 先要找出异常值, 连续数值型特征以及回归问题中的标签都可能存在这样的异常值. 可以利用箱线图的原理来确定异常值; 也可以设定一个边界占比阈值如1%, 将最大的1%与最小的1%样本当做异常值.</p>
<p>确定异常值后, 一般来说有两种比较简单的处理方式:</p>
<ul>
<li>若异常值样本较少, 则可以考虑直接删除.</li>
<li>可以设定一个最大最小数值, 将超出该阈值的异常值变成该数值.</li>
</ul>
<p>异常值处理的意义, 是可以让模型学习得更好, 同时预测的结果更加客观真实.</p>
<p>举个栗子, 有一个线性回归模型, 有5个特征. 当某个样本中, 第1个特征中出现了一个特别大的值, 此时最终的预测值将会几乎完全被第1个特征决定, 这样的预测结果的可信度就会降低.</p>
<h2 id="类别特征编码"><a href="#类别特征编码" class="headerlink" title="类别特征编码"></a>类别特征编码</h2><p>对于几乎所有的机器学习算法来说, 只能接受数值型特征, 所以一些字符型的特征, 需要先进行数值编码才行. 并且, 即便是数值型的特征, 但是本质上仍然是类别特征(如月份), 那么仍然可以尝试不同的类别特征编码方法, 来提高模型的表现.</p>
<p>常见的类别特征编码方法有以下一些:</p>
<ul>
<li><p>直接编码.</p>
<p>这个方法简单粗暴, 就是将每个类别对应到一个整数上, 对应的方式一般是随机的. </p>
<p>如果类别特征较少, 类别特征内的类别数量也少(如3个), 只是想随便跑一个baseline模型, 可以一试, 但若想要更好的模型表现, 这样的编码是不行的.</p>
</li>
<li><p>独热编码.</p>
<p>独热编码是非常经典的类别特征编码方法, 将特征的每个类别变成一列bool型特征, 表示是否是该类别.</p>
<p>一般说来, 独热编码能够让模型学习到不错的表现.</p>
<p>要说缺点, 那就是当类别特征较多, 且特征内类别过多时, 可能会产生大量的列, 这对于数据的保存于训练都不友好. 当然可以做一些优化, 比如将一些占比较少的类别合并为一类, 或者设定最大类别数等.</p>
</li>
<li><p>频率编码.</p>
<p>即按类别的出现频率进行编码, 让模型知道哪些类别出现几率更高, 是否与标签相关.</p>
<p>这个方法感觉更偏向于特征衍生, 可以在其它类别特征编码基础上, 附加此编码方法得到更多特征.</p>
</li>
<li><p>哈希编码.</p>
<p>哈希编码的方法可能更多用于文本编码上, 但也可以用在类别特征编码上. 其原理是运用哈希函数, 将原本的特征向量(行), 转变为其它的数值向量. 可以预设转换后的向量长度, 既可以扩展原本的长度, 也可以进行降维.</p>
<p>哈希编码在一些情况下有较好的效果. 不过由于其特性, 会把一些不同的特征, 编码成同样的向量. 并且编码后, 解释性变差(每一列不再清楚含义).</p>
</li>
<li><p>标签编码.</p>
<p>标签编码是我个人比较喜欢的一种编码方式. 其核心思想是, 结合标签的信息来对类别特征进行编码, 例如对于特征的每一类, 编码为该类下标签的均值.</p>
<p>但此方法由于直接利用到了标签信息, 所以相比前面的方法, 容易引起过拟合. 为了防止或者减轻过拟合, 有不少改良版本. 比如其中一种改良方法为, 编码值为先验值与标签均值的加权平均:</p>
<script type="math/tex; mode=display">
编码值_{类别i}=\alpha_{类别i} \times 全样本标签均值+(1-\alpha_{类别i})\times 标签均值_{类别i} \\[7mm]
\alpha_{类别i}=1-{\rm exp}\lgroup\beta\times\frac{样本量_{类别i}}{全体样本量}\rgroup</script><p>当某个类别占据的样本量较多时, 给予较多的信任, 主要由标签均值进行编码; 而当某类别占据样本量较少时, 其编码值更多由先验值决定.</p>
<p>除了上面的方法, 还有一些方法, 比如使用交叉验证的方式, 把原数据集分成$N$份, 每次在$N-1$份上得到编码值, 将其赋予到剩下的那一份上.</p>
<p>之所以比较喜欢标签编码, 是因为一来只要注意一下过拟合的问题, 多数情况下能够让模型有不错的表现. 同时相比独热编码, 不会出现特征维度爆炸的情况.</p>
</li>
</ul>
<h2 id="连续特征分箱"><a href="#连续特征分箱" class="headerlink" title="连续特征分箱"></a>连续特征分箱</h2><p>为啥要对连续特征进行分箱呢? 对于类别特征的编码是必要的, 而连续特征的分箱在一些时候也是有用的.</p>
<p>首先列举一些常用的分箱方法:</p>
<ul>
<li><p>等距/等频分箱.</p>
<p>对于等距一般不太推荐. 等频分箱根据实际情况, 设置合适的分箱数.</p>
</li>
<li><p>决策树分箱.</p>
<p>借助决策树的叶子节点的划分, 来进行分箱.</p>
</li>
<li><p>WOE.</p>
<p>WOE这一套分箱与编码的方法, 算是分箱中的佼佼者了. 在线性模型中用得比较多.</p>
</li>
</ul>
<p>然后说一下连续特征分箱的一些用途:</p>
<ul>
<li><p>提升特征稳定性.</p>
<p>分箱某种程度上, 会损失特征的一些信息, 但是却可以提高稳定性, 让模型现在学到的规则, 将来也一样适用.</p>
</li>
<li><p>将非线性转变为线性.</p>
<p>比如在使用线性模型时, 如果特征与标签之间不是线性相关的, 而是比如U型的关系, 那么经过分箱编码(如WOE)后, 可以与标签拥有线性相关性, 可以很好地学习.</p>
</li>
<li><p>免除异常值影响.</p>
<p>经过分箱, 异常值同样也会被分到某一箱中, 与该箱中其它样本无异, 相当于顺便做了异常值处理.</p>
</li>
</ul>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>由于重点是讲结构化数据的范畴, 所以这里不做过多讲解. 其核心思想是把原本的非结构化数据, 或者稀疏的数据转换成相对稠密的结构化数据.</p>
<p>比如对于文本, 图片音频等数据, 可以利用深度学习的方法, 采用有监督或者无监督的方式, 将原始数据表示为结构化的向量数据. 对于稀疏的数据, 同样使用一些方法如PCA, 或者自编码器等进行降维.</p>
<p>这里再说一下为啥稀疏数据要特殊对待. 因为一般来说稀疏数据, 往往伴随着高纬度特征(如上万维), 此时无论是一些模型, 还是对计算资源来说, 都是一项困难的事情. 此时要么考虑将其降维后再用模型进行学习, 要么直接使用一些能够应对稀疏数据的模型, 如线性模型.</p>
<h2 id="特征衍生"><a href="#特征衍生" class="headerlink" title="特征衍生"></a>特征衍生</h2><p>说实话, 特征衍生是玄学(≧∇≦)ﾉ</p>
<p>为啥说是玄学呢? 因为你不知道你衍生的特征, 对于模型是否有提升…</p>
<p>甚至有时候, 可能还有神奇的负提升o(TヘTo)</p>
<p>所以应该怎么办呢? 我认为一个比较好的做法是, 先尽量地进行特征衍生, 然后再依赖各种特征选择的方法, 来去其糟泊, 留其精华.</p>
<p>总体来说, 特征衍生可以分为两种方法:</p>
<ul>
<li><p>按业务理解进行衍生.</p>
<p>即首先了解各个字段的含义, 然后根据自己的理解与猜想, 去构建新的特征. 具体说来有如下:</p>
<ul>
<li>特征直接相互组合(如两两相乘, 两两相除等), 这样得到的特征有时候可以与目标变量之前存在更强的相关性.</li>
<li>构建统计量特征. 比如一些时候, 一条样本对应多条历史数据, 那么根据这些历史数据, 就可以设定不同的时间窗口, 使用不同的统计量得到不同的特征.</li>
</ul>
</li>
<li><p>暴力衍生特征.</p>
<p>暴力衍生特征的思想是, 既然我不知道什么样的衍生特征可能对模型有提升, 那我把所有情况下的衍生特征都构建出来不就行了吗(&gt;▽&lt;)</p>
<p>此外, 在一些特殊情况下, 不知道原始特征的字段含义, 或者即使知道含义但没有领域知识时, 暴力衍生是一个不错的办法.</p>
<p>比如构建基于两两相乘的衍生特征, 若原始特征为10个, 则对应的衍生特征为45个. 当特征很多的时候, 需要谨慎使用, 不然可能会内存不够(≧∇≦)ﾉ</p>
<p>在原始特征比较多, 例如上千个的时候, 一种折中的暴力衍生策略, 是先利用一些方式进行评估(如树模型的特征重要性), 然后根据评估结果, 将重要的部分特征进行衍生.</p>
<p>除了上面按业务理解进行衍生的形式, 还可以对类别特征进行组合/拆分; 基于某特征做分组, 计算其它特征在分组上的统计值(如均值)等.</p>
</li>
</ul>
<p>此外, 还有根据决策树, 符号学习等方法来寻找高阶(两个以上)组合特征的方法, 也可以进行尝试.</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们不能迷信特征工程, 企图利用各种花里胡哨的操作, 在一份辣鸡数据集上玩出花.</p>
<p>但我们要重视特征工程, 掌握一些基本的处理方式, 应对各种情况.</p>
<p>哦还有一点, 特征工程有时候是需要想象力的, 需要一些脑洞, 一些天马行空.</p>
<p>尝试, 验证, 尝试, 验证…正所谓实践出真知. 而在不停的实践中, 也要注意经验的积累, 使得在以后的实践中, 能够尽量减少不必要的尝试, 能用更少的时间, 做出更好的模型.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-数据清洗</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</url>
    <content><![CDATA[<p>关于数据清洗, 若拿到手的是已经整理好的结构化数据, 是非常轻松且简单的; 而若是拿到了比较原始的, 杂乱的数据, 那可能要在这一步耗费不少时间. 总体来说, 数据清洗是简单的, 可能繁琐的, 但也是很重要的一步.</p>
<a id="more"></a>
<h1 id="整理数据"><a href="#整理数据" class="headerlink" title="整理数据"></a>整理数据</h1><p>这里主要以平时用到的最多的表格型数据来说, 使用<code>Pytohn</code>的<code>pandas</code>包, 可以进行有效地处理.</p>
<p>若原文件时<code>excel</code>或者<code>csv</code>等格式, 则直接读取即可; 若原文件是<code>json</code>这样的格式或类似的格式, 一般是先将其转变为标准的形式, 然后利用如<code>pd.read_json(string)</code>的方式进行读取.</p>
<p>我们要明确, 一份数据中, 哪些是<strong>ID相关特征</strong>, 这些特征可用于多份数据之间的关联, 但对于分析以及建模来说, 可能没有额外帮助. 同时, 还有一些<strong>基础信息特征</strong>, 如日期等, 这些可以用来分析数据, 也可以考虑参与建模. 这两大类的特征, 一般来说有一个特点, 即100%的覆盖率, 没有缺失.</p>
<p>除开以上两类特征, 总体来说还剩下<strong>标签列</strong>和<strong>建模特征</strong>. 标签列不必多说, 对于监督学习来说是必要的. 对于建模特征, 如果按数据类型来进行一些细分的话, 大致可以包含以下一些形式:</p>
<ul>
<li>无序类别特征.</li>
<li>有序类别特征.</li>
<li>离散数值特征.</li>
<li>连续数值特征.</li>
<li>日期时间特征.</li>
</ul>
<p>对于类别特征, 原始数据大概率是<code>object</code>或者<code>string</code>形式, 考虑到后续入模需要数值型特征, 一种简单的方式, 就是将类别型特征直接进行数值编码, 从字符串映射到整数. 但是这种方式, 除非该类别特征属性的数量很少(比如5个以下), 否则不太适合直接入模. 关于类别特征编码, 我准备在<strong>特征工程</strong>篇章详细叙述.</p>
<p>对于日期时间类的特征, 在整理数据时, 将其整理成统一的格式即可(如<code>%Y-%m-%D %H:%M:%S</code>), 可用于后续分析之用. 若要作为入模特征, 需要考虑<code>特征抽取</code>的方式, 同样也会在<strong>特征工程</strong>篇章说明.</p>
<p>此外, 对于<strong>缺失值</strong>的处理也是非常重要的. 不同的原始数据, 对缺失值的表示是不一样的. 常见的缺失值表示方式可能有:</p>
<ul>
<li>空值.</li>
<li>null, NaN等字符串.</li>
<li>-9999999等特殊数值.</li>
</ul>
<p>同时有些时候一份数据是由多份数据拼接而成, 其中可能有多种缺失值的表示, 要留意一下. 处理的方式, 一般来说统一转化成<code>pandas</code>默认的缺失形式就行.</p>
<h1 id="了解数据"><a href="#了解数据" class="headerlink" title="了解数据"></a>了解数据</h1><p>虽然有别于专门的数据分析, 最终目的是为了建模, 但是早期的对数据尽可能地了解, 是对后续的一些工作有帮助的, 并且可能指导后续的一些尝试和方向, 这个过程一般也可以被称为<strong>EDA</strong>(Exploratory Data Analysis).</p>
<p>我们可以查看以及了解的东西有哪些呢?</p>
<ul>
<li><p>数据集有多少样本, 多少列.</p>
</li>
<li><p>样本集随时间的分布, 即各个时间段的样本数量.</p>
</li>
<li><p>特征中有多少类别特征, 数值特征, 日期…</p>
</li>
<li><p>特征是否具有可解释性(知道特征含义), 是刻画什么维度的数据.</p>
</li>
<li><p>各特征的覆盖率.</p>
<script type="math/tex; mode=display">
覆盖率=1-缺失率</script></li>
<li><p>标签列的分布(如均值).</p>
</li>
<li><p>标签列在各个组群(如类别特征)上的分布.</p>
</li>
<li><p>……</p>
</li>
</ul>
<p>这里推荐一个可以辅助进行EDA的包<code>Pandas Profiling</code>, 其GitHub地址<a href="https://github.com/pandas-profiling/pandas-profiling" target="_blank" rel="noopener">在这里</a>. 这个包可以用一两行命令, 展示出数据集中各个特征的多种统计信息, 也包括特征之间的相关性, 使用后会生成一个html文档. 建议没有使用过的同学尝试一下~</p>
<p>通过对数据进行EDA, 可以了解手里这份数据整体的内容, 可以据此来制定一些尝试. </p>
<p>比如发现数据随时间分布有较长的跨度, 可以样本按时间切分, 查看前后样本的一些差异. </p>
<p>比如发现部分特征覆盖率极低(低于10%), 可以考虑在建模前, 直接先删除这部分特征. </p>
<p>比如根据业务理解, 认为某些特征的组合(如两两相除)可以表示更强的信息, 则可据此衍生特征. </p>
<p>比如发现标签列分布不均匀(如1多0少), 此时可以考虑对0进行上采样, 或者增加样本权重等方式</p>
<p>……</p>
<p>总之, 数据清洗这一步, 慢慢做就好了, 将原始数据进行统一化的整理, 并进行查看与分析, 为后续建模做一个良好的开端.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-样本设计</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E6%A0%B7%E6%9C%AC%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>一般在一些数据挖掘比赛中, 比赛的样本, 样本的特征, 标签啥的, 都是比赛方提供的. 作为参赛者, 大部分时候不需要准备额外的数据, 充分利用给定的数据就行.</p>
<p>但是在实际生产环境中, 通常是由自己, 或者一个小团体, 针对某个问题来做样本设计. 那么什么是<strong>样本设计</strong>呢, 其实就是反过来, <strong>设计样本</strong>o(*≧▽≦)ツ</p>
<a id="more"></a>
<p>咳…认真点, 比如我们想通过数据, 针对某个问题进行建模, 来实现对业务的优化, 大致会有以下几种情况:</p>
<ul>
<li><p>对于每个样本(比如人), 在这个问题上, 采集哪些维度的信息(特征), 可能是有帮助的?</p>
</li>
<li><p>可以采集大量的数据进行建模, 但是每一份样本的采集是有成本的.</p>
</li>
<li><p>样本集(比如用户客群), 包含了一些群组, 但是这些群组并不均衡, 也不稳定, 应该怎么做?</p>
</li>
<li>若采用有监督算法, 标签如何定义, 可以更好地解决问题呢?</li>
</ul>
<p>针对以上一些问题, 下面逐一进行阐述.</p>
<h1 id="特征维度"><a href="#特征维度" class="headerlink" title="特征维度"></a>特征维度</h1><p>这里先举一个比较扯栗子吧, 比如我想建模预测股市大盘的涨跌, 然后通过观察历史数据, 我发现股市的涨跌尽然和小明的考试成绩呈现很高的正相关, 皮尔森相关系数高达0.6. 这好像是一个不错的特征, 可以帮助模型预测股市的变化.</p>
<p>但是真的是这样吗? 明显不是呀ヽ(✿ﾟ▽ﾟ)ノ</p>
<p>第一, 股市大盘长期来看, 是持续性增长的, 小明也许以前比较贪玩, 后来慢慢好好学习, 考试分数也上去了. 第二, 小明下次考试缺考, 股市是不是得熔断? 小明毕业了, 没有考试了, 股市是不是得关门?</p>
<p>对没错, 就是<strong>相关性 VS 因果性</strong>. 通常我们希望用于建模的特征, 是与问题本身有一定的因果性的, 这样才能够具有一定的<strong>解释性</strong>, 以及更好的<strong>稳定性</strong>.</p>
<p>通常, 一个特征和我们的问题(或者说标签y), 相关性是很好计算的, 几行代码, 皮尔森, 斯皮尔曼, 肯德尔, 互信息, 方差检验…线性与非线性的相关性都能计算.</p>
<p>因果性怎么计算呢? 无法计算, 只能靠人的一些经验进行判断. 一个特征与问题标签有因果性, 也应该有相关性; 但若发现有相关性, 能说明有因果性吗?</p>
<p>所以, 我们想优先找有<strong>因果性</strong>的特征, 但由于因果性难以判断, 退而求其次, 寻找有<strong>相关性</strong>的特征. 而最终判断是否保留某个特征, 可以结合<strong>特征选择</strong>的手段, <strong>模型表现</strong>等来决策.</p>
<h1 id="样本维度"><a href="#样本维度" class="headerlink" title="样本维度"></a>样本维度</h1><p>很多时候, 数据并不是免费资源, 有很多依靠卖数据而存在的公司. 当作为甲方, 向数据公司(乙方)索求数据(特征)时, 是要考虑成本的.</p>
<p>即要保证足量的样本, 来使模型学习得更好, 又要缩减成本. 对此, 我们需要考虑我们全部可以用于建模的样本有哪些, 然后在这些样本中进行<strong>选择</strong>, 使用其中部分样本来购买数据进行建模.</p>
<p>那么, 如何进行选择呢?</p>
<p>首先, 可以考虑<strong>时间维度</strong>. 在一个随时间会发生变化的场景中, 如果我们想观察一些事物随时间的变化, 最好的办法就是把时间线拉长. 同时, 我们是在<strong>未来</strong>使用我们的模型, <strong>近期</strong>的样本理论上和未来可能有更相似, 因此要尽可能使用近期的样本.</p>
<p>时间维度上, 就成了<strong>长期 VS 近期</strong>, 我们要找到一种平衡, 使得我们的样本同时满足以下两点:</p>
<ul>
<li>较长的时间跨度.</li>
<li>包含大量近期的样本.</li>
</ul>
<p>然后, 如果还想进一步对样本进行缩减, 可以对样本进行<strong>随机抽样</strong>. 随机抽样的方式, 通常有两种:</p>
<ul>
<li>完全随机抽样.</li>
<li>分层随机抽样.</li>
</ul>
<p>对于<strong>完全随机抽样</strong>, 不必多说. 对于<strong>分层随机抽样</strong>, 比如我们要做的是二分类模型, 已有0, 1标签, 并且假设正负样本比例不均衡, 正样本远少于负样本. 那么采用分层随机抽样时, 即可以使正负样本在抽样后仍维持原有比例, 也可以分别对正负样本进行不同程度的抽样. 如保留所有正样本, 仅仅对负样本下采样.</p>
<h1 id="组群平衡"><a href="#组群平衡" class="headerlink" title="组群平衡"></a>组群平衡</h1><p>这里仍然举个栗子╰(<em>°▽°</em>)╯</p>
<p>假如我们有一帮用户, 他们组成了我们的样本集. 而用户中存在一些我们人为划分的组群, 如用户是从哪看到我们的广告, 而进行注册的, 可以称之为注册渠道.</p>
<p>可能的注册渠道有很多, 不同注册渠道的用户可能有不同的性质和表现, 一般会进行对应的分析. 那我们能够使用<code>注册渠道</code>这个特征参与建模吗?</p>
<p>如果使用<code>注册渠道</code>参与建模, 可能会出现如下两个问题:</p>
<ul>
<li>高基数的类别特征, 容易导致过拟合.</li>
<li>这样的特征随时间可能并不稳定.</li>
</ul>
<p>首先说第一点, 对于高基数的类别特征, 如果不进行比较细致的人工处理, 直接使用一些类别特征的编码方法, 相对其它特征类型, 是容易出现过拟合的. 即这个特征可能在训练集上是一个强特征, 但实际上特征本身并没有这么强.</p>
<p>然后是第二点, 这是更关键的一点, 即特征随时间变化太大. 比如有的渠道现在有, 未来也许就关闭了, 有的渠道现在没有, 未来加入了. 对于这种情况模型基于这个特征学到的东西, 就没用了(。・・)ノ</p>
<p>那么不把这样的特征参与建模, 就完全没用了吗? 是有用的嗷ヾ(´･ω･｀)ﾉ</p>
<p>如前所述, 组群若随时间发生变化, 那么我用现在已有的样本, 去预测未来的样本, 就需要考虑组群之间的<strong>平衡</strong>问题. 比如<code>组群A</code>, 在现有的全部样本中, 占比超过50%, 而<code>组群B</code>现在占比较少, 仅有10%. 同时通过业务了解到, 未来<code>组群A</code>占比会减少, <code>组群B</code>占比会增加. </p>
<p>问: 此时因该怎么做?</p>
<p>答: 将现有样本中的<code>组群A</code>进行下采样, 如减少一半的样本, 使模型在学习过程中, 不会过度倾向<code>组群A</code>的特点.</p>
<h1 id="标签设计"><a href="#标签设计" class="headerlink" title="标签设计"></a>标签设计</h1><p>尽管有不少无监督学习算法, 但不少情况下, 还是有监督学习算法更好用一些. 而对于有监督学习算法, 用于预测的<strong>标签</strong>是必不可少的.</p>
<p>一些场景下, 标签的获取, 以及定义都比较简单和直接; 而另一些场景下, 标签的获取可能是一件<strong>高成本</strong>的事情, 同时需要一定的<strong>表现期</strong>, 没有现成的<strong>定义方式</strong>.</p>
<p>对于标签成本, 表现期长短, 一般来说是不太好控制的. 而对于标签的定义, 是可控并直接影响模型表现的关键因素.</p>
<p>首先, 要明确应该以什么指标来作为标签, 能够使得模型进行学习后, 预测的结果更好地解决我们的问题. 一些时候问题比较直接, 可能就是想预测用户的年龄, 收入, 性别; 而一些时候问题相对模糊, 比如想衡量一个用户的信用, 对某件商品的兴趣. 对于后者, 一般可以通过用户的一些<strong>行为表现</strong>, 来进行表征, 如利用是否按期还款来作为信用的标签, 按是否购买某商品来作为兴趣的标签.</p>
<p>然后, 进一步, 对于某个问题, 我们是把标签保持为原有的连续值, 形成一个回归问题; 还是将其按某阈值划分0, 1标签, 形成一个二分类排序问题等. 这要根据实际的情况进行决定, 如果原始标签是是连续值, 并且其值是相对稳定, 与观察时间窗口无关, 比如某个用户对电影的评分, 是固定值, 此时可直接采用连续标签. 而一些情况下, 标签值是会随着观察窗口变化的, 比如信用卡还款中的逾期时间, <code>用户A</code>今天累计逾期了30天, 到明天可能就31天了, <code>用户B</code>今天累计逾期20天, 是否说明<code>用户B</code>比<code>用户A</code>信用好呢, 其实是因为<code>用户B</code>的<code>最后还款日期</code>相比<code>用户A</code>迟10天, 而且这都算严重逾期, 因此可以设定一个阈值如10天, 将逾期超10天的都定义为正样本.</p>
<p>此外, 在进行标签设计时, 按某阈值划分正负样本, 可能处于阈值附近的样本, 会使得模型学起来较为困难, 从而降低模型效果. 这时候可以尝试将处于阈值附近的样本过滤掉, 不参与建模. 当然, 这都是要看实际表现来进行决策的, 有时候增加模型学习的难度, 模型或许可以获得更好的表现(๑•̀ㅂ•́)و✧</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据建模-解决问题流程</title>
    <url>/2020/06/06/%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1-%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>生活中, 工作中我们会遇到很多问题, 其中大部分问题是我们以前遇到过的, 我们可以凭借经验进行解决. 然而当我们遇到一个之前没有遇到过的问题时, 我们应该怎么做呢?</p>
<p>作为一个菜鸟小白, 其实自知没有水平写关于这个讨论的文章, 毕竟现在自己遇到问题的时候几乎都是去搬别人的方法. 我个人认为这项能力, 即<code>分析问题 &amp; 转化问题</code>的能力, 是非常重要的, 这也是为什么把这个话题放到了<code>数据建模</code>的第一篇.</p>
<a id="more"></a>
<p>如果, 能够在一开始遇到问题的时候, 明确想要达到的目的, 然后用合理的思路, 将其转化为数学语言, 或者定量的描述, 或者优化目标, 后续的工作大概率是有意义的. 反之, 一开始就错误理解了问题, 或者没有很好地描述问题, 则后续很可能做无用功.</p>
<h1 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h1><p>一开始遇到一个问题的时候, 最好先按某种顺序进行梳理, 如从前往后, 或者从整体到局部.</p>
<p>重点在于明确我们要做什么, 可利用的资源有哪些, 难点可能在哪.</p>
<p>这里我就胡扯一个栗子吧, 真的是胡扯ヾ(´･ω･｀)ﾉ</p>
<p>比如我想财富自由, 或者目标放低点, 我想躺着赚钱, 请问有没有什么法子?</p>
<p>有的, 只要买对了股票, 就可以躺着赚钱(。・・)ノ</p>
<p>那怎么买股票呢? 这就是一个模糊的问题, 我们先把模糊的问题具体化.</p>
<p>首先我能用来买股票的钱是有限的.</p>
<p>其次股市有风险, 可能躺着赚钱, 也可能被别人躺着赚钱.</p>
<p>对于散户来说, 一般牛市的时候, 容易赚钱, 熊市的时候, 容易亏钱.</p>
<p>别把鸡蛋都放在一个篮子里.</p>
<p>收益越大, 风险越大.</p>
<p>…</p>
<p>大概这些就是我的一些经验信息, 结合这些信息, 我想做的事情就是:</p>
<blockquote>
<p>用有限的钱, 去买一种股票组合, 使得期望收益最大化.</p>
</blockquote>
<h1 id="转化问题"><a href="#转化问题" class="headerlink" title="转化问题"></a>转化问题</h1><p>有了这个明确的目标, 那下一步考虑是否可以转化成一些定量描述, 或者数学模型.</p>
<p>这一看应该是可以的, <code>最大化</code>就是一个优化问题, <code>有限的钱</code>就是限制条件.</p>
<p>假设咱们有收集数据的能力, 收集到了几种股票的历史长期表现, 再经过一些简单的算法, 可以定量得到每个股票的<code>收益</code>与<code>风险</code>.</p>
<p>现在我们要最大化如下目标函数:</p>
<script type="math/tex; mode=display">
\max(\sum_i资金_i(收益_i-风险_i))\\[7mm]
s.t.\quad 资金_i\ge0\quad i\in I\\[7mm]
\sum_i资金_i=总资金</script><p>相信聪明的同学已经发现了, 这个优化问题根本不需要所谓的优化o(〃’▽’〃)o</p>
<p>只要把钱全部拿来买<code>收益-风险</code>最大的这一个股票, 就行了.</p>
<p>啊…完美解决问题, （＾∀＾●）ﾉｼ</p>
<p>但是有些不太对劲, 咱们其实并没有完全用到上一节中咱们梳理得到的经验或者知识:</p>
<blockquote>
<p>别把鸡蛋放在一个篮子里.</p>
<p>牛市容易赚钱, 熊市容易亏钱.</p>
</blockquote>
<p>能不能把这些经验或者知识也加入到模型中呢? 试试吧(o゜▽゜)o☆</p>
<p>对于第一条经验, 其实是让我们尽量分散投资, 那我们可以在目标函数中添加一个惩罚机制, 如果越集中购买, 就惩罚越重, <code>L2</code>正则在这里可以达到这样的效果.</p>
<p>对于第二条经验, 可以通过根据当前大盘的局势判断, 来选择<code>总资金</code>的量, 牛市的时候用更多的资金投资, 熊市的时候用少量的资金投资.</p>
<p>总结起来, 现在的目标函数变成了下面这个样子:</p>
<script type="math/tex; mode=display">
\max(\sum_i资金_i(收益_i-风险_i))-\lambda\sum_i资金_i^2 \\[7mm]
s.t.\quad 资金_i\ge0\quad i\in I\\[7mm]
\sum_i资金_i=总资金\times\beta</script><p>其中的$\lambda$是<code>L2</code>正则参数, 越大, 则资金越分散, 越小则越集中.</p>
<p>$\beta$是大盘局势, 牛市时值大, 熊市时值小.</p>
<p>通过调节以上两个超参数, 可以控制<code>购买总金额</code>以及<code>资金分散程度</code>.</p>
<h1 id="寻找相似问题"><a href="#寻找相似问题" class="headerlink" title="寻找相似问题"></a>寻找相似问题</h1><p>现在我们有了具体的数学优化函数, 那如合进一步去解决这个问题呢?</p>
<p>我认为最好的方法, 仍然是首先借鉴前人的智慧. 如果存在一个类似的问题, 前人已经解决过, 那这时候我们可以借鉴, 甚至直接照搬其中的做法. 这样可以达到事半功倍的效果（。＾▽＾）</p>
<p>那有的同学就要问了, 我怎么知道哪里有相似的问题呢, 别人的办法我看不懂怎么办, 看懂了也不会做怎么办</p>
<p>摸摸头 (。・・)ノ</p>
<p>确实是这样的, 所以这就涉及到一个平时积累的过程了. </p>
<p>一些问题的解法, 或许我们平时生活工作中是没有用到过的, 但是出于好奇心感兴趣, 或者就是认为以后某一天这个会派上用场, 而去学习理解, 并归纳消化.</p>
<p>如此, 有了一定的知识库作为支撑, 才能够看得更远(ง •_•)ง</p>
<p>还是以上一个小结瞎举的栗子来说, 这个问题是一个很典型的<code>带约束的优化问题</code>, 并且符合<code>KKT</code>条件, 那么就可以利用<code>拉格朗日乘子法</code>转化成<code>无约束优化问题</code>, 进而运用<code>无约束优化问题</code>的一些方法, 如<code>SGD</code>进行解决.</p>
<p>具体来做的话, 当然可以自己写代码, 也可以调用已有的轮子. 比如<code>scipy</code>中有关于<code>带约束的优化问题</code>的方法, 查阅对应API, 几行代码就可以得到问题的解♪(^∇^*)</p>
<h1 id="更多尝试-amp-优化"><a href="#更多尝试-amp-优化" class="headerlink" title="更多尝试 &amp; 优化"></a>更多尝试 &amp; 优化</h1><p>通常, 我们在处理一个没怎么做过的问题时, 优先的方法, 一般不是一些特别复杂的办法, 而是一些尽可能简单而有效的方法.</p>
<p>当我们相对简单的方法奏效后, 后续可以根据对结果的分析, 提出更多的尝试与优化方案, 进行迭代升级.</p>
<p>比如, 当我们面对一个问题, 数据很少, 甚至没有数据的时候, 就不要生搬硬套一些基于大量数据的机器学习模型. </p>
<p>结合对数据的分析, 和个人的经验以及业务理解, 提出一些定量的建议或者规则, 就是不错的.</p>
<p>再比如, 经过一定的积累, 在有了一定数据的基础上, 可以尝试用一些机器学习算法.</p>
<p>但不同机器学习模型, 对于数据的维度与量的要求是不一样的. 假如数据量少, 或者数据非常稀疏, 或许线性模型是一个很好的选择; 假如数据量大, 同时其中蕴含着很多非线性关系, 可能神经网络这一类模型更加合适.</p>
<p>总之, 在面对一个新的难题时, 应该先从简单的解决方法入手, 并将其结果表现作为基准, 然后再逐步地尝试优化, 最终得到一个更好的结果.</p>
]]></content>
      <categories>
        <category>传统机器学习</category>
      </categories>
      <tags>
        <tag>传统机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>二分网络的部分方法与应用</title>
    <url>/2020/05/24/%E5%9B%BE%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86%E7%BD%91%E7%BB%9C%E7%9A%84%E9%83%A8%E5%88%86%E6%96%B9%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>除了社交网络这一常见的网络类型外, 二分网络也是一种经常出现的网络. 所谓二分网络, 即节点被分为了两大类, 其中的边总是存在于一类节点到另一类节点之间. 例如用户与商品的二分网络, 投资者与股票的二分网络等.</p>
<p>特别是用户与商品构成的商品偏好网络, 有着很大的研究价值, 对于构建推荐系统来说必不可少的. 推荐系统一般考虑的是, 想什么样的用户推荐什么样的商品, 会使得用户喜欢, 点击率更高一些. 比如豆瓣电影, 每部电影下, 有用户对该电影的评分, 评分的由高到低, 表示了用户对其的喜欢与不喜欢.</p>
<a id="more"></a>
<p>这样, 用户对产品的评分, 或者是否购买某产品, 构成了一个二分网络. 仍然以豆瓣电影为例, 一个简单的二分网络如下图:</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>对于该网络, 以矩阵来表示为:</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p>其中每个格点的值, 表示该用户对该电影的评分. 接下来, 就以推荐这一应用为目的, 来介绍与二分网络相关的一些方法.</p>
<h1 id="基于内容"><a href="#基于内容" class="headerlink" title="基于内容"></a>基于内容</h1><p>其实基于内容进行推荐的方法, 并没有充分利用到二分网络的特点, 在这里进行简要介绍.</p>
<p>基于内容的推荐, 核心的步骤为3个:</p>
<ul>
<li>构建商品的特征向量.</li>
<li>构建用户的特征向量.</li>
<li>计算商品-用户的相似性.</li>
</ul>
<p>对于特征向量的构造, 没有固定的方法, 根据实际情况可以很简单, 也可以很复杂.</p>
<p>对于相似性评估指标, 一般来说余弦相似度就是不错的相似度评估指标(遇事不决选余弦).</p>
<p>基于内容进行推荐有着如下的优缺点:</p>
<ul>
<li>优点:<ul>
<li>可以应用到一个新商品上.</li>
<li>不会倾向于流行商品.</li>
<li>如果特征向量构建解释性强, 则最终推荐结果解释性强.</li>
</ul>
</li>
<li>缺点:<ul>
<li>有时候构建有效特征比较困难.</li>
<li>推荐范围可能比较狭窄.</li>
<li>对新用户无效.</li>
<li>没有充分利用到二分网络的整体结构信息.</li>
</ul>
</li>
</ul>
<h1 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h1><p>在推荐领域, 协同过滤是一种有效的方法, 分为基于用户与基于物品的协同过滤. 之所以叫做协同过滤, 指的是利用其他的用户或者商品一起, 过滤出值得推荐的商品.</p>
<ul>
<li><p>基于用户的协同过滤.</p>
<p>基于用户进行协同过滤, 基本思想是首先根据二分网络, 找到与目标用户相似的那些用户, 然后再根据相似的用户会喜欢哪些商品, 推荐给目标用户.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
<li><p>基于物品的协同过滤.</p>
<p>基于物品进行协同过滤, 基本思想是首先根据二分网络, 找到与目标用户所喜欢的商品相似的商品, 然后将这些商品推荐给用户.</p>
<p><img src="fig_3.png" alt="fig_3"></p>
</li>
<li><p>相似性计算.</p>
<p>用户与物品的相似性计算是类似的, 将每个用户对每个物品的打分(没有记录则为空)整理为一个矩阵. 每个用户的向量可以使用对物品的打分构成; 每个物品的向量可以用各个用户对它的打分构成. 向量中的空值可以用0来填充.</p>
<p>得到向量后, 相似性就可以利用余弦相似度$Cosine_Similarity=\frac{AB}{|A||B|}$进行计算了.</p>
<p>然而这里存在的一个问题是, 对于用户没有评分的物品, 其实有两种可能:</p>
<ul>
<li><p>用户还没接触过.</p>
</li>
<li><p>用户提前知道自己不喜欢该物品, 不参与打分.</p>
</li>
</ul>
<p>因此直接使用0来进行填充, 是不太好的. 一种更好的方法是, 是计算出用户对每个物品打分的平均值, 再将用户对物品的打分基础上减去这个平均值.</p>
<p><img src="fig_4.png" alt="fig_4"></p>
<p>进行这样的处理后, 0可以视作用户对物品的一个平均评分, 比之前的情形要好一些.</p>
<p>使用协同过滤, 有如下优缺点:</p>
</li>
<li><p>优点:</p>
<ul>
<li>不需要去精心设计物品与用户的特征向量, 只要有二分网络即可.</li>
<li>在有一定数据积累的情况下, 效果一般比基于内容的方法好.</li>
</ul>
</li>
<li><p>缺点:</p>
<ul>
<li>冷启动问题, 无论是对新用户, 还是新物品都难以进行推荐.</li>
<li>倾向推荐比较流行的商品.</li>
<li>如果二分网络对应的矩阵非常稀疏, 那么效果将会不好.</li>
</ul>
</li>
</ul>
<h1 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h1><p>我们知道对称方阵$A$, 可以用下式的特征分解表示:</p>
<script type="math/tex; mode=display">
A=W\Sigma W^{-1}</script><p>其中$W$为酉矩阵, $\Sigma$为对角矩阵, 对角值为特征值.</p>
<p>那么对于任意的距阵$A_{m\times n}$, 可以进行类似的分解吗? 也是可以的:</p>
<script type="math/tex; mode=display">
A=U\Sigma V^{T}</script><p>这样的分解称为奇异值分解(SVD), 其中, $U$是$m\times m$的酉矩阵, $V$是$n\times n$的酉矩阵, $\Sigma$是$m\times n$的矩阵, 除了主对角线外的元素都是0, 主对角线上的每个元素称为奇异值.</p>
<p>这里暂且不细说矩阵分解的具体过程, 可以采用解析的方式, 也可以采用迭代的方式.</p>
<p>当使用奇异值分解后, 在奇异值矩阵中也是按照从大到小排列, 而且奇异值的减小特别快, 在很多情况下, 前10%甚至1%的奇异值就占了全部奇异值之和的99%以上的比例. 也就是说, 可以用最大的$k$个奇异值和对应的左右奇异向量来近似描述矩阵:</p>
<script type="math/tex; mode=display">
A_{m\times n}\thickapprox U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}</script><p>如果在原矩阵中, 一些格点上存在缺失, 经过改动的矩阵分解方法, 可以在进行分解时只考虑非缺失部分. 当得到分解后的矩阵后, 可以对原本缺失的格点进行预测.</p>
<p>将其运用到二分网络, 商品推荐中, 如下图:</p>
<p><img src="fig_5.png" alt="fig_5"></p>
<p>可以对每个物品, 每个用户, 得到他们对应的$k$维特征向量, 要计算某个用户对某个物品的打分或者评价, 最简单的方法就是将两者的特征向量点乘.</p>
<p>对于SVD, 以及改进的方法SVD++, 在推荐系统中的效果是很好的, 相比协同过滤, 在面对稀疏数据的时候, 可以有更好的表现.</p>
<h1 id="信息推测"><a href="#信息推测" class="headerlink" title="信息推测"></a>信息推测</h1><p>在上面协同过滤的方法中, 可以知道某个用户与哪些用户更加相似, 那么利用这个相似度, 可以做更多的事情, 比如可以用于推测用户的信息.</p>
<p><img src="fig_6.png" alt="fig_6"></p>
<p>同样, 由矩阵分解得到的特征, 也能够用来做类似的事情, 例如利用特征与某用户信息的标签(如年龄), 构建模型进行预测, 通常会取得不错的效果.</p>
<h1 id="Surprise"><a href="#Surprise" class="headerlink" title="Surprise"></a>Surprise</h1><p><a href="https://github.com/NicolasHug/Surprise" target="_blank" rel="noopener">Surprise</a>是Python下一个推荐系统库.</p>
<p>使用方式与scikit包相似.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> surprise <span class="keyword">import</span> SVD</span><br><span class="line"><span class="keyword">from</span> surprise <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> surprise.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the movielens-100k dataset (download it if needed).</span></span><br><span class="line">data = Dataset.load_builtin(<span class="string">'ml-100k'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the famous SVD algorithm.</span></span><br><span class="line">algo = SVD()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run 5-fold cross-validation and print results.</span></span><br><span class="line">cross_validate(algo, data, measures=[<span class="string">'RMSE'</span>, <span class="string">'MAE'</span>], cv=<span class="number">5</span>, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Evaluating RMSE, MAE of algorithm SVD on 5 split(s).</span><br><span class="line"></span><br><span class="line">            Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std</span><br><span class="line">RMSE        0.9311  0.9370  0.9320  0.9317  0.9391  0.9342  0.0032</span><br><span class="line">MAE         0.7350  0.7375  0.7341  0.7342  0.7375  0.7357  0.0015</span><br><span class="line">Fit time    6.53    7.11    7.23    7.15    3.99    6.40    1.23</span><br><span class="line">Test time   0.26    0.26    0.25    0.15    0.13    0.21    0.06</span><br></pre></td></tr></table></figure>
<p>其中包含了一些主流的应用于二分网络的推荐算法, 包括协同过滤, SVD等.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a href="http://grouplens.org/datasets/movielens/100k" target="_blank" rel="noopener">Movielens 100k</a></th>
<th>RMSE</th>
<th>MAE</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD" target="_blank" rel="noopener">SVD</a></td>
<td>0.934</td>
<td>0.737</td>
<td>0:00:11</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp" target="_blank" rel="noopener">SVD++</a></td>
<td>0.92</td>
<td>0.722</td>
<td>0:09:03</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF" target="_blank" rel="noopener">NMF</a></td>
<td>0.963</td>
<td>0.758</td>
<td>0:00:15</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/slope_one.html#surprise.prediction_algorithms.slope_one.SlopeOne" target="_blank" rel="noopener">Slope One</a></td>
<td>0.946</td>
<td>0.743</td>
<td>0:00:08</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBasic" target="_blank" rel="noopener">k-NN</a></td>
<td>0.98</td>
<td>0.774</td>
<td>0:00:10</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans" target="_blank" rel="noopener">Centered k-NN</a></td>
<td>0.951</td>
<td>0.749</td>
<td>0:00:10</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline" target="_blank" rel="noopener">k-NN Baseline</a></td>
<td>0.931</td>
<td>0.733</td>
<td>0:00:12</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/co_clustering.html#surprise.prediction_algorithms.co_clustering.CoClustering" target="_blank" rel="noopener">Co-Clustering</a></td>
<td>0.963</td>
<td>0.753</td>
<td>0:00:03</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly" target="_blank" rel="noopener">Baseline</a></td>
<td>0.944</td>
<td>0.748</td>
<td>0:00:01</td>
</tr>
<tr>
<td><a href="http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor" target="_blank" rel="noopener">Random</a></td>
<td>1.514</td>
<td>1.215</td>
<td>0:00:01</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>社交网络-组群发现</title>
    <url>/2020/05/23/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C-%E7%BB%84%E7%BE%A4%E5%8F%91%E7%8E%B0/</url>
    <content><![CDATA[<p>对于社交网络, 主要的研究与应用包括以下三个方面:</p>
<ul>
<li>信息推测.</li>
<li>组群发现.</li>
<li>消息传播.</li>
</ul>
<p>这里着重介绍一些组群发现的理论, 以及使用真实的数据进行试验.</p>
<a id="more"></a>
<h1 id="组群分类"><a href="#组群分类" class="headerlink" title="组群分类"></a>组群分类</h1><h2 id="显式组群-amp-隐式组群"><a href="#显式组群-amp-隐式组群" class="headerlink" title="显式组群 &amp; 隐式组群"></a>显式组群 &amp; 隐式组群</h2><p>在一般的社交网络中, 组群有显式与隐式之分.</p>
<p>比如在豆瓣网上的各种小组就属于<strong>显式群组</strong>.</p>
<p>而<strong>隐式群组</strong>, 是通过观察和分析节点之间的交互特征发现的群组. 比如通过一些人平时的通话记录, 来推断哪些人是朋友, 一起工作等.</p>
<p>这里所说的组群发现, 一般默认指的是隐式组群发现.</p>
<h2 id="可重叠组群-amp-不可重叠组群"><a href="#可重叠组群-amp-不可重叠组群" class="headerlink" title="可重叠组群 &amp; 不可重叠组群"></a>可重叠组群 &amp; 不可重叠组群</h2><p><img src="fig_0.png" alt="fig_0"></p>
<p>网络中的节点可以同时属于多个组群, 这样的组群称为<strong>可重叠组群</strong>.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p>网络中的每个节点只属于一个组群, 组群之间没有共同节点, 则称为<strong>不可重叠组群</strong>.</p>
<h1 id="组群发现的意义"><a href="#组群发现的意义" class="headerlink" title="组群发现的意义"></a>组群发现的意义</h1><ul>
<li><p>单个节点.</p>
<p>通过组群发现, 将节点划分到了不同的团体, 可以通过整个团体的信息, 来对单个节点进行推测, 更好地理解单个节点.</p>
</li>
<li><p>群体.</p>
<p>有时候, 一些事实很难甚至无法通过单个节点看出来, 但是却能够从整体的行为中挖掘得到. 对于划分得到的组群, 它们之间的区别与联系可以通过划分结果进行分析.</p>
</li>
</ul>
<h1 id="组群发现算法"><a href="#组群发现算法" class="headerlink" title="组群发现算法"></a>组群发现算法</h1><ul>
<li><p>核心思想.</p>
<ol>
<li>高内聚: 组群内部节点之间交互密集.</li>
<li>低耦合: 组群之间的交互稀疏.</li>
</ol>
</li>
<li><p>组群发现 vs 聚类.</p>
<ul>
<li>聚类: 各数据点之间没有交互(连边), 根据节点的属性(如人的性别, 年龄, 学历等)来进行划分.</li>
<li>组群发现: 节点之间有交互(连边), 根据其交互行为来进行划分.</li>
</ul>
</li>
<li><p>经典算法Louvain.</p>
<ul>
<li><p>模块度(Modularity).</p>
<p>用以衡量组分划分的好坏的一个非常重要的指标, 越大说明划分效果越好. 计算公式为:</p>
<script type="math/tex; mode=display">
Q=\sum_i(组群_i内部的连边之和比例-组群_i连边之和比例的平方)</script><p>对于无向图, 一般看做有向图处理, 即一条无向连边, 等价于两条有向连边.</p>
<p>一般Q值在0.3~0.7.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">C1</th>
<th style="text-align:center">C2</th>
<th style="text-align:center">C3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">C1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">C2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">6</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">C3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">8</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
Q=\frac{6+6+8}{2\times 12}-(\frac{6+1}{2\times 12})^2-(\frac{6+1}{2\times 12})^2-(\frac{8+1+1}{2\times 12})^2=0.49</script><ul>
<li><p>算法流程.</p>
<ol>
<li>将每个节点作为一个组群.</li>
<li>尝试将某个节点向邻近节点的组群合并, 计算合并后的模块度增量, 选择增加最大的组群进行移动. 若增量为负数, 则不移动. 遍历所有节点, 并循环处理, 当稳定时停止本次划分.</li>
<li>将现阶段组群看做一个节点, 组群内的边作为”新节点”的自环边, 同时边的权重为之前边的权重和. 再进行第二步.</li>
</ol>
<p>从算法可以看出, Louvain算法是一种层次型的组群发现, 可以得到一系列不同层次的组群以及对应的Q值. 最终可以选择最大的Q值划分, 最为最优划分.</p>
</li>
</ul>
<h1 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h1><p>对于Louvain算法, 在Github上有开源的包<a href="https://github.com/taynaud/python-louvain" target="_blank" rel="noopener">python-louvain</a>, 利用这个包来对网络进行组群划分.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> community  <span class="comment"># python-louvain 在调用时的名称为community</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_list_df = pd.read_csv(<span class="string">'network_links.csv'</span>)</span><br><span class="line">G = nx.from_pandas_edgelist(edge_list_df)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用best_partition将会得到最佳组群划分的一个字典, 其中key为节点, value为组群编号.</span></span><br><span class="line">node_group = community.best_partition(G)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 得到不同节点的组群编号列表.</span></span><br><span class="line">values = [node_group.get(node) <span class="keyword">for</span> node <span class="keyword">in</span> G.nodes()]</span><br><span class="line"><span class="comment">#对不同组群的节点进行不同着色.</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line">nx.draw(G, cmap=plt.cm.RdYlBu, node_color=values, with_labels=<span class="literal">True</span>, edge_color=<span class="string">'gray'</span>, node_size=<span class="number">200</span>, pos=nx.spring_layout(G, k=<span class="number">0.2</span>, iterations=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p><img src="fig_3.png" alt="fig_3"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>使用NetworkX分析网络</title>
    <url>/2020/05/23/%E5%9B%BE%E7%AE%97%E6%B3%95/%E4%BD%BF%E7%94%A8NetworkX%E5%88%86%E6%9E%90%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>NetworkX是一个Python包, 其中包含了丰富的与网络分析相关的方法, 对于中小型的网络来说, 用来进行分析是非常合适的.</p>
<p>这里将使用NetworkX包对一份网络数据进行构建, 绘图, 统计等, 以展示其基本功能.</p>
<a id="more"></a>
<h1 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">edge_list_df = pd.read_csv(<span class="string">'network_links.csv'</span>)</span><br><span class="line">edge_list_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>这是以边表形式保存的数据, 其中<code>source</code>和<code>target</code>表示节点对, <code>value</code>表示边权重, 这里暂不考虑.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建网络对象.</span></span><br><span class="line">G = nx.from_pandas_edgelist(edge_list_df)</span><br></pre></td></tr></table></figure>
<h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><p>对于NetworkX, 提供静态的可视化网络, 对于节点不是特别多的情况下, 使用可视化能够非常直观地了解网络.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>,</span><br><span class="line">        edge_color=<span class="string">'grey'</span>,</span><br><span class="line">        node_color=<span class="string">'blue'</span>,</span><br><span class="line">        node_size=<span class="number">10</span>,</span><br><span class="line">        pos=nx.spring_layout(G,k=<span class="number">0.2</span>,iterations=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<p><img src="fig_1.png" alt="fig_1"></p>
<h1 id="获取网络信息"><a href="#获取网络信息" class="headerlink" title="获取网络信息"></a>获取网络信息</h1><ul>
<li><p>各节点度数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.degree(G)</span><br></pre></td></tr></table></figure>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
<li><p>节点中心性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 度数占比.</span></span><br><span class="line">nx.degree_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中介中心性.</span></span><br><span class="line">nx.betweenness_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接近中心性.</span></span><br><span class="line">nx.closeness_centrality(G)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征向量中心性.</span></span><br><span class="line">nx.eigenvector_centrality(G)</span><br></pre></td></tr></table></figure>
</li>
<li><p>最大连通子图.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取出网络的最大子图</span></span><br><span class="line">Sub_G=G.subgraph(list(nx.connected_components(G))[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络平均聚类系数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算网络的聚类系数</span></span><br><span class="line">nx.average_clustering(G)</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络平均路径长度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算网络的平均路径</span></span><br><span class="line">nx.average_shortest_path_length(G)</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>网络生成模型</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>大多数时候, 我们可以去采集获取现实中真实的图结构的数据, 对其进行分析和研究.</p>
<p>而如果想通过一些方法, 来生成类似的数据, 可以怎么做呢? 如果一个相对简单的过程, 就能够生成与现实世界中非常相似的网络, 那么有如下好处:</p>
<ol>
<li>可以用生成的网络, 代替真实网络进行研究和实验.</li>
<li>可以更加清楚地了解真实世界网络的生成方式.</li>
</ol>
<a id="more"></a>
<h1 id="网络相似度衡量指标"><a href="#网络相似度衡量指标" class="headerlink" title="网络相似度衡量指标"></a>网络相似度衡量指标</h1><p>那么, 怎么知道生成的网络与真实世界网络之间是否相似呢? 需要一些衡量指标.</p>
<h2 id="节点度数分布"><a href="#节点度数分布" class="headerlink" title="节点度数分布"></a>节点度数分布</h2><p>首先是节点的度分布, 在现实世界中, 节点的度数分布通常服从幂律分布:</p>
<script type="math/tex; mode=display">
P_d=\alpha d^{-\beta}</script><p>其中$\alpha,\beta$均为参数, $d$为度数.</p>
<p>大多数节点的度数较小, 小部分节点的度数很大.</p>
<h2 id="聚类系数"><a href="#聚类系数" class="headerlink" title="聚类系数"></a>聚类系数</h2><p>以社交网络为例, A有两个朋友B和朋友C, 那么B和C本身可能也是朋友.</p>
<p>也就是说, 真实世界的网络一般具有不低的聚类系数.</p>
<h2 id="平均路径长度"><a href="#平均路径长度" class="headerlink" title="平均路径长度"></a>平均路径长度</h2><p>有一个著名的理论可能大家都听过, 叫做<strong>六度分隔</strong>理论, 或者也叫<strong>小世界</strong>理论. 大致就是说, 原本互不相识的人, 平均下来经过6个两两相识的人, 就能连接到一起.</p>
<p>而在真实的网络中, 平均路径长度可能比6还要小, 例如以前统计过, Facebook中用户的平均路径长度为4.7, YouTube为5.1.</p>
<h1 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h1><h2 id="随机网络"><a href="#随机网络" class="headerlink" title="随机网络"></a>随机网络</h2><p>随机网络模型, 指的是固定节点数, 对于所有可能存在的边, 以概率p决定是否出现.</p>
<p>在10个节点, 不同的p参数下, 随机网络的表现如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">概率p</th>
<th style="text-align:center">平均度数</th>
<th style="text-align:center">直径(最长距离)</th>
<th style="text-align:center">最大连通子图</th>
<th style="text-align:center">平均距离</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">0.055</td>
<td style="text-align:center">0.8</td>
<td style="text-align:center">2</td>
<td style="text-align:center">4</td>
<td style="text-align:center">1.5</td>
</tr>
<tr>
<td style="text-align:center">0.11</td>
<td style="text-align:center">1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">7</td>
<td style="text-align:center">2.66</td>
</tr>
<tr>
<td style="text-align:center">1.0</td>
<td style="text-align:center">9</td>
<td style="text-align:center">1</td>
<td style="text-align:center">10</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>随着p的增加, 平均度数到达1附近时, 网络会发生”相变”. 其最大连通子图的大小, 会突然变大, 几乎囊括所有节点; 同时, 网络的直径也会达到最大, 但后续又会降低.</p>
<p>可以总结下随机网络的性质:</p>
<ul>
<li>平均度数小于1.<ul>
<li>分散的簇.</li>
<li>直径小.</li>
<li>平均路径长度小.</li>
</ul>
</li>
<li>平均度数约等于1.<ul>
<li>最大连通子图的大小会迅速增大.</li>
<li>直径最大.</li>
<li>平均路径长度大.</li>
</ul>
</li>
<li>平均度数大于1.<ul>
<li>几乎所有节点都被连接.</li>
<li>直径减小.</li>
<li>平均路径长度减小.</li>
</ul>
</li>
</ul>
<p>最后随机网络与真实的网络有哪些差别呢?</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">随机网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">泊松分布</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较小</td>
</tr>
</tbody>
</table>
</div>
<p>也就是说, 如果用随机网络来模拟真实网络, 那么除了平均路径长度, 在度分布以及聚类系数上都不符合.</p>
<h2 id="小世界网络"><a href="#小世界网络" class="headerlink" title="小世界网络"></a>小世界网络</h2><p>小世界网络也被称为WS(Watts-Strogatz)网络, 是一种进阶的随机模型.</p>
<p>生成小世界网络的过程如下:</p>
<ol>
<li><p>生成一个符合期望度数的规则网络.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
</li>
<li><p>以概率p重连其中的某些边.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
</li>
</ol>
<p>在不同的概率p下, 平均距离与聚类系数的变化如下图:</p>
<p><img src="fig_3.png" alt="fig_3"></p>
<p>可以看到, 随着p的增大, 在中间的一段区域, 聚类系数减小缓慢, 而平均距离减小很快, 选择合适的p, 可以获得高聚类系数与低平均距离的网络.</p>
<p>小世界网络与真实的网络对比:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">小世界网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">集中在期望度数附近</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较大</td>
</tr>
</tbody>
</table>
</div>
<p>除了度分布, 小世界网络在平均路径长度与聚类系数上, 都可以较好地模拟真实网络.</p>
<h2 id="优先连接网络"><a href="#优先连接网络" class="headerlink" title="优先连接网络"></a>优先连接网络</h2><p>如何让生成的网络度分布符合幂率分布呢? 遵循富者更富的原则, 可以在生成网络时, 一个一个地加入节点, 对于新加入的节点, 更倾向于连接网络中度数较大的节点, 连接到节点$i$相对概率为:</p>
<script type="math/tex; mode=display">
p(v_i)=\frac{d_i}{\sum_j d_j}</script><p>优先连接网络与真实网络比较:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">真实网络</th>
<th style="text-align:center">优先连接网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">幂率分布</td>
<td style="text-align:center">集中在期望度数附近</td>
</tr>
<tr>
<td style="text-align:center">平均路径长度</td>
<td style="text-align:center">较短</td>
<td style="text-align:center">较短</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">较小</td>
</tr>
</tbody>
</table>
</div>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上介绍了三种经典并简单的生成网络的方法, 它们相比真实网络, 都有一些不一样, 但同样有其价值.</p>
<p>与真实网络对比, 做一个总结:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">随机网络</th>
<th style="text-align:center">小世界网络</th>
<th style="text-align:center">优先连接网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">构造方式</td>
<td style="text-align:center">以概率随机连边</td>
<td style="text-align:center">以概率在规则网络随机重连边</td>
<td style="text-align:center">新加入节点优先连接高度数节点</td>
</tr>
<tr>
<td style="text-align:center">度分布</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">符合</td>
</tr>
<tr>
<td style="text-align:center">聚类系数</td>
<td style="text-align:center">不符合</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">不符合</td>
</tr>
<tr>
<td style="text-align:center">平均距离</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">符合</td>
<td style="text-align:center">符合</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>节点的相似性</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E8%8A%82%E7%82%B9%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7/</url>
    <content><![CDATA[<p>如何判断一个网络中, 某两个节点的相似性呢? 一般来说有两种方法:</p>
<ul>
<li>如果节点带有属性, 那么可以根据属性是否相似来衡量.</li>
<li>根据节点在网络中所处的位置.</li>
</ul>
<p>这里主要讲第二种方法.</p>
<a id="more"></a>
<h1 id="结构等价性-Structural-Equivalence"><a href="#结构等价性-Structural-Equivalence" class="headerlink" title="结构等价性(Structural Equivalence)"></a>结构等价性(Structural Equivalence)</h1><p>这种方法非常简单, 两个节点的共有邻居节点的数目定义了两个节点之间的相似程度.</p>
<p>公式为:</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=v_i与v_j的共同邻居数量</script><p>此公式的一个问题是, 没有考虑到各个节点本身的度数, 比如同样都是10个共同的节点, 但一些节点的度数很高, 一些节点度数很低, 那么这种情况下是不能一概而论的.</p>
<p>因此需要做一些调整, 有两种进阶的计算方法:</p>
<ul>
<li><p>Jaccard Similarity</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=\frac{v_i与v_j的共同邻居数量}{v_i与v_j的所有邻居数量}</script></li>
<li><p>Cosine Similarity</p>
<script type="math/tex; mode=display">
相似度(v_i, v_j)=\frac{v_i与v_j的共同邻居数量}{\sqrt{v_i邻居数量\times v_j邻居数量}}</script></li>
</ul>
<h1 id="规则等价性-Regular-Equivalence"><a href="#规则等价性-Regular-Equivalence" class="headerlink" title="规则等价性(Regular Equivalence)"></a>规则等价性(Regular Equivalence)</h1><p>在衡量等价性时, 不仅仅看相同的邻居, 对于不同的邻居也要看它们是否相似.</p>
<p>计算公式为:</p>
<script type="math/tex; mode=display">
\sigma(v_i, v_j)=\alpha\sum_{k.l}A_{i,k}A_{j,l}\sigma(v_k,v_l)</script><p><img src="fig_0.png" alt="fig_0"></p>
<p>显然这个计算方式相比结构等价性要复杂很多, 并且是利用迭代的方式进行计算的.</p>
<p>一种相对简单的近似算法如下:</p>
<script type="math/tex; mode=display">
\sigma(v_i, v_j)=\alpha\sum_{k}A_{i,k}\sigma(v_k,v_j)</script><p><img src="fig_1.png" alt="fig_1"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>网络的传递性</title>
    <url>/2020/05/21/%E5%9B%BE%E7%AE%97%E6%B3%95/%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%A0%E9%80%92%E6%80%A7/</url>
    <content><![CDATA[<h1 id="传递性-Transitivity"><a href="#传递性-Transitivity" class="headerlink" title="传递性(Transitivity)"></a>传递性(Transitivity)</h1><p>网络的传递性, 可以用于衡量一个网络中, 各个节点之间相互连通的程度. 在一些网络中, 节点倾向于相互孤立, 那么传递性就较差; 在一些网络中, 节点之间倾向于相互连接, 那么传递性一般就会高一些.</p>
<p>传递性的思想用一句通俗的话来说, 就是: 我朋友的朋友也是我的朋友.</p>
<a id="more"></a>
<p><img src="fig_0.png" alt="fig_0"></p>
<h1 id="聚集系数"><a href="#聚集系数" class="headerlink" title="聚集系数"></a>聚集系数</h1><p>那么, 如何定量地去刻画一个网络的传递性呢? 可以用到聚集系数, 其计算方法非常简单:</p>
<script type="math/tex; mode=display">
聚集系数=\frac{构成三角形的三元节点数量 \times 3}{连通的三元节点数量}</script><p>也就是说, 节点之间两两相互连接, 尽量构成更多的三角形, 那么就拥有更大的聚集系数, 也就拥有更高的传递性.</p>
<h1 id="结构平衡理论"><a href="#结构平衡理论" class="headerlink" title="结构平衡理论"></a>结构平衡理论</h1><p>这里再说一下与传递性相关的理论, 结构平衡理论, 同样用通俗的话来说就是:</p>
<ul>
<li>我朋友的朋友就是我朋友.</li>
<li>我朋友的敌人就是我敌人.</li>
<li>我敌人的敌人就是我朋友.</li>
<li>我敌人的朋友就是我敌人.</li>
</ul>
<p>在这样的理论下, 就会出现符合这种描述的网络结构, 称为平衡网络; 以及不符合这种描述的网络, 称为不平衡网络.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<p><img src="fig_2.png" alt="fig_2"></p>
<p>使用计算的方式判断网络是否平衡的方法:</p>
<p><img src="fig_3.png" alt="fig_3"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>节点的中心性</title>
    <url>/2020/05/20/%E5%9B%BE%E7%AE%97%E6%B3%95/%E8%8A%82%E7%82%B9%E7%9A%84%E4%B8%AD%E5%BF%83%E6%80%A7/</url>
    <content><![CDATA[<p>在一个由节点和边组成的网络中, 很多时候想知道哪些节点是相对重要的, 或者进一步对所有节点的重要性进行排序.</p>
<p>根据不同的场景需求, 可以选择不同的方式来度量节点的中心性(重要性).</p>
<a id="more"></a>
<h1 id="基于度数"><a href="#基于度数" class="headerlink" title="基于度数"></a>基于度数</h1><p>基于度数是非常自然的想法, 一个节点的度数越大, 通常是越重要的节点.</p>
<h2 id="点度中心性-Degree-Centrality"><a href="#点度中心性-Degree-Centrality" class="headerlink" title="点度中心性(Degree Centrality)"></a>点度中心性(Degree Centrality)</h2><p>直接利用度数的大小对节点中心性进行排序, 简单, 应用广泛, 可解释性强.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>但该方法显然存在明显的缺点, 邻居节点的重要性没有考虑. 因为在一些场景下, 可以通过非常规方式增加度数, 如一些社交平台上买假粉.</p>
<h2 id="特征向量中心性-Eigenvector-Centrality"><a href="#特征向量中心性-Eigenvector-Centrality" class="headerlink" title="特征向量中心性(Eigenvector Centrality)"></a>特征向量中心性(Eigenvector Centrality)</h2><p>于是, 在考虑某个节点中心性时, 也同时考虑其邻居节点的中心性. 不仅需要邻居节点多, 还需要邻居节点是重要的.</p>
<p>给定一个网络, 对应一个邻接矩阵$A$, 以及一个各节点度数向量$X$.</p>
<p>利用$AX$会得到什么呢? 得到一个新的向量$X$, 其中的每个值代表原本每个节点的邻居节点的度数(重要性)和.</p>
<p>经过反复迭代, 最终向量$X$会收敛稳定, 对于这种情况, 正是线性代数中的特征向量:</p>
<script type="math/tex; mode=display">
AX=\lambda X</script><p>其中$\lambda$是邻接矩阵$A$的特征根, $X$是对应的特征向量, 其各个值也就是各节点的中心性.</p>
<h2 id="PageRank-Centrality"><a href="#PageRank-Centrality" class="headerlink" title="PageRank Centrality"></a>PageRank Centrality</h2><p>特征向量中心性对于无向网络中心性的度量效果是较好的, 但是对于有向网络却并不适用.</p>
<p>下面介绍大名鼎鼎的PageRank算法.</p>
<p>PageRank的基本思想非常简单:</p>
<ol>
<li><p>预设一个阻尼系数$\beta$, 表示在一个页面上跳转到下一个页面的概率.</p>
</li>
<li><p>首先初始化所有节点的重要性为$(1-\beta)$.</p>
</li>
<li><p>每一次迭代时, 某个节点的重要性等于:</p>
<script type="math/tex; mode=display">
PR_i=(1-\beta)+\beta(\sum_{j\in I}PR_j/C_j)</script><p>其中, $PR_i$表示节点$i$在某一次迭代中的重要性, $I$表示连接向节点$i$的邻居节点, $C_j$表示节点$j$的出度.</p>
</li>
<li><p>当迭代后, 各节点前后重要性变化很小时, 停止迭代, 得到结果.</p>
</li>
</ol>
<p>以上就是PageRank的简单版流程, 同时考虑到了邻居节点的数量和重要性, 可以很好地对节点中心性进行排序.</p>
<h1 id="中介中心性-Betweenness-Centrality"><a href="#中介中心性-Betweenness-Centrality" class="headerlink" title="中介中心性(Betweenness Centrality)"></a>中介中心性(Betweenness Centrality)</h1><p>除了可以基于节点度数来进行衡量节点中心性外, 使用betweenness在一些场景下, 也是一种很好的方法.</p>
<p>中介中心性, 强调了某个节点作为”交通枢纽”的作用, 即如果该节点堵塞, 在网络流通将会受到较大影响.</p>
<p>假设要求某个节点$i$的中心性, 计算方式如下:</p>
<ol>
<li><p>计算网络中, 除了节点$i$外, 任意两两节点之间的最短路径.</p>
</li>
<li><p>统计有多少条最短路径经过节点$i$.</p>
</li>
<li><p>节点$i$中心性为:</p>
<script type="math/tex; mode=display">
中心性_i=\frac{经过节点i的最短路径数量}{除节点i的最短路径数量}</script></li>
</ol>
<h1 id="接近中心性-Closeness-Centrality"><a href="#接近中心性-Closeness-Centrality" class="headerlink" title="接近中心性(Closeness Centrality)"></a>接近中心性(Closeness Centrality)</h1><p>想象一种场景下, 需要找到一个节点作为初始点, 想周围扩散一些事物(如观点), 那应该怎样评价各节点的中心性呢?</p>
<p>很自然地想到, 利用平均最短距离进行衡量.</p>
<p>计算方式非常简单:</p>
<script type="math/tex; mode=display">
中心性_i=\frac{1}{平均最短距离_i} \\
平均最短距离_i=\frac{1}{n-1}\sum_{j\ne i}最短路径_{i,j}</script>]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>图基本概念</title>
    <url>/2020/05/17/%E5%9B%BE%E7%AE%97%E6%B3%95/%E5%9B%BE%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>作为一个还只会一点皮毛的初学者, 在这里把自己的所学记录下来, 有问题的地方希望大家可以指点(暂时没开评论, 不过有邮箱~). 以后随着学习的深入, 可能还会对现在的文章进行编辑.</p>
<p>在我们的生活中, 各种学科中, 一些事物可以被抽象成的节点与连边的结构. 比如:</p>
<ul>
<li>人与人之间的社交, 每个人可以被看成一个节点, 两个人是否认识决定他们之间是否拥有连边, 这样可以构成一个网络, 称为社交网络.</li>
<li>互联网中, 网站可以作为节点, 超链接作为边.</li>
<li>在电商中, 用户与商品都可以作为节点, 用户是否购买商品可以作为边.</li>
</ul>
<a id="more"></a>
<p>节点与边组成的抽象网络, 学术上可以被称为<strong>复杂网络</strong>, 属于系统科学的一个重要部分. 围绕复杂网络展开的研究, 可以包括网络本身的一些静态性质, 如节点重要性等; 也可以包括网络的一些动态演变, 比如疾病传播等.</p>
<p>同时, 更进一步, 节点可以有自己的类别和属性, 边可以有方向, 也可以有属性, 这样就可以构成更加丰富的网络. 而基于这样一些丰富的网络, 可以用来做更多的分析与创造. 比如基于图结构的数据, 构建模型, 来进行预测或表示; 结合自然语言处理技术, 构建<strong>知识图谱</strong>, 也是可以大有作为的.</p>
<p>若想在一些现实问题中, 运用图算法相关的理论和工具, 有两个关键要点:</p>
<ol>
<li>如何将现实场景中的对象转化为网络结构.</li>
<li>选择合适的方法处理网络结构数据, 以获得期望的结果.</li>
</ol>
<p>谈到图, 或者图论, 都会提及<strong>柯尼斯堡七桥问题</strong>.</p>
<p><img src="fig_0.png" alt="fig_0"></p>
<p>问题是能否每座桥只经过一次, 遍历所有桥? 经过大神欧拉的一顿操作, 证明这是不可能的, 同时还对这一类问题做了总结和抽象: 对于连通有限图, 度数为奇数的节点, 数量只能为0或者2, 只有这样才能所有边都经过且只经过一次.</p>
<p>这里的度数, 就是节点的连边数量. 通俗地理解, 就是一个节点, 由一条边到达, 必然需要另一条边离开, 所以需要偶数的度数. 除非这个节点作为起点或者终点.</p>
<h1 id="图的基本结构"><a href="#图的基本结构" class="headerlink" title="图的基本结构"></a>图的基本结构</h1><h2 id="节点-amp-边"><a href="#节点-amp-边" class="headerlink" title="节点 &amp; 边"></a>节点 &amp; 边</h2><p>节点与边是组成图的基本单元.</p>
<p><img src="fig_1.png" alt="fig_1"></p>
<h2 id="有向图-amp-无向图"><a href="#有向图-amp-无向图" class="headerlink" title="有向图 &amp; 无向图"></a>有向图 &amp; 无向图</h2><p>根据边是否带有方向(指向性), 可以把图划分为有向图和无向图.</p>
<p><img src="fig_2.png" alt="fig_2"></p>
<h2 id="度数"><a href="#度数" class="headerlink" title="度数"></a>度数</h2><p>节点的度数指的是其周围边的数量, 如果是有向图, 那么还可以细分为<strong>出度</strong>和<strong>入度</strong>.</p>
<p><img src="fig_3.png" alt="fig_3"></p>
<p><img src="fig_4.png" alt="fig_4"></p>
<h2 id="度数分布"><a href="#度数分布" class="headerlink" title="度数分布"></a>度数分布</h2><p>度数的分布指的是具有不同度数的节点, 其数量在整体节点中的占比.</p>
<p><img src="fig_5.png" alt="fig_5"></p>
<p>与其它一些场景下, 经常出现正态分布不一样, 在显示中的不少网络, 其度数分布服从幂律分布, 即绝大部分节点的度数较小, 极少部分节点度数很大.</p>
<h1 id="计算机储存图的方法"><a href="#计算机储存图的方法" class="headerlink" title="计算机储存图的方法"></a>计算机储存图的方法</h1><h2 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h2><p><img src="fig_6.png" alt="fig_6"></p>
<p>在一个正方矩阵中, 行列数目等于节点数目, 格点数值1表示对应两个节点相连, 或者其中一个节点连向另外一个节点.</p>
<p>如果是无向图, 那么是对称矩阵.</p>
<p>用邻接矩阵, 有一些问题:</p>
<ul>
<li><p>通常非常稀疏, 即大量格点为0.</p>
</li>
<li><p>若节点较多, 则会耗费大量储存空间.</p>
</li>
</ul>
<h2 id="邻接表"><a href="#邻接表" class="headerlink" title="邻接表"></a>邻接表</h2><p>只记录相互连接的节点与边的信息, 相比邻接矩阵, 可以节省很多空间.</p>
<p><img src="fig_7.png" alt="fig_7"></p>
<p><img src="fig_8.png" alt="fig_8"></p>
<p>并且具体的储存方式可以根据有向图, 还是无向图进行调整.</p>
<p>其优点是便于查询每个节点的连接情况, 缺点是数据仍然存在冗余.</p>
<h2 id="边表"><a href="#边表" class="headerlink" title="边表"></a>边表</h2><p>只储存边, 其占用空间最小, 是当前比较常用的储存方式.</p>
<p>边表中一条数据, 代表一条边. 在有向图中, 代表一个节点指向另一个节点.</p>
<p><img src="fig_9.png" alt="fig_9"></p>
<p><img src="fig_10.png" alt="fig_10"></p>
]]></content>
      <categories>
        <category>图算法</category>
      </categories>
      <tags>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/2020/05/13/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>正则表达式(regular expression)描述了一种字符串匹配的模式(pattern), 利用这种模式, 可以判断某字符串中是否包含可匹配的子字符串; 可以对匹配到的字符串进行移除, 替换等操作.</p>
<a id="more"></a>
<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="基本匹配"><a href="#基本匹配" class="headerlink" title="基本匹配"></a>基本匹配</h2><p>最简单最基本的匹配方式, 就是直接逐位进行匹配.</p>
<p>例如正则表达式为<code>the</code>, 则会去尝试匹配<code>the</code>.</p>
<p>同时, 基本匹配是大小写敏感的.</p>
<h2 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h2><p>复杂一些的正则表达式, 需要<strong>元字符</strong>来参与.</p>
<p>元字符不代表他们本身的字面意思, 都有特殊的含义. 同时一些元字符写在方括号中的时候会转变用法. 以下是元字符的介绍:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">元字符</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">句号匹配任意单个字符, 除了换行符.</td>
</tr>
<tr>
<td style="text-align:center"><code>[]</code></td>
<td style="text-align:center">匹配方括号内的任意字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>[^]</code></td>
<td style="text-align:center">匹配除了方括号内的任意字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>*</code></td>
<td style="text-align:center">匹配*前的字符0到多次.</td>
</tr>
<tr>
<td style="text-align:center"><code>+</code></td>
<td style="text-align:center">匹配+前的字符1到多次.</td>
</tr>
<tr>
<td style="text-align:center"><code>?</code></td>
<td style="text-align:center">标记?之前的字符为可选.</td>
</tr>
<tr>
<td style="text-align:center"><code>{m, n}</code></td>
<td style="text-align:center">匹配<code>num</code>个大括号之前的字符或者字符集<code>(m &lt;= num &lt;= n)</code>.</td>
</tr>
<tr>
<td style="text-align:center"><code>(xyz)</code></td>
<td style="text-align:center">字符集, 匹配与<code>xyz</code>完全相同的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>竖线</code></td>
<td style="text-align:center">或运算符, 匹配符合前或后的字符(由于显示问题, 以中文示意).</td>
</tr>
<tr>
<td style="text-align:center"><code>\</code></td>
<td style="text-align:center">转义字符, 将字符特殊用法消除或者赋予特殊用法.</td>
</tr>
<tr>
<td style="text-align:center"><code>^</code></td>
<td style="text-align:center">从开始进行匹配.</td>
</tr>
<tr>
<td style="text-align:center"><code>$</code></td>
<td style="text-align:center">从末端进行匹配.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="简写字符集"><a href="#简写字符集" class="headerlink" title="简写字符集"></a>简写字符集</h2><p>正则表达式提供一些常用的字符集简写, 如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">简写</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">除换行符外的所有字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\w</code></td>
<td style="text-align:center">匹配所有字母数字, 等价于<code>[a-zA-Z0-9_]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\W</code></td>
<td style="text-align:center">匹配所有非字母数字, 即符号, 等价于<code>[^\w]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\d</code></td>
<td style="text-align:center">匹配数字, 等价于<code>[0-9]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\D</code></td>
<td style="text-align:center">匹配非数字, 等价于<code>[^\d]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\s</code></td>
<td style="text-align:center">匹配所有空格字符, 等价于<code>[\t\n\f\r]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>\f</code></td>
<td style="text-align:center">匹配一个换页符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\n</code></td>
<td style="text-align:center">匹配一个换行符.</td>
</tr>
<tr>
<td style="text-align:center"><code>\r</code></td>
<td style="text-align:center">匹配一个回车符</td>
</tr>
<tr>
<td style="text-align:center"><code>\t</code></td>
<td style="text-align:center">匹配一个制表符</td>
</tr>
</tbody>
</table>
</div>
<h2 id="前后预查"><a href="#前后预查" class="headerlink" title="前后预查"></a>前后预查</h2><p>在进行匹配的时候, 还要检查匹配字符串前或后的字符串是否满足指定的模式.</p>
<p>一般配合<code>()</code>使用.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>?=</code></td>
<td style="text-align:center">后面存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?!</code></td>
<td style="text-align:center">后面不存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?&lt;=</code></td>
<td style="text-align:center">前面存在</td>
</tr>
<tr>
<td style="text-align:center"><code>?&lt;!</code></td>
<td style="text-align:center">前面不存在</td>
</tr>
</tbody>
</table>
</div>
<h2 id="标志"><a href="#标志" class="headerlink" title="标志"></a>标志</h2><p>标志也可以称作修正符, 用以控制搜索及返回结果的模式.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标志</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>i</code></td>
<td style="text-align:center">忽略大小写.</td>
</tr>
<tr>
<td style="text-align:center"><code>g</code></td>
<td style="text-align:center">全局搜索.</td>
</tr>
<tr>
<td style="text-align:center"><code>m</code></td>
<td style="text-align:center">多行修饰符, 让<code>^</code>与<code>$</code>工作范围在每行的起始.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="贪婪匹配-amp-惰性匹配"><a href="#贪婪匹配-amp-惰性匹配" class="headerlink" title="贪婪匹配 &amp; 惰性匹配"></a>贪婪匹配 &amp; 惰性匹配</h2><p>正则表达式默认采用的贪婪匹配模式, 在该模式下会匹配尽可能长的字符串.</p>
<p>使用<code>?</code>将贪婪匹配转换为惰性匹配, 常配合<code>*</code>, <code>+</code>等元字符使用.</p>
<h1 id="re模块"><a href="#re模块" class="headerlink" title="re模块"></a>re模块</h1><p>在Python中的re模块是用来专门处理正则表达式的, 下面介绍一下基础的常用方法.</p>
<h2 id="match-amp-search"><a href="#match-amp-search" class="headerlink" title="match &amp; search"></a>match &amp; search</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.match(pattern, string, flags=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">re.search(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>match</code>可用于从一开始进行匹配, 若没有匹配到结果, 则返回<code>None</code>.</p>
<p><code>search</code>与<code>match</code>非常相似, 只是它可以从任意位置开始匹配.</p>
<p>参数说明:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>pattern</code></td>
<td style="text-align:center">正则表达式.</td>
</tr>
<tr>
<td style="text-align:center"><code>string</code></td>
<td style="text-align:center">要匹配的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>flag</code></td>
<td style="text-align:center">标志位, 用于控制正则表达式的匹配方式.</td>
</tr>
</tbody>
</table>
</div>
<p>可选标志位:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">修饰符</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>re.I</code></td>
<td style="text-align:center">忽略大小写.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.M</code></td>
<td style="text-align:center">多行匹配, 影响<code>^</code>和<code>$</code>.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.S</code></td>
<td style="text-align:center">使<code>.</code>匹配包含换行符在内的所有字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.L</code></td>
<td style="text-align:center">由当前环境解析字符, 影响<code>\w, \W, \B, \b</code>, 不推荐使用.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.U</code></td>
<td style="text-align:center">由Unicode字符集解析字符.</td>
</tr>
<tr>
<td style="text-align:center"><code>re.X</code></td>
<td style="text-align:center">这个标记允许你编写更具可读性更友好的正则表达式.</td>
</tr>
</tbody>
</table>
</div>
<p>常用的是忽略大小写以及多行匹配.</p>
<p>返回结果为匹配对象:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">匹配对象方法</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>group(num=0)</code></td>
<td style="text-align:center">返回匹配到的整个字符串. 可以通过输入1及以上的单个或多个数字, 来获取对应模式的小组的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>groups()</code></td>
<td style="text-align:center">返回一个包含所有小组字符串的元组.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.sub(pattern, repl, string, count=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>sub</code>可用于替换给定字符串中匹配项.</p>
<p>参数说明:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>pattern</code></td>
<td style="text-align:center">正则表达式.</td>
</tr>
<tr>
<td style="text-align:center"><code>repl</code></td>
<td style="text-align:center">用于替换的字符串, 也可以是函数.</td>
</tr>
<tr>
<td style="text-align:center"><code>string</code></td>
<td style="text-align:center">要匹配的字符串.</td>
</tr>
<tr>
<td style="text-align:center"><code>count</code></td>
<td style="text-align:center">匹配替换的最大次数, 默认0表示全部替换.</td>
</tr>
<tr>
<td style="text-align:center"><code>flag</code></td>
<td style="text-align:center">标志位, 用于控制正则表达式的匹配方式.</td>
</tr>
</tbody>
</table>
</div>
<p>举个栗子来说明<code>repl</code>为函数时的情形:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将匹配的数字乘于2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(s)</span>:</span></span><br><span class="line">    value = int(s.group())</span><br><span class="line">    <span class="keyword">return</span> str(value * <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">string_ = <span class="string">'aaa123bbb234'</span></span><br><span class="line">print(re.sub(<span class="string">'\d+'</span>, func, string_))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aaa246bbb468</span><br></pre></td></tr></table></figure>
<h2 id="findall-amp-finditer"><a href="#findall-amp-finditer" class="headerlink" title="findall &amp; finditer"></a>findall &amp; finditer</h2><p>利用<code>match</code>与<code>search</code>进行匹配时, 只能匹配一个结果.</p>
<p>而<code>findall</code>与<code>finditer</code>可以返回所有匹配项, 根据不同情况选择使用. 相比<code>findall</code>一次性返回所有结果, <code>finditer</code>返回一个迭代器.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.findall(pattern, string, flags=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">re.finditer(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>其中的参数, 与前面方法相同.</p>
<h2 id="split"><a href="#split" class="headerlink" title="split"></a>split</h2><p>与字符串的<code>split</code>方法类似, 将匹配项作为分割边界, 返回分割后的结果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.split(pattern, string, maxsplit=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>其中的<code>maxsplit</code>参数表示最大分割次数, 其余参数与前面方法相同.</p>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>编程基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac配置优化</title>
    <url>/2020/03/27/%E5%85%B6%E5%AE%83/Mac%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>话说为啥会用到Mac Pro呢, 原本我用的小黑, 原因是近期为了搭梯子, 使得连接公司的VPN损坏Σ(っ °Д °;)っ</p>
<p>然后经过一番原因排查, 知道了是因为这个梯子和Windows下的EC会起冲突.</p>
<p>其实如果排除玩游戏, Office, 以及一些必要的社交工具如微信, 钉钉(这排除的有点多啊喂), 我个人是更喜欢Ubuntu的, WIndows系统实在一言难尽つ﹏⊂</p>
<p>于是我将目光投向了Mac OS, 买Mac Pro, 是的我看中的是苹果系统. 但在人生第一次开箱后, 我被震惊了, 这一眼看过去就很高级的做工, 这低调内敛, 精益求精的态度, 我甚至想下一部手机也换Iphone好了(｡･∀･)ﾉﾞ</p>
<a id="more"></a>
<h1 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h1><p>有一说一, 对于习惯了常规键盘布局的我来说, 已经形成了肌肉记忆, 在编辑文字或者写代码的时候, 效率还行. 而Mac的键盘布局发生了变化, 一开始用得真的不习惯, 希望后面慢慢适应吧.</p>
<p>同时, Mac上有些常规的键没有, 比如<code>Home</code>, <code>End</code>, <code>PageUp</code>, <code>PageDown</code>等等, 这些可以由组合键完成.</p>
<ul>
<li><p>Home: fn left or cmd left</p>
</li>
<li><p>End: fn right or cmd right</p>
</li>
<li><p>PageUp: fn up</p>
</li>
<li><p>PageDown: fn down</p>
</li>
<li><p>Delete: fn backspace</p>
</li>
<li><p>按单词移动: option left or option right</p>
</li>
<li><p>iTerm2快捷键设置:</p>
<p>总体思路是查询原有快捷键, 将其配置赋予自定义的快捷键.</p>
<ul>
<li><p>按单词移动:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>option left Send Escape Sequence b</p>
<p>option right Send Escape Sequence f</p>
</li>
<li><p>按单词删除:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>Left Option: Normal &gt; Esc+</p>
</li>
<li><p>整行删除:</p>
<p>Preferences &gt; Keys</p>
<p>cmd backspace Send Hex Codes 0x15</p>
</li>
<li><p>移动到句首/尾:</p>
<p>Preferences &gt; Keys</p>
<p>cmd left/right Send Hex Codes 0x01/0x05</p>
</li>
<li><p>撤销:</p>
<p>Preferences &gt; Profiles &gt; Keys</p>
<p>cmd z Send Hex Codes 0x01/0x1f</p>
</li>
</ul>
</li>
</ul>
<h1 id="触控板"><a href="#触控板" class="headerlink" title="触控板"></a>触控板</h1><p>系统偏好设置 &gt; 触控板</p>
<p><img src="chukongban_0.png" alt="触控板0"></p>
<p><img src="chukongban_1.png" alt="触控板1"></p>
<p>系统偏好设置 &gt; 辅助功能 &gt; 鼠标与触控板 &gt; 触控板选项</p>
<p><img src="chukongban_2.png" alt="触控板2"></p>
<h1 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h1><p>原本Mac自带的输入法有如下一些问题:</p>
<ul>
<li>智能输入体验差.</li>
<li>中英文切换是原本的大写锁定键. 按逗号, 句号键无法查看更多候选.</li>
<li>虽然设定里面有设定半角模式, 但实际上没用. 这一点对于俺这样的小码农来说是最难受的.</li>
</ul>
<p>所以尝试了一下<strong>搜狗输入法Mac版</strong>, 不得不说, 真滴好用, 不愧是<strong>中文输入法界永远滴神!</strong></p>
<p>需要进行一些设置:</p>
<ul>
<li>全局半角.</li>
<li>关闭TouchBar功能, 关闭小图标(语音, 截图这些功能对我来说多余了).</li>
</ul>
<p>相比自带的输入法, 上面提到的问题都不再是问题, 而且如果愿意可以自定义更加适合自己的东西, 比如斗图, 颜表情…用搜狗输入法写到这里, 我还没有翻过提示页, 我这么说, 你懂吧.</p>
<h1 id="文本编辑器"><a href="#文本编辑器" class="headerlink" title="文本编辑器"></a>文本编辑器</h1><p>Mac自带的文本编辑器, 很难使, 只能说可以正常打字吧, 嗯.</p>
<p>所以需要一款功能多一些, 但仍然轻量级的文本编辑器, 这里我选择的是<strong>Sublime Text</strong>.</p>
<p>安装方法是直接到官网下载安装.</p>
<p>配置命令行方法:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ln -sv &quot;/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl&quot; /usr/local/bin/subl</span><br></pre></td></tr></table></figure>
<h1 id="基础APP"><a href="#基础APP" class="headerlink" title="基础APP"></a>基础APP</h1><p>包含但不限于:</p>
<ul>
<li>钉钉</li>
<li>微信</li>
<li>Chrome</li>
<li>EeasyConnect</li>
<li>ClashX</li>
<li>Tencent lemon</li>
<li>The Unarchiver</li>
</ul>
<h1 id="生产工具"><a href="#生产工具" class="headerlink" title="生产工具"></a>生产工具</h1><ul>
<li><p>Anaconda3</p>
<p>可在清华镜像下载. 安装完成后, 需要将路径写入环境变量<code>~/.zsh_profile</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Anaconda3.</span><br><span class="line">export PATH=&quot;/Users/opt/anaconda3/bin:$PATH&quot;</span><br></pre></td></tr></table></figure>
<p>安装Jupyter插件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  # 安装nbextensions.</span><br><span class="line">  $ pip install jupyter_contrib_nbextensions</span><br><span class="line">  $ jupyter contrib nbextension install --user</span><br><span class="line">  # 安装nbextensions_configurator.</span><br><span class="line">  $ pip install jupyter_nbextensions_configurator</span><br><span class="line">  $ jupyter nbextensions_configurator enable --user</span><br><span class="line">  # 安装yapf, 使用代码格式标准化工具时需要.</span><br><span class="line">$ pip install yapf</span><br></pre></td></tr></table></figure>
<p>安装完成后, 勾选如下配置:</p>
<ul>
<li><code>Codefolding</code> &amp; <code>Codefolding in Editor</code>: 代码折叠.</li>
<li><code>Move selected cells</code>: 按住<code>option</code>可上下移动<code>cells</code>.</li>
<li><code>Toggle all line numbers</code>: 显示行数.</li>
<li><code>AutoSaveTime</code>: 自动按时保存.</li>
<li><code>Hide Header</code>: <code>control H</code>隐藏/展现工具栏.</li>
<li><code>Highlight selected word</code>: 变量高亮.</li>
<li><code>Table of Contents</code>: 目录.</li>
<li><code>Code prettify</code>: 代码格式美化.</li>
<li><code>ExecuteTime</code>: 显示代码执行时间.</li>
<li><code>ScrollDown</code>: 输出自动下滑.</li>
</ul>
</li>
<li><p>PyCharm</p>
<p>下载安装, 并配置解释器路径和主题.</p>
</li>
<li><p>Clion</p>
<p>下载安装, 并破解.</p>
</li>
<li><p>Office</p>
<p>下载官网最新版, 然后寻找激活方法. 可选择装部分应用.</p>
</li>
<li><p>Git</p>
<p>一般自带. 需要创建公钥私钥, GitHub关联.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure>
</li>
<li><p>Node.js</p>
<p>官网下载安装.</p>
</li>
<li><p>typora</p>
<p>官网下载安装.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在~/.zsh_profile中添加如下, 使其可在终端快速启动.</span><br><span class="line">alias typora=&quot;open -a typora&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>java8</p>
<p>为了配合<code>Spark</code>的使用.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 更新brew库.</span><br><span class="line">$ brew tap homebrew/cask-versions</span><br><span class="line"># 制定版本安装.</span><br><span class="line">$ brew cask install adoptopenjdk8</span><br><span class="line"># 查看是否安装成功.</span><br><span class="line">$ java -version</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="终端"><a href="#终端" class="headerlink" title="终端"></a>终端</h1><ul>
<li><p>homebrew</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure>
<p>在执行<code>brew update</code>时, 基本卡死, 解决方法为更换国内镜像源.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 切换brew</span><br><span class="line">$ git -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-core</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-cask</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.git</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ brew config</span><br><span class="line">$ brew update -verbose</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 恢复源</span><br><span class="line"># 恢复 brew</span><br><span class="line">$ git -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-core</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git</span><br><span class="line"></span><br><span class="line"># 切换homebrew-cask</span><br><span class="line">$ git -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>iTerm2</p>
<p>官网下载安装.</p>
<p>设置为默认终端.</p>
<p>调整字体大小15, 光标闪动, 色彩对比度.</p>
<p>preference =&gt; Keys =&gt; Hotkey =&gt; Show/hide iTerm2 with a system-wide hotkey, 设置为command ..</p>
<p>preference  =&gt; profiles =&gt; colors =&gt; Color Presets =&gt; Solarized Light.</p>
</li>
<li><p>oh-my-zsh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看安装的shell.</span><br><span class="line">$ cat /etc/shells</span><br><span class="line"># 查看当前shell.</span><br><span class="line">$ echo $SHELL</span><br><span class="line"># 切换shell.</span><br><span class="line">$ chsh -s /bin/zsh</span><br><span class="line"># 安装oh-my-zsh</span><br><span class="line">$ sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置oh-my-zsh.</span><br><span class="line">$ open -t .zshrc</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主题列表: https://github.com/ohmyzsh/ohmyzsh/wiki/themes</span><br><span class="line">ZSH_THEME=avit</span><br><span class="line"># 插件.</span><br><span class="line">plugins=(git osx zsh-autosuggestions zsh-syntax-highlighting)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 插件中的zsh-autosuggestions 和 zsh-syntax-highlighting 是自定义安装的插件, 需要用 git 将插件 clone 到指定插件目录下：</span><br><span class="line"># 自动提示插件</span><br><span class="line">git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions</span><br><span class="line"># 语法高亮插件</span><br><span class="line">git clone git://github.com/zsh-users/zsh-syntax-highlighting $ZSH_CUSTOM/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="DS-Store"><a href="#DS-Store" class="headerlink" title=".DS_Store"></a>.DS_Store</h1><p><code>.DS_Store</code>这个文件没什么用, 但是却会在很多传输文件的时候带来不必要的问题. 比如在使用<code>Git</code>的时候.</p>
<p>有没有什么办法能够完全禁止呢? 暂时没找到.</p>
<p>删除所有目录下的<code>.DS_Store</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo find / -name &quot;.DS_Store&quot; -depth -exec rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<p>这条命令可以临时删除, 但后续只要文件夹有一些改动, 又回自动生成.</p>
<p>针对<code>Git</code>, 可以设定全局的<code>.gitignore_global</code>进行一定程度上的帮助. 创建<code>~/.gitignore_global</code>, 其中写入:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># .gitignore_global</span><br><span class="line">####################################</span><br><span class="line">######## OS generated files ########</span><br><span class="line">####################################</span><br><span class="line">.DS_Store</span><br><span class="line">.DS_Store?</span><br></pre></td></tr></table></figure>
<p>同时在<code>~./gitconfig</code>中引入设置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[user]</span><br><span class="line">	name = xxx</span><br><span class="line">	email = xxx</span><br><span class="line">[core]</span><br><span class="line">	excludesfile = /Users/shy/.gitignore_global</span><br></pre></td></tr></table></figure>
<p>或者输入如下命令(推荐):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global core.excludesfile ~/.gitignore_global</span><br></pre></td></tr></table></figure>
<h1 id="TouchBar"><a href="#TouchBar" class="headerlink" title="TouchBar"></a>TouchBar</h1><p>最开始我觉得Mac的TouchBar是鸡肋, 花里胡哨的, 还我<code>F1~F12</code>(ﾟДﾟ*)ﾉ</p>
<p>比如Pycharm里面的<code>运行</code>快捷键是<code>shift F10</code>, 你这都没有这个键怎么办啊? 哦…变成<code>cmd shift R</code>了啊, 那没事了…</p>
<p>TouchBar的存在, 可以在系统层面, 或者一些APP上, 将一些功能直接放上去, 只要设置合理, 在熟练使用以后, 我相信效率是会提高的.</p>
<p>系统层面的TouchBar设置, 这个根据自己喜好就行. 我是将<code>调度管理</code>,<code>截屏</code>, <code>静音</code>设置成默认, 同时将<code>调度管理</code>, <code>截屏</code>, <code>音量调节</code>, <code>亮度调节</code>, <code>锁屏</code>, <code>睡眠</code>放到详细页.</p>
<p>一些APP, 比如PyCharm是支持定制TouchBar的. 对PyCharm来说, 可以在<code>Preferences &gt; Appearance &amp; Behavior &gt; Menus and Tollbars &gt; Touch Bar</code>中进行设置.</p>
<p>但是关于显示图标, 需要自己指定图案. 我在谷歌上搜索<code>touchbar icon download</code>这个关键词, 找到了<a href="https://community.folivora.ai/t/v3-update-native-apple-touch-bar-icon-pack-for-btt/3310/18" target="_blank" rel="noopener">这个网站</a>, 提供了TouchBar图标的下载地址.</p>
<p>然鹅, 直接使用里面的图标, 仍然会觉得有些蛋疼(* ￣︿￣), 因为你看自带的TouchBar图标, 都是亮亮的, 咱这图标是黑黑的, 白天用着还行, 光线不好的时候用着一片黑啊…</p>
<p>于是, 我想到了利用Python来修改图标颜色, 经过一番尝试, 成功了（＃￣～￣＃）</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">path=<span class="string">'image.png'</span></span><br><span class="line">im = Image.open(path)</span><br><span class="line">im.show()</span><br><span class="line"></span><br><span class="line">print(im.format)</span><br><span class="line">print(im.mode)</span><br><span class="line">print(im.size)</span><br><span class="line"></span><br><span class="line">source = im.split()</span><br><span class="line">source = (source[<span class="number">0</span>].point(<span class="keyword">lambda</span> i: <span class="number">255</span>), source[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">im = Image.merge(im.mode, source)</span><br><span class="line"></span><br><span class="line">im.show()</span><br><span class="line">im.save(<span class="string">'new_image.png'</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码的思路, 是咱们手里的原始图片, 是<code>LA</code>格式的, 众所周知<code>RGB</code>格式有三个通道, 而这种格式呢有两个通道. 第一个通道控制颜色, 0 ~ 255, 其中0是黑色, 255是白色; 第二个通道控制是否透明, 0 ~ 255, 0表示透明, 255表示不透明. 所以我们只要将第一个通道的数值, 全部改为255即可o(≧口≦)o</p>
<p>由此, 我在PyCharm的TouchBar上, 配置了<code>Home</code>, <code>End</code>等功能, 某种程度上加快了编辑效率.</p>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>删除一些无用APP</p>
<p>菜单栏 &gt; 关于本机 &gt; 储存空间 &gt; 管理</p>
<p>在其中可以看到一些可删除的系统自带的APP, 删除后可以节省几个G的空间.</p>
</li>
<li><p>设置访达的侧边栏</p>
<p>访达 &gt; 偏好设置</p>
<p>设置开启新窗口为用户根目录. 并将其显示到边栏中.</p>
</li>
</ul>
<h1 id="键盘-1"><a href="#键盘-1" class="headerlink" title="键盘"></a>键盘</h1><p>为了拥有更好的体验, 我买了一块外接无线键盘. 主要就是为了<code>Hone</code>, <code>End</code>, <code>PageUp</code>, <code>PageDown</code>, <code>Delete</code>键. 然后当我买入后, 发现, 这些键是有了, 但是在一些APP上的功能却不是我所想的. 比如想要移动光标到当前行第一列或者最后一列, 使用<code>cmd left/right</code>肯定是可以的, 但是<code>Home</code>和<code>End</code>却不一定行…还好与代码编辑相关的PyCharm与Jupyter可以与预期一致.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(三)</title>
    <url>/2020/03/20/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%B8%89/</url>
    <content><![CDATA[<p>这一篇是本系列的终篇, 主要介绍一个方法.</p>
<p>问题是这样的, 我在笔记本A上搭建博客, 并进行文章的编辑和推送部署. 现在我换了一台笔记本B, 那如果我想在B上面进行继续创作应该怎么办呢? 以后某一天由需要用A进行创作呢?</p>
<p>相信很多同学已经想到了, 没错, 还是基于<code>Github</code>来进行操作.</p>
<a id="more"></a>
<h1 id="创建一个用于管理博客的仓库"><a href="#创建一个用于管理博客的仓库" class="headerlink" title="创建一个用于管理博客的仓库"></a>创建一个用于管理博客的仓库</h1><p>在篇章一中, 我们创建了一个名为<code>xiaoming.github.io</code>的仓库, 这是我们用来管理博客的仓库吗?</p>
<p>当然不是啦, 对比一下咱们本地和它的文件布局就知道, 这个仓库存放的, 是我们<code>$ hexo d</code>时, 推送的博客相关文件的仓库, 并不包含其它的文件.</p>
<p>于是我们还需要创建一个仓库, 比如命名为<code>my_blog</code>.</p>
<h1 id="本地远端关联"><a href="#本地远端关联" class="headerlink" title="本地远端关联"></a>本地远端关联</h1><p>创建好<code>my_blog</code>以后, 进入到<code>笔记本A</code>本地博客的根目录(假设也叫<code>my_blog</code>), 执行一波如下操作, 经常使用<code>Github</code>的同学应该很熟悉:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd my_blog/</span><br><span class="line">$ git init</span><br><span class="line">$ git remote add origin git@github.com:xiaoming/my_blog.git  # 根据自己实际地址进行修改.</span><br></pre></td></tr></table></figure>
<p>那这时候可能有同学就说说了, 啊我知道了, 接下来<code>add commit push</code>就完事了.</p>
<p>但是此时存在一个问题,  在<code>my_blog</code>中, 存在几个<code>Git子模块</code>, 就是一个大的Git仓库里面, 还有小的Git仓库.</p>
<p>还记得篇章一中, 咱们在<code>theme/</code>中利用clone的<code>Next</code>主题吗, 这是一个子模块; 后来我们在执行<code>$ hexo d</code>时, 是将<code>my_blog/.deploy_git/</code>这个子模块push到了Github.</p>
<p>子模块的存在, 在一些开发项目中可以带来很多便利, 但是在咱们这里并不需要, 鉴于本宝宝不怎么使用Git子模块(对, 我就不会​​), 所以需要一些额外的操作, 消除子模块的存在, 以此来保证后续<code>add commit push</code>的顺利进行.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以下操作只用执行一次, 除非以后跟新主题.</span><br><span class="line">$ cd my_blog/theme/</span><br><span class="line">$ rm -rf .git/</span><br></pre></td></tr></table></figure>
<p>在根目录添加<code>.gitignore</code>文件, 并将<code>.deploy_git/</code>写入.</p>
<p>Ok, 现在假设已经执行<code>$ hexo d</code>了, 就可以执行如下代码, 推送到远端的<code>my_blog</code>仓库了:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git add -A</span><br><span class="line">$ git commit -m &quot;happy day&quot;</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>
<h1 id="在另外一台电脑上"><a href="#在另外一台电脑上" class="headerlink" title="在另外一台电脑上"></a>在另外一台电脑上</h1><p>经过上面的一顿操作, 接下来就是要在<code>笔记本B</code>上写博客了.</p>
<p>首先我们要保证<code>笔记本B</code>上也有相应的轮子<code>Git &amp; Node.js</code>, 照着篇章一就ok.</p>
<p>然后, 假设我们也在<code>笔记本B</code>上创建了一个<code>my_blog</code>的文件夹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd my_blog/</span><br><span class="line">$ git init</span><br><span class="line">$ git remote add origin git@github.com:xiaoming/my_blog.git</span><br><span class="line">$ git pull origin master</span><br></pre></td></tr></table></figure>
<p>为了验证是否移植成功, 可以在本地看一下效果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
<p>如果在<code>localhost:4000</code>中看到了自己的博客, 说明一切尽在掌握.</p>
<p>那接下来, 就可以在<code>笔记本B</code>写博客, 推送部署了.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(二)</title>
    <url>/2020/03/20/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%BA%8C/</url>
    <content><![CDATA[<p>在上一篇中, 记录了使用<code>Hexo</code>来搭建博客的基础操作, 这一篇紧接上一篇, 来介绍更多的东西.</p>
<p>因为后续可能为博客添加更多的功能, 所以本篇章后续还会更新.</p>
<a id="more"></a>
<h1 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h1><p>一般来说都会保留一个页面, 来展示博主的一些信息, <code>about</code>或者<code>关于</code>页面就是用来做这个的.</p>
<p>其文件路径在<code>source/about/index.md</code>, 按照自己想法, 利用<code>Markdown</code>的语法进行编辑就好.</p>
<h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><p>尽管一个良好的博客, 有<code>分类</code>或者<code>标签</code>, 来帮助大家快速找到自己想要看的文章, 但是当文章数量日渐增长, 同时没有找到对应的<code>分类</code>时, 怎么办? </p>
<p>此时就需要<code>本地搜索</code>.</p>
<p>感恩大神, 如果没有额外的需求, 我们只需要做如下操作, 就能拥有<code>本地搜索</code>的功能.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-generator-searchdb</span><br></pre></td></tr></table></figure>
<p>在根目录下的<code>_config.yml</code>中添加如下配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  format: html</span><br></pre></td></tr></table></figure>
<p>重新编译博客后, 会发现多了一个<code>搜索</code>的功能, 敲好用!</p>
<h1 id="流量统计"><a href="#流量统计" class="headerlink" title="流量统计"></a>流量统计</h1><p>我想大部分同学还是想知道, 自己的博客/文章被多少人看过, 被看过多少次这样的信息.</p>
<p>统计这样的信息有不少方法和途径, 这里我使用的是一款叫做<code>不蒜子</code>的插件.</p>
<p>使用方法非常简单, 再次感恩大神.</p>
<p>只需要将主题文件中的<code>_config.yml</code>修改一下就行, 就这么简单.​</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Show Views / Visitors of the website / page with busuanzi.</span><br><span class="line"># Get more information on http://ibruce.info/2015/04/04/busuanzi</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br><span class="line">  total_visitors: true</span><br><span class="line">  total_visitors_icon: user</span><br><span class="line">  total_views: true</span><br><span class="line">  total_views_icon: eye</span><br><span class="line">  post_views: true</span><br><span class="line">  post_views_icon: eye</span><br></pre></td></tr></table></figure>
<p>然后在网页的底部会显示整个博客的流量, 文章的开头会显示本篇文章的流量.</p>
<p>这里再说一句, 在本地测试<code>$ hexo s</code>时看到的流量数据是虚假的.</p>
<h1 id="插图"><a href="#插图" class="headerlink" title="插图"></a>插图</h1><p>想要在文章中插图, 方式主要有两种: </p>
<ul>
<li>将图片放置与网络图床, 然后直接引用就行.</li>
<li>将图片放置于博客根目录下的某目录中, 然后再用相对路径/绝对路径引用.</li>
</ul>
<p>这里介绍相对路径的方法, 这样我认为更加方便管理一些. 方法非常简单, 即在文章存在的文件夹中, 创建一个同文章名的文件夹, 用于存放该文章的图片.</p>
<p>如文章路径为<code>source/_post/happy_day.md</code>, 放置图片的文件夹路径为<code>source/_post/happy_day/</code></p>
<p>而在文章中插入图片(假设图片路径为<code>source/_post/happy_day/happy.png</code>)时, 语法为<code>![我是图](happy.png)</code></p>
<p>如果觉得每次同时创建文章和放置图片的同名文件夹麻烦, 可以修改<code>_config.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
<p>以后每次创建文章<code>$ hexo new xxx</code>时, 就会同时出现<code>xxx.md</code>和<code>xxx/</code>.</p>
<h1 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h1><p>使用默认的渲染引擎<code>hexo-renderer-marked</code>时, 会和Latex各种冲突, 公式换行为<code>\\</code>, 下标<code>_</code>等.</p>
<p>这个问题比较蛋疼, 试了好些办法, 最后选择的一种办法是更换引擎, 更换为<code>hexo-renderer-kramed</code>这个渲染引擎, 该引擎其实就是在<code>hexo-renderer-marked</code>基础上, 针对<code>mathjax</code>进行了改进, 使其优先支持Latex公式语法.</p>
<p>更换引擎代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm uninstall hexo-renderer-marked --save</span><br><span class="line">$ npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>若更换过程中报错, 则根据错误信息提示, 进行相应操作即可.</p>
<p>但是<code>hexo-renderer-kramed</code>只能够解决单行的渲染问题, 行内的仍然会出问题, 需要手工矫正一下, 修改<code>node_modules\kramed\lib\rules\inline.js</code>:</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改第 11 行</span></span><br><span class="line"><span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 修改第 20 行</span></span><br><span class="line"><span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> 重启 hexo</span><br><span class="line">hexo clean </span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<p>最后, 若发现浏览器中仍然无法正常显示换行, 尝试更换浏览器. 在Windows系统下, 火狐浏览器即使更换了引擎也不行, 但谷歌浏览器可以.</p>
<p>Chrome大法好啊(｡･∀･)ﾉﾞ</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>创建属于自己的博客(一)</title>
    <url>/2020/03/17/%E5%85%B6%E5%AE%83/%E5%88%9B%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2-%E4%B8%80/</url>
    <content><![CDATA[<p>这里开始记录自己构建博客的流程, 帮助别人也是帮助自己.</p>
<p>当然, 每个人的需求是不一样的. 有的人不需要一个自己独有的博客, 因为现在有各种很好的流量平台, 知乎, 微信公众号等. 同时, 不同人的审美是不一样的, 也许我觉得简约明了就很漂亮, 一些人觉得充满各种图案颜色更好看. 对于博客的一些功能, 如社交链接, 打赏, 广告位等, 我暂时是没有啥兴趣的, 而可能一部分人希望加上这些功能, 能够更加充分地展示自己~</p>
<a id="more"></a>
<p>所以, 我这里会将重要的, 我自己需要的一些步骤和流程做记录. 我个人更喜欢Linux系统, 但是工作需要与他人交互, 所以现在使用更多的反而是Windows和Mac OS. 后面的操作都是基于Windows的, 其它的系统如Mac OS, Linux也是类似的.</p>
<p>如果以后有一天, 你看了我的流程, 发现自己在用的时候出错了, 或者你想添加不一样的更多的东西, 建议谷歌​​.</p>
<h1 id="借助别人的轮子"><a href="#借助别人的轮子" class="headerlink" title="借助别人的轮子"></a>借助别人的轮子</h1><p>说起来, 搭建博客, 写网页, 最专业的应该是偏开发, 前端的程序员. 而咱们如果从头去学, 成本太高, 也没必要.</p>
<p>很多大神程序员已经帮我们造好了各种轮子, 我们只要好好利用起来就行了.</p>
<blockquote>
<p>合理使用别人的轮子, 是对造轮子的人的一种尊重.</p>
</blockquote>
<p>而在搭建博客这块, 有几个(我使用)关键元素:</p>
<ul>
<li><strong>Hexo</strong>: 什么是Hexo? Hexo是一个快速, 简洁且高效的博客框架. Hexo使用Markdown(或其它渲染引擎)解析文章, 在几秒内, 即可利用靓丽的主题生成静态页面. <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo官网</a>, 中文的哟.</li>
<li><strong>Next</strong>: 什么是Next? Next是一款可以与Hexo搭配使用的主题风格. <a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">Next主题Github地址</a>.</li>
<li><strong>Github</strong>: 什么是Github? 啊…这…就不解释了哈哈.</li>
</ul>
<p>那么用举栗子的说法, 来解释一下这三个元素, 就是比如你想做一件衣服, 一件能够去参加艺术品展示的衣服. 那么Hexo就是裁缝, 负责将布料做成衣服; Next是设计师, 告诉裁缝怎么做漂亮一些; Github是模特, 将衣服穿上向大家展示.</p>
<p>需要说的是, 与上述的三个轮子, 具有相似功能的其它轮子也有一些, 不一定非要使用这几个轮子.</p>
<p>同时需要说明的是我使用的<code>Hexo</code>版本是<code>v3.9.0</code>, <code>Next</code>版本是<code>v7.2.0</code>.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><ul>
<li><a href="https://git-scm.com/download" target="_blank" rel="noopener">安装地址</a>.</li>
</ul>
<h2 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h2><ul>
<li><a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">安装地址</a>.</li>
</ul>
<h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>在Windows下使用PowerShell或者GitBash(更加推荐GitBash), 输入如下代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h1 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h1><h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>安装 Hexo 完成后, 执行下列命令, Hexo 将会在指定文件夹中新建所需要的文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ cd &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
<p>新建完成后, 指定文件夹的目录如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>_config.yml</strong></p>
<p>网站的配置信息, 可以在此配置大部分的参数. 后续再做详细说明.</p>
</li>
<li><p><strong>scaffold</strong></p>
<p>模板文件夹, 文章的布局会根据模板来进行建立.</p>
</li>
<li><p><strong>source</strong></p>
<p>资源文件夹是存放用户资源(文章)的地方. </p>
<p>除 <code>_posts</code> 文件夹之外, 开头命名为 <code>_</code> (下划线)的文件 / 文件夹和隐藏的文件将会被忽略.</p>
<p>Markdown 和 HTML 文件会被解析并放到 <code>public</code> 文件夹, 而其他文件会被拷贝过去.</p>
</li>
<li><p><strong>themes</strong></p>
<p>主题文件夹, Hexo根据选定的主题来生成静态页面.</p>
</li>
</ul>
<h2 id="远端"><a href="#远端" class="headerlink" title="远端"></a>远端</h2><p>上文说到, 需要借助Github来帮助我们展示博客, 因此需要在Github上创建对应的仓库.</p>
<p>用于网站的仓库, 相比普通仓库多了一些限制和操作.</p>
<ul>
<li><p><strong>仓库名称</strong></p>
<p>必须满足如下格式: Github用户名.github.io, 如名字叫xiaoming, 就是xiaoming.github.io.</p>
</li>
<li><p><strong>仓库公开</strong></p>
<p>要给大家伙看的嘛, 自然不能是private的, 必须是public的.</p>
</li>
<li><p><strong>Github Pages</strong></p>
<p>进入到仓库的settings中, 往下面拉到Github Pages的版块, 进行相应配置.</p>
<p>可以填入自定义域名, 这个后续会加以说明.</p>
<p>建议勾选<code>Enforce HTTPS</code>, 这样Github会免费把你的网站升级成HTTPS, 更加安全.</p>
</li>
</ul>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="添加主题"><a href="#添加主题" class="headerlink" title="添加主题"></a>添加主题</h2><p>我有点忘了是否安装Hexo并建站以后, <code>themes</code>文件夹下会自带<code>Next</code>主题.</p>
<p>若没有, 需要自己安装主题. 以<code>Next</code>主题为例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd themes/</span><br><span class="line">$ git init</span><br><span class="line">$ git clone https://github.com/theme-next/hexo-theme-next.git</span><br></pre></td></tr></table></figure>
<p>主题文件夹的一般布局:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── languages</span><br><span class="line">├── layout</span><br><span class="line">├── scripts</span><br><span class="line">└── source</span><br></pre></td></tr></table></figure>
<h2 id="Hexo-config-yml"><a href="#Hexo-config-yml" class="headerlink" title="Hexo_config.yml"></a>Hexo_config.yml</h2><p>一下对部分我认为较为重要, 以及基础的参数进行介绍.</p>
<p><strong>网站相关</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>title</code></td>
<td style="text-align:center">网站标题</td>
</tr>
<tr>
<td style="text-align:center"><code>subtitle</code></td>
<td style="text-align:center">网站副标题</td>
</tr>
<tr>
<td style="text-align:center"><code>description</code></td>
<td style="text-align:center">网站描述</td>
</tr>
<tr>
<td style="text-align:center"><code>keywords</code></td>
<td style="text-align:center">网站的关键词。使用半角逗号 <code>,</code> 分隔多个关键词。</td>
</tr>
<tr>
<td style="text-align:center"><code>author</code></td>
<td style="text-align:center">您的名字</td>
</tr>
<tr>
<td style="text-align:center"><code>language</code></td>
<td style="text-align:center">网站使用的语言, 我使用的zh-Hans</td>
</tr>
<tr>
<td style="text-align:center"><code>timezone</code></td>
<td style="text-align:center">网站时区。Hexo 默认使用您电脑的时区。请参考 <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" target="_blank" rel="noopener">时区列表</a> 进行设置，如 <code>America/New_York</code>, <code>Japan</code>, 和 <code>UTC</code> 。一般的，对于中国大陆地区可以使用 <code>Asia/Shanghai</code>。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>网址相关</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>url</code></td>
<td style="text-align:center">网址, 修改为Github仓库地址, 如<a href="https://xiaoming.github.io/" target="_blank" rel="noopener">https://xiaoming.github.io/</a></td>
</tr>
<tr>
<td style="text-align:center"><code>root</code></td>
<td style="text-align:center">网站根目录, 默认为/</td>
</tr>
</tbody>
</table>
</div>
<p><strong>主题相关</strong></p>
<p>指定使用的主题. 如上文中我们在<code>theme</code>文件中安装了<code>Next</code>主题, 则可将其对应文件名进行设置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theme: hexo-theme-next</span><br></pre></td></tr></table></figure>
<p><strong>部署相关</strong></p>
<p>这一部分的配置, 是告诉Hexo你要用什么方法来将博客部署到服务器上. 当然我是用的Github.</p>
<p>首先安装hexo-deployer-git:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>然后对应修改配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:xiaoming/xiaoming.github.io.git</span><br></pre></td></tr></table></figure>
<p>到这里, 基础的配置就完成了, 接下来进行主题的相关配置.</p>
<h2 id="主题-config-yml"><a href="#主题-config-yml" class="headerlink" title="主题 _config.yml"></a>主题 _config.yml</h2><p>相比Hexo的_config.yml, 主题文件中的_config.yml有更多的内容, 下面针对部分内容进行修改.</p>
<ul>
<li><p><strong>菜单设置</strong></p>
<p>这里设置你的博客有哪些主要的版块, 我的设置保留了主页, 关于, 分类, 归档.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home</span><br><span class="line">  about: /about/ || user</span><br><span class="line">  tags: /tags/ || tags</span><br><span class="line">  categories: /categories/ || th</span><br><span class="line">  archives: /archives/ || archive</span><br><span class="line">  #schedule: /schedule/ || calendar</span><br><span class="line">  #sitemap: /sitemap.xml || sitemap</span><br><span class="line">  #commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo new page tags</span><br><span class="line">$ hexo new page categories</span><br><span class="line">$ hexo new page about</span><br></pre></td></tr></table></figure>
<p>编辑对应的index.md文件.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tags</span><br><span class="line">date: 2020-09-20 15:41:00</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comment: false</span><br><span class="line"></span><br><span class="line"># categories</span><br><span class="line">date: 2019-07-23 22:11:08</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line"></span><br><span class="line"># about</span><br><span class="line">date: 2019-07-23 01:01:43</span><br><span class="line">comment: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>子主题设置</strong></p>
<p>可以认为<code>Next</code>主题下包含了4个子主题: <code>Muse</code>, <code>Mist</code>, <code>Pisces</code>, <code>Gemini</code>.</p>
<p>根据个人喜好进行选择, 不满意可更换. 我选择的<code>Mist</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>侧边栏设置</strong></p>
<p>打开侧边栏:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">site_state: true</span><br></pre></td></tr></table></figure>
<p>目录显示相关:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  # Automatically add list number to toc.</span><br><span class="line">  number: true</span><br><span class="line">  # If true, all words will placed on next lines if header width longer then sidebar width.</span><br><span class="line">  wrap: false</span><br><span class="line">  # If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span><br><span class="line">  expand_all: false</span><br><span class="line">  # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` in Front-matter.</span><br><span class="line">  max_depth: 6</span><br></pre></td></tr></table></figure>
<p>在左边还是右边 , 宽度, 是否自动显示:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  # Sidebar Position.</span><br><span class="line">  #position: left</span><br><span class="line">  position: right</span><br><span class="line">  </span><br><span class="line">  # Manual define the sidebar width. If commented, will be default for:</span><br><span class="line">  # Muse | Mist: 320</span><br><span class="line">  # Pisces | Gemini: 240</span><br><span class="line">  #width: 300</span><br><span class="line"></span><br><span class="line">  # Sidebar Display (only for Muse | Mist), available values:</span><br><span class="line">  #  - post    expand on posts automatically. Default.</span><br><span class="line">  #  - always  expand for all pages automatically.</span><br><span class="line">  #  - hide    expand only when click on the sidebar toggle icon.</span><br><span class="line">  #  - remove  totally remove sidebar including sidebar toggle.</span><br><span class="line">  display: hide</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>代码版块设置</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">codeblock:</span><br><span class="line">  # Code Highlight theme</span><br><span class="line">  # Available values: normal | night | night eighties | night blue | night bright</span><br><span class="line">  # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line">  highlight_theme: normal</span><br><span class="line">  # Add copy button on codeblock</span><br><span class="line">  copy_button:</span><br><span class="line">    enable: true</span><br><span class="line">    # Show text copy result.</span><br><span class="line">    show_result: true</span><br><span class="line">    # Available values: default | flat | mac</span><br><span class="line">    style:</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>字体设置</strong></p>
<p>暂时没有进行相关设置, 感觉默认的就还好.</p>
<p>如果要设置的话, 根据自带的帮助说明应该也能设置.</p>
<p>设置过多的字体的话, 博客的加载速度会变慢.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">font:</span><br><span class="line">  enable: false</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>数学公式</strong></p>
<p>如果文章里面包含了数学公式, 需要利用进行如下配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line">  # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">  # That is it only render those page which has `mathjax: true` in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: true</span><br><span class="line"></span><br><span class="line">  engine: mathjax</span><br><span class="line">  #engine: katex</span><br></pre></td></tr></table></figure>
<p>同时, 在需要解析数学公式的文章前面, 添加<code>mathjax: true</code>.</p>
</li>
</ul>
<p>更多的配置, 以后会进行补充说明.</p>
<h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>现在, 安装了需要的轮子, 并完成了基本的配置, 可以开始写博客了.</p>
<p>下面介绍几个基础但是够用的操作, 需要说的是以下操作均需要在博客文件夹中进行, 就是<code>hexo init</code>的那个.</p>
<ul>
<li><p><strong>新建文章</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>
<p>默认会在<code>source/_post/</code>中生成对应的md文件. 我们可以将文章写在里面.</p>
</li>
<li><p><strong>生成静态网页</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>本地查看</strong></p>
<p>在部署到Github之前, 可以先看看自己的博客, 文章是啥样的, 需不需要修改, 是否满足自己预期.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>部署</strong></p>
<p>觉得一切OK后, 将其部署到Github上.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>清空历史信息</strong></p>
<p>类似于删除Git仓库的历史commit, 不会影响当前现有的内容.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>等成功部署之后, 就可以在对应网址下, 看到自己的博客了. </p>
<p>需要说的是部署是由延时的, 有时候需要十来分钟的时间. 所以如果你是第一次部署, 打开自己的链接, 发现404, 不要慌, 起来喝一杯热水.</p>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
</search>

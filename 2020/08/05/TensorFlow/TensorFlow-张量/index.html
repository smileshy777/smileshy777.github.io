<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    fancybox: false,
    mediumzoom: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="程序 = 数据结构 + 算法 TensorFlow程序 = 张量数据结构 + 计算图算法 张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似. 从行为特性来看, 有两种类型的张量, 分别是常量constant和变量Variable. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值. 张量的操">
<meta name="keywords" content="TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow-张量">
<meta property="og:url" content="https://smileshy777.github.io/2020/08/05/TensorFlow/TensorFlow-张量/index.html">
<meta property="og:site_name" content="阿枂蛋糕店">
<meta property="og:description" content="程序 = 数据结构 + 算法 TensorFlow程序 = 张量数据结构 + 计算图算法 张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似. 从行为特性来看, 有两种类型的张量, 分别是常量constant和变量Variable. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值. 张量的操">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-10-10T14:36:42.937Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow-张量">
<meta name="twitter:description" content="程序 = 数据结构 + 算法 TensorFlow程序 = 张量数据结构 + 计算图算法 张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似. 从行为特性来看, 有两种类型的张量, 分别是常量constant和变量Variable. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值. 张量的操">





  
  
  <link rel="canonical" href="https://smileshy777.github.io/2020/08/05/TensorFlow/TensorFlow-张量/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>TensorFlow-张量 | 阿枂蛋糕店</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">阿枂蛋糕店</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">在下月小白</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://smileshy777.github.io/2020/08/05/TensorFlow/TensorFlow-张量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="月小白">
      <meta itemprop="description" content="这是一家有爱的小店">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿枂蛋糕店">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow-张量

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-08-05 21:00:51" itemprop="dateCreated datePublished" datetime="2020-08-05T21:00:51+08:00">2020-08-05</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-10-10 22:36:42" itemprop="dateModified" datetime="2020-10-10T22:36:42+08:00">2020-10-10</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>程序 = 数据结构 + 算法</p>
<p>TensorFlow程序 = 张量数据结构 + 计算图算法</p>
<p>张量与计算图是TensorFlow的两个核心概念, 张量就是多维数组. TensorFlow中的张量与numpy中的array很类似.</p>
<p>从行为特性来看, 有两种类型的张量, 分别是常量<strong>constant</strong>和变量<strong>Variable</strong>. 顾名思义, 常量在计算图中不能再被赋值, 而变量在计算图中可以被重新赋值.</p>
<p>张量的操作主要包括张量的<strong>结构操作</strong>和张量的<strong>数学运算</strong>. 张量结构操作诸如: 张量创建, 索引切片, 维度变换, 合并分割. 张量数学运算主要有: 标量运算, 向量运算, 矩阵运算.</p>
<a id="more"></a>
<h1 id="常量张量"><a href="#常量张量" class="headerlink" title="常量张量"></a>常量张量</h1><p>张量的数据类型基本和numpy.array的数据类型对应.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">1</span>)  <span class="comment"># 默认整数tf.int32</span></span><br><span class="line">b = tf.constant(<span class="number">1</span>, dtype=tf.int64)  <span class="comment"># tf.int64</span></span><br><span class="line">c = tf.constant(<span class="number">5.20</span>)  <span class="comment"># 默认小数tf.float32</span></span><br><span class="line">d = tf.constant(<span class="number">5.20</span>, dtype=tf.double)  <span class="comment">#tf.double</span></span><br><span class="line">e = tf.constant(<span class="string">'are you ok?'</span>)  <span class="comment"># tf.string</span></span><br><span class="line">f = tf.constant(<span class="literal">True</span>)  <span class="comment"># tf.bool</span></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64)</span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode)  <span class="comment"># 不等价</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>张量可以有各种不同的维度, 标量是0维的张量, 向量是1维的张量, 矩阵是2维的张量, 彩色图像有rgb三个通道, 可以表示为3维张量, 视频有时间维, 可以表示为4维张量. 通俗来说, 有几层中括号, 就是几维张量.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment"># 标量, 0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank与numpy的ndim方法查看具体维度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(0, shape=(), dtype=int32)</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 向量, 1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector))</span><br><span class="line">print(vector.numpy().ndim)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(1, shape=(), dtype=int32)</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                     [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 矩阵, 2维张量</span></span><br><span class="line">print(tf.rank(matrix))</span><br><span class="line">print(np.ndim(matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(2, shape=(), dtype=int32)</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], </span><br><span class="line">                       [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line"></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[[1 2]</span><br><span class="line">  [3 4]]</span><br><span class="line"></span><br><span class="line"> [[5 6]</span><br><span class="line">  [7 8]]], shape=(2, 2, 2), dtype=int32)</span><br><span class="line">tf.Tensor(3, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>可以用<code>tf.cast</code>来来改变张量中的数据类型.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.cast(a, tf.float32)</span><br><span class="line"></span><br><span class="line">print(a.dtype, b.dtype)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;dtype: &apos;int32&apos;&gt; &lt;dtype: &apos;float32&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以用numpy中的方法, 将TensorFlow中的张量转化为numpy中的张量. 用shape方法查看张量的尺寸.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">print(a.numpy())  <span class="comment"># 转换成np.array</span></span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1 2]</span><br><span class="line">(2,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="string">'你好啊!'</span>)</span><br><span class="line"></span><br><span class="line">print(a.numpy())</span><br><span class="line">print(a.numpy().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b&apos;\xe4\xbd\xa0\xe5\xa5\xbd\xe5\x95\x8a!&apos;</span><br><span class="line">你好啊!</span><br></pre></td></tr></table></figure>
<h1 id="变量张量"><a href="#变量张量" class="headerlink" title="变量张量"></a>变量张量</h1><p>模型中需要被训练的参数一般被设置为变量.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常量值不可改变, 常量的重新赋值相当于创造新的内存空间.</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a = a + tf.constant([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(c)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([1 2], shape=(2,), dtype=int32)</span><br><span class="line">140680756570256</span><br><span class="line">tf.Tensor([2 3], shape=(2,), dtype=int32)</span><br><span class="line">140680756571312</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变, 可以通过assign, assign_add等方法进行赋值.</span></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br><span class="line"></span><br><span class="line">a.assign_add([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(id(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br><span class="line">&lt;tf.Variable &apos;a:0&apos; shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)&gt;</span><br><span class="line">140680760772624</span><br></pre></td></tr></table></figure>
<h1 id="张量的结构操作"><a href="#张量的结构操作" class="headerlink" title="张量的结构操作"></a>张量的结构操作</h1><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>张量创建的许多方法和numpy中创建array的方法很像.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>, <span class="number">10</span>, delta=<span class="number">2</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1 3 5 7 9]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">100</span>)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0 0.101010099 0.202020198 ... 9.79797935 9.89899 10]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.zeros_like(a, dtype=tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1 1]</span><br><span class="line"> [1 1]]</span><br><span class="line">[[0 0]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[5 5]</span><br><span class="line"> [5 5]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[8.34453773 2.33366609 8.79651928 0.466492176 8.03496838]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[-2.27333117 -1.65921044]</span><br><span class="line"> [-0.263356805 -0.809234142]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正态分布随机，对两倍方差进行截断</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">2</span>, <span class="number">2</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1.57495022 -0.52902168]</span><br><span class="line"> [-0.704079211 -0.923798501]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print()</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 对角阵</span></span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[1 0]</span><br><span class="line"> [0 1]]</span><br><span class="line"></span><br><span class="line">[[1 0 0]</span><br><span class="line"> [0 2 0]</span><br><span class="line"> [0 0 3]]</span><br></pre></td></tr></table></figure>
<h2 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h2><p>张量的索引切片方式和numpy几乎是一样的, 切片时支持缺省参数和省略号. </p>
<p>对于<code>tf.Variable</code>可以通过索引和切片对部分元素进行修改.</p>
<p>对于提取张量的连续子区域, 也可以使用<code>tf.slice</code>.</p>
<p>此外, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p><code>tf.boolean_mask</code>功能最为强大, 它可以实现<code>tf.gather</code>, <code>tf.gather_nd</code>的功能, 并且<code>tf.boolean_mask</code>还可以实现布尔索引.</p>
<p>如果要通过修改张量的某些元素得到新的张量, 可以使用<code>tf.where</code>, <code>tf.scatter_nd</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>, <span class="number">5</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[4 5 7 8 4]</span><br><span class="line"> [7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]</span><br><span class="line"> [4 6 5 9 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[4 5 7 8 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[4 6 5 9 4]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :])</span><br><span class="line">tf.print(tf.slice(t, [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">3</span>, <span class="number">5</span>]))  <span class="comment">#tf.slice(input, begin_vector, size_vector)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br><span class="line">[[7 8 8 7 7]</span><br><span class="line"> [0 5 7 2 6]</span><br><span class="line"> [0 6 2 8 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>, :<span class="number">4</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[7 8]</span><br><span class="line"> [0 7]</span><br><span class="line"> [0 2]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对变量来说，还可以使用索引和切片修改部分元素</span></span><br><span class="line">x = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">x[<span class="number">1</span>, :].assign(tf.constant([<span class="number">0.0</span>, <span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1 2]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[[0 4]</span><br><span class="line">  [7 2]]</span><br><span class="line"></span><br><span class="line"> [[4 0]</span><br><span class="line">  [7 5]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省略号可以表示多个冒号</span></span><br><span class="line">tf.print(a[..., <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[4 2]</span><br><span class="line"> [0 5]]</span><br></pre></td></tr></table></figure>
<p>以上切片方式相对规则, 对于不规则的切片提取, 可以使用<code>tf.gather</code>, <code>tf.gather_nd</code>, <code>tf.boolean_mask</code>.</p>
<p>考虑班级成绩册的例子, 有4个班级, 每个班级5个学生, 每个学生6门科目成绩. 可以用一个(4, 5, 6)的张量来表示.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), minval=<span class="number">0</span>, maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.print(scores)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [22 67 12 59 33 61]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [43 94 21 25 69 53]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [77 34 29 78 64 96]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [14 79 72 81 90 86]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [5 18 37 63 36 27]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [79 62 29 13 62 3]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [19 51 84 98 81 97]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [89 69 94 84 13 54]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生, 第2个学生, 第4个学生的第1门课程, 第3门课程, 第5门课程成绩</span></span><br><span class="line">q = tf.gather(tf.gather(scores, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], axis=<span class="number">1</span>), [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[84 14 4]</span><br><span class="line">  [29 94 60]</span><br><span class="line">  [1 10 58]]</span><br><span class="line"></span><br><span class="line"> [[31 1 29]</span><br><span class="line">  [74 14 45]</span><br><span class="line">  [70 41 12]]</span><br><span class="line"></span><br><span class="line"> [[24 80 70]</span><br><span class="line">  [32 36 55]</span><br><span class="line">  [22 2 10]]</span><br><span class="line"></span><br><span class="line"> [[66 16 56]</span><br><span class="line">  [87 29 71]</span><br><span class="line">  [73 41 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 抽取第1个班级第1个学生, 第2个班级的第2个学生, 第3个班级的第3个学生的全部成绩</span></span><br><span class="line"><span class="comment"># indices的长度为采样样本的个数, 每个元素为采样位置的坐标</span></span><br><span class="line">s = tf.gather_nd(scores, indices=[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]</span><br><span class="line"> [89 69 94 84 13 54]]</span><br></pre></td></tr></table></figure>
<p>以上<code>tf.gather</code>和<code>tf.gather_nd</code>的功能也可以用<code>tf.boolean_mask</code>来实现.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 抽取每个班级第0个学生, 第2个学生, 第4个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores, [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>], axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[28 84 29 14 65 4]</span><br><span class="line">  [62 29 21 94 19 60]</span><br><span class="line">  [81 1 32 10 44 58]]</span><br><span class="line"></span><br><span class="line"> [[89 31 68 1 55 29]</span><br><span class="line">  [14 74 84 14 11 45]</span><br><span class="line">  [83 70 77 41 65 12]]</span><br><span class="line"></span><br><span class="line"> [[87 24 0 80 74 70]</span><br><span class="line">  [74 32 81 36 97 55]</span><br><span class="line">  [52 22 30 2 3 10]]</span><br><span class="line"></span><br><span class="line"> [[25 66 80 16 70 56]</span><br><span class="line">  [20 87 3 29 79 71]</span><br><span class="line">  [6 73 9 41 39 31]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 抽取第0个班级第0个学生, 第1个班级的第1个学生, 第2个班级的第2个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(</span><br><span class="line">    scores,</span><br><span class="line">    [[<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>], </span><br><span class="line">     [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[28 84 29 14 65 4]</span><br><span class="line"> [77 34 29 78 64 96]</span><br><span class="line"> [74 32 81 36 97 55]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用tf.boolean_mask可以实现布尔索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到矩阵中小于0的元素</span></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">tf.print(c, <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">tf.print(tf.boolean_mask(c, c &lt; <span class="number">0</span>), <span class="string">"\n"</span>)</span><br><span class="line">tf.print(c[c &lt; <span class="number">0</span>])  <span class="comment"># 布尔索引, 为boolean_mask的语法糖形式</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[-1 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 -3 3]] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3] </span><br><span class="line"></span><br><span class="line">[-1 -1 -2 -3]</span><br></pre></td></tr></table></figure>
<p>以上这些方法仅能提取张量的部分元素值, 但不能更改张量的部分元素值得到新的张量.</p>
<p>如果要通过修改张量的部分元素值得到新的张量, 可以使用<code>tf.where</code>和<code>tf.scatter_nd</code>.</p>
<p><code>tf.where</code>可以理解为<code>if</code>的张量版本, 此外它还可以用于找到满足条件的所有元素的位置坐标.</p>
<p><code>tf.scatter_nd</code>的作用和<code>tf.gather_nd</code>有些相反, <code>tf.gather_nd</code>用于收集张量的给定位置的元素, 而<code>tf.scatter_nd</code>可以将某些值插入到一个给定shape的全0的张量的指定位置处.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找到张量中小于0的元素, 将其换成np.nan得到新的张量</span></span><br><span class="line"><span class="comment"># tf.where和np.where作用类似, 可以理解为if的张量版本</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">-2</span>], [<span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>]], dtype=tf.float32)</span><br><span class="line">d = tf.where(c &lt; <span class="number">0</span>, np.nan, c)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[nan 1 nan]</span><br><span class="line"> [2 2 nan]</span><br><span class="line"> [3 nan 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果where只有一个参数, 将返回所有满足条件的位置坐标</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(indices)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0 0]</span><br><span class="line"> [0 2]</span><br><span class="line"> [1 2]</span><br><span class="line"> [2 1]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量</span></span><br><span class="line"></span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>]], [c[<span class="number">0</span>, <span class="number">0</span>], c[<span class="number">2</span>, <span class="number">1</span>]], c.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0 1 -1]</span><br><span class="line"> [2 2 -2]</span><br><span class="line"> [3 0 3]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scatter_nd的作用和gather_nd有些相反</span></span><br><span class="line"><span class="comment"># 可以将某些值插入到一个给定shape的全0的张量的指定位置处</span></span><br><span class="line"></span><br><span class="line">indices = tf.where(c &lt; <span class="number">0</span>)</span><br><span class="line">tf.print(tf.scatter_nd(indices, tf.gather_nd(c, indices), c.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[-1 0 -1]</span><br><span class="line"> [0 0 -2]</span><br><span class="line"> [0 -3 0]]</span><br></pre></td></tr></table></figure>
<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>维度变换相关函数主要有 <code>tf.reshape</code>, <code>tf.squeeze</code>, <code>tf.expand_dims</code>, <code>tf.transpose</code>.</p>
<p><code>tf.reshape</code>可以改变张量的形状. 可以改变张量的形状, 但是其本质上不会改变张量元素的存储顺序, 所以该操作实际上非常迅速, 并且是可逆的.</p>
<p><code>tf.squeeze</code>可以减少维度.</p>
<p><code>tf.expand_dims</code>可以增加维度.</p>
<p><code>tf.transpose</code>可以交换维度.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>], minval=<span class="number">0</span>, maxval=<span class="number">255</span>, dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改成(3, 6)形状的张量</span></span><br><span class="line"></span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.print(b.shape)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([3, 6])</span><br><span class="line">[[95 119 125 231 120 219]</span><br><span class="line"> [179 202 37 124 246 167]</span><br><span class="line"> [211 93 165 94 31 189]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改回成(1,3,3,2)形状的张量</span></span><br><span class="line"></span><br><span class="line">c = tf.reshape(b, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p>如果张量在某个维度上只有一个元素, 利用<code>tf.squeeze</code>可以消除这个维度.</p>
<p>和<code>tf.reshape</code>相似, 它本质上不会改变张量元素的存储顺序. 张量的各个元素在内存中是线性存储的, 其一般规律是, 同一层级中的相邻元素的物理地址也相邻.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = tf.squeeze(a)</span><br><span class="line">tf.print(s.shape)</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([3, 3, 2])</span><br><span class="line">[[[95 119]</span><br><span class="line">  [125 231]</span><br><span class="line">  [120 219]]</span><br><span class="line"></span><br><span class="line"> [[179 202]</span><br><span class="line">  [37 124]</span><br><span class="line">  [246 167]]</span><br><span class="line"></span><br><span class="line"> [[211 93]</span><br><span class="line">  [165 94]</span><br><span class="line">  [31 189]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = tf.expand_dims(s, axis=<span class="number">0</span>)  <span class="comment"># 在第0维插入长度为1的一个维度</span></span><br><span class="line">tf.print(d.shape)</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([1, 3, 3, 2])</span><br><span class="line">[[[[95 119]</span><br><span class="line">   [125 231]</span><br><span class="line">   [120 219]]</span><br><span class="line"></span><br><span class="line">  [[179 202]</span><br><span class="line">   [37 124]</span><br><span class="line">   [246 167]]</span><br><span class="line"></span><br><span class="line">  [[211 93]</span><br><span class="line">   [165 94]</span><br><span class="line">   [31 189]]]]</span><br></pre></td></tr></table></figure>
<p><code>tf.transpose</code>可以交换张量的维度, 与<code>tf.reshape</code>不同, 它会改变张量元素的存储顺序.</p>
<p><code>tf.transpose</code>常用于图片存储格式的变换上.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch, Height, Width, Channel</span></span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>, <span class="number">600</span>, <span class="number">600</span>, <span class="number">4</span>],</span><br><span class="line">                      minval=<span class="number">0</span>,</span><br><span class="line">                      maxval=<span class="number">255</span>,</span><br><span class="line">                      dtype=tf.int32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Channel, Height, Width, Batch</span></span><br><span class="line">s = tf.transpose(a, perm=[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">tf.print(s.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([100, 600, 600, 4])</span><br><span class="line">TensorShape([4, 600, 600, 100])</span><br></pre></td></tr></table></figure>
<h2 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h2><p>和numpy类似, 可以用<code>tf.concat</code>和<code>tf.stack</code>方法对多个张量进行合并, 可以用<code>tf.split</code>方法把一个张量分割成多个张量.</p>
<p><code>tf.concat</code>和<code>tf.stack</code>有略微的区别, <code>tf.concat</code>是连接, 不会增加维度, 而<code>tf.stack</code>是堆叠, 会增加维度.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(6, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [ 3.,  4.],</span><br><span class="line">       [ 5.,  6.],</span><br><span class="line">       [ 7.,  8.],</span><br><span class="line">       [ 9., 10.],</span><br><span class="line">       [11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 6), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.,  5.,  6.,  9., 10.],</span><br><span class="line">       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 3.,  4.]],</span><br><span class="line"></span><br><span class="line">       [[ 5.,  6.],</span><br><span class="line">        [ 7.,  8.]],</span><br><span class="line"></span><br><span class="line">       [[ 9., 10.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([a, b, c], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=</span><br><span class="line">array([[[ 1.,  2.],</span><br><span class="line">        [ 5.,  6.],</span><br><span class="line">        [ 9., 10.]],</span><br><span class="line"></span><br><span class="line">       [[ 3.,  4.],</span><br><span class="line">        [ 7.,  8.],</span><br><span class="line">        [11., 12.]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>, <span class="number">10.0</span>], [<span class="number">11.0</span>, <span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.concat([a, b, c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.split</code>是<code>tf.concat</code>的逆运算, 可以指定分割份数平均分割, 也可以通过指定每份的记录数量进行分割.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.split(value, num_or_size_splits, axis)</span></span><br><span class="line">tf.print(tf.split(c, <span class="number">3</span>, axis=<span class="number">0</span>))  <span class="comment"># 指定分割份数, 平均分割</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.print(tf.split(c, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], axis=<span class="number">0</span>))  <span class="comment"># 指定每份的记录数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[[1 2]</span><br><span class="line"> [3 4]], </span><br><span class="line"> </span><br><span class="line"> [[5 6]</span><br><span class="line"> [7 8]], </span><br><span class="line"> </span><br><span class="line"> [[9 10]</span><br><span class="line"> [11 12]]]</span><br></pre></td></tr></table></figure>
<h1 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h1><p>张量的数学运算符可以分为标量运算符, 向量运算符, 以及矩阵运算符.</p>
<h2 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h2><p>加减乘除乘方, 以及三角函数, 指数, 对数等常见函数, 逻辑比较运算符等都是标量运算符.</p>
<p>标量运算符的特点是对张量实施逐元素运算.</p>
<p>有些标量运算符对常用的数学运算符进行了重载. 并且支持类似numpy的广播特性.</p>
<p>许多标量运算符都在<code>tf.math</code>模块下.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">-3</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>, <span class="number">6</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">a + b  <span class="comment"># 运算符重载</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 6.,  8.],</span><br><span class="line">       [ 4., 12.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a - b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ -4.,  -4.],</span><br><span class="line">       [-10.,  -4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a * b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[  5.,  12.],</span><br><span class="line">       [-21.,  32.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a / b</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.2       ,  0.33333334],</span><br><span class="line">       [-0.42857143,  0.5       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  4.],</span><br><span class="line">       [ 9., 16.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a**<span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1.       , 1.4142135],</span><br><span class="line">       [      nan, 2.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a % <span class="number">3</span>  <span class="comment"># mod的运算符重载, 等价于m = tf.math.mod(a, 3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 1.,  2.],</span><br><span class="line">       [-0.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a // <span class="number">3</span>  <span class="comment"># 地板除法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[ 0.,  0.],</span><br><span class="line">       [-1.,  1.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a &gt;= <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) &amp; (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False,  True],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a &gt;= <span class="number">2</span>) | (a &lt;= <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[ True,  True],</span><br><span class="line">       [ True,  True]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a == <span class="number">5</span>  <span class="comment"># tf.equal(a, 5)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=bool, numpy=</span><br><span class="line">array([[False, False],</span><br><span class="line">       [False, False]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>, <span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>, <span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a, b, c])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.print(tf.maximum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[5 8]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.print(tf.minimum(a, b))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1 6]</span><br></pre></td></tr></table></figure>
<h2 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h2><p>向量运算符只在一个特定轴上运算, 将一个向量映射到一个标量或者另外一个向量. 许多向量运算符都以<code>reduce</code>开头.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a))</span><br><span class="line">tf.print(tf.reduce_mean(a))</span><br><span class="line">tf.print(tf.reduce_max(a))</span><br><span class="line">tf.print(tf.reduce_min(a))</span><br><span class="line">tf.print(tf.reduce_prod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">45</span><br><span class="line">5</span><br><span class="line">9</span><br><span class="line">1</span><br><span class="line">362880</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[6]</span><br><span class="line"> [15]</span><br><span class="line"> [24]]</span><br><span class="line">[[12 15 18]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p))</span><br><span class="line">tf.print(tf.reduce_any(q))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a, b: a + b, tf.range(<span class="number">10</span>))</span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">45</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a))</span><br><span class="line">tf.print(tf.math.cumprod(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1 3 6 ... 28 36 45]</span><br><span class="line">[1 2 6 ... 5040 40320 362880]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a))</span><br><span class="line">tf.print(tf.argmin(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values, indices = tf.math.top_k(a, <span class="number">3</span>, sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values)</span><br><span class="line">tf.print(indices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[8 7 5]</span><br><span class="line">[5 2 3]</span><br></pre></td></tr></table></figure>
<h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><p>矩阵必须是二维的, 类似<code>tf.constant([1, 2, 3])</code>这样的不是矩阵.</p>
<p>矩阵运算包括: 矩阵乘法, 矩阵转置, 矩阵逆, 矩阵求迹, 矩阵范数, 矩阵行列式, 矩阵求特征值, 矩阵分解等运算.</p>
<p>除了一些常用的运算外, 大部分和矩阵有关的运算都在<code>tf.linalg</code>子包中.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">a @ b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span><br><span class="line">array([[2, 4],</span><br><span class="line">       [6, 8]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[1., 3.],</span><br><span class="line">       [2., 4.]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵逆, 必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3.0</span>, <span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[-2.0000002 ,  1.0000001 ],</span><br><span class="line">       [ 1.5000001 , -0.50000006]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.trace(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.477226&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵特征值</span></span><br><span class="line">tf.linalg.eigvalsh(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵qr分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">q, r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q @ r)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[-0.316227794 -0.948683321]</span><br><span class="line"> [-0.948683321 0.316227734]]</span><br><span class="line">[[-3.1622777 -4.4271884]</span><br><span class="line"> [0 -0.632455349]]</span><br><span class="line">[[1.00000012 1.99999976]</span><br><span class="line"> [3 4]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵svd分解</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]], dtype=tf.float32)</span><br><span class="line">v, s, d = tf.linalg.svd(a)</span><br><span class="line">tf.matmul(tf.matmul(s, tf.linalg.diag(v)), d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span><br><span class="line">array([[0.9999996, 1.9999996],</span><br><span class="line">       [2.9999998, 4.       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><p>TensorFlow的广播规则和numpy是一样的:</p>
<ul>
<li>如果张量的维度不同, 将维度较小的张量进行扩展, 直到两个张量的维度都一样.</li>
<li>如果两个张量在某个维度上的长度是相同的, 或者其中一个张量在该维度上的长度为1, 那么我们就说这两个张量在该维度上是相容的.</li>
<li>如果两个张量在所有维度上都是相容的, 它们就能使用广播.</li>
<li>广播之后, 每个维度的长度将取两个张量在该维度长度的较大值.</li>
<li>在任何一个维度上, 如果一个张量的长度为1, 另一个张量长度大于1, 那么在该维度上, 就好像是对第一个张量进行了复制.</li>
</ul>
<p><code>tf.broadcast_to</code>以显式的方式按照广播机制扩展张量的维度.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment"># 等价于 b + tf.broadcast_to(a, b.shape)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [2, 3, 4],</span><br><span class="line">       [3, 4, 5]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.broadcast_to(a, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [1, 2, 3],</span><br><span class="line">       [1, 2, 3]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 静态形状, TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape, b.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([3, 3])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算广播后计算结果的形状, 动态形状, Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c), tf.shape(d))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广播效果</span></span><br><span class="line">c + d  <span class="comment"># 等价于 tf.broadcast_to(c, [3, 3]) + tf.broadcast_to(d, [3, 3])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span><br><span class="line">array([[2, 3, 4],</span><br><span class="line">       [3, 4, 5],</span><br><span class="line">       [4, 5, 6]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/05/24/图算法/二分网络的部分方法与应用/" rel="next" title="二分网络的部分方法与应用">
                <i class="fa fa-chevron-left"></i> 二分网络的部分方法与应用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/08/05/TensorFlow/TensorFlow-自动微分机制/" rel="prev" title="TensorFlow-自动微分机制">
                TensorFlow-自动微分机制 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
  <p class="site-author-name" itemprop="name">月小白</p>
  <div class="site-description motion-element" itemprop="description">这是一家有爱的小店</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    

    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>













          
          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#常量张量"><span class="nav-number">1.</span> <span class="nav-text">常量张量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#变量张量"><span class="nav-number">2.</span> <span class="nav-text">变量张量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#张量的结构操作"><span class="nav-number">3.</span> <span class="nav-text">张量的结构操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建张量"><span class="nav-number">3.1.</span> <span class="nav-text">创建张量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#索引切片"><span class="nav-number">3.2.</span> <span class="nav-text">索引切片</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#维度变换"><span class="nav-number">3.3.</span> <span class="nav-text">维度变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#合并分割"><span class="nav-number">3.4.</span> <span class="nav-text">合并分割</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#张量的数学运算"><span class="nav-number">4.</span> <span class="nav-text">张量的数学运算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#标量运算"><span class="nav-number">4.1.</span> <span class="nav-text">标量运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#向量运算"><span class="nav-number">4.2.</span> <span class="nav-text">向量运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵运算"><span class="nav-number">4.3.</span> <span class="nav-text">矩阵运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#广播机制"><span class="nav-number">4.4.</span> <span class="nav-text">广播机制</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">月小白</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>



        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  















  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

</body>
</html>
